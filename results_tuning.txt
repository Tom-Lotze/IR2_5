dnn_hidden_units : 256, 128, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.304836711885123 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.307666143780903 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.30454271893719 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.413787738313849 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 1375.731150527966 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.505024108867258 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.295782396460957 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.297680414146392 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.297161407970893 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.337986516026821 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.358010348120773 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.347550996994041 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.5369108949648 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 8.892340668300223 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.742516945233125 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.305514015900019 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.31582228495221 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.304248297448561 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.303581454147442 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.307586399380166 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.303636294010325 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.410367890774954 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 322.5935064472756 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.593057774186477 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.295731264630243 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.297695612804847 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.296847081528893 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.304433012269018 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.307576951143259 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.303585279074668 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.487225913775301 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 9.264659792414939 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.502935849572291 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.295775857451418 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.2974463246422525 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : AdamW
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.2976554024437785 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.363722923626154 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.352328101162771 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.335182066866296 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.309505282320409 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.306405152754886 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.310813466839356 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.604365652752927 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.5854004144852665 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.517559508351125 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.365179862745703 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.353751216507 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.33604361572172 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.3122134001187185 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.30949481721485 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.312858193645987 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.604745989567182 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.585750660319321 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.5179197397514494 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.3973058758128065 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 9.42983876245321 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.505330005926511 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 26514.14373895596 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 1113915.3971249745 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 2979.7333543122986 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.294448223238667 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.2946107729418035 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.296477079746504 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.414775224663473 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 8.869372060526135 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.545879093287876 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 31346.27613251722 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 1589403.182719485 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.01
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5240.241784674375 (4533 batches).
dnn_hidden_units : 256, 128, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=128, out_features=32, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.302920116669238 (4533 batches).
dnn_hidden_units : 2000, 100, 16
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=2000, out_features=100, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=100, out_features=16, bias=True)
  (5): LeakyReLU(negative_slope=0.02)
  (6): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.319195988726306 (4533 batches).
dnn_hidden_units : 300, 32
learning_rate : 0.0001
nr_epochs : 100
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : RMSprop
amsgrad : True
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): LeakyReLU(negative_slope=0.02)
  (2): Linear(in_features=300, out_features=32, bias=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.304752653728067 (4533 batches).
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.307019337015701 (4533 batches).
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.311795296691097 (4533 batches).
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.315829692824692 (4533 batches).
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.319329312561417 (4533 batches).
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.325968726433956 (4533 batches).
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.317995832951527 (4533 batches).
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.343471323894187 (4533 batches).
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.369240474706188 (4533 batches).
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.353712302708032 (4533 batches).
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.387197083007308 (4533 batches).
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.386399897405072 (4533 batches).
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.391162905647295 (4533 batches).
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.306234086714332 (4533 batches).
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.311448280881788 (4533 batches).
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.314024798369897 (4533 batches).
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.307192044912415 (4533 batches).
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.296719334232289 (4533 batches).
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.305016849547487 (4533 batches).
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.305981876306809 (4533 batches).
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.312292382466249 (4533 batches).
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.312210463265196 (4533 batches).
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.307019337015701 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.228065745314567 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.176544272108055 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.113108773566018 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.029875712075981 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.90844572124275 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.748817864564668 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 4.539568292852692 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 4.309844422913692 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 4.050812896529282 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 3.8246507617960748 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 3.6165530183918557 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 3.4178402622540793 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 3.235214359971195 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 3.06276377036875 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 2.906235572375553 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 2.7650591101231967 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 2.622752350462264 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 2.5033495922686115 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 2.4051866983107693 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 2.29907666976789 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 2.211936864761125 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 2.122968295782627 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 2.040641858059393 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 1.9686021170762367 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 1.907517931798213 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 1.8583023090736508 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 1.8005854843298172 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 1.740372616961227 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 1.6973159444429426 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 1.6529138529508445 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 1.6133837495290915 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 1.5770970465015055 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 1.5397431181804144 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 1.5080258190835676 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 1.4713627098873177 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 1.4443113861587855 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 1.421892228819218 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 1.3931150729296462 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 1.3901019042463405 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 1.3365402716281212 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 1.3273935697967845 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 1.291099231950684 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 1.296719617107807 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 1.2711292460590384 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 1.2586451239527232 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 1.2389413713356952 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 1.2277833538920102 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 1.1920273071477514 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 1.1983328281505046 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 1.1799756225248639 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 1.1586607666585504 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 1.1477890927586996 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 1.116375995979466 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 1.113529487629087 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 1.0992140400897528 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 1.0756523009153778 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 1.0756012070147427 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 1.0679516040368924 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 1.0462960657722609 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 1.0321913978009725 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 1.0414765484034423 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 1.022468931636036 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 1.0107750864039975 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 0.9914373457201685 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 0.9708313070798115 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 0.958731476936597 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 0.9499490633984946 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 0.9532941007078969 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 0.949377800310234 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 0.9299500141347872 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 0.9110530182906604 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 0.9014975830728503 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 0.9110745357975327 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 0.8992374906595406 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 0.8786854953750616 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 0.8621468406450846 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 0.8630000539904736 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 0.8608022892834097 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 0.863535102395172 (4533 batches).
Loss on test set: 6.987314744786302
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.311795296691097 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.236857711493036 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.190446648065444 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.135961659494257 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.068674097202115 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.958366733235869 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.79011609994521 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 4.520534760636945 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 4.214648947833049 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 3.8597843707488804 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 3.5293228216658306 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 3.2065882451837227 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 2.9450000964751077 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 2.6810204350354 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 2.4688707224230826 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 2.249763720436073 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 2.0559855428011296 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 1.9012974125776705 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 1.7689186948867868 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 1.6600136881747205 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 1.5462046873482704 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 1.4313987398250145 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 1.3764462479062094 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 1.312431465621529 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 1.2585498521344014 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 1.2341553172823505 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 1.170376018179507 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 1.1276414770047027 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 1.1012397791089903 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 1.0929834752298602 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 1.070581586375264 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 1.0412007951852162 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 1.0320020353468002 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 1.0018649318972723 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 0.9919612475254008 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 0.9358476566842342 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 0.9270493522164787 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 0.9230093135591535 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 0.9005904929059482 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 0.889186360130977 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 0.8688555925282426 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 0.8623312556145998 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 0.8634769999340568 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 0.8537064963652156 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 0.8397967650395025 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 0.8269445040211256 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 0.8172162314032498 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 0.8358444571725164 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 0.8189841702764539 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 0.8072438965073464 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 0.8247777636450777 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 0.779027135771104 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 0.757733214119794 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 0.7641167952992415 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 0.7383850736384515 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 0.7436069761539121 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 0.7350215913278757 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 0.7556075593910679 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 0.7428078198246206 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 0.7298626941648272 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 0.7095739863946058 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 0.7156792847727693 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 0.7175971653379012 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 0.6692668307575068 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 0.6692821245131813 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 0.6752267495856206 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 0.6547793084646649 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 0.6468965547771978 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 0.6380161026370675 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 0.6498326066262591 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 0.6621894202181342 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 0.6756812189388244 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 0.669662257594408 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 0.6521100509059553 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 0.6316730104141905 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 0.6328699374035208 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 0.6452714942827962 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 0.6356181245672763 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 0.6265202527908913 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 0.624234823544978 (4533 batches).
Loss on test set: 7.372337054764328
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.315829692824692 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.22672370921006 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.173901211208206 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.105730865140006 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.017184539002192 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.9025457311144685 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.731609516419403 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 4.506990537037525 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 4.249008954794181 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 3.9981343352502736 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 3.7370048748218747 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 3.4970737878465767 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 3.297547550843616 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 3.0889065214374223 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 2.912329068555754 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 2.7506947445206733 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 2.612712217263504 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 2.4843666030033624 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 2.3779147094957565 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 2.2464343634118262 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 2.181787297093471 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 2.0790245479209886 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 1.9895406519830188 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 1.9206521300035624 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 1.8412962871922738 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 1.7961537787818551 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 1.741474015742192 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 1.695164390281418 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 1.6377567188013422 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 1.589753052453555 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 1.5399586544929948 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 1.5209537761747352 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 1.4794036682580918 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 1.4403154164940586 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 1.4028318558482442 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 1.3704895697732964 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 1.366393441774449 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 1.328503350377372 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 1.296169063027479 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 1.2665182247103746 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 1.248965101304655 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 1.2239323698235693 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 1.2052580951040164 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 1.1862676542818533 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 1.175258625176288 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 1.1566044262622908 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 1.13459614575067 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 1.105001626817722 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 1.0780348750486717 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 1.0717201550525466 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 1.0617784689409224 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 1.0507548112605494 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 1.0240483902396416 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 1.0246271840223748 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 0.9970202767379548 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 0.9993737827967577 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 0.9940015300204363 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 0.9736921617310489 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 0.970046520979363 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 0.9678598802964892 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 0.9308828224041539 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 0.9142737985643467 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 0.9017281524644712 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 0.9020324106277442 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 0.8875411788497622 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 0.8788875033070177 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 0.8773543974592174 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 0.8744019702053901 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 0.8541594684196313 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 0.8527348130947966 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 0.856269114897947 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 0.846290910813342 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 0.8348143564932872 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 0.8276447658799958 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 0.8246488796673204 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 0.8165221468071103 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 0.8136997272669546 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 0.8017706204573103 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 0.7916130780180953 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 0.8119039108770035 (4533 batches).
Loss on test set: 7.122160956984813
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.319329312561417 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.26716556302922 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.254060587029885 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.238181354692906 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.222235472844501 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.209008446098452 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.199249264343458 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.18519431122507 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.173731045732954 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.15248557395975 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.144655577636255 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.131823632132263 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.120538604356794 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.10970140279273 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.10031773100987 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.08324572660264 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.073112465386987 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.056470567254917 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.04753840800655 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.035035800229049 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.020328426424199 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.009962071705517 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.003053496143241 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 4.991151927573618 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 4.978998499570923 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 4.969185212998103 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 4.962193894433628 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 4.95178586052607 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 4.928108094308173 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 4.930929933968638 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 4.922902194811444 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 4.912344753597578 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 4.911516484876566 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 4.9030362970119485 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 4.899891090372028 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 4.89293635639691 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 4.882565347703382 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 4.8793496650600705 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 4.866586503684323 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 4.856986903707739 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 4.856779408539049 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 4.843443771295402 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 4.848194014426496 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 4.845761943726042 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 4.831273641987087 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 4.831667194915034 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 4.820597893008854 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 4.8149892230079105 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 4.8197597986600424 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 4.824611722042354 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 4.821180337710857 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 4.825506605482406 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 4.817770421991489 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 4.800478921053874 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 4.8036797899640264 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 4.801417637194804 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 4.7944509507461595 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 4.7931325856933995 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 4.781623775494064 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 4.779435855240951 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 4.781629695656703 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 4.7744549781235275 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 4.776639795792489 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 4.769162423058953 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 4.7746727384664664 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 4.775629219592288 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 4.7628581164137245 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 4.761296271186461 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 4.758464402613881 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 4.763534558284843 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 4.759129664452429 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 4.755298533898275 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 4.758489253161733 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 4.754252575352679 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 4.765307196396853 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 4.746483854813053 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 4.747279288580474 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 4.7378004637875355 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 4.7417053742411905 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 4.736575709150875 (4533 batches).
Loss on test set: 5.453326353206694
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.325968726433956 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.293018290966572 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.274326654911673 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.257783473340086 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.246681975326921 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.235061691753022 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.227816459071733 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.217826514535463 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.212673805892376 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.206389979642405 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.1959337954512765 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.19503428639888 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.185641806698413 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.181557839408027 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.178177445028992 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.172165141893543 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.165462777429455 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.157706706881023 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.157638201091875 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.154240622745032 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.146217468503871 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.146875321773038 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.144082429825589 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.136045890903199 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.136533129538707 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.1372412158352585 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.123629457425156 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 5.123715691881518 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 5.127862650170485 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 5.121958276227744 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 5.117946992087727 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 5.115766916137223 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 5.11499943738212 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 5.117881179197771 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 5.113493277015156 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 5.105134357410421 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 5.104132658344427 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 5.1085653031290486 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 5.099746990080257 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 5.103479124788099 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 5.095934830081665 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 5.093306826974497 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 5.093319957458286 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 5.094502856445818 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 5.090937604460789 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 5.091426280589917 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 5.085870016483763 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 5.0804998771312775 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 5.076628554098764 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 5.080423995329796 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 5.075089830450735 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 5.079857211606854 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 5.077808337708117 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 5.073037094636493 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 5.074958743111282 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 5.066946416731678 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 5.071104895771405 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 5.068943200076149 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 5.064323451065107 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 5.064259718591583 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 5.06600228837592 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 5.064555035517386 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 5.04665894215664 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 5.057343851420728 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 5.047714077135124 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 5.0445530826541045 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 5.043213686089944 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 5.048343675909204 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 5.0357807261407865 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 5.041750360871161 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 5.029186850348346 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 5.03265032623873 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 5.031667690113525 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 5.028502733600131 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 5.02740489840902 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 5.027296571234007 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 5.023748619625328 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 5.028432611582323 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 5.019119347611879 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 5.020518379707934 (4533 batches).
Loss on test set: 5.316213749586983
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.317995832951527 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.270048184135011 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.25745723041351 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.24058898988213 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.229570681202524 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.229145284733077 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.215129835227454 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.202200678466252 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.186281959058011 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.175290428406088 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.170547092379047 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.157863734758849 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.146897070920154 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.132002414469421 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.121164183466213 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.112532584866545 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.097664980495999 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.087245574014329 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.083207577315751 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.073058615213668 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.064344042504094 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.052286583673026 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.040075093303956 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.032439975937865 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.016307165652691 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.016496733274781 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.003511317118677 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 4.995928608027352 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 4.990264786886059 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 4.983934013701001 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 4.980474502315012 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 4.976662407785542 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 4.967481561904182 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 4.953459310079289 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 4.945549676164791 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 4.9468836555437825 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 4.93812397103317 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 4.931201679854369 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 4.930646540864582 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 4.9330010289207555 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 4.922484432207205 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 4.903836265045261 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 4.902656853001769 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 4.902019331727679 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 4.903051994847799 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 4.895558470399496 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 4.880952077986993 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 4.878362829259708 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 4.886609637419067 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 4.8736180444177135 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 4.865448559537619 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 4.86553640927591 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 4.851867094040969 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 4.860166510814024 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 4.845908223386108 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 4.847626142251473 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 4.844657532247833 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 4.849989780402988 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 4.833307778325113 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 4.828597197515823 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 4.830958446747889 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 4.827594324436878 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 4.8182481470261616 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 4.81468041853586 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 4.812583421966558 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 4.804376273131018 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 4.793157511532714 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 4.798196714733863 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 4.8060462993290045 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 4.790835311929306 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 4.790432861801701 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 4.784458302515326 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 4.783057971976857 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 4.780802018235331 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 4.784260974179139 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 4.7849874347349735 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 4.771096399820275 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 4.774466728022786 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 4.765444534571261 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 4.7727726672675574 (4533 batches).
Loss on test set: 5.483474793689504
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.343471323894187 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.297295944911083 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.28906448949075 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.285069660571877 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.2808383142293325 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.2760386398867904 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.2786244214462075 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.276699383264605 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.276552140620694 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.2753280359415875 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.274477916391854 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.274456249465749 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.27651971786353 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.276787445530996 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.2753831734726715 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.277241687098271 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.2764238820232725 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.274936975508159 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.273764396881441 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.274436223580562 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.26978042539634 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.276861702793617 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.274452471349141 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.276036239080599 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.270202037871554 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.27077501719029 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.271203129043807 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 5.2720505501717465 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 5.269393288613207 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 5.274092356811 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 5.274214344652657 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 5.273187018311263 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 5.274297715863881 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 5.273585214721113 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 5.27400072981079 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 5.276706330328621 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 5.2725616054715845 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 5.273502948266793 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 5.2753946199970425 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 5.274356548383277 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 5.274483266306481 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 5.271706029460469 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 5.276415559279322 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 5.274113460037321 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 5.273531723201288 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 5.2730407213338975 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 5.270162048940493 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 5.27481777560703 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 5.2699172607247675 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 5.277414333970242 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 5.273198478116175 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 5.278097503368833 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 5.2725062029739895 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 5.271720268304001 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 5.272025991690598 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 5.274718070882271 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 5.272758855373686 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 5.269859412398634 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 5.27321937014245 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 5.268674918076283 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 5.271969355049444 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 5.2699338886455385 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 5.268960215636181 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 5.2717491811720345 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 5.277354595573327 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 5.271764397252982 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 5.266266328252864 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 5.270152727026511 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 5.276189207083969 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 5.276046583280642 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 5.270485735205186 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 5.269961795269351 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 5.273828286039048 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 5.27182654471369 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 5.274950763944124 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 5.2750958724445836 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 5.269875500813873 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 5.2743236720890705 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 5.273003265633111 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 5.276690318204172 (4533 batches).
Loss on test set: 5.2572484527753875
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.369240474706188 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.2970548053727144 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.2832391035101836 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.2810029376297365 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.281218175090703 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.278914664587437 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.282078562102622 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.280000495931683 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.281116215093442 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.282708616044225 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.276590709967143 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.278421795686402 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.274610819746637 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.272610830545373 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.274288295174234 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.273720734092591 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.2716738699341406 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.270991139253612 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.274746771182757 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.272966563583288 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.271637736291463 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.273587897916306 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.27427084354436 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.270551200052931 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.276992420920888 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.273226184111779 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.269387621951108 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 5.270469939579849 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 5.27026025388458 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 5.270874894167608 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 5.272051063707904 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 5.273296470624207 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 5.274717308395145 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 5.27842750624003 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 5.271108382891641 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 5.271553651073662 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 5.2709656928039506 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 5.2743996261314345 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 5.270822134918391 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 5.275685263684641 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 5.271300670407925 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 5.268457592835417 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 5.272413288642312 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 5.268809094009298 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 5.269645220405503 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 5.272926067006843 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 5.271571762443982 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 5.273135380399903 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 5.26807189679372 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 5.268614418493999 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 5.270951906011616 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 5.272284439941813 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 5.27033045839953 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 5.269275506416378 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 5.27175470777045 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 5.269469298131414 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 5.276603321642006 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 5.27329598158473 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 5.266184085703517 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 5.2744599964191075 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 5.27458217578983 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 5.2729607396453995 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 5.269228359628875 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 5.2687266153293075 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 5.269522797921991 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 5.267923942391719 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 5.267828619219782 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 5.26738889835908 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 5.264989296568958 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 5.273564046512186 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 5.264300021829653 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 5.2694318308726125 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 5.26777545689958 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 5.265333442137832 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 5.265796705518315 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 5.267652276162935 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 5.266970165854596 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 5.2683754517967385 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 5.271848454029387 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 5.266999243953594 (4533 batches).
Loss on test set: 5.2629960102597915
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.353712302708032 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.295194371407221 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.289445860060324 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.281335032146655 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.279711874502268 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.287727567157076 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.283883016629643 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.282844864793521 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.2784978413776455 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.277163312789122 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.280700968954439 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.276445211005637 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.278001459615793 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.27247695832912 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.273156069457439 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.272403114472639 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.266731318958332 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.268070943277192 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.272457565794237 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.274578126802788 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.269597658752317 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.273309134974165 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.268061469780959 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.268999544866244 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.2663448246114966 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.268308420331699 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.267454064710235 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 5.266835769464605 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 5.268343192863591 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 5.26749594493494 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 5.270100103672729 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 5.267817351732879 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 5.2684051021161045 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 5.265029349838322 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 5.266743786782585 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 5.268752966196942 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 5.268611273464714 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 5.267882281650961 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 5.2699282159874725 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 5.270129748114471 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 5.268442361974937 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 5.268376605849431 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 5.268798221980082 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 5.269762338798829 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 5.27293818256804 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 5.270266402643458 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 5.267874240283537 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 5.264398603927949 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 5.271243411790397 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 5.266432414984772 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 5.266481656021507 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 5.266544438686535 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 5.266948911046025 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 5.2677593241588925 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 5.266633898854756 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 5.268751400605275 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 5.26784584484378 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 5.271032280383351 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 5.267449336732536 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 5.269530482831812 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 5.265956271228584 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 5.267750385092658 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 5.267035471191213 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 5.26673152002046 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 5.265971039216197 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 5.26794814613779 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 5.265380815549215 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 5.2681180561664425 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 5.267599050503757 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 5.266130463035065 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 5.266340792192257 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 5.270955303686753 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 5.268705572312488 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 5.268646984654706 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 5.268221602254742 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 5.27220034615083 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 5.265826187306304 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 5.265731462389855 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 5.2688015573681675 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 5.2701006527113226 (4533 batches).
Loss on test set: 5.272463539356306
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.387197083007308 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.37104430315433 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.370180118801892 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.366760518728122 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.35993657894753 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.357897427035777 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.36426945785116 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.367533534166962 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.364944798265785 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.369725045912213 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.366606281565273 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.365810626749169 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.3669122109580405 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.369405858029652 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.367788454725231 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.368699881563959 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.370870626343551 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.368273042643699 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.3685984831757825 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.3665915065692165 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.363198980626591 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.37036382889027 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.365089701641061 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.369046307783003 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.362599730780917 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.363284717799022 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.364313936538578 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 5.370232521711847 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 5.365185964812302 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 5.367012253836925 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 5.367516127743523 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 5.3656721007158605 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 5.368592358269282 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 5.3628809661652745 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 5.361642548744973 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 5.362249285607261 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 5.356198581577846 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 5.361528620008809 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 5.358960988688306 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 5.363717730080032 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 5.361133286068305 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 5.360680390687644 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 5.362104089244848 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 5.358857081868963 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 5.35864740564101 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 5.359585747112903 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 5.356691770487318 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 5.363772767941315 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 5.357625671577538 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 5.361405473525441 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 5.360678935745234 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 5.364639304974419 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 5.356203224783619 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 5.355448037445111 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 5.3572302063437665 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 5.359841145665446 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 5.359069510324622 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 5.356913391509751 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 5.359326541634182 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 5.35408046493829 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 5.356911961432195 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 5.3552272989869305 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 5.3549668330797005 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 5.357392577390757 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 5.3601695782244665 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 5.359454322539758 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 5.35386535683768 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 5.356521376231741 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 5.363430618082691 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 5.3620890543841115 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 5.3566190680209935 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 5.356555484644551 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 5.35871540316677 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 5.358140546676473 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 5.360375272157359 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 5.361093234153292 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 5.357549031470328 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 5.3591445185295035 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 5.3566547194354035 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 5.36063013647714 (4533 batches).
Loss on test set: 5.3529849375313265
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.386399897405072 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.380628719107352 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.368007004379359 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.375477092236629 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.367591156145345 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.372117979306141 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.364772597909263 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.359602342713308 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.3585543724550675 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.358535738291657 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.3555597368781465 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.358768520714457 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.360815546001369 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.357448412492105 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.359542954015384 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.354579697664069 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.354172598992177 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.356841182645625 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.359304024109166 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.357660697912396 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.356170961747968 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.35499689246602 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.356509627515897 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.356528297260637 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.3605584798205275 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.359740986022995 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.352553469467289 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 5.357162411813044 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 5.356821938616115 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 5.353962545320737 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 5.359301615150565 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 5.355444219092409 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 5.355521719423636 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 5.3591363432653 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 5.355284536447637 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 5.355612717925542 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 5.355136772996143 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 5.358434063641935 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 5.355292928473933 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 5.3576275921935625 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 5.354603890312341 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 5.353320312321173 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 5.356658643361829 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 5.3508140817531125 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 5.355255868243982 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 5.357030047109104 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 5.355094131257343 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 5.358751609681486 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 5.354286528280601 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 5.35326243450172 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 5.355565902743938 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 5.355685338565326 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 5.354178343233672 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 5.352504040030962 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 5.358066333213338 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 5.355424739589708 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 5.362159873393627 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 5.359248381352125 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 5.3519586064603555 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 5.358072519854629 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 5.359046615724292 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 5.356001224866009 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 5.353689413735495 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 5.352158119918862 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 5.355650980846524 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 5.350660521069452 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 5.35193813407287 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 5.351992846508308 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 5.353066592586769 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 5.359040434329472 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 5.350357272005386 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 5.357464730331973 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 5.353625343318231 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 5.35217390361801 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 5.353269301501168 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 5.3545983920764275 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 5.353647324427742 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 5.3538691899830635 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 5.354662990033587 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 5.354305612785833 (4533 batches).
Loss on test set: 5.385280677083838
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : Adam
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.391162905647295 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.362362492933932 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.363830496242918 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.3583801813454865 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.356296310250185 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.362693894259219 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.3624088096481906 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.360564175384239 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.358793811505661 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.358257054362159 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.36383650343912 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.356983070601802 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.358964777113769 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.354719853153982 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.364548777077021 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.358043569932572 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.356164463040267 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.354984833642975 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.3605947508970795 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.359039197240527 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.362989311946193 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.360293096944298 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.357149377738091 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.35703323325547 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.357586500504252 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.354598131646338 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.357369555297235 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 5.354991531190613 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 5.356606045254224 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 5.35423763744252 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 5.3594797103709535 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 5.355062125509158 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 5.36059209021306 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 5.355627212610146 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 5.355975973245826 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 5.356860100578689 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 5.356737006481136 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 5.357392286849342 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 5.3590026482719475 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 5.3586615614088435 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 5.35958898003044 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 5.359106306469049 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 5.35542739552692 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 5.355598419424768 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 5.359031273701742 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 5.35632834756924 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 5.3563336760377664 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 5.351370136718889 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 5.359359549313698 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 5.354247493720754 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 5.3553022491256055 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 5.354944564361013 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 5.351789979765859 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 5.358743788573801 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 5.354132293195302 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 5.355547668180165 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 5.355997093355527 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 5.35994562164143 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 5.357304155747674 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 5.356904597438095 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 5.353936798770302 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 5.355441826662164 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 5.354484499776282 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 5.353704718598829 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 5.353972528723989 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 5.354400584825532 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 5.352009249685512 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 5.355081230053543 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 5.354773989858255 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 5.35401672978236 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 5.354602765831862 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 5.360837629239553 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 5.354811683596298 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 5.3555318379991235 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 5.355475347772645 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 5.359564946288439 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 5.35345354567767 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 5.354332616083043 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 5.357163720853592 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 5.357958806131735 (4533 batches).
Loss on test set: 5.356637151675907
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.306234086714332 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.159102059793924 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.0526607541382615 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 4.923893500171222 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 4.776496178446925 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.601552698671647 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.409700288826231 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 4.212678883214042 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 4.026906801432457 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 3.777104710890919 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 3.5695632722318344 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 3.3625198807801575 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 3.17787475260367 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 2.9970896368044615 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 2.8584542129704054 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 2.6999916413118052 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 2.540296711282385 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 2.4220365187518516 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 2.284035355201126 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 2.183391619624088 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 2.063478923195013 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 1.9801689180059199 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 1.8970354840820156 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 1.7995433224672885 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 1.7250126744050367 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 1.6545103515861788 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 1.5977092157090287 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 1.549448828691419 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 1.502926766948013 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 1.4576064491019511 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 1.402539011220019 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 1.353636701967499 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 1.3071651121861567 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 1.3030042041421015 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 1.2446644705453773 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 1.2110405152151187 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 1.1902186279853868 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 1.1677968837082477 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 1.151525708403173 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 1.1174157047223077 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 1.0794838104268034 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 1.0650710227498412 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 1.0500509512955691 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 1.0407941874824242 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 1.010074486351555 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 0.9916584577362926 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 0.9725352810683852 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 0.9613925425350969 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 0.926028606297558 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 0.93078706650026 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 0.9402483326169058 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 0.9121025478553225 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 0.894854504042263 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 0.8699900761736483 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 0.8602306509376914 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 0.856759097921041 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 0.8292697464824117 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 0.8254475341663965 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 0.8147015074099923 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 0.7940520479167347 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 0.7923019762244836 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 0.7758475246560014 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 0.7732353838579349 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 0.7665355648215102 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 0.7677789798555799 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 0.76482106299819 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 0.7560661151272136 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 0.7406685974932968 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 0.7383230542790776 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 0.7381616455519039 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 0.765453894954432 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 0.7095406892486106 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 0.7022246041247026 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 0.6943034307368408 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 0.6840569899702424 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 0.6789431054693249 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 0.6737201191746266 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 0.6593860823380849 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 0.6863127744785608 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 0.6612316725429204 (4533 batches).
Loss on test set: 6.456781767792117
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.311448280881788 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.133190711944171 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 4.971353847749602 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 4.748816458694513 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 4.492161641419657 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.178222286972073 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 3.841568642017096 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 3.477902763073423 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 3.1444139711751125 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 2.8298010952604704 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 2.5359767289080684 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 2.2501009537661916 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 2.024843660695858 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 1.828398881876624 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 1.643681023482742 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 1.4694828530586557 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 1.3467762245791073 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 1.2272763968618559 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 1.1245978266927756 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 1.0457507439225604 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 0.9676717530608099 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 0.9033807755575101 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 0.8663264066769855 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 0.7961610789537535 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 0.746350389234467 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 0.7308441457118179 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 0.6722955431615757 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 0.665843692243086 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 0.6345818313822797 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 0.6159801292624559 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 0.6183824980137971 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 0.5796894389415271 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 0.5648723169240814 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 0.5336735924919946 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 0.5114237240873345 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 0.49096199732701984 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 0.48487229737624304 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 0.4703791017448721 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 0.4649132515700111 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 0.4443138181545233 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 0.4448520100237701 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 0.4215597907504892 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 0.41933157495840373 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 0.4044746520265217 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 0.3944465169843764 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 0.3807875096235821 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 0.38104508218256655 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 0.3739986508877604 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 0.36699112425744207 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 0.37049249224997555 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 0.35541299162829076 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 0.3435664269945794 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 0.33953223505751623 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 0.34408449412901065 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 0.3312531336648901 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 0.32459725735073397 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 0.31810300095229127 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 0.32779429213672107 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 0.30816556856253147 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 0.29983581107100743 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 0.2914117905127405 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 0.3099932189420204 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 0.2988502832337928 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 0.2819893307903337 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 0.28067818244834125 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 0.294416919324326 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 0.28175050110042343 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 0.2650580690027651 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 0.2615495921422216 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 0.2619641501790978 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 0.25841629397581806 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 0.2518954275546979 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 0.2625618913750656 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 0.24766528146115854 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 0.23877605087701753 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 0.23718480923197244 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 0.23650874419702012 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 0.23087924661329717 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 0.22481211706890772 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 0.22742886882059205 (4533 batches).
Loss on test set: 6.114195690832718
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.314024798369897 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.170047701864665 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.079043487221259 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 4.966268212937793 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 4.844731518850405 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.707765120374376 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.557283076602524 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 4.390536567403759 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 4.205828816743973 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 4.008310518410988 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 3.8329411518907746 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 3.644856455650599 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 3.4731790209721183 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 3.2928020602000516 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 3.138487721920119 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 2.971629204167295 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 2.8301083674473975 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 2.6835549704396855 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 2.555050386883895 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 2.4403309167128326 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 2.33863980148687 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 2.276183496385701 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 2.146784062801595 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 2.0552510193397264 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 1.9744941300965029 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 1.893298400873752 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 1.8212768099651466 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 1.7630425254332949 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 1.6957061302969583 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 1.6543227750130003 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 1.6268810107564389 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 1.5520271359362035 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 1.4984891338903805 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 1.4649656803934563 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 1.4224930993510796 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 1.3864572252654987 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 1.3575263575242105 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 1.3177705159414532 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 1.299753938543331 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 1.255322569461419 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 1.2272227322545084 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 1.2233213455622332 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 1.1861430431093414 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 1.1623767378404917 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 1.1415546151638662 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 1.1240175603018832 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 1.1042257015302221 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 1.0765829502142426 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 1.050806705713167 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 1.0368562381629935 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 1.021251569494043 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 1.000515324993689 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 0.9977384819485824 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 0.9757693570535387 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 0.9600202228200901 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 0.9492876665852019 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 0.9361568436950294 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 0.933053183445151 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 0.9173700871542277 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 0.8979318964936311 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 0.8832869036240265 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 0.8825238975446639 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 0.8667216340608742 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 0.8524558397218299 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 0.8500225461071068 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 0.8367442330290039 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 0.8294941294398394 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 0.8255133101436809 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 0.8017440798028583 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 0.7974201168362836 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 0.7832065370536234 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 0.7792611351089185 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 0.7694713231673337 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 0.7731690162161237 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 0.7685952976415048 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 0.7679027981633728 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 0.7526270818945695 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 0.7487045084038976 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 0.7425775427298122 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 0.7402879229654099 (4533 batches).
Loss on test set: 37.601833964229
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.307192044912415 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.213501251193564 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.141843595621524 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.055227083965771 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 4.940562977134432 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.810158415632797 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.659979264335918 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 4.478935241580877 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 4.286111833966123 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 4.063180299013518 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 3.853452139978453 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 3.6608788048145446 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 3.480879629641844 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 3.279097634502628 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 3.1157491183224404 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 2.9709972336464254 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 2.823265454678669 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 2.655375259793833 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 2.520452479140111 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 2.393975961078168 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 2.296668241401658 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 2.185699450471347 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 2.0968060066190852 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 2.0251986496092966 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 1.9286853592705575 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 1.8662578163953272 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 1.7885607569890387 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 1.7160183217400302 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 1.655552164472914 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 1.6037804777788427 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 1.5588401297570431 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 1.4843130866975782 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 1.4295648031188193 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 1.418837418007635 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 1.3596191712589998 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 1.3413114688818801 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 1.296025732245741 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 1.262852080486848 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 1.2362110109031266 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 1.220540176055538 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 1.1782968095200703 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 1.236393631254104 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 1.1592672976823193 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 1.199673357492195 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 1.1186562885636924 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 1.1200273533366307 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 1.0836626905335671 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 1.0540931935331928 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 1.0339703357511605 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 1.0310751522252057 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 1.08111628887079 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 1.0182182231953727 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 0.9957491522451491 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 0.9752533839201523 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 0.954556589931769 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 0.9366407444359134 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 0.938298717811753 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 0.9098036963646311 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 0.9022428260098329 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 0.8858861887189375 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 0.8706778402843145 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 0.8524969010269723 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 0.8953806392301162 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 0.8615695688238266 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 0.8337981292569898 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 0.8175285418180738 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 0.8040778486014148 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 0.8183419430674029 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 0.7987631074796574 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 0.7837377078675051 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 0.7686668497335823 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 0.7704696590119894 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 0.7702722175265816 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 0.7486364304108807 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 0.7445107036073582 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 0.7316449407908056 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 0.7233738431584985 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 0.7112034212821713 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 0.7337871502707685 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 0.7065007838042741 (4533 batches).
Loss on test set: 6.404305842337721
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.296719334232289 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.198803750379391 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.104883654344274 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 4.984073749302293 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 4.8421683092136245 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.652026861202577 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.4216175311294945 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 4.138477587539472 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 3.8624445972867605 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 3.561449664766731 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 3.258278848371205 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 2.958862229343442 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 2.6808104835255055 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 2.4547981079671914 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 2.2225843867915316 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 2.016085229848569 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 1.8232173988381521 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 1.661082894490987 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 1.5093666063997622 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 1.3593944879608282 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 1.2508524921314883 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 1.1306188904388361 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 1.0417307259762705 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 0.9727701692984326 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 0.9057519764740315 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 0.8561906387362552 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 0.8132568387441648 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 0.7791865719188503 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 0.7352608417060342 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 0.7532788699572117 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 0.6952888249591594 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 0.6586049650330404 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 0.6230204546305943 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 0.6028118245739194 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 0.5684548047154044 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 0.5465883810994393 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 0.5396035941563811 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 0.5242659326206368 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 0.5248797665365137 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 0.4945167982266645 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 0.47720256805117495 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 0.48146937163281645 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 0.4569254842353214 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 0.4413338343177493 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 0.425574138563331 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 0.4457311715290506 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 0.40849729233967685 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 0.40569920766216855 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 0.4164342288311654 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 0.3973612629892927 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 0.3832265337686283 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 0.37580000027889265 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 0.3646041673838027 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 0.3506962246428571 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 0.34030986918736816 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 0.33022629979405344 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 0.3315438870541931 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 0.33273173233392667 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 0.3212023844053657 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 0.3117276940493954 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 0.30894743584420226 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 0.33515270460111835 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 0.321082868538312 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 0.3041988155729995 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 0.30101759565762803 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 0.3709946956552077 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 0.33624251188902227 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 0.3071699354492708 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 0.29209432533118085 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 0.28819613217298307 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 0.31487995504670563 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 0.28018518036673945 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 0.2979631413833645 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 0.27622739683870395 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 0.2667790719043129 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 0.26278053990840755 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 0.256254667068176 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 0.25679764562522095 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 0.24962228539605463 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 0.2696826126325375 (4533 batches).
Loss on test set: 6.7867648548741055
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 1e-05
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.305016849547487 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.2131230668361 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.145094631971599 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.054890788077467 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 4.9564622539362055 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.833943184705962 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.6960960020956914 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 4.522786337598754 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 4.325567655575978 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 4.122005370734623 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 3.924836001936868 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 3.7161896124649805 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 3.5161804393403924 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 3.326466834140145 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 3.1407579942541672 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 2.9825255867400338 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 2.8211567399387665 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 2.6861123155737565 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 2.5573747798014814 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 2.436993836528177 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 2.3513235720924373 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 2.2265766065114914 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 2.1275182502858296 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 2.040083072121349 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 1.9611744089103444 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 1.8764113404617755 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 1.78916706317391 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 1.7516514467083275 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 1.6775934332166054 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 1.6490269387541505 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 1.594386915267237 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 1.5391151944769945 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 1.489981835044406 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 1.453602707658202 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 1.419600102369185 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 1.3861120776272282 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 1.3388030932172097 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 1.313386117770344 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 1.2893139918776713 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 1.2628171317149492 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 1.229884109833949 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 1.2045991893848678 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 1.1729721669958255 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 1.1612452649528449 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 1.1351163375332949 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 1.1132808129124208 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 1.1096387553919815 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 1.0710814882463475 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 1.0416018137428478 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 1.0423007256939214 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 1.027130406360581 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 1.0031419032237505 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 0.9967665107386174 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 0.9816970588479536 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 0.9669673167729572 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 0.9631569938248892 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 0.9395912496256771 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 0.9279207773235443 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 0.9186076308530133 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 0.9106234371688438 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 0.8956500077396203 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 0.8807040661796944 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 0.8747514103116669 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 0.86947726073537 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 0.85388230780555 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 0.8338169152344033 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 0.8395395833787712 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 0.8249209479414652 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 0.8179375782145482 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 0.8101757401080681 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 0.8004771131383591 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 0.8037072589447134 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 0.7919601628467681 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 0.7815695235087807 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 0.7732713966871396 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 0.771756618018448 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 0.7671697504494644 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 0.7571664653371666 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 0.7405702164669897 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 0.7570858623231717 (4533 batches).
Loss on test set: 8.355090228462808
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.305981876306809 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.1667223558661535 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.070653095903865 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 4.961400820513106 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 4.841927369146349 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.712444319936473 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.571929841911506 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 4.42353105447741 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 4.272441429858331 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 4.096075279176722 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 3.936165370847541 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 3.792924386919031 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 3.6451535934963264 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 3.506286159496341 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 3.3635946560610797 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 3.2733869344199857 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 3.139660344320024 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 3.0435728367369563 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 2.938881034741148 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 2.8503451964436004 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 2.7568843886995853 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 2.700423756097816 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 2.619493624153133 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 2.5697278641731356 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 2.5053366445992764 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 2.4775698602186407 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 2.4385153901989876 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 2.3937990699820557 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 2.352811490400669 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 2.3124446680076125 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 2.2889920603668825 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 2.239405970192892 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 2.2037096545708357 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 2.228325932251547 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 2.1778470781329133 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 2.1718212063741715 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 2.1601990766670434 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 2.1122358065027025 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 2.1099009460667073 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 2.140572255510593 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 2.1181167136641914 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 2.1081480735018374 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 2.0322254130889217 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 2.0363108993838908 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 2.035537001301904 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 2.01940096485123 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 2.002582272866849 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 1.99579304751907 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 1.9903325477782239 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 1.9841099389927976 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 2.0977133302901088 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 2.0231490105607506 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 1.9619534057732477 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 1.9342146839312888 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 1.9132427780827543 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 1.9263364032959533 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 1.9027375686768266 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 1.912484831692182 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 1.8921668786640176 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 1.88040314345546 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 1.877097348779762 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 1.8635384160544524 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 1.8803134621070654 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 1.8589978823460886 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 1.8702117292041576 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 1.8497199000387614 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 1.816196378817654 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 1.8608118367594861 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 1.8418422649231752 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 1.8193993756386084 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 1.873339581418558 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 1.8055523046983306 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 1.8051346157287727 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 1.8506530733290503 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 1.7905471160294546 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 1.783099120716609 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 1.7956475540486072 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 1.7507733981616709 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 1.76219066713205 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 1.8197728989667616 (4533 batches).
Loss on test set: 8.880205521008996
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.312292382466249 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.141166994278725 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 4.995650509980086 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 4.812897898281433 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 4.6138243303245305 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.381922511477989 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.1428167679807935 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 3.880803961273116 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 3.6476164273465677 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 3.386637802987127 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 3.1899358516835545 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 2.935476696940939 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 2.7301114133039164 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 2.5964561244614623 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 2.4436439649437585 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 2.289587001558069 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 2.1488982067514724 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 2.043317364582604 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 1.93672254137874 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 1.8573741472217122 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 1.7962337718903032 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 1.6988766122253531 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 1.6604184042419947 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 1.5880217854481724 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 1.533519711697993 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 1.5299244071247033 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 1.5556914702422724 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 1.4712059446749193 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 1.398490504653228 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 1.3551368383107636 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 1.3435158647470276 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 1.3146167704595153 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 1.2869729023848362 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 1.297863769170966 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 1.4994153303011821 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 1.3507182559357187 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 1.2898008772836678 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 1.2427518346367254 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 1.194556876589677 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 1.1817258553422894 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 1.1913048782978208 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 1.1981048820601534 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 1.1589630855094932 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 1.1758499685309198 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 1.1658278732814722 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 1.165664707262627 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 1.1309031754659813 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 1.1356767888445742 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 1.1287259983813922 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 1.1433925139550314 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 1.1573184100442997 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 1.1693917538501453 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 1.141161982343998 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 1.2819904237163795 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 1.2115826475522966 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 1.1269007823669355 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 1.095879785688592 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 1.2022560758028444 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 1.127449944566055 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 1.132150763027352 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 1.0857533424346488 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 1.0595155936839853 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 1.1531714082007478 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 1.0805245698083454 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 1.0547964052105487 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 1.3173692279795055 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 1.1848054921557354 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 1.0665102853282986 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 1.015458343924078 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 0.9885115481990341 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 1.0713393856016658 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 1.0592631668924049 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 1.021222603415091 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 1.0462771178067665 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 1.0030001887199818 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 0.9924788191806079 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 1.0016625107071033 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 1.0284387310385414 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 1.0826261633743703 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 1.0552948454529303 (4533 batches).
Loss on test set: 9.961755564971515
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.001
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.312210463265196 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.1711499556363245 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.089702993019932 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 4.992091917712878 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 4.892277098270065 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 4.783055537167223 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 4.675228127890435 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 4.537103220797416 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 4.40459370379045 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 4.264480719786198 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 4.134731252593708 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 3.9888862811411925 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 3.8525226944044624 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 3.722636834165 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 3.5886890366854267 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 3.4633483242415286 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 3.3433163106086474 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 3.2251368085464422 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 3.121372577859002 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 3.0286671719276534 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 2.9221295866644357 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 2.8525922186532977 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 2.75878251920387 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 2.6991401490012255 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 2.626556243639815 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 2.5559767410635765 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 2.504876677337241 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 2.468748764304854 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 2.402235869669162 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 2.3687909134118574 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 2.3299240526013256 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 2.2848335320182174 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 2.2498388973366943 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 2.222346136592593 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 2.197320169679855 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 2.1579598463872873 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 2.151818063988634 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 2.1391764399680824 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 2.10629068534947 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 2.0747949561022407 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 2.0544429437203724 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 2.070125739161026 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 2.0274354539353037 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 2.0297996020727327 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 1.9906333940078584 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 1.9969307669090588 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 1.9972440538331686 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 1.936457717107038 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 1.9402321660963084 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 1.9411219185797184 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 1.9245781558464097 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 1.9084400788932454 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 1.9011988175012617 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 1.890473782503392 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 1.8768980159366102 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 1.8563850745180703 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 1.8995964813845168 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 1.8618152896392275 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 1.8505746374297505 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 1.8418184716444312 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 1.8299670222182687 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 1.8041900190958882 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 1.805995567913169 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 1.7839467593265532 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 1.7982767947246612 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 1.7942375033311488 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 1.778800170994373 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 1.7835421469099548 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 1.7941629386700306 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 1.7758911487718516 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 1.766468437274718 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 1.754986541379821 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 1.797793640065819 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 1.7439775184871817 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 1.7399977401487354 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 1.743510633049594 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 1.7688883343609179 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 1.7386118546506308 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 1.720552625310045 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 1.718235560226672 (4533 batches).
Loss on test set: 7.782253116338555
dnn_hidden_units : 256, 128, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=256, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=128, out_features=32, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.306235796432739 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.236429330083989 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.211159237194293 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.190629446346082 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.172636906174459 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.160167060070892 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.153850331360106 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.143678937083492 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.133923727262107 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.122731278123062 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.118598228491828 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.111677640798785 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.111980943550423 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.106909917096758 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.102089023434929 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.100447015206913 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.095159255351132 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.0921249572157254 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.08569424391161 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.083558678653312 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.075053413709004 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.083115917390318 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.079015586093749 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.078077711939102 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.069489552883709 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.065468707030481 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.070308865707704 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 5.0690447206453015 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 5.064287780071762 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 5.063164932375945 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 5.073175680129807 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 5.06399785870094 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 5.065316321267057 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 5.063299391933974 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 5.0649707490130815 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 5.067955752200647 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 5.064335297465245 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 5.063398256145144 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 5.063293395362204 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 5.062147186682394 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 5.063940861170954 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 5.060814111090222 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 5.0699998321266015 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 5.066857895971054 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 5.065594712262014 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 5.067806151432363 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 5.06630864206224 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 5.063128408074983 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 5.05836944761009 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 5.064906507355736 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 5.065720889272738 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 5.0693258197792 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 5.0670016072534345 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 5.067261041801759 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 5.066237930164888 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 5.069246758747753 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 5.062107896294626 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 5.063350521121729 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 5.063738255594941 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 5.056016083850309 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 5.061341477576508 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 5.060517044538645 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 5.060812590770734 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 5.0596852627858775 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 5.067268402587492 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 5.06302786165322 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 5.059668938108503 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 5.058248739118427 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 5.072537897728727 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 5.0689997726945775 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 5.065030122997218 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 5.060894061391937 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 5.065375409877356 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 5.06268029028339 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 5.066757831961646 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 5.063336182108731 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 5.063963337815573 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 5.063475294484489 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 5.062165031232188 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 5.063130478129964 (4533 batches).
Loss on test set: 5.475276574661263
dnn_hidden_units : 2000, 100, 16
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.297102430969733 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.233743922444282 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.202616404598185 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.179845120137347 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.165857671560527 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.149386029513813 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.134764073241133 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.121827924669696 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.1162571961480525 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.108579540789167 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.096295783174188 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.091104929478206 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.083450362339415 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.0786592291859955 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.075500036795461 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.07233687452257 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.061967524502837 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.0602957296655475 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.057592340712776 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.0595659867123 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.047318685509736 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.051783374529813 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.05618996858702 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.044080002587178 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.049074772412536 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.0423071323155355 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.0390605639469905 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 5.035516896916888 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 5.035663284267676 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 5.035828468764351 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 5.038297117321848 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 5.044436404832856 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 5.040724914962294 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 5.040805875295154 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 5.036482677458665 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 5.037698918321973 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 5.036264470521383 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 5.039497127602386 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 5.03453474291476 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 5.034899620591161 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 5.03137602173359 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 5.032003843466125 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 5.03520337148873 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 5.027524631517285 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 5.029600189138716 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 5.030998957017755 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 5.034112077608241 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 5.036176980527958 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 5.030610807968138 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 5.029326238880456 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 5.030764772998243 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 5.035314098654692 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 5.035541656788889 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 5.03473461960263 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 5.033321499653586 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 5.032423298957988 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 5.036452842947402 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 5.035569565174676 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 5.031197180659624 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 5.036508469031448 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 5.034910137227848 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 5.041288479561051 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 5.034984660048687 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 5.027805550022681 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 5.035408524364502 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 5.0265679120912266 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 5.021590352111007 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 5.027991130431967 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 5.028774770027277 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 5.036367139331348 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 5.026671347806397 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 5.034848744101853 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 5.027048869662114 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 5.023317664841955 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 5.0250149763579826 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 5.030177254735201 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 5.03247839692358 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 5.025155459942997 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 5.029876560479002 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 5.0330377315649155 (4533 batches).
Loss on test set: 5.391197196855359
dnn_hidden_units : 300, 32
dropout_percentages : 0, 0, 0
learning_rate : 0.01
nr_epochs : 80
batch_size : 200
eval_freq : 100
data_dir : dataloader/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : False
weightdecay : 0.01
momentum : 0
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)


Epoch: 0
Average batch loss (epoch 0: 5.3037944124703795 (4533 batches).


Epoch: 1
Average batch loss (epoch 1: 5.233772042753311 (4533 batches).


Epoch: 2
Average batch loss (epoch 2: 5.207259069839745 (4533 batches).


Epoch: 3
Average batch loss (epoch 3: 5.17844534447565 (4533 batches).


Epoch: 4
Average batch loss (epoch 4: 5.161753526539721 (4533 batches).


Epoch: 5
Average batch loss (epoch 5: 5.155657363416553 (4533 batches).


Epoch: 6
Average batch loss (epoch 6: 5.144395004564265 (4533 batches).


Epoch: 7
Average batch loss (epoch 7: 5.1263474038492465 (4533 batches).


Epoch: 8
Average batch loss (epoch 8: 5.117380869288647 (4533 batches).


Epoch: 9
Average batch loss (epoch 9: 5.10701570154788 (4533 batches).


Epoch: 10
Average batch loss (epoch 10: 5.1033787248993505 (4533 batches).


Epoch: 11
Average batch loss (epoch 11: 5.089161047195457 (4533 batches).


Epoch: 12
Average batch loss (epoch 12: 5.08428385229845 (4533 batches).


Epoch: 13
Average batch loss (epoch 13: 5.070861559920876 (4533 batches).


Epoch: 14
Average batch loss (epoch 14: 5.067486715663907 (4533 batches).


Epoch: 15
Average batch loss (epoch 15: 5.062046355215519 (4533 batches).


Epoch: 16
Average batch loss (epoch 16: 5.056376783678975 (4533 batches).


Epoch: 17
Average batch loss (epoch 17: 5.0543394398668235 (4533 batches).


Epoch: 18
Average batch loss (epoch 18: 5.055807439586118 (4533 batches).


Epoch: 19
Average batch loss (epoch 19: 5.052584937069555 (4533 batches).


Epoch: 20
Average batch loss (epoch 20: 5.046381676812428 (4533 batches).


Epoch: 21
Average batch loss (epoch 21: 5.041662499469662 (4533 batches).


Epoch: 22
Average batch loss (epoch 22: 5.038592547631911 (4533 batches).


Epoch: 23
Average batch loss (epoch 23: 5.038302561764998 (4533 batches).


Epoch: 24
Average batch loss (epoch 24: 5.030427095486632 (4533 batches).


Epoch: 25
Average batch loss (epoch 25: 5.0311050316684165 (4533 batches).


Epoch: 26
Average batch loss (epoch 26: 5.0288978928949515 (4533 batches).


Epoch: 27
Average batch loss (epoch 27: 5.025926679366213 (4533 batches).


Epoch: 28
Average batch loss (epoch 28: 5.024361002269024 (4533 batches).


Epoch: 29
Average batch loss (epoch 29: 5.028527622898236 (4533 batches).


Epoch: 30
Average batch loss (epoch 30: 5.025818241515645 (4533 batches).


Epoch: 31
Average batch loss (epoch 31: 5.027884086078296 (4533 batches).


Epoch: 32
Average batch loss (epoch 32: 5.022840328282824 (4533 batches).


Epoch: 33
Average batch loss (epoch 33: 5.015729024713728 (4533 batches).


Epoch: 34
Average batch loss (epoch 34: 5.015862014260535 (4533 batches).


Epoch: 35
Average batch loss (epoch 35: 5.017551431902034 (4533 batches).


Epoch: 36
Average batch loss (epoch 36: 5.014675131833977 (4533 batches).


Epoch: 37
Average batch loss (epoch 37: 5.015982956489716 (4533 batches).


Epoch: 38
Average batch loss (epoch 38: 5.014149896361668 (4533 batches).


Epoch: 39
Average batch loss (epoch 39: 5.014559811034162 (4533 batches).


Epoch: 40
Average batch loss (epoch 40: 5.015116383375617 (4533 batches).


Epoch: 41
Average batch loss (epoch 41: 5.014760011609438 (4533 batches).


Epoch: 42
Average batch loss (epoch 42: 5.013290394280832 (4533 batches).


Epoch: 43
Average batch loss (epoch 43: 5.010860107393683 (4533 batches).


Epoch: 44
Average batch loss (epoch 44: 5.019503100066161 (4533 batches).


Epoch: 45
Average batch loss (epoch 45: 5.01296207028037 (4533 batches).


Epoch: 46
Average batch loss (epoch 46: 5.008855994163747 (4533 batches).


Epoch: 47
Average batch loss (epoch 47: 5.0057710026001105 (4533 batches).


Epoch: 48
Average batch loss (epoch 48: 5.010368249496824 (4533 batches).


Epoch: 49
Average batch loss (epoch 49: 5.00673048072321 (4533 batches).


Epoch: 50
Average batch loss (epoch 50: 5.009743505983249 (4533 batches).


Epoch: 51
Average batch loss (epoch 51: 5.002392822245008 (4533 batches).


Epoch: 52
Average batch loss (epoch 52: 5.003131278219167 (4533 batches).


Epoch: 53
Average batch loss (epoch 53: 5.010994164827458 (4533 batches).


Epoch: 54
Average batch loss (epoch 54: 5.008431695271821 (4533 batches).


Epoch: 55
Average batch loss (epoch 55: 5.009438103170498 (4533 batches).


Epoch: 56
Average batch loss (epoch 56: 5.005936993738423 (4533 batches).


Epoch: 57
Average batch loss (epoch 57: 5.0146193937463215 (4533 batches).


Epoch: 58
Average batch loss (epoch 58: 5.007028081284015 (4533 batches).


Epoch: 59
Average batch loss (epoch 59: 5.014021787847506 (4533 batches).


Epoch: 60
Average batch loss (epoch 60: 5.007251644184488 (4533 batches).


Epoch: 61
Average batch loss (epoch 61: 5.012440982557621 (4533 batches).


Epoch: 62
Average batch loss (epoch 62: 5.0082694769379 (4533 batches).


Epoch: 63
Average batch loss (epoch 63: 5.00696422353949 (4533 batches).


Epoch: 64
Average batch loss (epoch 64: 5.007581465322661 (4533 batches).


Epoch: 65
Average batch loss (epoch 65: 5.009492208527435 (4533 batches).


Epoch: 66
Average batch loss (epoch 66: 5.005353406936213 (4533 batches).


Epoch: 67
Average batch loss (epoch 67: 5.006709382598721 (4533 batches).


Epoch: 68
Average batch loss (epoch 68: 5.0101155964133754 (4533 batches).


Epoch: 69
Average batch loss (epoch 69: 5.0068914515278395 (4533 batches).


Epoch: 70
Average batch loss (epoch 70: 5.0042871308069 (4533 batches).


Epoch: 71
Average batch loss (epoch 71: 5.003686719190041 (4533 batches).


Epoch: 72
Average batch loss (epoch 72: 5.009076854368985 (4533 batches).


Epoch: 73
Average batch loss (epoch 73: 5.002353756574508 (4533 batches).


Epoch: 74
Average batch loss (epoch 74: 5.004639658001173 (4533 batches).


Epoch: 75
Average batch loss (epoch 75: 5.009789098500416 (4533 batches).


Epoch: 76
Average batch loss (epoch 76: 5.003146258037979 (4533 batches).


Epoch: 77
Average batch loss (epoch 77: 5.006799653905553 (4533 batches).


Epoch: 78
Average batch loss (epoch 78: 5.000512903017528 (4533 batches).


Epoch: 79
Average batch loss (epoch 79: 5.007673551882179 (4533 batches).
Loss on test set: 5.351866722352473
