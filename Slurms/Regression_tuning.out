********************************************************************************************
** WARNING: 'The 'pre2019' module environment is deprecated. Please consider switching
             to the '2019' or '2020' module environment. You can read more about our
             software policy on this page:
             https://userinfo.surfsara.nl/documentation/software-policy-lisacartesius

             If you have any question, please contact us via http://servicedesk.surfsara.nl.'
********************************************************************************************
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.837162017822266 / Valid loss: 14.915881393069313
Model is saved in epoch 0, overall batch: 0
Training loss: 6.321648120880127 / Valid loss: 9.074903215680804
Model is saved in epoch 0, overall batch: 100
Training loss: 4.496370792388916 / Valid loss: 6.700826163518997
Model is saved in epoch 0, overall batch: 200
Training loss: 6.916571617126465 / Valid loss: 5.88400741985866
Model is saved in epoch 0, overall batch: 300
Training loss: 5.420005798339844 / Valid loss: 5.594571342922392
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.56787109375 / Valid loss: 5.586402600152152
Model is saved in epoch 1, overall batch: 500
Training loss: 4.816599369049072 / Valid loss: 5.70923262323652
Training loss: 5.809679985046387 / Valid loss: 5.6599036262148905
Training loss: 4.620092391967773 / Valid loss: 5.689857108252389
Training loss: 4.591750144958496 / Valid loss: 5.585323681150164
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.559162139892578 / Valid loss: 5.598988962173462
Training loss: 6.443928241729736 / Valid loss: 5.627868438902356
Training loss: 5.298986434936523 / Valid loss: 5.563427464167277
Model is saved in epoch 2, overall batch: 1200
Training loss: 6.769243240356445 / Valid loss: 5.624234923862276
Training loss: 4.6555585861206055 / Valid loss: 5.65996219090053

Epoch: 3
Training loss: 4.282618999481201 / Valid loss: 5.731308887118385
Training loss: 4.53293514251709 / Valid loss: 5.643079412551153
Training loss: 5.500177383422852 / Valid loss: 5.553071648733956
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.4861159324645996 / Valid loss: 5.787145058314006
Training loss: 5.150965690612793 / Valid loss: 5.668568588438488

Epoch: 4
Training loss: 5.594768524169922 / Valid loss: 5.853413159506662
Training loss: 5.42860221862793 / Valid loss: 5.6452359812600275
Training loss: 4.522441864013672 / Valid loss: 5.685219810122535
Training loss: 5.50386905670166 / Valid loss: 5.618183122362409
Training loss: 4.196699142456055 / Valid loss: 5.617743512562343

Epoch: 5
Training loss: 6.768355369567871 / Valid loss: 5.65682133265904
Training loss: 5.613252639770508 / Valid loss: 5.799467668079195
Training loss: 4.041893482208252 / Valid loss: 5.664829506192889
Training loss: 4.827764511108398 / Valid loss: 5.71393240974063
Training loss: 4.052933692932129 / Valid loss: 5.666227908361526

Epoch: 6
Training loss: 7.671290397644043 / Valid loss: 5.799550624120803
Training loss: 5.091148376464844 / Valid loss: 5.824727507999965
Training loss: 4.61021614074707 / Valid loss: 5.7646747566404795
Training loss: 4.658868789672852 / Valid loss: 5.7716200624193466
Training loss: 4.697964668273926 / Valid loss: 5.795917574564616

Epoch: 7
Training loss: 5.79011344909668 / Valid loss: 5.877698117210752
Training loss: 4.209721088409424 / Valid loss: 5.862874333063761
Training loss: 4.430708408355713 / Valid loss: 5.705767858596075
Training loss: 6.781694412231445 / Valid loss: 5.762449757258097
Training loss: 4.246590614318848 / Valid loss: 5.78728544598534

Epoch: 8
Training loss: 3.924065351486206 / Valid loss: 5.83439956619626
Training loss: 2.9888992309570312 / Valid loss: 5.96227236248198
Training loss: 4.779830455780029 / Valid loss: 6.100723237083072
Training loss: 7.33599328994751 / Valid loss: 5.921323054177421
Training loss: 4.720703125 / Valid loss: 5.817060343424479

Epoch: 9
Training loss: 2.892019033432007 / Valid loss: 6.133499579202561
Training loss: 5.535035610198975 / Valid loss: 5.995180908838908
Training loss: 6.33676815032959 / Valid loss: 5.987954693748837
Training loss: 4.203104019165039 / Valid loss: 6.153385725475493

Epoch: 10
Training loss: 4.013599395751953 / Valid loss: 5.98314365432376
Training loss: 4.632652759552002 / Valid loss: 5.97621294430324
Training loss: 3.2779626846313477 / Valid loss: 6.008858798799061
Training loss: 4.759273529052734 / Valid loss: 6.131637986501058
Training loss: 4.585094451904297 / Valid loss: 5.959823199680874

Epoch: 11
Training loss: 3.114126682281494 / Valid loss: 6.134550952911377
Training loss: 3.6853957176208496 / Valid loss: 6.090132826850528
Training loss: 5.358453273773193 / Valid loss: 6.224783979143415
Training loss: 2.5956954956054688 / Valid loss: 6.1188843727111815
Training loss: 3.719893455505371 / Valid loss: 6.276047842843192

Epoch: 12
Training loss: 3.6657378673553467 / Valid loss: 6.076996558053153
Training loss: 3.218977451324463 / Valid loss: 6.0561834312620615
Training loss: 4.7692484855651855 / Valid loss: 6.359050519125802
Training loss: 4.457049369812012 / Valid loss: 6.213277517046247
Training loss: 5.01511812210083 / Valid loss: 6.392970439365932

Epoch: 13
Training loss: 2.956104278564453 / Valid loss: 6.4112162521907265
Training loss: 3.0886781215667725 / Valid loss: 6.208647732507615
Training loss: 4.232881546020508 / Valid loss: 6.157359572819301
Training loss: 3.561152458190918 / Valid loss: 6.152854896727062
Training loss: 3.7970731258392334 / Valid loss: 6.322664006551107

Epoch: 14
Training loss: 4.155691146850586 / Valid loss: 6.345315472284953
Training loss: 3.6166415214538574 / Valid loss: 6.284171361014956
Training loss: 4.08259916305542 / Valid loss: 6.611380770092919
Training loss: 3.2750813961029053 / Valid loss: 6.293654296511695
Training loss: 4.174644470214844 / Valid loss: 6.2990826288859045

Epoch: 15
Training loss: 3.213860511779785 / Valid loss: 6.347236401694161
Training loss: 4.276656150817871 / Valid loss: 6.279622881753104
Training loss: 2.9252915382385254 / Valid loss: 6.285624454134987
Training loss: 3.940978527069092 / Valid loss: 6.302877723603022
Training loss: 2.7040579319000244 / Valid loss: 6.152196180252802

Epoch: 16
Training loss: 2.6727325916290283 / Valid loss: 6.286171763283866
Training loss: 2.5984115600585938 / Valid loss: 6.341372231074742
Training loss: 3.003373146057129 / Valid loss: 6.423793250038511
Training loss: 3.5334396362304688 / Valid loss: 6.232769477935064
Training loss: 4.082033157348633 / Valid loss: 6.4230293796176

Epoch: 17
Training loss: 3.3699309825897217 / Valid loss: 6.417585284369332
Training loss: 2.9296908378601074 / Valid loss: 6.367472698574974
Training loss: 3.509660005569458 / Valid loss: 6.327985863458543
Training loss: 3.9207959175109863 / Valid loss: 6.544802683875674
Training loss: 3.0456295013427734 / Valid loss: 6.286824775877453

Epoch: 18
Training loss: 1.9493565559387207 / Valid loss: 6.585811683109829
Training loss: 2.327925682067871 / Valid loss: 6.33869411604745
Training loss: 3.3072962760925293 / Valid loss: 6.205015979494367
Training loss: 3.463420867919922 / Valid loss: 6.326635442461286
Training loss: 3.320037364959717 / Valid loss: 6.7879645483834405

Epoch: 19
Training loss: 3.06874942779541 / Valid loss: 6.340102713448661
Training loss: 3.215500831604004 / Valid loss: 6.429939526603335
Training loss: 4.0716657638549805 / Valid loss: 6.423725293931507
Training loss: 2.37337589263916 / Valid loss: 6.5445363884880425

Epoch: 20
Training loss: 2.3457398414611816 / Valid loss: 6.440985686438424
Training loss: 1.7707871198654175 / Valid loss: 6.545266896202451
Training loss: 2.5070748329162598 / Valid loss: 6.536194635572888
Training loss: 2.7934162616729736 / Valid loss: 6.72126221429734
Training loss: 3.8534679412841797 / Valid loss: 6.448253388631912

Epoch: 21
Training loss: 2.3502917289733887 / Valid loss: 6.492001615251813
Training loss: 2.1942405700683594 / Valid loss: 6.621150116693406
Training loss: 1.962101697921753 / Valid loss: 6.555552900405157
Training loss: 3.1594676971435547 / Valid loss: 6.499924144290742
Training loss: 2.24603271484375 / Valid loss: 6.7195437681107295

Epoch: 22
Training loss: 2.3957300186157227 / Valid loss: 6.610055968874977
Training loss: 2.194551467895508 / Valid loss: 6.738111436934698
Training loss: 1.8851962089538574 / Valid loss: 6.520402663094657
Training loss: 3.333493232727051 / Valid loss: 6.661181120645432
Training loss: 2.8018596172332764 / Valid loss: 6.463453136171613

Epoch: 23
Training loss: 2.189365863800049 / Valid loss: 6.352718355542137
Training loss: 3.0165586471557617 / Valid loss: 6.495504140853882
Training loss: 2.5757973194122314 / Valid loss: 6.7852575279417495
Training loss: 2.2704977989196777 / Valid loss: 6.991298580169678
Training loss: 2.9257986545562744 / Valid loss: 6.538015070415678

Epoch: 24
Training loss: 3.1179487705230713 / Valid loss: 6.710946455455962
Training loss: 1.9269769191741943 / Valid loss: 6.80631187529791
Training loss: 1.649042010307312 / Valid loss: 6.679555266244071
Training loss: 3.9975156784057617 / Valid loss: 6.981930946168445
Training loss: 4.160137176513672 / Valid loss: 6.601296043395996

Epoch: 25
Training loss: 1.817405104637146 / Valid loss: 6.525548998514811
Training loss: 2.1409313678741455 / Valid loss: 6.555835996355329
Training loss: 3.107058525085449 / Valid loss: 6.638503749029977
Training loss: 2.7382726669311523 / Valid loss: 6.61816656930106
Training loss: 3.333390712738037 / Valid loss: 6.753121360143026

Epoch: 26
Training loss: 2.4708471298217773 / Valid loss: 6.649518689655122
Training loss: 2.7714309692382812 / Valid loss: 6.6133708545139855
Training loss: 2.1485538482666016 / Valid loss: 6.751154018583752
Training loss: 1.6650460958480835 / Valid loss: 6.6242997941516695
Training loss: 1.9717754125595093 / Valid loss: 6.74485034942627

Epoch: 27
Training loss: 1.6469314098358154 / Valid loss: 6.953308359781901
Training loss: 1.8618615865707397 / Valid loss: 6.733510671343122
Training loss: 2.041104316711426 / Valid loss: 6.886810947599865
Training loss: 1.931503176689148 / Valid loss: 6.619090361822219
Training loss: 2.5580036640167236 / Valid loss: 6.821873787471226

Epoch: 28
Training loss: 1.5385011434555054 / Valid loss: 6.659838971637544
Training loss: 1.7731494903564453 / Valid loss: 6.692001256488618
Training loss: 1.728772521018982 / Valid loss: 6.667549192337763
Training loss: 1.8270306587219238 / Valid loss: 6.654129259926933
Training loss: 2.0673420429229736 / Valid loss: 6.577630733308338

Epoch: 29
Training loss: 2.7276411056518555 / Valid loss: 6.843769595736549
Training loss: 2.127434730529785 / Valid loss: 6.647874754951114
Training loss: 2.453810214996338 / Valid loss: 6.6172443276359925
Training loss: 1.671478271484375 / Valid loss: 6.693059303646996

Epoch: 30
Training loss: 2.0080127716064453 / Valid loss: 7.11651654016404
Training loss: 1.561330795288086 / Valid loss: 6.698612953367688
Training loss: 1.6121060848236084 / Valid loss: 6.559704471769787
Training loss: 1.7051646709442139 / Valid loss: 6.64299593880063
Training loss: 1.6784619092941284 / Valid loss: 7.1095340501694455

Epoch: 31
Training loss: 2.3522281646728516 / Valid loss: 6.911559325172788
Training loss: 1.4498409032821655 / Valid loss: 7.057667073749361
Training loss: 1.6331439018249512 / Valid loss: 6.628508349827357
Training loss: 1.6992955207824707 / Valid loss: 6.795309952327183
Training loss: 1.5071725845336914 / Valid loss: 6.610900533766974

Epoch: 32
Training loss: 1.2511436939239502 / Valid loss: 6.7114046142214825
Training loss: 0.8893759250640869 / Valid loss: 6.673373392650059
Training loss: 1.4834980964660645 / Valid loss: 7.110977726890927
Training loss: 1.3652238845825195 / Valid loss: 7.030661805470785
Training loss: 1.8031435012817383 / Valid loss: 6.7797932579403835

Epoch: 33
Training loss: 1.2866421937942505 / Valid loss: 6.713643391927083
Training loss: 1.3970049619674683 / Valid loss: 6.7061740670885355
Training loss: 1.451191782951355 / Valid loss: 6.689867546444847
Training loss: 1.8697974681854248 / Valid loss: 6.5489379451388405
Training loss: 1.399939775466919 / Valid loss: 6.675028769175212

Epoch: 34
Training loss: 1.729358434677124 / Valid loss: 6.6116670290629065
Training loss: 1.8761258125305176 / Valid loss: 6.696357499985468
Training loss: 1.2593483924865723 / Valid loss: 6.755461143312
Training loss: 1.8508589267730713 / Valid loss: 6.67362307593936
Training loss: 1.2893332242965698 / Valid loss: 6.892020570664179

Epoch: 35
Training loss: 1.9507832527160645 / Valid loss: 6.883137471335274
Training loss: 1.733031153678894 / Valid loss: 6.826664790653047
Training loss: 1.8207049369812012 / Valid loss: 6.690479891640799
Training loss: 1.3027081489562988 / Valid loss: 6.753717490604946
Training loss: 1.4026691913604736 / Valid loss: 6.969638152349563

Epoch: 36
Training loss: 1.4988181591033936 / Valid loss: 6.801313227698916
Training loss: 1.5757747888565063 / Valid loss: 6.851889274233863
Training loss: 1.561713695526123 / Valid loss: 6.994311546144031
Training loss: 1.9421688318252563 / Valid loss: 6.75202943938119
Training loss: 1.7795913219451904 / Valid loss: 6.827785414741153

Epoch: 37
Training loss: 1.2505899667739868 / Valid loss: 6.637202489943731
Training loss: 1.3493841886520386 / Valid loss: 6.937481321607318
Training loss: 1.3231477737426758 / Valid loss: 6.910037145160493
Training loss: 2.5942821502685547 / Valid loss: 6.7532611756097705
Training loss: 1.8419468402862549 / Valid loss: 6.935796387990316

Epoch: 38
Training loss: 1.6510015726089478 / Valid loss: 6.816902519407726
Training loss: 1.3953044414520264 / Valid loss: 6.675641187032064
Training loss: 1.0813665390014648 / Valid loss: 6.907058129991804
Training loss: 1.5544873476028442 / Valid loss: 6.699519486654372
Training loss: 1.7450093030929565 / Valid loss: 7.016932487487793

Epoch: 39
Training loss: 1.4885809421539307 / Valid loss: 6.669003811336699
Training loss: 1.214550256729126 / Valid loss: 6.807244332631429
Training loss: 1.7696404457092285 / Valid loss: 6.746223190852574
Training loss: 1.050588607788086 / Valid loss: 6.554284985860189
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1700): 5.485486314410255
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.837162017822266 / Valid loss: 14.915936270214262
Model is saved in epoch 0, overall batch: 0
Training loss: 6.12880277633667 / Valid loss: 8.900763366335914
Model is saved in epoch 0, overall batch: 100
Training loss: 4.323542594909668 / Valid loss: 6.402808729807536
Model is saved in epoch 0, overall batch: 200
Training loss: 7.096892833709717 / Valid loss: 5.7258542424156555
Model is saved in epoch 0, overall batch: 300
Training loss: 5.484593391418457 / Valid loss: 5.68491484778268
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.500814437866211 / Valid loss: 5.644232831682477
Model is saved in epoch 1, overall batch: 500
Training loss: 4.799864292144775 / Valid loss: 5.719497399103074
Training loss: 6.020102500915527 / Valid loss: 5.683052428563436
Training loss: 4.827229022979736 / Valid loss: 5.638913354419526
Model is saved in epoch 1, overall batch: 800
Training loss: 4.754312515258789 / Valid loss: 5.605361220950172
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.4673919677734375 / Valid loss: 5.596947209040324
Model is saved in epoch 2, overall batch: 1000
Training loss: 6.371668815612793 / Valid loss: 5.622010866800944
Training loss: 5.4122209548950195 / Valid loss: 5.565366404397147
Model is saved in epoch 2, overall batch: 1200
Training loss: 6.633395195007324 / Valid loss: 5.641259615761893
Training loss: 4.752178192138672 / Valid loss: 5.661753824778965

Epoch: 3
Training loss: 4.524855613708496 / Valid loss: 5.771837811242967
Training loss: 4.466701984405518 / Valid loss: 5.771574167978196
Training loss: 5.610308647155762 / Valid loss: 5.693822735831851
Training loss: 3.3497064113616943 / Valid loss: 5.8220074789864675
Training loss: 5.0577569007873535 / Valid loss: 5.803938765752883

Epoch: 4
Training loss: 5.287466526031494 / Valid loss: 6.000545213336037
Training loss: 5.0073161125183105 / Valid loss: 5.640106809706915
Training loss: 4.718023300170898 / Valid loss: 5.732356409799485
Training loss: 5.703123092651367 / Valid loss: 5.68685344060262
Training loss: 4.314734935760498 / Valid loss: 5.673736856097267

Epoch: 5
Training loss: 7.5135498046875 / Valid loss: 5.693895592008318
Training loss: 5.789935111999512 / Valid loss: 5.856143906002953
Training loss: 4.177235126495361 / Valid loss: 5.749339212690081
Training loss: 5.0239410400390625 / Valid loss: 5.735393238067627
Training loss: 4.25374698638916 / Valid loss: 5.651074150630406

Epoch: 6
Training loss: 7.509098529815674 / Valid loss: 5.951577511287871
Training loss: 4.658641338348389 / Valid loss: 5.789364512761434
Training loss: 4.707670211791992 / Valid loss: 5.820272298086257
Training loss: 4.209489345550537 / Valid loss: 5.9609222389402845
Training loss: 4.779896259307861 / Valid loss: 5.874375052679152

Epoch: 7
Training loss: 5.479825973510742 / Valid loss: 5.832112414496286
Training loss: 4.412835121154785 / Valid loss: 5.913601014727638
Training loss: 5.15217924118042 / Valid loss: 5.807562435240973
Training loss: 6.546024322509766 / Valid loss: 6.091290494373866
Training loss: 3.975801944732666 / Valid loss: 5.9543367136092415

Epoch: 8
Training loss: 3.879277229309082 / Valid loss: 5.849135242189679
Training loss: 3.228504180908203 / Valid loss: 5.8951952888852075
Training loss: 5.032069206237793 / Valid loss: 6.026354367392404
Training loss: 7.191525459289551 / Valid loss: 5.911967845190139
Training loss: 4.494899749755859 / Valid loss: 6.035407586324783

Epoch: 9
Training loss: 3.0031275749206543 / Valid loss: 6.156684160232544
Training loss: 5.949456214904785 / Valid loss: 6.033263272330874
Training loss: 5.969379425048828 / Valid loss: 6.0588438442775185
Training loss: 4.667335510253906 / Valid loss: 6.5985494886125835

Epoch: 10
Training loss: 4.200283050537109 / Valid loss: 5.970475650969005
Training loss: 4.628461837768555 / Valid loss: 6.030444939931233
Training loss: 3.2585201263427734 / Valid loss: 6.265200873783656
Training loss: 4.706254959106445 / Valid loss: 6.062587551843553
Training loss: 4.4075727462768555 / Valid loss: 6.045963058017549

Epoch: 11
Training loss: 3.3456153869628906 / Valid loss: 6.181611524309431
Training loss: 3.5586533546447754 / Valid loss: 6.090458565666562
Training loss: 4.903547286987305 / Valid loss: 6.191925257728213
Training loss: 2.4388833045959473 / Valid loss: 6.095465853100731
Training loss: 3.9706268310546875 / Valid loss: 6.215403990518479

Epoch: 12
Training loss: 3.199537754058838 / Valid loss: 6.123444779713949
Training loss: 3.675410509109497 / Valid loss: 6.04188753309704
Training loss: 4.752285003662109 / Valid loss: 6.381112566448393
Training loss: 4.182088851928711 / Valid loss: 6.289709806442261
Training loss: 5.126612663269043 / Valid loss: 6.423132651192802

Epoch: 13
Training loss: 2.3974640369415283 / Valid loss: 6.5476192883082796
Training loss: 3.3979318141937256 / Valid loss: 6.3972301755632675
Training loss: 3.8797688484191895 / Valid loss: 6.109548793520246
Training loss: 3.4825565814971924 / Valid loss: 6.297402867816744
Training loss: 3.892670154571533 / Valid loss: 6.265243498484294

Epoch: 14
Training loss: 4.258686065673828 / Valid loss: 6.248270970299131
Training loss: 3.506922960281372 / Valid loss: 6.424976916540237
Training loss: 3.579984664916992 / Valid loss: 6.268884602047148
Training loss: 2.8963637351989746 / Valid loss: 6.089796488625662
Training loss: 5.411527633666992 / Valid loss: 6.343798678261893

Epoch: 15
Training loss: 3.446889877319336 / Valid loss: 6.370626399630592
Training loss: 3.7140092849731445 / Valid loss: 6.31248779296875
Training loss: 3.645853042602539 / Valid loss: 6.38014649890718
Training loss: 4.530153274536133 / Valid loss: 6.397414516267323
Training loss: 3.24245548248291 / Valid loss: 6.187521798270089

Epoch: 16
Training loss: 2.5922584533691406 / Valid loss: 6.2293953010014125
Training loss: 2.982247829437256 / Valid loss: 6.211810974847703
Training loss: 2.5441575050354004 / Valid loss: 6.337695961906796
Training loss: 3.367429256439209 / Valid loss: 6.295891611916678
Training loss: 5.247204780578613 / Valid loss: 6.349566593624297

Epoch: 17
Training loss: 3.461641550064087 / Valid loss: 6.615265871229626
Training loss: 3.231224536895752 / Valid loss: 6.289047159467425
Training loss: 3.7544331550598145 / Valid loss: 6.376487940833682
Training loss: 3.636866569519043 / Valid loss: 6.400738595780872
Training loss: 3.4415621757507324 / Valid loss: 6.345764455341158

Epoch: 18
Training loss: 1.8702073097229004 / Valid loss: 6.573735850197928
Training loss: 2.9930543899536133 / Valid loss: 6.636401226407005
Training loss: 2.7388834953308105 / Valid loss: 6.31845706758045
Training loss: 3.387256145477295 / Valid loss: 6.447086370558965
Training loss: 3.462031364440918 / Valid loss: 6.920049294971284

Epoch: 19
Training loss: 3.470409631729126 / Valid loss: 6.450645485378447
Training loss: 3.294431209564209 / Valid loss: 6.67798859278361
Training loss: 3.789059638977051 / Valid loss: 6.615155551547096
Training loss: 2.644801139831543 / Valid loss: 6.490891738164993

Epoch: 20
Training loss: 1.9290547370910645 / Valid loss: 6.556801162447248
Training loss: 2.302342414855957 / Valid loss: 6.948937715802874
Training loss: 2.266610622406006 / Valid loss: 6.482009115673247
Training loss: 3.7872307300567627 / Valid loss: 6.503599275861467
Training loss: 3.591747283935547 / Valid loss: 6.508960285640899

Epoch: 21
Training loss: 2.606153964996338 / Valid loss: 6.53984587306068
Training loss: 2.1273131370544434 / Valid loss: 6.486808545248849
Training loss: 1.8278112411499023 / Valid loss: 6.525311785652524
Training loss: 3.187305212020874 / Valid loss: 6.658507637750535
Training loss: 2.321197509765625 / Valid loss: 6.387731377283732

Epoch: 22
Training loss: 2.7624619007110596 / Valid loss: 6.46795904976981
Training loss: 2.2693018913269043 / Valid loss: 6.657334799993606
Training loss: 1.994324803352356 / Valid loss: 6.642488200323922
Training loss: 3.6808464527130127 / Valid loss: 6.674407609303793
Training loss: 2.1488826274871826 / Valid loss: 7.074438985188802

Epoch: 23
Training loss: 2.1255831718444824 / Valid loss: 6.426779544921148
Training loss: 3.1388704776763916 / Valid loss: 6.5098453090304425
Training loss: 2.490809679031372 / Valid loss: 6.726235371544248
Training loss: 2.68780517578125 / Valid loss: 7.153549375988188
Training loss: 3.0423965454101562 / Valid loss: 6.642450770877656

Epoch: 24
Training loss: 3.6806275844573975 / Valid loss: 6.521728583744594
Training loss: 1.8800277709960938 / Valid loss: 6.603795110611689
Training loss: 1.9695465564727783 / Valid loss: 6.947842616126651
Training loss: 4.2604241371154785 / Valid loss: 6.82161739894322
Training loss: 3.7602572441101074 / Valid loss: 6.642365110488165

Epoch: 25
Training loss: 1.8582741022109985 / Valid loss: 6.486184027081444
Training loss: 2.1267518997192383 / Valid loss: 6.50988891238258
Training loss: 3.0108418464660645 / Valid loss: 6.724067649387178
Training loss: 1.8824973106384277 / Valid loss: 6.547827999932426
Training loss: 3.629687786102295 / Valid loss: 6.600648991266886

Epoch: 26
Training loss: 2.2193617820739746 / Valid loss: 6.604435677755447
Training loss: 2.8785219192504883 / Valid loss: 6.5727783203125
Training loss: 2.281158208847046 / Valid loss: 6.567785315286546
Training loss: 1.9649802446365356 / Valid loss: 6.746059308733259
Training loss: 2.224547863006592 / Valid loss: 6.857538936251686

Epoch: 27
Training loss: 1.8287663459777832 / Valid loss: 7.011423946562267
Training loss: 1.1954638957977295 / Valid loss: 7.377293432326544
Training loss: 2.0665483474731445 / Valid loss: 6.845571928932554
Training loss: 1.9092025756835938 / Valid loss: 6.484798206601824
Training loss: 2.544341564178467 / Valid loss: 6.672819939113799

Epoch: 28
Training loss: 1.6333237886428833 / Valid loss: 6.678816572825114
Training loss: 2.0031042098999023 / Valid loss: 6.609366482780093
Training loss: 2.0415210723876953 / Valid loss: 6.569502626146589
Training loss: 2.0526862144470215 / Valid loss: 6.847147328513009
Training loss: 1.9470784664154053 / Valid loss: 6.651528013320196

Epoch: 29
Training loss: 2.8961644172668457 / Valid loss: 6.82469752629598
Training loss: 2.277413845062256 / Valid loss: 6.812755425771077
Training loss: 2.0856733322143555 / Valid loss: 6.746902256920224
Training loss: 2.2661213874816895 / Valid loss: 6.6444622357686365

Epoch: 30
Training loss: 2.0045461654663086 / Valid loss: 6.974981276194255
Training loss: 1.8900396823883057 / Valid loss: 6.856097185044062
Training loss: 1.9783506393432617 / Valid loss: 6.795916178112939
Training loss: 1.9315468072891235 / Valid loss: 6.763410241263253
Training loss: 1.8503329753875732 / Valid loss: 6.9763692946661084

Epoch: 31
Training loss: 1.7289401292800903 / Valid loss: 6.798738990511213
Training loss: 1.126389980316162 / Valid loss: 6.941630499703543
Training loss: 1.8905670642852783 / Valid loss: 6.64700810341608
Training loss: 1.4692373275756836 / Valid loss: 6.900840850103469
Training loss: 2.243116855621338 / Valid loss: 6.686894925435384

Epoch: 32
Training loss: 1.6129814386367798 / Valid loss: 6.755930587223598
Training loss: 1.372246503829956 / Valid loss: 6.796795465832665
Training loss: 1.73654043674469 / Valid loss: 7.225426464989072
Training loss: 1.8575539588928223 / Valid loss: 6.755146932601929
Training loss: 2.2185287475585938 / Valid loss: 6.795727012270973

Epoch: 33
Training loss: 1.6458555459976196 / Valid loss: 6.823284539722261
Training loss: 1.6938562393188477 / Valid loss: 6.569893900553385
Training loss: 1.5155669450759888 / Valid loss: 6.970025307791573
Training loss: 2.0409913063049316 / Valid loss: 6.737284047263009
Training loss: 1.5719642639160156 / Valid loss: 6.588404598690214

Epoch: 34
Training loss: 2.011026382446289 / Valid loss: 6.5215638160705565
Training loss: 1.5447661876678467 / Valid loss: 6.605663467588879
Training loss: 1.4837052822113037 / Valid loss: 6.810631670270648
Training loss: 1.4572601318359375 / Valid loss: 6.520424534025646
Training loss: 2.4802474975585938 / Valid loss: 6.709116636003767

Epoch: 35
Training loss: 1.7185500860214233 / Valid loss: 6.896216496967134
Training loss: 2.0737054347991943 / Valid loss: 6.691364165714809
Training loss: 2.153855562210083 / Valid loss: 6.818346806934902
Training loss: 1.7239768505096436 / Valid loss: 6.579823911757696
Training loss: 1.7640429735183716 / Valid loss: 6.6795789241790775

Epoch: 36
Training loss: 2.442856788635254 / Valid loss: 6.5096642062777565
Training loss: 2.3220176696777344 / Valid loss: 6.652154847553798
Training loss: 1.6376338005065918 / Valid loss: 6.669115729559035
Training loss: 2.2474472522735596 / Valid loss: 6.736176293236869
Training loss: 2.0168442726135254 / Valid loss: 6.707385508219401

Epoch: 37
Training loss: 1.2530968189239502 / Valid loss: 6.631649069559007
Training loss: 1.2431079149246216 / Valid loss: 6.710849098932176
Training loss: 1.9138009548187256 / Valid loss: 6.6571816626049225
Training loss: 3.1394598484039307 / Valid loss: 6.631879692985898
Training loss: 1.6344846487045288 / Valid loss: 6.892250724065871

Epoch: 38
Training loss: 1.4844468832015991 / Valid loss: 6.715578960237049
Training loss: 1.5901527404785156 / Valid loss: 6.56353333791097
Training loss: 1.222459316253662 / Valid loss: 6.8364028658185685
Training loss: 1.328528642654419 / Valid loss: 6.541275365012033
Training loss: 1.6287486553192139 / Valid loss: 6.631556638081869

Epoch: 39
Training loss: 1.065624713897705 / Valid loss: 6.603000218527658
Training loss: 1.4644267559051514 / Valid loss: 6.7342379206702825
Training loss: 1.2081091403961182 / Valid loss: 6.696993836902437
Training loss: 1.1483948230743408 / Valid loss: 6.764512293679374
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1200): 5.45414776120867
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.974810600280762 / Valid loss: 15.079355621337891
Model is saved in epoch 0, overall batch: 0
Training loss: 3.929624319076538 / Valid loss: 10.01613549277896
Model is saved in epoch 0, overall batch: 100
Training loss: 4.945060729980469 / Valid loss: 7.867315601167225
Model is saved in epoch 0, overall batch: 200
Training loss: 5.312336444854736 / Valid loss: 7.729926050276983
Model is saved in epoch 0, overall batch: 300
Training loss: 4.5894670486450195 / Valid loss: 7.2454054718925835
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.649003982543945 / Valid loss: 7.222748595192319
Model is saved in epoch 1, overall batch: 500
Training loss: 5.153219699859619 / Valid loss: 7.04908868925912
Model is saved in epoch 1, overall batch: 600
Training loss: 4.219559669494629 / Valid loss: 7.065793655032203
Training loss: 4.304048538208008 / Valid loss: 6.646097800845191
Model is saved in epoch 1, overall batch: 800
Training loss: 6.165659427642822 / Valid loss: 6.327544900349208
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.018426418304443 / Valid loss: 6.382928446360997
Training loss: 4.904507637023926 / Valid loss: 6.337966878073556
Training loss: 5.0709028244018555 / Valid loss: 6.457627130690075
Training loss: 6.324923992156982 / Valid loss: 6.0761410486130485
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.16085147857666 / Valid loss: 5.842829238800776
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.676373481750488 / Valid loss: 6.108869350524175
Training loss: 4.239077568054199 / Valid loss: 6.309275193441482
Training loss: 6.253137588500977 / Valid loss: 5.856848516918364
Training loss: 5.616181373596191 / Valid loss: 5.969755220413208
Training loss: 5.320956707000732 / Valid loss: 6.09609758967445

Epoch: 4
Training loss: 3.6187496185302734 / Valid loss: 6.002272773924328
Training loss: 4.789478778839111 / Valid loss: 5.995575191861107
Training loss: 4.375545024871826 / Valid loss: 5.982308167502993
Training loss: 5.153312683105469 / Valid loss: 5.741563665299188
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.937215328216553 / Valid loss: 5.834632106054396

Epoch: 5
Training loss: 5.585419654846191 / Valid loss: 5.835564072926839
Training loss: 5.299285411834717 / Valid loss: 5.872577533267793
Training loss: 5.6225738525390625 / Valid loss: 5.586501886731102
Model is saved in epoch 5, overall batch: 2700
Training loss: 4.143985271453857 / Valid loss: 5.6697545324053085
Training loss: 5.782642364501953 / Valid loss: 5.668521279380435

Epoch: 6
Training loss: 5.314078330993652 / Valid loss: 5.747020603361584
Training loss: 3.083893299102783 / Valid loss: 5.8057350771767755
Training loss: 4.9767913818359375 / Valid loss: 5.741637786229451
Training loss: 4.863372802734375 / Valid loss: 5.789477779751732
Training loss: 5.2445597648620605 / Valid loss: 5.6988407975151425

Epoch: 7
Training loss: 4.700043678283691 / Valid loss: 5.833611906142462
Training loss: 2.948272228240967 / Valid loss: 5.716319095520746
Training loss: 3.813563585281372 / Valid loss: 5.71502412387303
Training loss: 5.874004364013672 / Valid loss: 5.798386151450021
Training loss: 3.7324180603027344 / Valid loss: 5.795398258027577

Epoch: 8
Training loss: 4.703058242797852 / Valid loss: 5.814695165270851
Training loss: 4.289566993713379 / Valid loss: 5.889622181937808
Training loss: 3.652061700820923 / Valid loss: 5.862942166555495
Training loss: 4.614336013793945 / Valid loss: 5.782935841878255
Training loss: 4.795854568481445 / Valid loss: 5.985102605819702

Epoch: 9
Training loss: 3.3813562393188477 / Valid loss: 5.821895899091448
Training loss: 5.047065258026123 / Valid loss: 5.855772608802432
Training loss: 5.416786193847656 / Valid loss: 5.794823206038702
Training loss: 3.9337074756622314 / Valid loss: 5.851015202204386

Epoch: 10
Training loss: 3.5116891860961914 / Valid loss: 5.933055366788592
Training loss: 3.238513469696045 / Valid loss: 5.897043741317022
Training loss: 4.4487409591674805 / Valid loss: 5.9764286949521015
Training loss: 3.900197744369507 / Valid loss: 5.869519951230004
Training loss: 5.045478820800781 / Valid loss: 5.948013080869402

Epoch: 11
Training loss: 4.285391807556152 / Valid loss: 5.826696391332717
Training loss: 3.383094549179077 / Valid loss: 6.00340830485026
Training loss: 4.498751640319824 / Valid loss: 5.9871574379148935
Training loss: 3.5685606002807617 / Valid loss: 5.95889573097229
Training loss: 4.0011749267578125 / Valid loss: 6.140608024597168

Epoch: 12
Training loss: 3.3395285606384277 / Valid loss: 5.9448877016703285
Training loss: 4.83809757232666 / Valid loss: 6.067343257722401
Training loss: 4.328939437866211 / Valid loss: 5.972809242066883
Training loss: 3.964447259902954 / Valid loss: 6.026036861964634
Training loss: 5.38778018951416 / Valid loss: 5.883649546759469

Epoch: 13
Training loss: 3.3951125144958496 / Valid loss: 6.024686629431589
Training loss: 4.032288074493408 / Valid loss: 5.982586022785732
Training loss: 2.5243687629699707 / Valid loss: 5.959162621271043
Training loss: 4.233318328857422 / Valid loss: 6.023497197741554
Training loss: 4.300228118896484 / Valid loss: 5.937843113853818

Epoch: 14
Training loss: 3.5228474140167236 / Valid loss: 6.017509449095953
Training loss: 3.3093559741973877 / Valid loss: 6.099296501704624
Training loss: 2.675687551498413 / Valid loss: 6.114383404595511
Training loss: 3.033686637878418 / Valid loss: 6.116743435178484
Training loss: 3.012389898300171 / Valid loss: 6.054568644932338

Epoch: 15
Training loss: 4.02061653137207 / Valid loss: 6.055615752083915
Training loss: 3.650696039199829 / Valid loss: 6.097236056554885
Training loss: 3.867623805999756 / Valid loss: 6.1731133506411595
Training loss: 3.6228604316711426 / Valid loss: 6.243953652608962
Training loss: 3.436934471130371 / Valid loss: 6.050313216163999

Epoch: 16
Training loss: 5.371993541717529 / Valid loss: 6.132154959724063
Training loss: 2.500264883041382 / Valid loss: 6.063794621967134
Training loss: 3.518578052520752 / Valid loss: 6.170742039453415
Training loss: 2.852395534515381 / Valid loss: 6.040096718924386
Training loss: 3.620931625366211 / Valid loss: 6.1278731096358525

Epoch: 17
Training loss: 2.217376232147217 / Valid loss: 6.157334986187163
Training loss: 3.0878190994262695 / Valid loss: 6.096075353168306
Training loss: 2.9533770084381104 / Valid loss: 6.3006620066506525
Training loss: 3.8293986320495605 / Valid loss: 6.20895285379319
Training loss: 3.580500602722168 / Valid loss: 6.117360937027704

Epoch: 18
Training loss: 2.808645248413086 / Valid loss: 6.1786402838570735
Training loss: 3.479077100753784 / Valid loss: 6.159821592058454
Training loss: 2.5845675468444824 / Valid loss: 6.188922284898304
Training loss: 3.334540367126465 / Valid loss: 6.18832042103722
Training loss: 4.8243913650512695 / Valid loss: 6.164945788610549

Epoch: 19
Training loss: 2.9163265228271484 / Valid loss: 6.224594656626383
Training loss: 2.961732864379883 / Valid loss: 6.223918038322812
Training loss: 3.014122486114502 / Valid loss: 6.170848955426897
Training loss: 2.92029070854187 / Valid loss: 6.180737023126511

Epoch: 20
Training loss: 2.194601535797119 / Valid loss: 6.166522482463292
Training loss: 2.871433734893799 / Valid loss: 6.407190922328404
Training loss: 3.073361873626709 / Valid loss: 6.170568423044114
Training loss: 3.5435807704925537 / Valid loss: 6.202758439381918
Training loss: 2.9011693000793457 / Valid loss: 6.207464034216745

Epoch: 21
Training loss: 3.5456979274749756 / Valid loss: 6.240266977037702
Training loss: 2.935246467590332 / Valid loss: 6.330668780917213
Training loss: 2.700850486755371 / Valid loss: 6.310687791733515
Training loss: 2.9033079147338867 / Valid loss: 6.307203658421835
Training loss: 2.1850178241729736 / Valid loss: 6.209167310169765

Epoch: 22
Training loss: 2.2891759872436523 / Valid loss: 6.330769522984823
Training loss: 1.899505376815796 / Valid loss: 6.290218385060628
Training loss: 3.486238956451416 / Valid loss: 6.3639313448043096
Training loss: 1.7069673538208008 / Valid loss: 6.332751753216698
Training loss: 2.9646401405334473 / Valid loss: 6.230524642126901

Epoch: 23
Training loss: 2.3562874794006348 / Valid loss: 6.17632512592134
Training loss: 2.380005359649658 / Valid loss: 6.35112718400501
Training loss: 3.509582042694092 / Valid loss: 6.397695382436116
Training loss: 3.5982918739318848 / Valid loss: 6.246183935801188
Training loss: 2.702132225036621 / Valid loss: 6.345465507961455

Epoch: 24
Training loss: 2.2627174854278564 / Valid loss: 6.342319166092645
Training loss: 3.78164005279541 / Valid loss: 6.390759297779629
Training loss: 1.5303059816360474 / Valid loss: 6.3811764194851825
Training loss: 2.327315092086792 / Valid loss: 6.308535478228614
Training loss: 2.8499107360839844 / Valid loss: 6.306713340395973

Epoch: 25
Training loss: 2.2803969383239746 / Valid loss: 6.359267214366368
Training loss: 2.5193426609039307 / Valid loss: 6.306432792118618
Training loss: 2.884253978729248 / Valid loss: 6.2603784197852725
Training loss: 2.5361461639404297 / Valid loss: 6.2261559032258535
Training loss: 2.386993408203125 / Valid loss: 6.297314058031355

Epoch: 26
Training loss: 2.5530710220336914 / Valid loss: 6.237109241031465
Training loss: 2.198209285736084 / Valid loss: 6.488093489692325
Training loss: 3.060041904449463 / Valid loss: 6.404162904194423
Training loss: 2.844738721847534 / Valid loss: 6.249684179396857
Training loss: 2.9595847129821777 / Valid loss: 6.210786020188104

Epoch: 27
Training loss: 2.708413600921631 / Valid loss: 6.274526062465849
Training loss: 2.570535898208618 / Valid loss: 6.345791773569016
Training loss: 3.3441100120544434 / Valid loss: 6.509140446072533
Training loss: 1.647505521774292 / Valid loss: 6.347559374854678
Training loss: 1.5587347745895386 / Valid loss: 6.2391820180983775

Epoch: 28
Training loss: 2.053422451019287 / Valid loss: 6.430519090379987
Training loss: 2.31203556060791 / Valid loss: 6.256513314020066
Training loss: 1.9401862621307373 / Valid loss: 6.336348522277105
Training loss: 1.5073602199554443 / Valid loss: 6.368467022123791
Training loss: 3.6718389987945557 / Valid loss: 6.385483948389689

Epoch: 29
Training loss: 1.8821072578430176 / Valid loss: 6.337580676305862
Training loss: 2.2935197353363037 / Valid loss: 6.405343355451311
Training loss: 1.5689257383346558 / Valid loss: 6.4443550927298405
Training loss: 2.8351268768310547 / Valid loss: 6.317043763115293

Epoch: 30
Training loss: 1.6224392652511597 / Valid loss: 6.397528866359166
Training loss: 2.5078248977661133 / Valid loss: 6.424987268447876
Training loss: 2.1793320178985596 / Valid loss: 6.4568513688587
Training loss: 2.093979597091675 / Valid loss: 6.604992264793033
Training loss: 2.6514930725097656 / Valid loss: 6.41667134875343

Epoch: 31
Training loss: 2.1950995922088623 / Valid loss: 6.444322783606393
Training loss: 3.904705762863159 / Valid loss: 6.566930559703282
Training loss: 2.288510799407959 / Valid loss: 6.409061910992577
Training loss: 1.7057218551635742 / Valid loss: 6.550293436504545
Training loss: 2.621029853820801 / Valid loss: 6.464821243286133

Epoch: 32
Training loss: 1.1135908365249634 / Valid loss: 6.362177462804885
Training loss: 1.5538415908813477 / Valid loss: 6.382092123939877
Training loss: 2.6540260314941406 / Valid loss: 6.3002514725639704
Training loss: 1.9155826568603516 / Valid loss: 6.2932327997116815
Training loss: 2.7869153022766113 / Valid loss: 6.28640554064796

Epoch: 33
Training loss: 1.6922328472137451 / Valid loss: 6.620463418960571
Training loss: 2.085153579711914 / Valid loss: 6.514740946179344
Training loss: 2.9447154998779297 / Valid loss: 6.4422690936497276
Training loss: 2.051938056945801 / Valid loss: 6.512041471118018
Training loss: 1.6837365627288818 / Valid loss: 6.440704749879383

Epoch: 34
Training loss: 2.691530227661133 / Valid loss: 6.379360984620594
Training loss: 2.5459742546081543 / Valid loss: 6.276802435375395
Training loss: 2.1764864921569824 / Valid loss: 6.462865736370995
Training loss: 2.1182925701141357 / Valid loss: 6.470126890000843
Training loss: 2.552856922149658 / Valid loss: 6.6840084938775925

Epoch: 35
Training loss: 1.7595739364624023 / Valid loss: 6.427336356753394
Training loss: 1.8610422611236572 / Valid loss: 6.659754877998715
Training loss: 2.3064184188842773 / Valid loss: 6.474856335776193
Training loss: 1.0283281803131104 / Valid loss: 6.521672326042538
Training loss: 2.0536422729492188 / Valid loss: 6.2972779455639065

Epoch: 36
Training loss: 1.6202760934829712 / Valid loss: 6.397838676543463
Training loss: 2.2938005924224854 / Valid loss: 6.381188265482584
Training loss: 1.6501612663269043 / Valid loss: 6.323330736160278
Training loss: 2.5100460052490234 / Valid loss: 6.4938723155430385
Training loss: 1.887010931968689 / Valid loss: 6.4209398133414135

Epoch: 37
Training loss: 1.4574249982833862 / Valid loss: 6.296026188986642
Training loss: 2.1464171409606934 / Valid loss: 6.489680006390526
Training loss: 1.2721610069274902 / Valid loss: 6.508605929783412
Training loss: 1.457706093788147 / Valid loss: 6.546540887015206
Training loss: 2.332759380340576 / Valid loss: 6.440020767847697

Epoch: 38
Training loss: 1.4070963859558105 / Valid loss: 6.46595246678307
Training loss: 1.6903269290924072 / Valid loss: 6.367259625026158
Training loss: 1.7485744953155518 / Valid loss: 6.3066899140675865
Training loss: 1.6495732069015503 / Valid loss: 6.401428738094512
Training loss: 3.092468738555908 / Valid loss: 6.321637825738816

Epoch: 39
Training loss: 1.9705837965011597 / Valid loss: 6.508266880398705
Training loss: 1.3893158435821533 / Valid loss: 6.5012483051845
Training loss: 1.6842048168182373 / Valid loss: 6.4130392869313555
Training loss: 2.3324475288391113 / Valid loss: 6.323225455057053
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2700): 5.424064309256417
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.974810600280762 / Valid loss: 15.079357937404088
Model is saved in epoch 0, overall batch: 0
Training loss: 3.8970508575439453 / Valid loss: 10.083162203289213
Model is saved in epoch 0, overall batch: 100
Training loss: 4.858211517333984 / Valid loss: 7.683267752329509
Model is saved in epoch 0, overall batch: 200
Training loss: 5.32241153717041 / Valid loss: 7.508594858078729
Model is saved in epoch 0, overall batch: 300
Training loss: 4.601586818695068 / Valid loss: 7.50973326365153

Epoch: 1
Training loss: 6.632889747619629 / Valid loss: 7.187362096423194
Model is saved in epoch 1, overall batch: 500
Training loss: 5.23984432220459 / Valid loss: 6.71925650097075
Model is saved in epoch 1, overall batch: 600
Training loss: 4.595717906951904 / Valid loss: 6.9976366406395325
Training loss: 4.359527111053467 / Valid loss: 6.738491024289813
Training loss: 6.216974258422852 / Valid loss: 6.367358505158197
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.20372200012207 / Valid loss: 6.503208337511335
Training loss: 4.973018646240234 / Valid loss: 6.306078906286331
Model is saved in epoch 2, overall batch: 1100
Training loss: 4.715391159057617 / Valid loss: 6.413575694674537
Training loss: 6.039654731750488 / Valid loss: 6.369389447711763
Training loss: 5.32902717590332 / Valid loss: 6.1124035926092235
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.291433334350586 / Valid loss: 6.257970194589524
Training loss: 4.173375129699707 / Valid loss: 6.267969749087379
Training loss: 6.301024913787842 / Valid loss: 5.951248409634545
Model is saved in epoch 3, overall batch: 1700
Training loss: 5.37359619140625 / Valid loss: 6.017318870907738
Training loss: 5.169442176818848 / Valid loss: 6.331444222586495

Epoch: 4
Training loss: 3.559009552001953 / Valid loss: 6.107466216314407
Training loss: 4.709726810455322 / Valid loss: 6.105103631246657
Training loss: 4.229971885681152 / Valid loss: 5.864901535851615
Model is saved in epoch 4, overall batch: 2200
Training loss: 5.359353542327881 / Valid loss: 5.773379023869833
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.914441108703613 / Valid loss: 5.786329709915888

Epoch: 5
Training loss: 5.956263542175293 / Valid loss: 5.720968400864374
Model is saved in epoch 5, overall batch: 2500
Training loss: 5.315683364868164 / Valid loss: 5.807951257342384
Training loss: 5.712196350097656 / Valid loss: 5.642246305374872
Model is saved in epoch 5, overall batch: 2700
Training loss: 4.085654258728027 / Valid loss: 5.780910896119617
Training loss: 6.271292209625244 / Valid loss: 5.712624252410162

Epoch: 6
Training loss: 5.130840301513672 / Valid loss: 5.687476153600784
Training loss: 3.091933250427246 / Valid loss: 5.9231539090474445
Training loss: 5.552817344665527 / Valid loss: 5.695908419291178
Training loss: 5.069817543029785 / Valid loss: 5.699677412850516
Training loss: 5.538537979125977 / Valid loss: 5.6819307963053385

Epoch: 7
Training loss: 4.462643146514893 / Valid loss: 5.7788836365654355
Training loss: 3.146519422531128 / Valid loss: 5.75828290212722
Training loss: 4.06857967376709 / Valid loss: 5.664429058347429
Training loss: 5.746145725250244 / Valid loss: 5.799544795354207
Training loss: 3.9478886127471924 / Valid loss: 5.7997472354343955

Epoch: 8
Training loss: 4.269272804260254 / Valid loss: 5.789078880491711
Training loss: 5.106515407562256 / Valid loss: 5.863066091991606
Training loss: 3.248530864715576 / Valid loss: 5.74711062113444
Training loss: 4.462248802185059 / Valid loss: 5.887119270506359
Training loss: 5.010972499847412 / Valid loss: 5.9571052846454435

Epoch: 9
Training loss: 4.1357502937316895 / Valid loss: 5.811591652461461
Training loss: 5.039675712585449 / Valid loss: 5.988161527542841
Training loss: 5.725947856903076 / Valid loss: 5.873299916585286
Training loss: 3.508232593536377 / Valid loss: 5.924721724646432

Epoch: 10
Training loss: 3.9701809883117676 / Valid loss: 5.903330269313994
Training loss: 3.2944369316101074 / Valid loss: 6.024832509812855
Training loss: 4.362634658813477 / Valid loss: 6.029488393238613
Training loss: 3.541721820831299 / Valid loss: 5.958083893003918
Training loss: 5.655309200286865 / Valid loss: 5.978225219817388

Epoch: 11
Training loss: 3.812756061553955 / Valid loss: 5.866888954525902
Training loss: 3.701171398162842 / Valid loss: 5.926668405532837
Training loss: 4.730501651763916 / Valid loss: 6.008674889519101
Training loss: 3.9459452629089355 / Valid loss: 5.927596480505807
Training loss: 3.98811936378479 / Valid loss: 5.9838336490449455

Epoch: 12
Training loss: 3.8312313556671143 / Valid loss: 5.919914963131859
Training loss: 4.432607650756836 / Valid loss: 6.261242019562494
Training loss: 3.506376266479492 / Valid loss: 5.954718426295689
Training loss: 4.123113632202148 / Valid loss: 5.9123107501438685
Training loss: 4.760827541351318 / Valid loss: 5.964912609826951

Epoch: 13
Training loss: 3.3310599327087402 / Valid loss: 6.085979441234044
Training loss: 4.0379791259765625 / Valid loss: 6.009891553152175
Training loss: 2.4510412216186523 / Valid loss: 5.969812558946155
Training loss: 3.856532096862793 / Valid loss: 6.001771792911348
Training loss: 4.4033708572387695 / Valid loss: 6.001361597151983

Epoch: 14
Training loss: 4.209575653076172 / Valid loss: 6.1129059292021255
Training loss: 2.747220993041992 / Valid loss: 6.069990884690058
Training loss: 2.3972554206848145 / Valid loss: 6.283354162034534
Training loss: 3.268388271331787 / Valid loss: 6.009899659383865
Training loss: 3.9856040477752686 / Valid loss: 6.041511022476923

Epoch: 15
Training loss: 3.7755157947540283 / Valid loss: 6.075896785372779
Training loss: 3.8318822383880615 / Valid loss: 6.116273475828625
Training loss: 3.6199264526367188 / Valid loss: 6.089779354277111
Training loss: 3.580625057220459 / Valid loss: 6.1025416442326135
Training loss: 2.6907036304473877 / Valid loss: 6.07503567196074

Epoch: 16
Training loss: 4.68853759765625 / Valid loss: 6.131625041507539
Training loss: 2.7428786754608154 / Valid loss: 6.164995232082549
Training loss: 3.2353055477142334 / Valid loss: 6.2639910607110885
Training loss: 3.1543428897857666 / Valid loss: 6.1350623221624465
Training loss: 3.349453926086426 / Valid loss: 6.247151061466762

Epoch: 17
Training loss: 2.730971336364746 / Valid loss: 6.262541051138015
Training loss: 3.2664551734924316 / Valid loss: 6.17430856795538
Training loss: 3.9775004386901855 / Valid loss: 6.220238883154733
Training loss: 3.740678548812866 / Valid loss: 6.197989913395473
Training loss: 3.6971216201782227 / Valid loss: 6.203253409976051

Epoch: 18
Training loss: 2.746091365814209 / Valid loss: 6.406766714368548
Training loss: 3.6165077686309814 / Valid loss: 6.245448008037749
Training loss: 2.568859100341797 / Valid loss: 6.405157343546549
Training loss: 2.9139151573181152 / Valid loss: 6.199696779251099
Training loss: 4.324885845184326 / Valid loss: 6.236435738063994

Epoch: 19
Training loss: 3.567808151245117 / Valid loss: 6.236133339291527
Training loss: 2.587642192840576 / Valid loss: 6.197660950251988
Training loss: 3.0054221153259277 / Valid loss: 6.2138457434518
Training loss: 2.8005058765411377 / Valid loss: 6.259041332063221

Epoch: 20
Training loss: 2.424088954925537 / Valid loss: 6.280584283102126
Training loss: 2.3140361309051514 / Valid loss: 6.34482877595084
Training loss: 3.1355016231536865 / Valid loss: 6.330723619461059
Training loss: 4.548201560974121 / Valid loss: 6.337889323915754
Training loss: 2.43822979927063 / Valid loss: 6.161221735818046

Epoch: 21
Training loss: 4.5820183753967285 / Valid loss: 6.315239872251238
Training loss: 2.5015413761138916 / Valid loss: 6.520346605210078
Training loss: 3.6193346977233887 / Valid loss: 6.336908692405338
Training loss: 2.4118728637695312 / Valid loss: 6.299349308013916
Training loss: 2.9153800010681152 / Valid loss: 6.233164723714193

Epoch: 22
Training loss: 2.4778242111206055 / Valid loss: 6.299068975448608
Training loss: 2.337512493133545 / Valid loss: 6.284277237029302
Training loss: 3.059979200363159 / Valid loss: 6.286080705551874
Training loss: 1.8821156024932861 / Valid loss: 6.413520688102359
Training loss: 2.951695203781128 / Valid loss: 6.212141752243042

Epoch: 23
Training loss: 2.6261754035949707 / Valid loss: 6.224995565414429
Training loss: 3.3707523345947266 / Valid loss: 6.544760209038144
Training loss: 3.183842658996582 / Valid loss: 6.4481736818949384
Training loss: 4.845515727996826 / Valid loss: 6.333111644926525
Training loss: 2.4577317237854004 / Valid loss: 6.373334071749732

Epoch: 24
Training loss: 2.6583077907562256 / Valid loss: 6.300913363411313
Training loss: 3.770781993865967 / Valid loss: 6.347001652490525
Training loss: 1.7579405307769775 / Valid loss: 6.492632202875047
Training loss: 3.0956897735595703 / Valid loss: 6.351877389635359
Training loss: 2.716897964477539 / Valid loss: 6.35678053810483

Epoch: 25
Training loss: 2.4018354415893555 / Valid loss: 6.448353131612142
Training loss: 1.9672175645828247 / Valid loss: 6.4567380700792585
Training loss: 2.40779972076416 / Valid loss: 6.237968826293946
Training loss: 2.5409581661224365 / Valid loss: 6.308175738652547
Training loss: 3.116288423538208 / Valid loss: 6.384935553868612

Epoch: 26
Training loss: 2.3167872428894043 / Valid loss: 6.334440499260312
Training loss: 3.5192179679870605 / Valid loss: 6.513653024037679
Training loss: 2.7171294689178467 / Valid loss: 6.473910027458555
Training loss: 3.0565924644470215 / Valid loss: 6.386180135181972
Training loss: 2.8967740535736084 / Valid loss: 6.409758971986316

Epoch: 27
Training loss: 2.517280101776123 / Valid loss: 6.299486346471877
Training loss: 3.153535842895508 / Valid loss: 6.399896047228858
Training loss: 3.088695526123047 / Valid loss: 6.575195314770653
Training loss: 2.3976845741271973 / Valid loss: 6.612061477842785
Training loss: 1.5150277614593506 / Valid loss: 6.381097636904035

Epoch: 28
Training loss: 2.1395387649536133 / Valid loss: 6.321034749348958
Training loss: 2.295440196990967 / Valid loss: 6.442019015266782
Training loss: 2.7613840103149414 / Valid loss: 6.46659429640997
Training loss: 2.1118521690368652 / Valid loss: 6.400010622115362
Training loss: 2.9334497451782227 / Valid loss: 6.351692744663784

Epoch: 29
Training loss: 2.3668737411499023 / Valid loss: 6.430507807504563
Training loss: 1.8097926378250122 / Valid loss: 6.539074607122512
Training loss: 1.8446271419525146 / Valid loss: 6.477677254449754
Training loss: 3.0529122352600098 / Valid loss: 6.495809718540737

Epoch: 30
Training loss: 1.357527494430542 / Valid loss: 6.397843951270694
Training loss: 2.213839054107666 / Valid loss: 6.487361483346849
Training loss: 2.2158212661743164 / Valid loss: 6.59742785862514
Training loss: 1.8483859300613403 / Valid loss: 6.3511535235813685
Training loss: 2.4330735206604004 / Valid loss: 6.487764476594471

Epoch: 31
Training loss: 1.9195928573608398 / Valid loss: 6.463054806845529
Training loss: 3.562452554702759 / Valid loss: 6.695628529503232
Training loss: 2.221142292022705 / Valid loss: 6.386126708984375
Training loss: 1.7940528392791748 / Valid loss: 6.564712479001
Training loss: 2.6270413398742676 / Valid loss: 6.487103925432478

Epoch: 32
Training loss: 2.1869750022888184 / Valid loss: 6.435642619360061
Training loss: 1.7468230724334717 / Valid loss: 6.5694083440871465
Training loss: 2.4497151374816895 / Valid loss: 6.433801333109538
Training loss: 1.9322798252105713 / Valid loss: 6.35598445165725
Training loss: 2.7137250900268555 / Valid loss: 6.438567027591524

Epoch: 33
Training loss: 1.6208133697509766 / Valid loss: 6.4857263746715725
Training loss: 2.5766761302948 / Valid loss: 6.468277281806582
Training loss: 2.2060132026672363 / Valid loss: 6.3934603009905135
Training loss: 2.5459489822387695 / Valid loss: 6.5722649460747125
Training loss: 1.6766809225082397 / Valid loss: 6.508320426940918

Epoch: 34
Training loss: 2.154005527496338 / Valid loss: 6.449422706876482
Training loss: 2.6031901836395264 / Valid loss: 6.285448362713768
Training loss: 1.3834600448608398 / Valid loss: 6.498811340332031
Training loss: 1.2077438831329346 / Valid loss: 6.427989768981933
Training loss: 2.345517873764038 / Valid loss: 6.474319476173037

Epoch: 35
Training loss: 2.1224069595336914 / Valid loss: 6.508041227431525
Training loss: 2.0935006141662598 / Valid loss: 6.707948550723848
Training loss: 1.8077785968780518 / Valid loss: 6.43701430502392
Training loss: 1.4777519702911377 / Valid loss: 6.6121843224480035
Training loss: 1.8212652206420898 / Valid loss: 6.507203735624041

Epoch: 36
Training loss: 1.5286717414855957 / Valid loss: 6.511645287559146
Training loss: 2.03294038772583 / Valid loss: 6.435057013375419
Training loss: 1.8067221641540527 / Valid loss: 6.399457332066127
Training loss: 3.2054972648620605 / Valid loss: 6.518290994280861
Training loss: 1.6270322799682617 / Valid loss: 6.38915327390035

Epoch: 37
Training loss: 1.2544121742248535 / Valid loss: 6.393241080783662
Training loss: 1.6830826997756958 / Valid loss: 6.520904818035308
Training loss: 1.7402255535125732 / Valid loss: 6.394684446425665
Training loss: 2.173485040664673 / Valid loss: 6.672368608202253
Training loss: 2.540238380432129 / Valid loss: 6.355571467535836

Epoch: 38
Training loss: 1.7430763244628906 / Valid loss: 6.393595977056594
Training loss: 1.1067616939544678 / Valid loss: 6.530601878393264
Training loss: 2.137836456298828 / Valid loss: 6.333166849045527
Training loss: 2.0675671100616455 / Valid loss: 6.532883650915963
Training loss: 2.7397730350494385 / Valid loss: 6.372574002402169

Epoch: 39
Training loss: 2.0905568599700928 / Valid loss: 6.649671422867548
Training loss: 1.9530506134033203 / Valid loss: 6.535980267751785
Training loss: 1.8659470081329346 / Valid loss: 6.556675888243175
Training loss: 2.2112059593200684 / Valid loss: 6.463347221556164
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2700): 5.427827094850086
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.614124298095703 / Valid loss: 16.246218199956985
Model is saved in epoch 0, overall batch: 0
Training loss: 6.312493801116943 / Valid loss: 5.877674141384307
Model is saved in epoch 0, overall batch: 100
Training loss: 6.751472473144531 / Valid loss: 6.418519231251308
Training loss: 6.353551864624023 / Valid loss: 6.0100365343548
Training loss: 8.439943313598633 / Valid loss: 5.937062224887666

Epoch: 1
Training loss: 5.2880048751831055 / Valid loss: 6.553929031462896
Training loss: 5.676802158355713 / Valid loss: 6.5628601165044875
Training loss: 6.814785957336426 / Valid loss: 5.897962449845814
Training loss: 5.9933319091796875 / Valid loss: 5.894264918281919
Training loss: 5.184691905975342 / Valid loss: 6.314388969966344

Epoch: 2
Training loss: 5.24749755859375 / Valid loss: 6.237087631225586
Training loss: 4.881400108337402 / Valid loss: 5.889023303985596
Training loss: 5.456589698791504 / Valid loss: 5.8669552235376266
Model is saved in epoch 2, overall batch: 1200
Training loss: 5.858824729919434 / Valid loss: 5.941099691390991
Training loss: 5.676375389099121 / Valid loss: 6.30075135912214

Epoch: 3
Training loss: 3.7753195762634277 / Valid loss: 5.854169527689616
Model is saved in epoch 3, overall batch: 1500
Training loss: 5.182581424713135 / Valid loss: 6.131450089954194
Training loss: 4.443765163421631 / Valid loss: 5.940154865809849
Training loss: 6.1273956298828125 / Valid loss: 5.8258265676952545
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.834737777709961 / Valid loss: 5.636628947939191
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.786046028137207 / Valid loss: 6.008568709237235
Training loss: 4.1309003829956055 / Valid loss: 5.8292631285531185
Training loss: 4.056604385375977 / Valid loss: 5.681003522872925
Training loss: 4.974031925201416 / Valid loss: 5.594757259459723
Model is saved in epoch 4, overall batch: 2300
Training loss: 7.265646934509277 / Valid loss: 6.141684493564424

Epoch: 5
Training loss: 3.7408804893493652 / Valid loss: 6.007342125120617
Training loss: 3.8601205348968506 / Valid loss: 5.809756867090861
Training loss: 5.639122009277344 / Valid loss: 5.930866800035749
Training loss: 5.966064453125 / Valid loss: 6.147733938126337
Training loss: 6.913714408874512 / Valid loss: 6.3387962818145756

Epoch: 6
Training loss: 4.543728828430176 / Valid loss: 5.737064334324428
Training loss: 3.713435649871826 / Valid loss: 6.195389681770688
Training loss: 4.558839321136475 / Valid loss: 5.801516303561983
Training loss: 3.6930909156799316 / Valid loss: 5.767184834253221
Training loss: 3.43778657913208 / Valid loss: 6.060870901743571

Epoch: 7
Training loss: 2.5260748863220215 / Valid loss: 6.334528777712867
Training loss: 3.9539318084716797 / Valid loss: 5.981876741136824
Training loss: 4.151986122131348 / Valid loss: 5.9609228610992435
Training loss: 4.748990535736084 / Valid loss: 5.899541691371373
Training loss: 4.8450164794921875 / Valid loss: 5.876784011295864

Epoch: 8
Training loss: 3.513190746307373 / Valid loss: 5.867927794229416
Training loss: 3.3383078575134277 / Valid loss: 6.403200283504668
Training loss: 4.856456756591797 / Valid loss: 5.98474702835083
Training loss: 4.073086738586426 / Valid loss: 6.054555684044248
Training loss: 3.50345516204834 / Valid loss: 5.904850323994954

Epoch: 9
Training loss: 3.0862855911254883 / Valid loss: 6.0184046745300295
Training loss: 3.699174404144287 / Valid loss: 6.136225648153395
Training loss: 4.011710166931152 / Valid loss: 6.073992263703119
Training loss: 2.95230770111084 / Valid loss: 6.2201897961752755

Epoch: 10
Training loss: 3.0942368507385254 / Valid loss: 6.039375414167131
Training loss: 2.6735379695892334 / Valid loss: 6.349157376516433
Training loss: 3.761002540588379 / Valid loss: 6.368217763446626
Training loss: 3.487499237060547 / Valid loss: 6.263705360321771
Training loss: 3.1756958961486816 / Valid loss: 6.254297251928421

Epoch: 11
Training loss: 3.5131170749664307 / Valid loss: 6.501405747731527
Training loss: 3.9575841426849365 / Valid loss: 6.300567958468482
Training loss: 3.2366466522216797 / Valid loss: 6.601554296130225
Training loss: 3.018425464630127 / Valid loss: 6.2089896996816
Training loss: 2.811051845550537 / Valid loss: 6.264829962594169

Epoch: 12
Training loss: 3.3285984992980957 / Valid loss: 6.273718418393816
Training loss: 2.6472301483154297 / Valid loss: 6.486787534895397
Training loss: 3.8602638244628906 / Valid loss: 6.4214356399717785
Training loss: 3.109874725341797 / Valid loss: 6.42090045838129
Training loss: 2.3828024864196777 / Valid loss: 6.428601385298229

Epoch: 13
Training loss: 2.4528050422668457 / Valid loss: 6.3347276574089415
Training loss: 2.4252939224243164 / Valid loss: 6.426772832870483
Training loss: 2.394383430480957 / Valid loss: 7.028439401444935
Training loss: 2.4825682640075684 / Valid loss: 7.204118792215983
Training loss: 2.8047752380371094 / Valid loss: 6.419840213230678

Epoch: 14
Training loss: 2.3648629188537598 / Valid loss: 6.743903085163661
Training loss: 2.6575355529785156 / Valid loss: 6.718045041674659
Training loss: 4.022311210632324 / Valid loss: 6.603086526053293
Training loss: 3.272007942199707 / Valid loss: 6.739283041726975
Training loss: 2.622654438018799 / Valid loss: 6.895591858455113

Epoch: 15
Training loss: 1.7326130867004395 / Valid loss: 6.650728604907081
Training loss: 2.9380221366882324 / Valid loss: 6.618429193042574
Training loss: 2.42022705078125 / Valid loss: 7.260886501130604
Training loss: 2.898822546005249 / Valid loss: 6.599203482128325
Training loss: 2.866872787475586 / Valid loss: 6.9973470460800895

Epoch: 16
Training loss: 2.9739990234375 / Valid loss: 6.6934902963184175
Training loss: 2.5816092491149902 / Valid loss: 6.908242003122965
Training loss: 1.7562224864959717 / Valid loss: 6.708404531933012
Training loss: 3.25703763961792 / Valid loss: 6.648017329261417
Training loss: 2.191534996032715 / Valid loss: 6.63601328531901

Epoch: 17
Training loss: 2.603316307067871 / Valid loss: 6.725612590426491
Training loss: 1.71140718460083 / Valid loss: 6.7424414998009095
Training loss: 2.154543399810791 / Valid loss: 7.011346744355701
Training loss: 4.760311126708984 / Valid loss: 6.799996628080096
Training loss: 3.2749075889587402 / Valid loss: 6.646798261006674

Epoch: 18
Training loss: 1.911954402923584 / Valid loss: 7.605169718606131
Training loss: 1.8104641437530518 / Valid loss: 6.87898773692903
Training loss: 2.2707479000091553 / Valid loss: 6.820659446716308
Training loss: 2.5440797805786133 / Valid loss: 7.098529025486537
Training loss: 2.8048744201660156 / Valid loss: 6.819531488418579

Epoch: 19
Training loss: 1.342199444770813 / Valid loss: 6.915588006519136
Training loss: 2.1838719844818115 / Valid loss: 6.873219172159831
Training loss: 2.18998646736145 / Valid loss: 6.90631685256958
Training loss: 2.0959606170654297 / Valid loss: 6.960575249081566

Epoch: 20
Training loss: 1.7444941997528076 / Valid loss: 7.751132924216134
Training loss: 1.7228646278381348 / Valid loss: 6.935852309635707
Training loss: 1.688985824584961 / Valid loss: 7.397377661296299
Training loss: 1.5284757614135742 / Valid loss: 7.283897490728469
Training loss: 1.975792407989502 / Valid loss: 6.926161784217471

Epoch: 21
Training loss: 1.8962749242782593 / Valid loss: 6.9615371022905626
Training loss: 2.929917335510254 / Valid loss: 7.710797343935285
Training loss: 2.1264147758483887 / Valid loss: 7.198719219934373
Training loss: 2.008678436279297 / Valid loss: 7.124734274546305
Training loss: 2.339188575744629 / Valid loss: 6.858554063524519

Epoch: 22
Training loss: 1.490449070930481 / Valid loss: 7.521955399286179
Training loss: 2.0341997146606445 / Valid loss: 6.933476284572056
Training loss: 2.1183369159698486 / Valid loss: 7.441724441165015
Training loss: 2.0302507877349854 / Valid loss: 7.263756829216367
Training loss: 2.061795473098755 / Valid loss: 7.1583199864342095

Epoch: 23
Training loss: 1.2833354473114014 / Valid loss: 6.949334153674898
Training loss: 1.8176262378692627 / Valid loss: 7.048804737272716
Training loss: 1.9420464038848877 / Valid loss: 7.3557206153869625
Training loss: 1.5673633813858032 / Valid loss: 8.028435209819248
Training loss: 1.9300129413604736 / Valid loss: 7.586660271599179

Epoch: 24
Training loss: 1.1075921058654785 / Valid loss: 7.081219405219668
Training loss: 1.6218693256378174 / Valid loss: 6.962452334449405
Training loss: 2.451991319656372 / Valid loss: 7.598558748336066
Training loss: 1.8583123683929443 / Valid loss: 7.044226214999244
Training loss: 1.5543689727783203 / Valid loss: 7.363834167662121

Epoch: 25
Training loss: 1.4588944911956787 / Valid loss: 7.047224880400158
Training loss: 1.363338828086853 / Valid loss: 7.4809204215095155
Training loss: 2.6761748790740967 / Valid loss: 7.440878899892171
Training loss: 1.6385794878005981 / Valid loss: 7.087056212198167
Training loss: 1.5942387580871582 / Valid loss: 6.972456391652425

Epoch: 26
Training loss: 1.3867101669311523 / Valid loss: 7.05612583614531
Training loss: 1.3126366138458252 / Valid loss: 7.312876774015881
Training loss: 1.4305706024169922 / Valid loss: 7.00689408211481
Training loss: 1.600358486175537 / Valid loss: 7.143165154684158
Training loss: 1.0999767780303955 / Valid loss: 7.021792706989107

Epoch: 27
Training loss: 1.5519499778747559 / Valid loss: 7.532787681761242
Training loss: 1.4256556034088135 / Valid loss: 7.037984857105074
Training loss: 1.4818164110183716 / Valid loss: 7.207201957702637
Training loss: 1.1080992221832275 / Valid loss: 7.1981124015081495
Training loss: 1.7512937784194946 / Valid loss: 8.178546510423933

Epoch: 28
Training loss: 0.9892725944519043 / Valid loss: 7.498099649520148
Training loss: 0.8134322166442871 / Valid loss: 7.128444948650542
Training loss: 1.1838250160217285 / Valid loss: 7.44603320076352
Training loss: 2.138397693634033 / Valid loss: 7.469186137971424
Training loss: 1.6358356475830078 / Valid loss: 7.085995810372489

Epoch: 29
Training loss: 1.0209286212921143 / Valid loss: 7.502628617059617
Training loss: 1.1807663440704346 / Valid loss: 7.044320363090152
Training loss: 1.577418565750122 / Valid loss: 7.292293062664213
Training loss: 1.6255059242248535 / Valid loss: 8.414864045097714

Epoch: 30
Training loss: 1.3804399967193604 / Valid loss: 7.514435395740327
Training loss: 1.4700089693069458 / Valid loss: 7.291317817143032
Training loss: 1.1095807552337646 / Valid loss: 8.780141062963576
Training loss: 1.3286161422729492 / Valid loss: 8.109500140235538
Training loss: 1.3284144401550293 / Valid loss: 8.24581402369908

Epoch: 31
Training loss: 1.4861667156219482 / Valid loss: 7.81942450205485
Training loss: 1.1575112342834473 / Valid loss: 7.554126403445289
Training loss: 1.01324462890625 / Valid loss: 7.5444410641988116
Training loss: 1.7202599048614502 / Valid loss: 7.0992251487005325
Training loss: 1.4342293739318848 / Valid loss: 7.072828533535912

Epoch: 32
Training loss: 1.0426090955734253 / Valid loss: 7.336283415839786
Training loss: 1.0363014936447144 / Valid loss: 7.945277372996013
Training loss: 0.9837038516998291 / Valid loss: 7.1434569267999555
Training loss: 1.0891979932785034 / Valid loss: 7.066995246069772
Training loss: 0.8330514430999756 / Valid loss: 7.561420595078241

Epoch: 33
Training loss: 0.9596468210220337 / Valid loss: 7.395766989390055
Training loss: 1.0598366260528564 / Valid loss: 7.304244036901565
Training loss: 1.0248370170593262 / Valid loss: 7.118258594331287
Training loss: 0.810280442237854 / Valid loss: 7.41495554787772
Training loss: 1.1251025199890137 / Valid loss: 7.36513474101112

Epoch: 34
Training loss: 1.4163641929626465 / Valid loss: 7.141156519026984
Training loss: 1.5910371541976929 / Valid loss: 7.330793689546131
Training loss: 1.2369128465652466 / Valid loss: 8.37219619296846
Training loss: 1.526697039604187 / Valid loss: 7.120884731837681
Training loss: 0.9038815498352051 / Valid loss: 7.211513819013323

Epoch: 35
Training loss: 0.8792237043380737 / Valid loss: 7.138654554457892
Training loss: 1.4404807090759277 / Valid loss: 7.04956301734561
Training loss: 0.9143431782722473 / Valid loss: 7.1539017177763435
Training loss: 0.9068232774734497 / Valid loss: 7.508437506357828
Training loss: 1.3264974355697632 / Valid loss: 7.260352843148368

Epoch: 36
Training loss: 1.282908320426941 / Valid loss: 7.493495441618419
Training loss: 1.3880871534347534 / Valid loss: 8.150945254734584
Training loss: 0.9525015354156494 / Valid loss: 7.255289763496036
Training loss: 0.8751879930496216 / Valid loss: 7.099060896464756
Training loss: 1.1796855926513672 / Valid loss: 7.078861613500686

Epoch: 37
Training loss: 1.6133344173431396 / Valid loss: 7.4149580024537585
Training loss: 0.8816770315170288 / Valid loss: 7.045775077456519
Training loss: 1.4987961053848267 / Valid loss: 7.081526411147345
Training loss: 0.6970174312591553 / Valid loss: 7.112975729079474
Training loss: 1.384056806564331 / Valid loss: 7.248935045514788

Epoch: 38
Training loss: 0.8146712779998779 / Valid loss: 7.329815383184524
Training loss: 1.5535697937011719 / Valid loss: 7.115889894394647
Training loss: 1.3118767738342285 / Valid loss: 7.181014431090582
Training loss: 1.3193309307098389 / Valid loss: 7.15113836924235
Training loss: 1.1030142307281494 / Valid loss: 7.360822000957671

Epoch: 39
Training loss: 0.7482557892799377 / Valid loss: 7.257923466818673
Training loss: 1.122933030128479 / Valid loss: 7.375155385335287
Training loss: 1.0373461246490479 / Valid loss: 7.673414779844738
Training loss: 0.69545578956604 / Valid loss: 8.135871514819918
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2300): 5.434702680224464
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.614124298095703 / Valid loss: 16.24617805480957
Model is saved in epoch 0, overall batch: 0
Training loss: 6.314384460449219 / Valid loss: 5.873962983630952
Model is saved in epoch 0, overall batch: 100
Training loss: 6.742455005645752 / Valid loss: 6.414893329711187
Training loss: 6.3054680824279785 / Valid loss: 5.997526246025449
Training loss: 8.426568984985352 / Valid loss: 5.948298111416045

Epoch: 1
Training loss: 5.283180236816406 / Valid loss: 6.462371099562872
Training loss: 5.6641035079956055 / Valid loss: 6.585086179914929
Training loss: 6.832809925079346 / Valid loss: 5.8953087693169
Training loss: 6.060865879058838 / Valid loss: 5.8377215271904355
Model is saved in epoch 1, overall batch: 800
Training loss: 5.198601722717285 / Valid loss: 6.339566221691313

Epoch: 2
Training loss: 5.279735088348389 / Valid loss: 6.2598349911826
Training loss: 4.757696151733398 / Valid loss: 5.903559587115333
Training loss: 5.442568778991699 / Valid loss: 5.873501759483701
Training loss: 5.892442226409912 / Valid loss: 5.963550633475894
Training loss: 5.66586971282959 / Valid loss: 6.212706127620879

Epoch: 3
Training loss: 3.8105478286743164 / Valid loss: 5.860588902518863
Training loss: 5.240318298339844 / Valid loss: 6.231671658016387
Training loss: 4.481975555419922 / Valid loss: 5.949922232400803
Training loss: 6.053610801696777 / Valid loss: 5.81203140985398
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.791852951049805 / Valid loss: 5.738376086098807
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.7765722274780273 / Valid loss: 5.88752806527274
Training loss: 4.282810211181641 / Valid loss: 5.858324257532756
Training loss: 4.2983808517456055 / Valid loss: 5.704022048768543
Model is saved in epoch 4, overall batch: 2200
Training loss: 5.1325788497924805 / Valid loss: 5.574386766978672
Model is saved in epoch 4, overall batch: 2300
Training loss: 7.425206184387207 / Valid loss: 6.29305339540754

Epoch: 5
Training loss: 3.840881586074829 / Valid loss: 5.970451087043399
Training loss: 3.7242236137390137 / Valid loss: 5.745420217514038
Training loss: 5.7313127517700195 / Valid loss: 5.93612067812965
Training loss: 5.873711585998535 / Valid loss: 6.232878619148618
Training loss: 7.2815070152282715 / Valid loss: 6.383413312548683

Epoch: 6
Training loss: 4.566384315490723 / Valid loss: 5.711750761667887
Training loss: 3.654489517211914 / Valid loss: 6.0255486760820665
Training loss: 4.543440341949463 / Valid loss: 5.774845729555403
Training loss: 3.955042600631714 / Valid loss: 5.73678571156093
Training loss: 3.3397040367126465 / Valid loss: 6.088093314852033

Epoch: 7
Training loss: 2.4698524475097656 / Valid loss: 6.4000096593584335
Training loss: 3.845987319946289 / Valid loss: 5.87925014722915
Training loss: 3.950101375579834 / Valid loss: 5.899894952774048
Training loss: 4.6281938552856445 / Valid loss: 5.947478823434739
Training loss: 4.79094123840332 / Valid loss: 5.8931543872469945

Epoch: 8
Training loss: 3.4594924449920654 / Valid loss: 5.873634202139718
Training loss: 3.3426880836486816 / Valid loss: 6.3229584262484595
Training loss: 4.909786701202393 / Valid loss: 6.004636884870983
Training loss: 3.9512505531311035 / Valid loss: 6.03764723823184
Training loss: 3.6393649578094482 / Valid loss: 5.865786604654222

Epoch: 9
Training loss: 3.2086548805236816 / Valid loss: 5.997382091340564
Training loss: 3.8216123580932617 / Valid loss: 6.135679662795294
Training loss: 4.063770771026611 / Valid loss: 6.032024279094878
Training loss: 3.1706948280334473 / Valid loss: 6.30267432530721

Epoch: 10
Training loss: 3.1424591541290283 / Valid loss: 6.0603664602552145
Training loss: 3.0880608558654785 / Valid loss: 6.385207834697905
Training loss: 3.846649169921875 / Valid loss: 6.300828202565511
Training loss: 3.7346034049987793 / Valid loss: 6.2989148480551584
Training loss: 3.426475763320923 / Valid loss: 6.2393267063867475

Epoch: 11
Training loss: 3.678056478500366 / Valid loss: 6.507187225705102
Training loss: 3.76822566986084 / Valid loss: 6.1475549334571475
Training loss: 3.411163330078125 / Valid loss: 6.852244320369902
Training loss: 2.6378989219665527 / Valid loss: 6.263643010457357
Training loss: 2.689464569091797 / Valid loss: 6.274562510989961

Epoch: 12
Training loss: 2.9794747829437256 / Valid loss: 6.184336453392392
Training loss: 2.715898036956787 / Valid loss: 6.444636308579218
Training loss: 3.6886110305786133 / Valid loss: 6.361548030944098
Training loss: 2.8041300773620605 / Valid loss: 6.3022130012512205
Training loss: 2.568040370941162 / Valid loss: 6.329010118756975

Epoch: 13
Training loss: 2.3152596950531006 / Valid loss: 6.301458013625372
Training loss: 2.8794116973876953 / Valid loss: 6.37997270311628
Training loss: 2.7264533042907715 / Valid loss: 6.886504223233177
Training loss: 2.6842286586761475 / Valid loss: 7.042316400437128
Training loss: 2.7099533081054688 / Valid loss: 6.31764003662836

Epoch: 14
Training loss: 2.3431878089904785 / Valid loss: 6.607242398034959
Training loss: 2.774752616882324 / Valid loss: 6.526934040160406
Training loss: 4.169761657714844 / Valid loss: 6.446983759743826
Training loss: 3.396596670150757 / Valid loss: 6.626510156903948
Training loss: 3.3804140090942383 / Valid loss: 6.835489025570097

Epoch: 15
Training loss: 1.872989535331726 / Valid loss: 6.719176853270758
Training loss: 3.324092149734497 / Valid loss: 6.604871463775635
Training loss: 2.7017624378204346 / Valid loss: 7.073549752008347
Training loss: 3.234288215637207 / Valid loss: 6.6431120600019185
Training loss: 2.7426950931549072 / Valid loss: 6.951867026374454

Epoch: 16
Training loss: 2.7611923217773438 / Valid loss: 6.586516566503615
Training loss: 2.562230348587036 / Valid loss: 6.751439980098179
Training loss: 1.7669031620025635 / Valid loss: 6.662464069184803
Training loss: 3.249032974243164 / Valid loss: 6.597451205480667
Training loss: 2.6242592334747314 / Valid loss: 6.619592119398571

Epoch: 17
Training loss: 2.709677219390869 / Valid loss: 6.68078464780535
Training loss: 2.0939927101135254 / Valid loss: 6.750819905598958
Training loss: 2.413755178451538 / Valid loss: 6.737290382385254
Training loss: 4.493892669677734 / Valid loss: 6.706868752979097
Training loss: 3.0763332843780518 / Valid loss: 6.674416973477318

Epoch: 18
Training loss: 2.1398589611053467 / Valid loss: 7.637948671976726
Training loss: 1.8110109567642212 / Valid loss: 6.936481457664853
Training loss: 2.232333183288574 / Valid loss: 6.795910789853051
Training loss: 2.2450058460235596 / Valid loss: 7.164916887737456
Training loss: 2.6688311100006104 / Valid loss: 6.94785061336699

Epoch: 19
Training loss: 1.5436604022979736 / Valid loss: 6.784035373869396
Training loss: 2.468120813369751 / Valid loss: 6.824343940189906
Training loss: 2.024228572845459 / Valid loss: 6.8645099299294605
Training loss: 2.4386065006256104 / Valid loss: 6.877210853213356

Epoch: 20
Training loss: 1.64559006690979 / Valid loss: 8.111710682369413
Training loss: 1.7512407302856445 / Valid loss: 7.062562313533965
Training loss: 1.806280493736267 / Valid loss: 7.172360436121623
Training loss: 1.7001737356185913 / Valid loss: 7.082927479062762
Training loss: 1.760143756866455 / Valid loss: 7.060633900052025

Epoch: 21
Training loss: 2.095679998397827 / Valid loss: 7.180015359606061
Training loss: 2.8810648918151855 / Valid loss: 7.998704101925805
Training loss: 2.1683034896850586 / Valid loss: 7.210312407357352
Training loss: 2.3391029834747314 / Valid loss: 7.262493583134242
Training loss: 2.31558895111084 / Valid loss: 6.955406529562814

Epoch: 22
Training loss: 1.6809015274047852 / Valid loss: 7.188275037493025
Training loss: 1.6644848585128784 / Valid loss: 7.019832997095017
Training loss: 2.3406434059143066 / Valid loss: 7.366858341580346
Training loss: 2.382323741912842 / Valid loss: 7.478049845922561
Training loss: 2.4359099864959717 / Valid loss: 6.957027580624535

Epoch: 23
Training loss: 1.3247907161712646 / Valid loss: 7.005834132149106
Training loss: 2.2961645126342773 / Valid loss: 7.43981910433088
Training loss: 2.0668392181396484 / Valid loss: 7.554634466625395
Training loss: 1.646104335784912 / Valid loss: 8.045743374597459
Training loss: 1.552138090133667 / Valid loss: 7.222824809664772

Epoch: 24
Training loss: 1.4878969192504883 / Valid loss: 7.087510976337251
Training loss: 1.3707078695297241 / Valid loss: 7.041772892361595
Training loss: 2.071194887161255 / Valid loss: 7.474981825692313
Training loss: 1.692209243774414 / Valid loss: 7.128767753782727
Training loss: 1.7842752933502197 / Valid loss: 7.094039376576742

Epoch: 25
Training loss: 1.6986860036849976 / Valid loss: 7.148991911751883
Training loss: 1.3893635272979736 / Valid loss: 7.0197115307762505
Training loss: 2.9924211502075195 / Valid loss: 7.629580883752732
Training loss: 1.7026264667510986 / Valid loss: 7.4481123924255375
Training loss: 1.396653413772583 / Valid loss: 7.161584826878139

Epoch: 26
Training loss: 1.0862104892730713 / Valid loss: 7.122552113305955
Training loss: 1.4241602420806885 / Valid loss: 7.060840588524228
Training loss: 1.2959259748458862 / Valid loss: 7.002166934240432
Training loss: 1.3091121912002563 / Valid loss: 7.019192130225045
Training loss: 1.0873165130615234 / Valid loss: 7.102841250101725

Epoch: 27
Training loss: 1.3726568222045898 / Valid loss: 7.147895245324998
Training loss: 1.5131418704986572 / Valid loss: 7.138808191390265
Training loss: 1.5093059539794922 / Valid loss: 7.271675886426653
Training loss: 1.3028316497802734 / Valid loss: 7.235350431714739
Training loss: 1.8973973989486694 / Valid loss: 8.578039882296608

Epoch: 28
Training loss: 0.8819840550422668 / Valid loss: 7.750169917515346
Training loss: 0.90674889087677 / Valid loss: 7.1788235301063175
Training loss: 1.1426377296447754 / Valid loss: 7.61647580464681
Training loss: 2.308912754058838 / Valid loss: 7.364917400905064
Training loss: 1.7796380519866943 / Valid loss: 7.025539003099714

Epoch: 29
Training loss: 0.9481138586997986 / Valid loss: 7.183351276034401
Training loss: 1.1295868158340454 / Valid loss: 7.459179464975993
Training loss: 1.7341907024383545 / Valid loss: 7.120388721284412
Training loss: 1.6306474208831787 / Valid loss: 8.244880744389125

Epoch: 30
Training loss: 1.393646001815796 / Valid loss: 7.17699218704587
Training loss: 1.827202320098877 / Valid loss: 7.288846792493548
Training loss: 1.179589867591858 / Valid loss: 8.415380305335635
Training loss: 1.3840478658676147 / Valid loss: 7.481537296658471
Training loss: 1.329949975013733 / Valid loss: 7.900766726902553

Epoch: 31
Training loss: 1.5662360191345215 / Valid loss: 7.4561122621808735
Training loss: 1.1808775663375854 / Valid loss: 7.617594891502744
Training loss: 0.8890745639801025 / Valid loss: 7.409750493367513
Training loss: 1.3258509635925293 / Valid loss: 7.426213014693487
Training loss: 1.3079094886779785 / Valid loss: 7.051150894165039

Epoch: 32
Training loss: 1.1225786209106445 / Valid loss: 7.122082115354992
Training loss: 1.3024959564208984 / Valid loss: 7.72701484135219
Training loss: 0.8819119930267334 / Valid loss: 7.0821830136435375
Training loss: 1.4432085752487183 / Valid loss: 7.097574633643741
Training loss: 1.0472990274429321 / Valid loss: 7.760989479791551

Epoch: 33
Training loss: 0.972916841506958 / Valid loss: 8.218047464461554
Training loss: 1.2468154430389404 / Valid loss: 7.172618157523019
Training loss: 1.211857795715332 / Valid loss: 7.1721400397164485
Training loss: 1.1816141605377197 / Valid loss: 7.500004768371582
Training loss: 0.9554792642593384 / Valid loss: 7.726143546331496

Epoch: 34
Training loss: 1.242715835571289 / Valid loss: 7.044647303081694
Training loss: 1.8403972387313843 / Valid loss: 7.33497736794608
Training loss: 1.0136256217956543 / Valid loss: 8.711034856523787
Training loss: 1.2766674757003784 / Valid loss: 7.685759489876883
Training loss: 1.1116559505462646 / Valid loss: 7.098791626521519

Epoch: 35
Training loss: 1.1536829471588135 / Valid loss: 7.462057227180118
Training loss: 1.3452540636062622 / Valid loss: 7.146763129461379
Training loss: 1.3834214210510254 / Valid loss: 7.308115536825998
Training loss: 1.0606666803359985 / Valid loss: 7.8055004891895114
Training loss: 1.6179044246673584 / Valid loss: 7.185946096692766

Epoch: 36
Training loss: 1.4946801662445068 / Valid loss: 7.21383923803057
Training loss: 1.6793901920318604 / Valid loss: 8.597657757713682
Training loss: 0.9782060384750366 / Valid loss: 7.124325447990781
Training loss: 1.1039769649505615 / Valid loss: 7.166340046837217
Training loss: 1.0508133172988892 / Valid loss: 7.283497002011254

Epoch: 37
Training loss: 1.737086296081543 / Valid loss: 7.369354461488269
Training loss: 0.761246919631958 / Valid loss: 7.1755920546395435
Training loss: 1.7604953050613403 / Valid loss: 7.288619890667143
Training loss: 0.8052189350128174 / Valid loss: 7.3041535649980815
Training loss: 0.9595177173614502 / Valid loss: 7.154173580805461

Epoch: 38
Training loss: 0.8200703859329224 / Valid loss: 7.258412994657244
Training loss: 1.3015737533569336 / Valid loss: 7.139923590705508
Training loss: 0.8698909282684326 / Valid loss: 7.111043158031645
Training loss: 1.3477221727371216 / Valid loss: 7.322104794638498
Training loss: 1.086107611656189 / Valid loss: 7.614036501021612

Epoch: 39
Training loss: 1.0358190536499023 / Valid loss: 7.217134280431838
Training loss: 0.8359359502792358 / Valid loss: 7.1579746132805235
Training loss: 0.9139401912689209 / Valid loss: 7.282702888761248
Training loss: 0.6878699064254761 / Valid loss: 8.432963702792213
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2300): 5.435509157180786
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.837162017822266 / Valid loss: 15.2834961664109
Model is saved in epoch 0, overall batch: 0
Training loss: 9.270496368408203 / Valid loss: 13.080676909855434
Model is saved in epoch 0, overall batch: 100
Training loss: 10.451250076293945 / Valid loss: 12.950587785811651
Model is saved in epoch 0, overall batch: 200
Training loss: 16.3831844329834 / Valid loss: 12.218393307640438
Model is saved in epoch 0, overall batch: 300
Training loss: 11.574783325195312 / Valid loss: 11.57171197618757
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 10.295727729797363 / Valid loss: 11.219212709154402
Model is saved in epoch 1, overall batch: 500
Training loss: 9.510900497436523 / Valid loss: 10.214341140928722
Model is saved in epoch 1, overall batch: 600
Training loss: 8.660006523132324 / Valid loss: 10.104317329043434
Model is saved in epoch 1, overall batch: 700
Training loss: 8.792652130126953 / Valid loss: 9.914145505995977
Model is saved in epoch 1, overall batch: 800
Training loss: 7.170626640319824 / Valid loss: 9.477253609611875
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 9.742366790771484 / Valid loss: 9.267871806735084
Model is saved in epoch 2, overall batch: 1000
Training loss: 9.761957168579102 / Valid loss: 9.172408203851608
Model is saved in epoch 2, overall batch: 1100
Training loss: 9.053597450256348 / Valid loss: 8.702584843408493
Model is saved in epoch 2, overall batch: 1200
Training loss: 10.378730773925781 / Valid loss: 8.092575232187906
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.441936016082764 / Valid loss: 7.754678276606969
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.545282363891602 / Valid loss: 7.220515001387823
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.791543006896973 / Valid loss: 7.064465545472645
Model is saved in epoch 3, overall batch: 1600
Training loss: 7.523123741149902 / Valid loss: 6.912587222598848
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.915858507156372 / Valid loss: 6.33345323517209
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.13422966003418 / Valid loss: 6.502891281672887

Epoch: 4
Training loss: 6.340506553649902 / Valid loss: 6.384247859319051
Training loss: 5.1642045974731445 / Valid loss: 6.277221495764596
Model is saved in epoch 4, overall batch: 2100
Training loss: 5.235794544219971 / Valid loss: 6.511738952000936
Training loss: 5.665506362915039 / Valid loss: 6.2641428788503015
Model is saved in epoch 4, overall batch: 2300
Training loss: 4.377581596374512 / Valid loss: 5.991610025224231
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 7.714526176452637 / Valid loss: 6.246255241121564
Training loss: 5.982294082641602 / Valid loss: 6.001102218173799
Training loss: 4.928891181945801 / Valid loss: 5.907224071593512
Model is saved in epoch 5, overall batch: 2700
Training loss: 4.474248886108398 / Valid loss: 6.014116632370722
Training loss: 4.319906711578369 / Valid loss: 5.890694584165301
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 7.877931118011475 / Valid loss: 6.01085870833624
Training loss: 4.465352535247803 / Valid loss: 5.960321773801531
Training loss: 4.527097702026367 / Valid loss: 6.149048855191185
Training loss: 4.457588195800781 / Valid loss: 5.930783008393787
Training loss: 4.944169998168945 / Valid loss: 5.901362185251145

Epoch: 7
Training loss: 5.128012180328369 / Valid loss: 5.964383463632493
Training loss: 4.28442907333374 / Valid loss: 6.12156331879752
Training loss: 5.1641950607299805 / Valid loss: 6.059687569027855
Training loss: 5.541784286499023 / Valid loss: 5.942587516421363
Training loss: 3.8061447143554688 / Valid loss: 5.961304124196371

Epoch: 8
Training loss: 3.9945218563079834 / Valid loss: 6.146809235073271
Training loss: 2.871046304702759 / Valid loss: 6.056254032679966
Training loss: 5.057506084442139 / Valid loss: 6.10595053264073
Training loss: 6.610231399536133 / Valid loss: 6.149382936386835
Training loss: 4.262786865234375 / Valid loss: 6.061170950390044

Epoch: 9
Training loss: 2.935258388519287 / Valid loss: 6.1979479880560016
Training loss: 5.459798812866211 / Valid loss: 6.098725014641172
Training loss: 6.255559921264648 / Valid loss: 6.163851222537812
Training loss: 4.387320518493652 / Valid loss: 6.107571987878709

Epoch: 10
Training loss: 3.923044204711914 / Valid loss: 6.193022133055187
Training loss: 4.661804676055908 / Valid loss: 6.247049971989223
Training loss: 3.040355682373047 / Valid loss: 6.221772391455514
Training loss: 4.0280561447143555 / Valid loss: 6.2986634163629445
Training loss: 3.4923925399780273 / Valid loss: 6.247436982109433

Epoch: 11
Training loss: 2.6925811767578125 / Valid loss: 6.305542255583264
Training loss: 3.3846864700317383 / Valid loss: 6.293109678086781
Training loss: 5.89821720123291 / Valid loss: 6.155214420954386
Training loss: 2.2990477085113525 / Valid loss: 6.232916418711344
Training loss: 3.3800435066223145 / Valid loss: 6.670308603559222

Epoch: 12
Training loss: 3.265151023864746 / Valid loss: 6.238013891946702
Training loss: 2.8195834159851074 / Valid loss: 6.394833051590693
Training loss: 4.868756294250488 / Valid loss: 6.4920951343718025
Training loss: 4.7588582038879395 / Valid loss: 6.29834368342445
Training loss: 4.743558883666992 / Valid loss: 6.437694858369373

Epoch: 13
Training loss: 2.2366185188293457 / Valid loss: 6.458022335597447
Training loss: 2.5929133892059326 / Valid loss: 6.377496828351702
Training loss: 4.029637336730957 / Valid loss: 6.453591773623512
Training loss: 3.4073357582092285 / Valid loss: 6.33673031216576
Training loss: 3.4579594135284424 / Valid loss: 6.441772195271083

Epoch: 14
Training loss: 2.977952003479004 / Valid loss: 6.369079444521949
Training loss: 3.4145753383636475 / Valid loss: 6.443074444362096
Training loss: 3.5150537490844727 / Valid loss: 6.519161885125296
Training loss: 3.2513813972473145 / Valid loss: 6.38863951365153
Training loss: 4.409554958343506 / Valid loss: 6.548127063115438

Epoch: 15
Training loss: 2.5812478065490723 / Valid loss: 6.615662227358137
Training loss: 3.755648136138916 / Valid loss: 6.691933856691633
Training loss: 3.105609178543091 / Valid loss: 6.684189860026041
Training loss: 3.5695648193359375 / Valid loss: 6.568692166464669
Training loss: 2.7014503479003906 / Valid loss: 6.426182969411214

Epoch: 16
Training loss: 2.3048791885375977 / Valid loss: 6.512158766246977
Training loss: 3.2531819343566895 / Valid loss: 6.5114536966596335
Training loss: 3.0846147537231445 / Valid loss: 6.61496476218814
Training loss: 3.5517778396606445 / Valid loss: 6.539891399656024
Training loss: 4.079935073852539 / Valid loss: 6.535179615020752

Epoch: 17
Training loss: 3.058849811553955 / Valid loss: 6.7119088445390975
Training loss: 2.4480905532836914 / Valid loss: 6.626170081184024
Training loss: 2.930450916290283 / Valid loss: 6.626270716530936
Training loss: 2.5775375366210938 / Valid loss: 6.571541856584095
Training loss: 3.269704818725586 / Valid loss: 6.53519412449428

Epoch: 18
Training loss: 1.8053901195526123 / Valid loss: 6.822737784612746
Training loss: 2.5936756134033203 / Valid loss: 6.84446971530006
Training loss: 2.2409820556640625 / Valid loss: 6.498491600581578
Training loss: 3.3129849433898926 / Valid loss: 6.8002783412025085
Training loss: 3.1538138389587402 / Valid loss: 6.731460998171852

Epoch: 19
Training loss: 3.1262407302856445 / Valid loss: 6.888977536701021
Training loss: 3.336179733276367 / Valid loss: 6.677623378662836
Training loss: 3.783430337905884 / Valid loss: 6.678013306572324
Training loss: 2.622832775115967 / Valid loss: 6.6155455044337685

Epoch: 20
Training loss: 1.7355537414550781 / Valid loss: 6.761165768759591
Training loss: 2.038595199584961 / Valid loss: 6.608478827703567
Training loss: 2.341146469116211 / Valid loss: 6.65658662432716
Training loss: 2.5555310249328613 / Valid loss: 6.69446926571074
Training loss: 3.718437671661377 / Valid loss: 6.762842078435988

Epoch: 21
Training loss: 3.185598850250244 / Valid loss: 6.720955394563221
Training loss: 2.628481388092041 / Valid loss: 6.6755155767713275
Training loss: 1.7215162515640259 / Valid loss: 6.6071224076407296
Training loss: 2.6187033653259277 / Valid loss: 6.921266701107934
Training loss: 2.2790238857269287 / Valid loss: 6.74558216276623

Epoch: 22
Training loss: 2.409714698791504 / Valid loss: 6.807433455330985
Training loss: 2.2345800399780273 / Valid loss: 6.6687979357583185
Training loss: 1.7515480518341064 / Valid loss: 6.657194001334054
Training loss: 3.2446675300598145 / Valid loss: 6.851481825964791
Training loss: 2.194568157196045 / Valid loss: 6.87226580665225

Epoch: 23
Training loss: 2.078226327896118 / Valid loss: 6.6901130176725845
Training loss: 2.8816890716552734 / Valid loss: 6.778680335907709
Training loss: 2.526526689529419 / Valid loss: 6.98379960287185
Training loss: 2.7585787773132324 / Valid loss: 6.8283093043736045
Training loss: 2.6679091453552246 / Valid loss: 6.780165785834903

Epoch: 24
Training loss: 2.7665085792541504 / Valid loss: 6.77118356795538
Training loss: 1.9781367778778076 / Valid loss: 6.823150262378511
Training loss: 1.8920328617095947 / Valid loss: 6.940486403873988
Training loss: 3.0672121047973633 / Valid loss: 6.854524753207252
Training loss: 2.9732232093811035 / Valid loss: 6.895390528724307

Epoch: 25
Training loss: 1.868127465248108 / Valid loss: 6.745377168201265
Training loss: 1.4663732051849365 / Valid loss: 6.778187175024123
Training loss: 2.882383346557617 / Valid loss: 6.9438522429693315
Training loss: 2.590557098388672 / Valid loss: 6.9068730853852776
Training loss: 2.517423629760742 / Valid loss: 6.811988208407447

Epoch: 26
Training loss: 2.1217093467712402 / Valid loss: 6.903079210008894
Training loss: 3.609403133392334 / Valid loss: 6.865424113046555
Training loss: 1.76810622215271 / Valid loss: 6.933608266285487
Training loss: 2.251715660095215 / Valid loss: 6.893740336100261
Training loss: 2.0581610202789307 / Valid loss: 6.881496302286783

Epoch: 27
Training loss: 1.3990869522094727 / Valid loss: 6.9159626915341335
Training loss: 1.951693058013916 / Valid loss: 6.79699985186259
Training loss: 2.1039581298828125 / Valid loss: 7.0770973909468875
Training loss: 1.6326615810394287 / Valid loss: 6.814780864261445
Training loss: 2.4987916946411133 / Valid loss: 6.965027395884196

Epoch: 28
Training loss: 1.3102104663848877 / Valid loss: 7.018441359202067
Training loss: 1.6543991565704346 / Valid loss: 6.897904981885637
Training loss: 2.3173065185546875 / Valid loss: 6.955596256256103
Training loss: 1.8734850883483887 / Valid loss: 7.08597639628819
Training loss: 2.0402956008911133 / Valid loss: 6.941676637104579

Epoch: 29
Training loss: 2.8803601264953613 / Valid loss: 6.964942868550619
Training loss: 2.0408992767333984 / Valid loss: 6.953803943452381
Training loss: 2.2748303413391113 / Valid loss: 6.917371758960543
Training loss: 1.9725494384765625 / Valid loss: 7.102265076410203

Epoch: 30
Training loss: 1.330241084098816 / Valid loss: 7.081876107624599
Training loss: 1.6040490865707397 / Valid loss: 6.8466790063040595
Training loss: 1.2453887462615967 / Valid loss: 6.943412424269177
Training loss: 1.5585827827453613 / Valid loss: 6.978875014895484
Training loss: 2.0128891468048096 / Valid loss: 6.9451807022094725

Epoch: 31
Training loss: 1.855429768562317 / Valid loss: 6.963055887676421
Training loss: 1.4952868223190308 / Valid loss: 6.898872707003639
Training loss: 2.025177478790283 / Valid loss: 6.890542563937959
Training loss: 1.8923357725143433 / Valid loss: 7.219995346523467
Training loss: 1.8034019470214844 / Valid loss: 6.939243521009173

Epoch: 32
Training loss: 1.1385068893432617 / Valid loss: 7.070692675454276
Training loss: 1.5145987272262573 / Valid loss: 6.92219401995341
Training loss: 1.86787748336792 / Valid loss: 6.977171593620664
Training loss: 1.496924877166748 / Valid loss: 6.866943132309687
Training loss: 2.275620460510254 / Valid loss: 7.078568354107085

Epoch: 33
Training loss: 1.9726040363311768 / Valid loss: 6.959355118161156
Training loss: 1.6794720888137817 / Valid loss: 6.980702195848737
Training loss: 1.8086023330688477 / Valid loss: 7.143823398862566
Training loss: 1.7449790239334106 / Valid loss: 7.140060224987212
Training loss: 1.492056131362915 / Valid loss: 6.968406309400286

Epoch: 34
Training loss: 2.099006175994873 / Valid loss: 6.857221848624093
Training loss: 1.7595958709716797 / Valid loss: 6.967633810497466
Training loss: 1.8644860982894897 / Valid loss: 6.983033452715192
Training loss: 1.9120746850967407 / Valid loss: 6.883328555879139
Training loss: 1.4667518138885498 / Valid loss: 7.062586793445406

Epoch: 35
Training loss: 2.138057231903076 / Valid loss: 7.002891622270856
Training loss: 1.6960785388946533 / Valid loss: 6.925501737140474
Training loss: 1.7056926488876343 / Valid loss: 6.9454043615432015
Training loss: 1.293278455734253 / Valid loss: 6.898009811128889
Training loss: 1.5005139112472534 / Valid loss: 6.830378977457682

Epoch: 36
Training loss: 2.316632032394409 / Valid loss: 6.888079279945011
Training loss: 2.6272125244140625 / Valid loss: 7.177445879436674
Training loss: 1.8782719373703003 / Valid loss: 7.111806279137021
Training loss: 1.4743413925170898 / Valid loss: 6.995121706099737
Training loss: 2.2039122581481934 / Valid loss: 7.101133387429374

Epoch: 37
Training loss: 1.3997788429260254 / Valid loss: 6.939696625300816
Training loss: 1.624755859375 / Valid loss: 7.116313364392235
Training loss: 1.2911114692687988 / Valid loss: 7.093980316888718
Training loss: 2.9683749675750732 / Valid loss: 7.078125381469727
Training loss: 1.553719401359558 / Valid loss: 7.263172917138963

Epoch: 38
Training loss: 1.6219964027404785 / Valid loss: 6.982560816265288
Training loss: 1.8720229864120483 / Valid loss: 7.021106079646519
Training loss: 1.884214162826538 / Valid loss: 7.065839535849435
Training loss: 1.349206805229187 / Valid loss: 6.907343017487299
Training loss: 2.1823487281799316 / Valid loss: 7.073323258899507

Epoch: 39
Training loss: 1.346667766571045 / Valid loss: 7.054816473098028
Training loss: 1.0200797319412231 / Valid loss: 7.092900430588495
Training loss: 1.7742663621902466 / Valid loss: 7.031734003339495
Training loss: 1.287956953048706 / Valid loss: 7.013495447522118
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2900): 5.759015203657604
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.837162017822266 / Valid loss: 15.283498836698985
Model is saved in epoch 0, overall batch: 0
Training loss: 9.337468147277832 / Valid loss: 13.12870230901809
Model is saved in epoch 0, overall batch: 100
Training loss: 10.471085548400879 / Valid loss: 12.772933365049816
Model is saved in epoch 0, overall batch: 200
Training loss: 16.25665855407715 / Valid loss: 12.185628936404274
Model is saved in epoch 0, overall batch: 300
Training loss: 11.558570861816406 / Valid loss: 11.528438336508614
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 10.393898963928223 / Valid loss: 11.277465924762545
Model is saved in epoch 1, overall batch: 500
Training loss: 9.527433395385742 / Valid loss: 10.102125889914376
Model is saved in epoch 1, overall batch: 600
Training loss: 8.540456771850586 / Valid loss: 10.145807416098458
Training loss: 8.843191146850586 / Valid loss: 9.782333133334205
Model is saved in epoch 1, overall batch: 800
Training loss: 6.967358112335205 / Valid loss: 9.391539292108446
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 9.608152389526367 / Valid loss: 9.440444946289062
Training loss: 9.839848518371582 / Valid loss: 9.14303195135934
Model is saved in epoch 2, overall batch: 1100
Training loss: 9.032489776611328 / Valid loss: 8.525283849807012
Model is saved in epoch 2, overall batch: 1200
Training loss: 9.989554405212402 / Valid loss: 7.879433261780512
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.6140594482421875 / Valid loss: 7.383952340625581
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.602643013000488 / Valid loss: 7.228540602184477
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.703982353210449 / Valid loss: 7.127873057410831
Model is saved in epoch 3, overall batch: 1600
Training loss: 7.485612392425537 / Valid loss: 6.990095583597819
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.17572021484375 / Valid loss: 6.478236293792724
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.075351715087891 / Valid loss: 6.497567095075335

Epoch: 4
Training loss: 6.533069610595703 / Valid loss: 6.403624380202521
Model is saved in epoch 4, overall batch: 2000
Training loss: 5.213667869567871 / Valid loss: 6.505305735270182
Training loss: 5.249142646789551 / Valid loss: 6.360685818535941
Model is saved in epoch 4, overall batch: 2200
Training loss: 5.864083766937256 / Valid loss: 6.382480035509382
Training loss: 4.596384048461914 / Valid loss: 5.98203543027242
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 7.4857401847839355 / Valid loss: 6.0228513218107675
Training loss: 5.692804336547852 / Valid loss: 5.886878363291422
Model is saved in epoch 5, overall batch: 2600
Training loss: 4.881307125091553 / Valid loss: 5.832114971251714
Model is saved in epoch 5, overall batch: 2700
Training loss: 4.3510942459106445 / Valid loss: 6.034158320654006
Training loss: 3.9262356758117676 / Valid loss: 5.8369706494467595

Epoch: 6
Training loss: 7.699655532836914 / Valid loss: 5.96585210164388
Training loss: 4.847851753234863 / Valid loss: 6.001114298048473
Training loss: 4.596900939941406 / Valid loss: 6.047256519680931
Training loss: 4.610586643218994 / Valid loss: 5.982103997185117
Training loss: 5.085550308227539 / Valid loss: 5.881579830532982

Epoch: 7
Training loss: 5.622108459472656 / Valid loss: 5.959532703672137
Training loss: 4.088942527770996 / Valid loss: 6.03710549218314
Training loss: 4.947952747344971 / Valid loss: 5.896861973262968
Training loss: 5.061216354370117 / Valid loss: 5.894902971812657
Training loss: 4.456655502319336 / Valid loss: 5.898124454134987

Epoch: 8
Training loss: 3.6018004417419434 / Valid loss: 6.179063787914457
Training loss: 2.9656026363372803 / Valid loss: 6.089604743321737
Training loss: 5.096795558929443 / Valid loss: 6.167295564923967
Training loss: 6.829076766967773 / Valid loss: 6.084403280984787
Training loss: 4.462895393371582 / Valid loss: 6.313490672338577

Epoch: 9
Training loss: 2.880573272705078 / Valid loss: 6.006802820024037
Training loss: 4.834118366241455 / Valid loss: 6.152325496219453
Training loss: 5.92473840713501 / Valid loss: 6.037313454491752
Training loss: 4.726726531982422 / Valid loss: 6.002599695750646

Epoch: 10
Training loss: 3.861489772796631 / Valid loss: 6.187625558035714
Training loss: 3.638434648513794 / Valid loss: 6.136696815490723
Training loss: 2.958760976791382 / Valid loss: 6.198276042938232
Training loss: 4.010465621948242 / Valid loss: 6.260055973416283
Training loss: 3.809246301651001 / Valid loss: 6.142164820716495

Epoch: 11
Training loss: 2.9922773838043213 / Valid loss: 6.215662002563477
Training loss: 4.1979079246521 / Valid loss: 6.181728694552467
Training loss: 5.071017742156982 / Valid loss: 6.215028694697788
Training loss: 2.4461045265197754 / Valid loss: 6.244600784210932
Training loss: 3.417936325073242 / Valid loss: 6.476818343571254

Epoch: 12
Training loss: 3.2287473678588867 / Valid loss: 6.296708502088275
Training loss: 2.337894916534424 / Valid loss: 6.311610510235741
Training loss: 4.452917098999023 / Valid loss: 6.436756688072568
Training loss: 5.273261547088623 / Valid loss: 6.236003237678891
Training loss: 4.668449878692627 / Valid loss: 6.311859208061581

Epoch: 13
Training loss: 2.5547592639923096 / Valid loss: 6.371951925186884
Training loss: 2.9440255165100098 / Valid loss: 6.4246975035894485
Training loss: 3.1097052097320557 / Valid loss: 6.374866735367548
Training loss: 3.083183765411377 / Valid loss: 6.288095306214832
Training loss: 3.159724235534668 / Valid loss: 6.363658687046596

Epoch: 14
Training loss: 3.2990493774414062 / Valid loss: 6.335981321334839
Training loss: 3.5112802982330322 / Valid loss: 6.454631971177601
Training loss: 3.780029773712158 / Valid loss: 6.444151926040649
Training loss: 2.69618821144104 / Valid loss: 6.360586400259109
Training loss: 3.924556255340576 / Valid loss: 6.390171507426671

Epoch: 15
Training loss: 2.896834373474121 / Valid loss: 6.371125947861445
Training loss: 3.774456739425659 / Valid loss: 6.58041417712257
Training loss: 3.5837159156799316 / Valid loss: 6.517592416490827
Training loss: 3.5470597743988037 / Valid loss: 6.575400861104329
Training loss: 2.104395866394043 / Valid loss: 6.479552602767944

Epoch: 16
Training loss: 2.2005035877227783 / Valid loss: 6.489500531696138
Training loss: 2.8413543701171875 / Valid loss: 6.481512966610136
Training loss: 3.0087890625 / Valid loss: 6.577570308957781
Training loss: 2.647289276123047 / Valid loss: 6.517626882734753
Training loss: 3.2080130577087402 / Valid loss: 6.507444184167045

Epoch: 17
Training loss: 3.653783082962036 / Valid loss: 6.6779146512349445
Training loss: 2.6925089359283447 / Valid loss: 6.754418023427328
Training loss: 3.1508922576904297 / Valid loss: 6.738591929844447
Training loss: 3.173459529876709 / Valid loss: 6.532311630249024
Training loss: 3.422039031982422 / Valid loss: 6.52841139066787

Epoch: 18
Training loss: 2.472057819366455 / Valid loss: 6.93850580851237
Training loss: 2.953815221786499 / Valid loss: 6.6909814471290225
Training loss: 2.7536709308624268 / Valid loss: 6.5051684470403766
Training loss: 3.4275310039520264 / Valid loss: 6.675982425326393
Training loss: 3.786526918411255 / Valid loss: 6.6698715981983

Epoch: 19
Training loss: 2.773038625717163 / Valid loss: 6.789806147984096
Training loss: 2.973466634750366 / Valid loss: 6.7102950663793655
Training loss: 3.1137843132019043 / Valid loss: 6.735904934292748
Training loss: 2.9876105785369873 / Valid loss: 6.6595268703642345

Epoch: 20
Training loss: 1.7182363271713257 / Valid loss: 6.691514183226086
Training loss: 1.976184606552124 / Valid loss: 6.735826642172677
Training loss: 3.073575973510742 / Valid loss: 6.707970081056867
Training loss: 2.685725212097168 / Valid loss: 6.754580815633138
Training loss: 4.023051738739014 / Valid loss: 6.560733599889846

Epoch: 21
Training loss: 2.458069086074829 / Valid loss: 6.773055830455962
Training loss: 2.413578510284424 / Valid loss: 7.049886394682384
Training loss: 2.301560163497925 / Valid loss: 6.785681747254872
Training loss: 2.842726469039917 / Valid loss: 6.953642777034214
Training loss: 1.945804238319397 / Valid loss: 6.836940924326579

Epoch: 22
Training loss: 2.259934902191162 / Valid loss: 6.872843519846598
Training loss: 2.3000693321228027 / Valid loss: 6.798649265652611
Training loss: 2.167013645172119 / Valid loss: 6.712696795236496
Training loss: 3.141737937927246 / Valid loss: 6.899725425811041
Training loss: 2.5790905952453613 / Valid loss: 6.910947745186942

Epoch: 23
Training loss: 2.0960655212402344 / Valid loss: 6.67741847038269
Training loss: 3.3518805503845215 / Valid loss: 6.821892216092064
Training loss: 2.455650568008423 / Valid loss: 6.958978003547305
Training loss: 2.7683334350585938 / Valid loss: 6.880842467716762
Training loss: 2.2763702869415283 / Valid loss: 6.730853135245187

Epoch: 24
Training loss: 3.19815731048584 / Valid loss: 6.7494945798601425
Training loss: 1.719386100769043 / Valid loss: 6.728380916232155
Training loss: 1.9606521129608154 / Valid loss: 6.828858230227516
Training loss: 3.116896152496338 / Valid loss: 6.840281282152448
Training loss: 3.3081583976745605 / Valid loss: 6.9501766613551546

Epoch: 25
Training loss: 2.454399585723877 / Valid loss: 6.707209543954758
Training loss: 1.9197838306427002 / Valid loss: 6.690315357844034
Training loss: 3.241041660308838 / Valid loss: 6.883922286260695
Training loss: 1.8899534940719604 / Valid loss: 6.9588452884129115
Training loss: 2.288874864578247 / Valid loss: 6.901658952803839

Epoch: 26
Training loss: 1.770735740661621 / Valid loss: 6.995528257460821
Training loss: 3.4755265712738037 / Valid loss: 7.024042424701509
Training loss: 1.8347399234771729 / Valid loss: 7.036837682269868
Training loss: 2.1749439239501953 / Valid loss: 7.053531969161261
Training loss: 1.8081185817718506 / Valid loss: 6.901599879491897

Epoch: 27
Training loss: 1.673015832901001 / Valid loss: 6.928506733122326
Training loss: 1.604276180267334 / Valid loss: 7.198403444744292
Training loss: 1.6222865581512451 / Valid loss: 7.033706115541004
Training loss: 1.65921950340271 / Valid loss: 6.874334335327148
Training loss: 2.473048686981201 / Valid loss: 6.944492029008411

Epoch: 28
Training loss: 1.5378669500350952 / Valid loss: 7.015197456450689
Training loss: 1.8015093803405762 / Valid loss: 6.9916008268083845
Training loss: 1.9766089916229248 / Valid loss: 6.994699639365787
Training loss: 1.7416216135025024 / Valid loss: 6.883074383508592
Training loss: 1.953261137008667 / Valid loss: 6.932537414914086

Epoch: 29
Training loss: 3.07003116607666 / Valid loss: 6.987576702662877
Training loss: 1.8310154676437378 / Valid loss: 7.0521077655610585
Training loss: 2.8272862434387207 / Valid loss: 7.063700666881743
Training loss: 2.2788286209106445 / Valid loss: 6.917208546683902

Epoch: 30
Training loss: 1.4949185848236084 / Valid loss: 7.1072888192676364
Training loss: 1.5352256298065186 / Valid loss: 6.960204242524647
Training loss: 1.4083081483840942 / Valid loss: 7.131359465916952
Training loss: 1.6285462379455566 / Valid loss: 6.929385425930931
Training loss: 1.927743673324585 / Valid loss: 7.21178871790568

Epoch: 31
Training loss: 1.6699020862579346 / Valid loss: 7.135052678698585
Training loss: 1.5917906761169434 / Valid loss: 7.04583564939953
Training loss: 1.8628137111663818 / Valid loss: 6.946859041849772
Training loss: 1.5777990818023682 / Valid loss: 6.913717067809332
Training loss: 1.5866717100143433 / Valid loss: 6.997977506546747

Epoch: 32
Training loss: 1.7267076969146729 / Valid loss: 7.044055057707287
Training loss: 1.5432480573654175 / Valid loss: 6.915479403450376
Training loss: 1.308708667755127 / Valid loss: 6.970333812350319
Training loss: 1.6392379999160767 / Valid loss: 6.970941743396577
Training loss: 1.599521279335022 / Valid loss: 7.009514374960037

Epoch: 33
Training loss: 2.101243734359741 / Valid loss: 6.881758158547537
Training loss: 2.0669312477111816 / Valid loss: 6.9807082948230565
Training loss: 1.617384910583496 / Valid loss: 6.9319901012239
Training loss: 1.8674535751342773 / Valid loss: 7.066746071406773
Training loss: 1.840469479560852 / Valid loss: 6.913739744822184

Epoch: 34
Training loss: 2.2447848320007324 / Valid loss: 6.95287534622919
Training loss: 2.484994411468506 / Valid loss: 7.146190606980097
Training loss: 1.560651183128357 / Valid loss: 7.076532808939616
Training loss: 1.7294807434082031 / Valid loss: 6.945773742312476
Training loss: 2.0467982292175293 / Valid loss: 7.105090881529309

Epoch: 35
Training loss: 2.117249011993408 / Valid loss: 7.1731133506411595
Training loss: 1.7489161491394043 / Valid loss: 6.929727263677687
Training loss: 2.0530526638031006 / Valid loss: 7.126970123109364
Training loss: 1.6504795551300049 / Valid loss: 7.124331092834472
Training loss: 1.3997752666473389 / Valid loss: 6.914969793955485

Epoch: 36
Training loss: 2.3274927139282227 / Valid loss: 7.038592397598993
Training loss: 2.086406707763672 / Valid loss: 6.99972441082909
Training loss: 1.612486720085144 / Valid loss: 7.042970907120478
Training loss: 1.1937956809997559 / Valid loss: 6.909320222763788
Training loss: 2.1385276317596436 / Valid loss: 7.037993322099958

Epoch: 37
Training loss: 1.9227222204208374 / Valid loss: 7.039531142371041
Training loss: 1.2065086364746094 / Valid loss: 6.9854926608857655
Training loss: 1.5246766805648804 / Valid loss: 7.04141876129877
Training loss: 2.6445322036743164 / Valid loss: 6.868468779609317
Training loss: 1.8065896034240723 / Valid loss: 7.137550417582194

Epoch: 38
Training loss: 1.3375141620635986 / Valid loss: 6.911687896365211
Training loss: 2.122457981109619 / Valid loss: 7.0597442536127
Training loss: 0.9794065356254578 / Valid loss: 7.171827563785372
Training loss: 1.4298466444015503 / Valid loss: 7.041316933858962
Training loss: 2.6569323539733887 / Valid loss: 7.061623032887777

Epoch: 39
Training loss: 1.3115887641906738 / Valid loss: 7.044741249084472
Training loss: 1.3318712711334229 / Valid loss: 7.14330472946167
Training loss: 2.081881523132324 / Valid loss: 7.0542549178713845
Training loss: 1.140518307685852 / Valid loss: 7.08347780136835
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2700): 5.6539663973308745
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.974810600280762 / Valid loss: 16.55236949012393
Model is saved in epoch 0, overall batch: 0
Training loss: 8.70136547088623 / Valid loss: 14.245676022484188
Model is saved in epoch 0, overall batch: 100
Training loss: 10.985286712646484 / Valid loss: 13.440915071396601
Model is saved in epoch 0, overall batch: 200
Training loss: 10.656763076782227 / Valid loss: 12.755789838518416
Model is saved in epoch 0, overall batch: 300
Training loss: 8.395356178283691 / Valid loss: 12.421933864411853
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 12.316364288330078 / Valid loss: 12.027312446775891
Model is saved in epoch 1, overall batch: 500
Training loss: 7.987654209136963 / Valid loss: 11.839864753541493
Model is saved in epoch 1, overall batch: 600
Training loss: 7.356596946716309 / Valid loss: 11.235434573037283
Model is saved in epoch 1, overall batch: 700
Training loss: 5.644240379333496 / Valid loss: 10.626446251642136
Model is saved in epoch 1, overall batch: 800
Training loss: 9.203102111816406 / Valid loss: 9.78315104529971
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.554454326629639 / Valid loss: 9.514952709561303
Model is saved in epoch 2, overall batch: 1000
Training loss: 6.4158477783203125 / Valid loss: 8.948331437792097
Model is saved in epoch 2, overall batch: 1100
Training loss: 6.2606000900268555 / Valid loss: 8.84548350742885
Model is saved in epoch 2, overall batch: 1200
Training loss: 7.386117935180664 / Valid loss: 8.641068753742037
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.550316333770752 / Valid loss: 8.201050349644252
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.750327110290527 / Valid loss: 8.15429934547061
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.07960844039917 / Valid loss: 7.982819704782395
Model is saved in epoch 3, overall batch: 1600
Training loss: 7.2280683517456055 / Valid loss: 7.532248551504953
Model is saved in epoch 3, overall batch: 1700
Training loss: 5.365880966186523 / Valid loss: 7.4194894109453475
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.925173759460449 / Valid loss: 7.388410441080729
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.307332992553711 / Valid loss: 7.0489781379699705
Model is saved in epoch 4, overall batch: 2000
Training loss: 4.5527753829956055 / Valid loss: 7.245543214253017
Training loss: 4.669344902038574 / Valid loss: 7.059760609127227
Training loss: 5.298105239868164 / Valid loss: 6.809281485421317
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.482419967651367 / Valid loss: 6.70197536604745
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 6.199690341949463 / Valid loss: 6.879143360682896
Training loss: 5.90436315536499 / Valid loss: 6.546163334165301
Model is saved in epoch 5, overall batch: 2600
Training loss: 6.123024940490723 / Valid loss: 6.791870587212699
Training loss: 3.8456273078918457 / Valid loss: 6.72538879939488
Training loss: 6.653940200805664 / Valid loss: 6.507582875660487
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 5.107178688049316 / Valid loss: 6.321098477499826
Model is saved in epoch 6, overall batch: 3000
Training loss: 3.0457191467285156 / Valid loss: 6.577341265905471
Training loss: 5.753632068634033 / Valid loss: 6.419461061840965
Training loss: 4.718350410461426 / Valid loss: 6.446895665214175
Training loss: 5.480368137359619 / Valid loss: 6.290567023413522
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 4.076953887939453 / Valid loss: 6.166208748590378
Model is saved in epoch 7, overall batch: 3500
Training loss: 3.0070300102233887 / Valid loss: 6.143975662049793
Model is saved in epoch 7, overall batch: 3600
Training loss: 4.326049327850342 / Valid loss: 6.272925431387765
Training loss: 5.792790412902832 / Valid loss: 6.494491232009161
Training loss: 4.072945594787598 / Valid loss: 6.568706637337094

Epoch: 8
Training loss: 4.05558967590332 / Valid loss: 6.4189517747788205
Training loss: 4.352888107299805 / Valid loss: 6.403832206271944
Training loss: 3.3718650341033936 / Valid loss: 6.099038539613996
Model is saved in epoch 8, overall batch: 4200
Training loss: 4.417362689971924 / Valid loss: 6.0202129954383485
Model is saved in epoch 8, overall batch: 4300
Training loss: 5.346861839294434 / Valid loss: 6.514694023132324

Epoch: 9
Training loss: 3.8026132583618164 / Valid loss: 6.304783212570917
Training loss: 5.040770530700684 / Valid loss: 6.20329986072722
Training loss: 5.220627784729004 / Valid loss: 5.977275462377639
Model is saved in epoch 9, overall batch: 4700
Training loss: 3.901183843612671 / Valid loss: 6.260566093808129

Epoch: 10
Training loss: 3.7062764167785645 / Valid loss: 6.2037997132255915
Training loss: 3.2748019695281982 / Valid loss: 6.184547749019805
Training loss: 4.695072174072266 / Valid loss: 6.092047641390846
Training loss: 4.142038345336914 / Valid loss: 6.280268171855382
Training loss: 4.7037811279296875 / Valid loss: 6.1553616705394925

Epoch: 11
Training loss: 4.583827018737793 / Valid loss: 6.133587142399379
Training loss: 4.942878723144531 / Valid loss: 6.228100263504755
Training loss: 4.289964199066162 / Valid loss: 6.343065661475772
Training loss: 4.546350479125977 / Valid loss: 6.485937002726963
Training loss: 3.8893330097198486 / Valid loss: 6.433869123458862

Epoch: 12
Training loss: 3.3251736164093018 / Valid loss: 6.422680773053851
Training loss: 4.947084903717041 / Valid loss: 6.294363353365943
Training loss: 4.3351826667785645 / Valid loss: 6.189020722252982
Training loss: 4.236030578613281 / Valid loss: 6.327136155537197
Training loss: 5.347254753112793 / Valid loss: 6.320085432415917

Epoch: 13
Training loss: 3.0994133949279785 / Valid loss: 6.152822726113456
Training loss: 4.2953290939331055 / Valid loss: 6.214527025676909
Training loss: 2.034816265106201 / Valid loss: 6.009234476089477
Training loss: 3.429177761077881 / Valid loss: 6.2825163659595304
Training loss: 3.3560314178466797 / Valid loss: 6.280302401951381

Epoch: 14
Training loss: 3.2871153354644775 / Valid loss: 6.196074347268968
Training loss: 3.157724142074585 / Valid loss: 6.122017973945255
Training loss: 2.4835498332977295 / Valid loss: 6.3743059498923165
Training loss: 3.5977437496185303 / Valid loss: 6.139410877227784
Training loss: 3.3340067863464355 / Valid loss: 6.199421482994443

Epoch: 15
Training loss: 3.9207794666290283 / Valid loss: 6.21509671438308
Training loss: 3.602047920227051 / Valid loss: 6.3103833039601644
Training loss: 3.3402018547058105 / Valid loss: 6.197571061906361
Training loss: 4.1732282638549805 / Valid loss: 6.270469311305455
Training loss: 3.4576759338378906 / Valid loss: 6.1721616563342865

Epoch: 16
Training loss: 4.711172103881836 / Valid loss: 6.159350256692796
Training loss: 3.422896385192871 / Valid loss: 6.222738856361026
Training loss: 3.254316568374634 / Valid loss: 6.207526722408477
Training loss: 2.9669203758239746 / Valid loss: 6.132795635859171
Training loss: 3.6404218673706055 / Valid loss: 6.170021695182437

Epoch: 17
Training loss: 3.010437488555908 / Valid loss: 6.382990403402419
Training loss: 2.8014063835144043 / Valid loss: 6.3290510177612305
Training loss: 4.385115623474121 / Valid loss: 6.495439563478742
Training loss: 3.5435776710510254 / Valid loss: 6.332905846550351
Training loss: 3.387672185897827 / Valid loss: 6.479948670523507

Epoch: 18
Training loss: 3.0461788177490234 / Valid loss: 6.406335812523252
Training loss: 4.057713508605957 / Valid loss: 6.240995021093459
Training loss: 3.1759886741638184 / Valid loss: 6.293910580589658
Training loss: 3.7296621799468994 / Valid loss: 6.129732508886428
Training loss: 3.753300905227661 / Valid loss: 6.3976552758898055

Epoch: 19
Training loss: 3.4761786460876465 / Valid loss: 6.352921712966192
Training loss: 2.439857006072998 / Valid loss: 6.304081362769717
Training loss: 3.066392421722412 / Valid loss: 6.249348592758179
Training loss: 3.039165496826172 / Valid loss: 6.365735546747843

Epoch: 20
Training loss: 2.9105148315429688 / Valid loss: 6.33059937613351
Training loss: 2.705996513366699 / Valid loss: 6.409055891491118
Training loss: 2.954709053039551 / Valid loss: 6.410965939930507
Training loss: 4.3270368576049805 / Valid loss: 6.274397482190813
Training loss: 2.7765283584594727 / Valid loss: 6.3818640095846995

Epoch: 21
Training loss: 2.6432671546936035 / Valid loss: 6.36698568207877
Training loss: 3.0460991859436035 / Valid loss: 6.57709854443868
Training loss: 2.672691583633423 / Valid loss: 6.37582345690046
Training loss: 3.1111321449279785 / Valid loss: 6.416103002003261
Training loss: 3.5809762477874756 / Valid loss: 6.41666765440078

Epoch: 22
Training loss: 3.199974536895752 / Valid loss: 6.530956740606399
Training loss: 2.2983837127685547 / Valid loss: 6.303909235908872
Training loss: 3.5143442153930664 / Valid loss: 6.328238537198021
Training loss: 2.574389696121216 / Valid loss: 6.475447825023106
Training loss: 2.953591823577881 / Valid loss: 6.3748125575837635

Epoch: 23
Training loss: 2.8940296173095703 / Valid loss: 6.358267956688291
Training loss: 2.942434787750244 / Valid loss: 6.548433031354632
Training loss: 3.307419776916504 / Valid loss: 6.505259409404936
Training loss: 3.966064453125 / Valid loss: 6.496108690897624
Training loss: 2.7566938400268555 / Valid loss: 6.3685977731432235

Epoch: 24
Training loss: 2.6902403831481934 / Valid loss: 6.522939271018618
Training loss: 4.334743022918701 / Valid loss: 6.54759018761771
Training loss: 2.3056435585021973 / Valid loss: 6.44618487131028
Training loss: 2.7331037521362305 / Valid loss: 6.50625110807873
Training loss: 2.9250988960266113 / Valid loss: 6.521501091548375

Epoch: 25
Training loss: 2.908693313598633 / Valid loss: 6.421031277520316
Training loss: 2.469942331314087 / Valid loss: 6.476067833673387
Training loss: 2.9219188690185547 / Valid loss: 6.424955138706026
Training loss: 2.614561080932617 / Valid loss: 6.397461334864299
Training loss: 2.584136486053467 / Valid loss: 6.477969555627732

Epoch: 26
Training loss: 2.762722969055176 / Valid loss: 6.4095110393705825
Training loss: 3.3524880409240723 / Valid loss: 6.515466265451341
Training loss: 3.0882301330566406 / Valid loss: 6.590816075461252
Training loss: 2.816793918609619 / Valid loss: 6.687951010749454
Training loss: 3.7239108085632324 / Valid loss: 6.559584249768938

Epoch: 27
Training loss: 3.436530351638794 / Valid loss: 6.542406041281564
Training loss: 2.541200637817383 / Valid loss: 6.452290596280779
Training loss: 2.941258192062378 / Valid loss: 6.444429088774181
Training loss: 2.6833863258361816 / Valid loss: 6.437177144913447
Training loss: 1.495704174041748 / Valid loss: 6.397539370400565

Epoch: 28
Training loss: 2.1773602962493896 / Valid loss: 6.535171910694667
Training loss: 2.2540476322174072 / Valid loss: 6.477545799527849
Training loss: 2.4387927055358887 / Valid loss: 6.450660008475894
Training loss: 1.588200330734253 / Valid loss: 6.657510652996245
Training loss: 3.398740291595459 / Valid loss: 6.577773897988456

Epoch: 29
Training loss: 2.1099095344543457 / Valid loss: 6.5490166982014975
Training loss: 2.2125017642974854 / Valid loss: 6.52668464978536
Training loss: 1.6618552207946777 / Valid loss: 6.618271693729219
Training loss: 4.2709174156188965 / Valid loss: 6.6208594685509095

Epoch: 30
Training loss: 1.867457628250122 / Valid loss: 6.525542820067633
Training loss: 3.3328680992126465 / Valid loss: 6.499105290004185
Training loss: 2.0264062881469727 / Valid loss: 6.794921400433495
Training loss: 1.995208740234375 / Valid loss: 6.531819918042138
Training loss: 2.9404726028442383 / Valid loss: 6.710780690965199

Epoch: 31
Training loss: 1.9838707447052002 / Valid loss: 6.623804187774658
Training loss: 4.376009941101074 / Valid loss: 6.882262545540219
Training loss: 2.5339043140411377 / Valid loss: 6.533879557110015
Training loss: 2.0100908279418945 / Valid loss: 6.545713197617304
Training loss: 2.509157180786133 / Valid loss: 6.723624193100703

Epoch: 32
Training loss: 1.9173997640609741 / Valid loss: 6.541768900553385
Training loss: 2.2786545753479004 / Valid loss: 6.505917549133301
Training loss: 2.650287628173828 / Valid loss: 6.545948900495257
Training loss: 1.9690611362457275 / Valid loss: 6.55668226650783
Training loss: 3.308513641357422 / Valid loss: 6.541249926884969

Epoch: 33
Training loss: 1.9530558586120605 / Valid loss: 6.571221160888672
Training loss: 2.4623780250549316 / Valid loss: 6.658609917050316
Training loss: 2.714895248413086 / Valid loss: 6.793728878384544
Training loss: 2.3882014751434326 / Valid loss: 6.631521368026734
Training loss: 1.4131853580474854 / Valid loss: 6.586081918080648

Epoch: 34
Training loss: 2.89035701751709 / Valid loss: 6.521342293421427
Training loss: 2.8388211727142334 / Valid loss: 6.640383021036784
Training loss: 1.7693336009979248 / Valid loss: 6.796726771763393
Training loss: 1.9101123809814453 / Valid loss: 6.566379463104974
Training loss: 1.886993646621704 / Valid loss: 6.64659548486982

Epoch: 35
Training loss: 2.7792742252349854 / Valid loss: 6.971371319180443
Training loss: 1.5548170804977417 / Valid loss: 6.930515589032854
Training loss: 1.8798646926879883 / Valid loss: 6.693567094348726
Training loss: 2.5838894844055176 / Valid loss: 6.728055590674991
Training loss: 2.495830535888672 / Valid loss: 6.6915949458167665

Epoch: 36
Training loss: 1.5715998411178589 / Valid loss: 6.841131737118675
Training loss: 2.0105791091918945 / Valid loss: 6.796933555603028
Training loss: 2.7285523414611816 / Valid loss: 6.839393182027908
Training loss: 2.9928665161132812 / Valid loss: 6.595377754029774
Training loss: 2.2552926540374756 / Valid loss: 6.862839403606596

Epoch: 37
Training loss: 2.437896728515625 / Valid loss: 6.615261127835228
Training loss: 1.7936416864395142 / Valid loss: 6.699729190553938
Training loss: 2.1711854934692383 / Valid loss: 6.665772778647287
Training loss: 2.1055924892425537 / Valid loss: 6.722544379461379
Training loss: 2.517709255218506 / Valid loss: 6.622157505580357

Epoch: 38
Training loss: 1.5203018188476562 / Valid loss: 6.610787225904919
Training loss: 1.910409688949585 / Valid loss: 6.834189505804153
Training loss: 2.199645519256592 / Valid loss: 6.62884951773144
Training loss: 1.6709034442901611 / Valid loss: 6.746328821636381
Training loss: 3.0342092514038086 / Valid loss: 6.687257635025751

Epoch: 39
Training loss: 2.775885820388794 / Valid loss: 6.863565195174444
Training loss: 1.8416836261749268 / Valid loss: 6.833250456764585
Training loss: 1.9078624248504639 / Valid loss: 7.161894325982956
Training loss: 2.391843318939209 / Valid loss: 6.653560810997373
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 4700): 5.812055001940046
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.974810600280762 / Valid loss: 16.552369571867445
Model is saved in epoch 0, overall batch: 0
Training loss: 8.707889556884766 / Valid loss: 14.460708491007487
Model is saved in epoch 0, overall batch: 100
Training loss: 11.042779922485352 / Valid loss: 13.101473735627675
Model is saved in epoch 0, overall batch: 200
Training loss: 10.774774551391602 / Valid loss: 12.519447531018939
Model is saved in epoch 0, overall batch: 300
Training loss: 8.296807289123535 / Valid loss: 12.279571951003302
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 12.590311050415039 / Valid loss: 11.78845098132179
Model is saved in epoch 1, overall batch: 500
Training loss: 7.950342178344727 / Valid loss: 11.487999679928734
Model is saved in epoch 1, overall batch: 600
Training loss: 7.3256425857543945 / Valid loss: 10.714784894670759
Model is saved in epoch 1, overall batch: 700
Training loss: 5.650008678436279 / Valid loss: 10.596611645108178
Model is saved in epoch 1, overall batch: 800
Training loss: 8.92613697052002 / Valid loss: 9.546946103232248
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.914602279663086 / Valid loss: 9.161828922090077
Model is saved in epoch 2, overall batch: 1000
Training loss: 6.316733360290527 / Valid loss: 8.78993950798398
Model is saved in epoch 2, overall batch: 1100
Training loss: 6.070747375488281 / Valid loss: 8.398796912602016
Model is saved in epoch 2, overall batch: 1200
Training loss: 7.4980316162109375 / Valid loss: 8.402114809127081
Training loss: 5.575911045074463 / Valid loss: 7.832435008457729
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.711507320404053 / Valid loss: 7.692528620220366
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.420430660247803 / Valid loss: 7.56651835214524
Model is saved in epoch 3, overall batch: 1600
Training loss: 7.446071624755859 / Valid loss: 7.129225113278344
Model is saved in epoch 3, overall batch: 1700
Training loss: 5.219752788543701 / Valid loss: 7.062804442360288
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.3660969734191895 / Valid loss: 7.1029695102146695

Epoch: 4
Training loss: 3.4371871948242188 / Valid loss: 6.919262590862456
Model is saved in epoch 4, overall batch: 2000
Training loss: 4.653868675231934 / Valid loss: 6.933352702004569
Training loss: 4.423190116882324 / Valid loss: 6.775892048790341
Model is saved in epoch 4, overall batch: 2200
Training loss: 5.37195348739624 / Valid loss: 6.479388722919283
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.419970512390137 / Valid loss: 6.400881817227318
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 6.078094959259033 / Valid loss: 6.535469849904378
Training loss: 6.118217468261719 / Valid loss: 6.404145585922968
Training loss: 5.451730728149414 / Valid loss: 6.293530003229777
Model is saved in epoch 5, overall batch: 2700
Training loss: 3.7637572288513184 / Valid loss: 6.299473660332816
Training loss: 6.637415409088135 / Valid loss: 6.370346353167579

Epoch: 6
Training loss: 5.352961540222168 / Valid loss: 6.192839091164725
Model is saved in epoch 6, overall batch: 3000
Training loss: 3.3044090270996094 / Valid loss: 6.349528498876662
Training loss: 5.663103103637695 / Valid loss: 6.188243366423107
Model is saved in epoch 6, overall batch: 3200
Training loss: 4.652130126953125 / Valid loss: 6.225884087880453
Training loss: 5.604889869689941 / Valid loss: 6.178103013265701
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 4.329785346984863 / Valid loss: 5.98281022480556
Model is saved in epoch 7, overall batch: 3500
Training loss: 2.7123255729675293 / Valid loss: 5.924131545566377
Model is saved in epoch 7, overall batch: 3600
Training loss: 3.821033239364624 / Valid loss: 5.872555955251058
Model is saved in epoch 7, overall batch: 3700
Training loss: 5.691341400146484 / Valid loss: 6.182291775658017
Training loss: 4.236863136291504 / Valid loss: 6.36239530926659

Epoch: 8
Training loss: 4.03973388671875 / Valid loss: 6.1577856041136245
Training loss: 4.428897380828857 / Valid loss: 6.364248938787551
Training loss: 3.3270459175109863 / Valid loss: 5.944466322944278
Training loss: 4.581570625305176 / Valid loss: 5.905164916174752
Training loss: 5.125185012817383 / Valid loss: 6.099470974150158

Epoch: 9
Training loss: 3.347670078277588 / Valid loss: 6.203627193541754
Training loss: 5.579195976257324 / Valid loss: 6.2531348387400305
Training loss: 4.91154670715332 / Valid loss: 6.123419389270601
Training loss: 3.780870199203491 / Valid loss: 6.2956660316103985

Epoch: 10
Training loss: 3.745436429977417 / Valid loss: 6.225443092981974
Training loss: 3.2058210372924805 / Valid loss: 6.048047771907988
Training loss: 5.089807510375977 / Valid loss: 6.067546624229068
Training loss: 3.6125547885894775 / Valid loss: 6.241335153579712
Training loss: 4.791022777557373 / Valid loss: 6.109609855924334

Epoch: 11
Training loss: 3.943895101547241 / Valid loss: 6.0003842217581616
Training loss: 4.123459815979004 / Valid loss: 6.0576596941266745
Training loss: 4.424143314361572 / Valid loss: 6.302247565133231
Training loss: 4.351879119873047 / Valid loss: 6.117789170855567
Training loss: 3.9067370891571045 / Valid loss: 6.031232143583752

Epoch: 12
Training loss: 4.0358686447143555 / Valid loss: 6.176133505503336
Training loss: 4.587762832641602 / Valid loss: 6.145347147896176
Training loss: 4.331214904785156 / Valid loss: 6.376840339388166
Training loss: 3.767732858657837 / Valid loss: 6.43914855775379
Training loss: 4.568564414978027 / Valid loss: 6.141239599954514

Epoch: 13
Training loss: 4.061190605163574 / Valid loss: 6.092191387358166
Training loss: 3.9032607078552246 / Valid loss: 6.1114232608250205
Training loss: 2.5895142555236816 / Valid loss: 6.1290577684129985
Training loss: 3.773637294769287 / Valid loss: 6.157378514607747
Training loss: 3.8950157165527344 / Valid loss: 6.309594077155704

Epoch: 14
Training loss: 3.390845537185669 / Valid loss: 6.160838728859311
Training loss: 2.970276117324829 / Valid loss: 6.105077684493292
Training loss: 2.854248523712158 / Valid loss: 6.463482561565581
Training loss: 3.4497461318969727 / Valid loss: 6.0897888070061095
Training loss: 3.6707401275634766 / Valid loss: 6.082696835199992

Epoch: 15
Training loss: 4.066150665283203 / Valid loss: 6.229313028426398
Training loss: 4.141180992126465 / Valid loss: 6.269577189854213
Training loss: 3.5793824195861816 / Valid loss: 6.23596981820606
Training loss: 3.520214557647705 / Valid loss: 6.179595976784116
Training loss: 3.4186038970947266 / Valid loss: 6.2499922479902

Epoch: 16
Training loss: 4.688672065734863 / Valid loss: 6.190205680756342
Training loss: 3.1307756900787354 / Valid loss: 6.198679469880604
Training loss: 3.7701308727264404 / Valid loss: 6.140914762587775
Training loss: 3.085020065307617 / Valid loss: 6.26811953726269
Training loss: 3.990222454071045 / Valid loss: 6.262054525102887

Epoch: 17
Training loss: 2.6818923950195312 / Valid loss: 6.4875619525001165
Training loss: 3.0801122188568115 / Valid loss: 6.1543329761141825
Training loss: 3.1337742805480957 / Valid loss: 6.405435841424125
Training loss: 3.52040433883667 / Valid loss: 6.256193226859683
Training loss: 3.9009628295898438 / Valid loss: 6.354850235439482

Epoch: 18
Training loss: 2.811091423034668 / Valid loss: 6.242488461449033
Training loss: 3.8893847465515137 / Valid loss: 6.216535064152309
Training loss: 2.323755979537964 / Valid loss: 6.132240374883016
Training loss: 3.5490450859069824 / Valid loss: 6.186054247901553
Training loss: 4.112862586975098 / Valid loss: 6.353633792059762

Epoch: 19
Training loss: 3.3204498291015625 / Valid loss: 6.28774429275876
Training loss: 3.270214557647705 / Valid loss: 6.313039809181577
Training loss: 2.795241355895996 / Valid loss: 6.256120127723331
Training loss: 2.979475259780884 / Valid loss: 6.34771272114345

Epoch: 20
Training loss: 3.1151955127716064 / Valid loss: 6.366818941207159
Training loss: 3.065889358520508 / Valid loss: 6.3299475806100025
Training loss: 2.78836727142334 / Valid loss: 6.369737834022159
Training loss: 3.975979804992676 / Valid loss: 6.277899238041469
Training loss: 2.4712305068969727 / Valid loss: 6.339984139941987

Epoch: 21
Training loss: 3.5439226627349854 / Valid loss: 6.359831562496367
Training loss: 2.846477746963501 / Valid loss: 6.707509086245582
Training loss: 2.958052635192871 / Valid loss: 6.489819358644032
Training loss: 2.926248788833618 / Valid loss: 6.3524065085819785
Training loss: 3.0031208992004395 / Valid loss: 6.339409984861102

Epoch: 22
Training loss: 2.9467663764953613 / Valid loss: 6.442695404234387
Training loss: 2.8467564582824707 / Valid loss: 6.354735360826765
Training loss: 2.582669258117676 / Valid loss: 6.362636543455578
Training loss: 2.505396842956543 / Valid loss: 6.468248603457496
Training loss: 3.307734251022339 / Valid loss: 6.380603474662418

Epoch: 23
Training loss: 2.47422194480896 / Valid loss: 6.414158848353795
Training loss: 3.363539218902588 / Valid loss: 6.501270207904634
Training loss: 3.8754851818084717 / Valid loss: 6.424981303442092
Training loss: 3.6688833236694336 / Valid loss: 6.378084446135022
Training loss: 3.218923807144165 / Valid loss: 6.379267049971081

Epoch: 24
Training loss: 3.4250805377960205 / Valid loss: 6.499880981445313
Training loss: 3.7328648567199707 / Valid loss: 6.573012526830038
Training loss: 2.140686511993408 / Valid loss: 6.361314410255069
Training loss: 3.324286460876465 / Valid loss: 6.535518178485689
Training loss: 2.9441184997558594 / Valid loss: 6.488043003990537

Epoch: 25
Training loss: 2.5472137928009033 / Valid loss: 6.540181536901565
Training loss: 2.644993305206299 / Valid loss: 6.764968838010516
Training loss: 2.6056559085845947 / Valid loss: 6.521359514054798
Training loss: 2.7862160205841064 / Valid loss: 6.533820758547102
Training loss: 2.559206008911133 / Valid loss: 6.420223374593825

Epoch: 26
Training loss: 2.6322524547576904 / Valid loss: 6.4200936135791595
Training loss: 3.0698423385620117 / Valid loss: 6.5217074689410985
Training loss: 3.430163621902466 / Valid loss: 6.453341715676444
Training loss: 3.034821033477783 / Valid loss: 6.515008796964373
Training loss: 3.1063356399536133 / Valid loss: 6.97014710789635

Epoch: 27
Training loss: 2.955613851547241 / Valid loss: 6.521930967058454
Training loss: 2.495922565460205 / Valid loss: 6.579936195555187
Training loss: 3.8977746963500977 / Valid loss: 6.65411400113787
Training loss: 2.927271842956543 / Valid loss: 6.532746251424154
Training loss: 1.8051294088363647 / Valid loss: 6.47120171501523

Epoch: 28
Training loss: 1.7849241495132446 / Valid loss: 6.457031104678199
Training loss: 2.9026811122894287 / Valid loss: 6.534122744060698
Training loss: 2.7089133262634277 / Valid loss: 6.511572980880738
Training loss: 1.7367826700210571 / Valid loss: 6.564100074768066
Training loss: 2.6985857486724854 / Valid loss: 6.594795224780128

Epoch: 29
Training loss: 2.616645336151123 / Valid loss: 6.54636538369315
Training loss: 1.918329119682312 / Valid loss: 6.485649331410726
Training loss: 2.3447835445404053 / Valid loss: 6.54831606774103
Training loss: 3.5546061992645264 / Valid loss: 6.8080213841937836

Epoch: 30
Training loss: 2.2780673503875732 / Valid loss: 6.5309799966358
Training loss: 2.0976104736328125 / Valid loss: 6.648607042857579
Training loss: 2.5105342864990234 / Valid loss: 6.727299735659645
Training loss: 2.648385524749756 / Valid loss: 6.583667276019142
Training loss: 3.189101219177246 / Valid loss: 6.695505821137202

Epoch: 31
Training loss: 2.1877143383026123 / Valid loss: 6.5974960690452935
Training loss: 4.309192657470703 / Valid loss: 6.923228400094168
Training loss: 2.6127476692199707 / Valid loss: 6.546850352060227
Training loss: 2.1553401947021484 / Valid loss: 6.535482674553281
Training loss: 2.378506660461426 / Valid loss: 6.844359027771723

Epoch: 32
Training loss: 1.7081596851348877 / Valid loss: 6.6482525780087425
Training loss: 1.976332426071167 / Valid loss: 6.634994767961048
Training loss: 1.847900390625 / Valid loss: 6.663324306124733
Training loss: 1.9501222372055054 / Valid loss: 6.684826535270328
Training loss: 2.430209159851074 / Valid loss: 6.608835633595785

Epoch: 33
Training loss: 2.6696152687072754 / Valid loss: 6.676922671000162
Training loss: 2.505021572113037 / Valid loss: 6.759893689836774
Training loss: 2.148618698120117 / Valid loss: 6.789099666050502
Training loss: 2.1543540954589844 / Valid loss: 6.675850418635777
Training loss: 1.7352347373962402 / Valid loss: 6.800111107599168

Epoch: 34
Training loss: 2.585118293762207 / Valid loss: 6.717469714936756
Training loss: 1.9464260339736938 / Valid loss: 6.716765222095308
Training loss: 3.0227880477905273 / Valid loss: 6.862506021772112
Training loss: 1.8821406364440918 / Valid loss: 6.69130125499907
Training loss: 2.1931567192077637 / Valid loss: 6.815370995657784

Epoch: 35
Training loss: 2.8168492317199707 / Valid loss: 6.794734235036941
Training loss: 1.6959773302078247 / Valid loss: 6.7078682468051
Training loss: 2.0679359436035156 / Valid loss: 6.745026738303048
Training loss: 1.8129535913467407 / Valid loss: 6.842005130222866
Training loss: 2.272477149963379 / Valid loss: 6.7322535287766225

Epoch: 36
Training loss: 1.6956254243850708 / Valid loss: 6.85287189029512
Training loss: 2.282928943634033 / Valid loss: 6.892512366885231
Training loss: 2.018312931060791 / Valid loss: 6.779949029286702
Training loss: 3.9493205547332764 / Valid loss: 6.736956049147106
Training loss: 1.7407948970794678 / Valid loss: 6.705288046882266

Epoch: 37
Training loss: 1.8497140407562256 / Valid loss: 6.68571187655131
Training loss: 1.879610538482666 / Valid loss: 6.772492174875168
Training loss: 1.6622142791748047 / Valid loss: 6.631539294833229
Training loss: 2.3621859550476074 / Valid loss: 6.713923583711897
Training loss: 2.4447147846221924 / Valid loss: 6.835279510134742

Epoch: 38
Training loss: 1.771315574645996 / Valid loss: 6.719412730989002
Training loss: 2.5848870277404785 / Valid loss: 6.779226820809501
Training loss: 2.3466925621032715 / Valid loss: 6.849966244470505
Training loss: 1.7591485977172852 / Valid loss: 6.762403199786232
Training loss: 3.091240167617798 / Valid loss: 6.728639091764178

Epoch: 39
Training loss: 2.400034189224243 / Valid loss: 6.7549871126810705
Training loss: 1.6511671543121338 / Valid loss: 6.740812288011823
Training loss: 1.7486194372177124 / Valid loss: 6.857661142803374
Training loss: 2.249389886856079 / Valid loss: 6.760822209857759
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 3700): 5.678481601533436
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.614124298095703 / Valid loss: 16.087527465820312
Model is saved in epoch 0, overall batch: 0
Training loss: 12.056886672973633 / Valid loss: 11.049335847582135
Model is saved in epoch 0, overall batch: 100
Training loss: 8.667441368103027 / Valid loss: 8.469164530436197
Model is saved in epoch 0, overall batch: 200
Training loss: 6.610881805419922 / Valid loss: 6.665668719155448
Model is saved in epoch 0, overall batch: 300
Training loss: 9.516958236694336 / Valid loss: 5.916311759040469
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.306789398193359 / Valid loss: 6.171028961454119
Training loss: 5.92024564743042 / Valid loss: 6.099041255315145
Training loss: 7.016142845153809 / Valid loss: 5.693788092476981
Model is saved in epoch 1, overall batch: 700
Training loss: 6.178069114685059 / Valid loss: 5.6127303759256995
Model is saved in epoch 1, overall batch: 800
Training loss: 5.065384864807129 / Valid loss: 5.838525438308716

Epoch: 2
Training loss: 5.029738426208496 / Valid loss: 5.89975457645598
Training loss: 4.650729656219482 / Valid loss: 5.641019825708298
Training loss: 5.2907867431640625 / Valid loss: 5.643225747063046
Training loss: 5.682273864746094 / Valid loss: 5.837302737008958
Training loss: 5.9404191970825195 / Valid loss: 5.866804204668317

Epoch: 3
Training loss: 3.7172226905822754 / Valid loss: 5.613911163239252
Training loss: 5.067708969116211 / Valid loss: 5.8510207789284845
Training loss: 4.3189778327941895 / Valid loss: 5.694520403089977
Training loss: 5.8950090408325195 / Valid loss: 5.576819170088995
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.796900749206543 / Valid loss: 5.673967186609904

Epoch: 4
Training loss: 3.578733205795288 / Valid loss: 5.850026114781698
Training loss: 4.449871063232422 / Valid loss: 5.625404936926706
Training loss: 4.304699897766113 / Valid loss: 5.651037386485508
Training loss: 4.99348258972168 / Valid loss: 5.557527669270834
Model is saved in epoch 4, overall batch: 2300
Training loss: 7.334284782409668 / Valid loss: 6.18645643279666

Epoch: 5
Training loss: 3.65525484085083 / Valid loss: 5.855544058481852
Training loss: 3.813103199005127 / Valid loss: 5.67670791262672
Training loss: 5.6100664138793945 / Valid loss: 5.755421195711408
Training loss: 5.900929927825928 / Valid loss: 5.884152755283174
Training loss: 7.032174587249756 / Valid loss: 6.4192167736235115

Epoch: 6
Training loss: 4.346477508544922 / Valid loss: 5.7760399773007345
Training loss: 3.526968479156494 / Valid loss: 5.851601884478614
Training loss: 4.669956207275391 / Valid loss: 5.748885209219797
Training loss: 3.571530342102051 / Valid loss: 5.6963978858221145
Training loss: 3.531813144683838 / Valid loss: 5.902502448218209

Epoch: 7
Training loss: 2.7362918853759766 / Valid loss: 6.428656360081264
Training loss: 3.9492926597595215 / Valid loss: 5.774646495637439
Training loss: 3.8594858646392822 / Valid loss: 5.825045206433251
Training loss: 4.7477922439575195 / Valid loss: 5.754758644104004
Training loss: 4.9118733406066895 / Valid loss: 5.7589592910948255

Epoch: 8
Training loss: 3.8961424827575684 / Valid loss: 5.852104032607306
Training loss: 3.2708544731140137 / Valid loss: 6.347462329410371
Training loss: 4.78618860244751 / Valid loss: 6.065475622812907
Training loss: 4.281925678253174 / Valid loss: 5.9268684001195995
Training loss: 4.017787933349609 / Valid loss: 5.745989295414516

Epoch: 9
Training loss: 3.5218849182128906 / Valid loss: 5.9354373000916985
Training loss: 4.133781433105469 / Valid loss: 5.840275507881528
Training loss: 3.7607765197753906 / Valid loss: 5.894435210455032
Training loss: 2.7901885509490967 / Valid loss: 6.266057118915376

Epoch: 10
Training loss: 3.123288154602051 / Valid loss: 5.837454175949096
Training loss: 3.0704755783081055 / Valid loss: 6.339847678229923
Training loss: 3.8336050510406494 / Valid loss: 5.97092265628633
Training loss: 3.833310604095459 / Valid loss: 6.114112106959025
Training loss: 3.4590981006622314 / Valid loss: 6.1402721632094615

Epoch: 11
Training loss: 3.929499626159668 / Valid loss: 5.962529200599307
Training loss: 4.145807266235352 / Valid loss: 6.185768740517752
Training loss: 3.3440308570861816 / Valid loss: 6.127988729022798
Training loss: 3.4584312438964844 / Valid loss: 5.949155691691807
Training loss: 3.279621124267578 / Valid loss: 6.039502884092785

Epoch: 12
Training loss: 3.621222972869873 / Valid loss: 5.883689251400176
Training loss: 3.0394716262817383 / Valid loss: 6.0771458557673865
Training loss: 4.1325178146362305 / Valid loss: 6.034179573967343
Training loss: 3.0040807723999023 / Valid loss: 6.0232042221795945
Training loss: 2.854198455810547 / Valid loss: 5.969091953550066

Epoch: 13
Training loss: 3.104808807373047 / Valid loss: 6.00121792838687
Training loss: 3.1830027103424072 / Valid loss: 6.085322911398752
Training loss: 2.393218994140625 / Valid loss: 6.331732777186803
Training loss: 2.426344871520996 / Valid loss: 6.523861126672654
Training loss: 2.9479174613952637 / Valid loss: 6.03162906283424

Epoch: 14
Training loss: 2.592374086380005 / Valid loss: 6.2103350048973445
Training loss: 2.8661532402038574 / Valid loss: 6.2178531328837074
Training loss: 4.136375427246094 / Valid loss: 6.1919983455113
Training loss: 3.5533032417297363 / Valid loss: 6.112909205754598
Training loss: 3.2784080505371094 / Valid loss: 6.174344260351998

Epoch: 15
Training loss: 2.2283263206481934 / Valid loss: 6.2234633241380966
Training loss: 3.9181599617004395 / Valid loss: 6.16379246030535
Training loss: 2.9330124855041504 / Valid loss: 6.204030363900321
Training loss: 3.5829243659973145 / Valid loss: 6.221891802833194
Training loss: 2.830202102661133 / Valid loss: 6.508477942148844

Epoch: 16
Training loss: 3.140605926513672 / Valid loss: 6.121224003746396
Training loss: 2.5654773712158203 / Valid loss: 6.344993275687808
Training loss: 1.9545323848724365 / Valid loss: 6.27119524592445
Training loss: 3.373751401901245 / Valid loss: 6.221555192129952
Training loss: 2.3811697959899902 / Valid loss: 6.309550580524263

Epoch: 17
Training loss: 2.878382921218872 / Valid loss: 6.360663695562454
Training loss: 2.32912540435791 / Valid loss: 6.282362084161668
Training loss: 2.1136627197265625 / Valid loss: 6.2921935853504
Training loss: 4.777216911315918 / Valid loss: 6.334358244850522
Training loss: 2.569046974182129 / Valid loss: 6.368570541200183

Epoch: 18
Training loss: 2.5781898498535156 / Valid loss: 6.755972489856538
Training loss: 1.826706886291504 / Valid loss: 6.368795363108317
Training loss: 2.420140266418457 / Valid loss: 6.510659726460775
Training loss: 2.30763578414917 / Valid loss: 6.294812075297037
Training loss: 3.3302817344665527 / Valid loss: 6.333620545977638

Epoch: 19
Training loss: 1.5976516008377075 / Valid loss: 6.48576211702256
Training loss: 2.5901286602020264 / Valid loss: 6.344803862344651
Training loss: 2.7897462844848633 / Valid loss: 6.393016188485282
Training loss: 2.650970935821533 / Valid loss: 6.467675265811739

Epoch: 20
Training loss: 2.00433611869812 / Valid loss: 7.142515575318109
Training loss: 1.938186526298523 / Valid loss: 6.429946910767328
Training loss: 2.2098238468170166 / Valid loss: 6.404380519049508
Training loss: 1.7910661697387695 / Valid loss: 6.376403633753458
Training loss: 1.450421929359436 / Valid loss: 6.500114286513556

Epoch: 21
Training loss: 2.1653475761413574 / Valid loss: 6.409874153137207
Training loss: 2.7698276042938232 / Valid loss: 6.518984290531703
Training loss: 2.479228973388672 / Valid loss: 6.54518989381336
Training loss: 2.9854793548583984 / Valid loss: 6.5308074996584935
Training loss: 1.862579345703125 / Valid loss: 6.49401243300665

Epoch: 22
Training loss: 1.4180105924606323 / Valid loss: 6.725166348048619
Training loss: 2.1364188194274902 / Valid loss: 6.5402091276078
Training loss: 1.9864726066589355 / Valid loss: 6.550913542792911
Training loss: 2.9977684020996094 / Valid loss: 6.50169708842323
Training loss: 2.177574396133423 / Valid loss: 6.517995225815546

Epoch: 23
Training loss: 1.2385241985321045 / Valid loss: 6.65178043728783
Training loss: 2.418281078338623 / Valid loss: 6.5899567149934315
Training loss: 2.4094910621643066 / Valid loss: 6.651432768503825
Training loss: 1.8574614524841309 / Valid loss: 7.40972170148577
Training loss: 1.8716758489608765 / Valid loss: 6.89384332384382

Epoch: 24
Training loss: 1.1766917705535889 / Valid loss: 6.501373933610462
Training loss: 1.977463960647583 / Valid loss: 6.560044549760365
Training loss: 2.777031898498535 / Valid loss: 6.562848626999628
Training loss: 1.7223939895629883 / Valid loss: 6.6148560501280285
Training loss: 2.08278226852417 / Valid loss: 6.711707719167074

Epoch: 25
Training loss: 1.866480827331543 / Valid loss: 6.5879603862762455
Training loss: 1.4612224102020264 / Valid loss: 6.671414007459368
Training loss: 2.2813661098480225 / Valid loss: 6.713812707719349
Training loss: 1.694542646408081 / Valid loss: 6.658231101717267
Training loss: 1.97568941116333 / Valid loss: 6.611661261603945

Epoch: 26
Training loss: 1.4539821147918701 / Valid loss: 6.506501490729196
Training loss: 1.328029751777649 / Valid loss: 6.755383825302124
Training loss: 1.615605354309082 / Valid loss: 6.662225568862189
Training loss: 1.5100882053375244 / Valid loss: 6.918625970113845
Training loss: 1.3746516704559326 / Valid loss: 6.743813060578846

Epoch: 27
Training loss: 1.6763134002685547 / Valid loss: 6.618010243915376
Training loss: 1.4645332098007202 / Valid loss: 7.157257970174154
Training loss: 1.8803110122680664 / Valid loss: 6.967989553724016
Training loss: 1.5002737045288086 / Valid loss: 6.588781997135707
Training loss: 2.457390069961548 / Valid loss: 6.8429658980596635

Epoch: 28
Training loss: 1.465237021446228 / Valid loss: 6.785743474960327
Training loss: 1.308342456817627 / Valid loss: 6.625928065890357
Training loss: 1.4695732593536377 / Valid loss: 6.696140947796049
Training loss: 2.150665283203125 / Valid loss: 6.798300450188773
Training loss: 1.5373048782348633 / Valid loss: 6.7029511224655876

Epoch: 29
Training loss: 1.0485671758651733 / Valid loss: 6.856891268775577
Training loss: 1.3392798900604248 / Valid loss: 7.113762224288214
Training loss: 1.8250758647918701 / Valid loss: 6.887157939729237
Training loss: 1.7294700145721436 / Valid loss: 6.68885509627206

Epoch: 30
Training loss: 1.4595952033996582 / Valid loss: 6.7571760858808245
Training loss: 1.5041697025299072 / Valid loss: 7.268757583981468
Training loss: 1.4500443935394287 / Valid loss: 8.325714106786819
Training loss: 1.338127851486206 / Valid loss: 7.485628654843285
Training loss: 1.4042515754699707 / Valid loss: 7.554272824242002

Epoch: 31
Training loss: 1.5080387592315674 / Valid loss: 6.668589115142822
Training loss: 1.3407613039016724 / Valid loss: 7.18814848718189
Training loss: 1.223374605178833 / Valid loss: 6.883691440309797
Training loss: 1.6795482635498047 / Valid loss: 6.757362567810786
Training loss: 1.6003329753875732 / Valid loss: 6.74280302411034

Epoch: 32
Training loss: 1.0312519073486328 / Valid loss: 6.987279456002372
Training loss: 1.3138964176177979 / Valid loss: 6.834060846056257
Training loss: 0.9696880578994751 / Valid loss: 6.931054939542498
Training loss: 1.6168485879898071 / Valid loss: 6.717241741362072
Training loss: 1.4054834842681885 / Valid loss: 6.913948685782296

Epoch: 33
Training loss: 0.8537076711654663 / Valid loss: 7.064179924556187
Training loss: 1.2497669458389282 / Valid loss: 6.786785507202149
Training loss: 1.5660241842269897 / Valid loss: 6.876961642219907
Training loss: 1.1233168840408325 / Valid loss: 6.8926937216804145
Training loss: 1.5230450630187988 / Valid loss: 6.882311961764381

Epoch: 34
Training loss: 1.2574561834335327 / Valid loss: 6.797718166169666
Training loss: 2.07930326461792 / Valid loss: 6.870088352475848
Training loss: 1.1239830255508423 / Valid loss: 6.787961056118919
Training loss: 1.7423996925354004 / Valid loss: 6.993563870021275
Training loss: 1.1535000801086426 / Valid loss: 6.907179600851876

Epoch: 35
Training loss: 1.477230191230774 / Valid loss: 7.18233912785848
Training loss: 1.524327278137207 / Valid loss: 6.926947051002866
Training loss: 1.6172294616699219 / Valid loss: 6.726479146594093
Training loss: 1.4078112840652466 / Valid loss: 6.740441122509185
Training loss: 1.493778944015503 / Valid loss: 6.930264318557013

Epoch: 36
Training loss: 1.5670979022979736 / Valid loss: 6.797344743637812
Training loss: 1.2248088121414185 / Valid loss: 7.196190125601632
Training loss: 1.204721212387085 / Valid loss: 6.843828691755022
Training loss: 1.2968089580535889 / Valid loss: 6.898756821950276
Training loss: 1.2068400382995605 / Valid loss: 7.008185631888253

Epoch: 37
Training loss: 1.6478290557861328 / Valid loss: 6.912013880411783
Training loss: 1.1548361778259277 / Valid loss: 6.9299376079014365
Training loss: 1.3905116319656372 / Valid loss: 6.887904139927455
Training loss: 1.1492000818252563 / Valid loss: 6.947041538783482
Training loss: 1.4742522239685059 / Valid loss: 7.003111196699597

Epoch: 38
Training loss: 1.2437067031860352 / Valid loss: 7.352901767549061
Training loss: 1.6764471530914307 / Valid loss: 6.932950201488676
Training loss: 1.2336440086364746 / Valid loss: 6.921104944319952
Training loss: 1.3183215856552124 / Valid loss: 7.144209575653076
Training loss: 1.578550934791565 / Valid loss: 7.20425248827253

Epoch: 39
Training loss: 1.0405535697937012 / Valid loss: 6.879515230088007
Training loss: 1.248581886291504 / Valid loss: 6.884702587127686
Training loss: 1.2284834384918213 / Valid loss: 7.222005707877023
Training loss: 1.1800615787506104 / Valid loss: 7.056732218606132
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2300): 5.440228291920254
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.614124298095703 / Valid loss: 16.087522606622606
Model is saved in epoch 0, overall batch: 0
Training loss: 12.071290969848633 / Valid loss: 11.133578641074044
Model is saved in epoch 0, overall batch: 100
Training loss: 8.667407989501953 / Valid loss: 8.524289231073288
Model is saved in epoch 0, overall batch: 200
Training loss: 6.58332633972168 / Valid loss: 6.679825108391898
Model is saved in epoch 0, overall batch: 300
Training loss: 9.505362510681152 / Valid loss: 5.900471185502552
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.304500579833984 / Valid loss: 6.210350590660458
Training loss: 5.942714214324951 / Valid loss: 6.081588572547549
Training loss: 6.972598075866699 / Valid loss: 5.683581147875104
Model is saved in epoch 1, overall batch: 700
Training loss: 6.0147223472595215 / Valid loss: 5.602641039802915
Model is saved in epoch 1, overall batch: 800
Training loss: 5.122608184814453 / Valid loss: 5.85454777535938

Epoch: 2
Training loss: 5.032231330871582 / Valid loss: 5.856461713427589
Training loss: 4.6706366539001465 / Valid loss: 5.613299558276222
Training loss: 5.376047611236572 / Valid loss: 5.6438181150527225
Training loss: 5.740701198577881 / Valid loss: 5.773166788192023
Training loss: 5.906232833862305 / Valid loss: 5.884750827153524

Epoch: 3
Training loss: 3.703645944595337 / Valid loss: 5.63907196181161
Training loss: 4.984380722045898 / Valid loss: 5.908667718796503
Training loss: 4.342468738555908 / Valid loss: 5.678438967750186
Training loss: 5.8961029052734375 / Valid loss: 5.591167613438198
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.792173385620117 / Valid loss: 5.676646291641962

Epoch: 4
Training loss: 3.4791336059570312 / Valid loss: 5.832518014453706
Training loss: 4.431839942932129 / Valid loss: 5.653993556613014
Training loss: 4.264151573181152 / Valid loss: 5.621275195621309
Training loss: 4.999271392822266 / Valid loss: 5.561749390193394
Model is saved in epoch 4, overall batch: 2300
Training loss: 7.24813175201416 / Valid loss: 6.1903331733885265

Epoch: 5
Training loss: 3.72489857673645 / Valid loss: 5.822183493205479
Training loss: 3.8471274375915527 / Valid loss: 5.735672789528256
Training loss: 5.572244644165039 / Valid loss: 5.757732028052921
Training loss: 5.985452651977539 / Valid loss: 5.88185175259908
Training loss: 7.114444732666016 / Valid loss: 6.47334900810605

Epoch: 6
Training loss: 4.410955429077148 / Valid loss: 5.755396970113119
Training loss: 3.532196283340454 / Valid loss: 5.862216815494356
Training loss: 4.532046318054199 / Valid loss: 5.729771384738741
Training loss: 3.6852195262908936 / Valid loss: 5.691962101345971
Training loss: 3.47744083404541 / Valid loss: 5.892813178471156

Epoch: 7
Training loss: 2.7762696743011475 / Valid loss: 6.3861676193418955
Training loss: 3.846487045288086 / Valid loss: 5.7909658318474175
Training loss: 3.90445876121521 / Valid loss: 5.807844320933024
Training loss: 4.63758659362793 / Valid loss: 5.745282089142572
Training loss: 5.029352188110352 / Valid loss: 5.76314940679641

Epoch: 8
Training loss: 3.8456430435180664 / Valid loss: 5.842270372027443
Training loss: 3.278148651123047 / Valid loss: 6.434168777011689
Training loss: 4.838931083679199 / Valid loss: 5.978652066276187
Training loss: 4.332193374633789 / Valid loss: 5.940128726050967
Training loss: 3.9573702812194824 / Valid loss: 5.753555865514846

Epoch: 9
Training loss: 3.4715895652770996 / Valid loss: 5.879982932408651
Training loss: 4.2776570320129395 / Valid loss: 5.853138694309053
Training loss: 3.7419252395629883 / Valid loss: 5.876702235993885
Training loss: 2.8077754974365234 / Valid loss: 6.290147670110067

Epoch: 10
Training loss: 3.043056011199951 / Valid loss: 5.864216961179461
Training loss: 3.0261712074279785 / Valid loss: 6.287557431629726
Training loss: 3.62774658203125 / Valid loss: 6.001371624356224
Training loss: 3.9524927139282227 / Valid loss: 5.996181808199201
Training loss: 3.4230613708496094 / Valid loss: 6.039452650433495

Epoch: 11
Training loss: 4.061244010925293 / Valid loss: 5.938161457152594
Training loss: 4.268978118896484 / Valid loss: 6.118732756660098
Training loss: 3.3553175926208496 / Valid loss: 6.192973459334601
Training loss: 3.408574342727661 / Valid loss: 5.964383627119519
Training loss: 3.2942991256713867 / Valid loss: 5.971445235751924

Epoch: 12
Training loss: 3.844095230102539 / Valid loss: 5.862830470857166
Training loss: 3.061495304107666 / Valid loss: 6.042269704455421
Training loss: 4.14484977722168 / Valid loss: 6.0071193331763855
Training loss: 3.003373146057129 / Valid loss: 5.995471311750866
Training loss: 2.787611246109009 / Valid loss: 5.957391277949015

Epoch: 13
Training loss: 3.1420397758483887 / Valid loss: 5.994767075493222
Training loss: 3.2997994422912598 / Valid loss: 6.039774501891364
Training loss: 2.590689182281494 / Valid loss: 6.277932271503267
Training loss: 2.275003433227539 / Valid loss: 6.4510944457281205
Training loss: 2.76580810546875 / Valid loss: 5.976442064557757

Epoch: 14
Training loss: 2.5141351222991943 / Valid loss: 6.254658131372361
Training loss: 3.057593822479248 / Valid loss: 6.214368218467349
Training loss: 4.077588081359863 / Valid loss: 6.163815452938988
Training loss: 3.608208656311035 / Valid loss: 6.120308199382964
Training loss: 3.179743766784668 / Valid loss: 6.138714947019305

Epoch: 15
Training loss: 2.0416390895843506 / Valid loss: 6.250296849296206
Training loss: 3.871297836303711 / Valid loss: 6.135744401386806
Training loss: 3.102846384048462 / Valid loss: 6.218904613313221
Training loss: 3.6060948371887207 / Valid loss: 6.188917648224604
Training loss: 2.726470947265625 / Valid loss: 6.471617784954253

Epoch: 16
Training loss: 2.983205795288086 / Valid loss: 6.106337690353394
Training loss: 2.6645395755767822 / Valid loss: 6.326631471088954
Training loss: 1.9112510681152344 / Valid loss: 6.239782996404738
Training loss: 3.291612148284912 / Valid loss: 6.218755460920788
Training loss: 2.492307186126709 / Valid loss: 6.256006197702317

Epoch: 17
Training loss: 2.6993892192840576 / Valid loss: 6.39477371715364
Training loss: 2.2808730602264404 / Valid loss: 6.260981482551212
Training loss: 2.1303560733795166 / Valid loss: 6.221762943267822
Training loss: 4.987066268920898 / Valid loss: 6.337104243323917
Training loss: 2.5193793773651123 / Valid loss: 6.254741809481666

Epoch: 18
Training loss: 2.801293134689331 / Valid loss: 6.548689978463309
Training loss: 1.8701677322387695 / Valid loss: 6.365664618355887
Training loss: 2.4555165767669678 / Valid loss: 6.543330651237851
Training loss: 2.350219488143921 / Valid loss: 6.263229047684442
Training loss: 3.2330055236816406 / Valid loss: 6.366013917468843

Epoch: 19
Training loss: 1.5313806533813477 / Valid loss: 6.466023295266288
Training loss: 2.691288709640503 / Valid loss: 6.330921836126418
Training loss: 2.86899471282959 / Valid loss: 6.34434092839559
Training loss: 2.6057288646698 / Valid loss: 6.375988222303844

Epoch: 20
Training loss: 2.035667657852173 / Valid loss: 7.681554926009405
Training loss: 2.082101345062256 / Valid loss: 6.390733051300049
Training loss: 2.096864700317383 / Valid loss: 6.40090530486334
Training loss: 1.9805805683135986 / Valid loss: 6.378478919892084
Training loss: 1.5865622758865356 / Valid loss: 6.388789431254069

Epoch: 21
Training loss: 2.087582588195801 / Valid loss: 6.361457440966651
Training loss: 2.778311252593994 / Valid loss: 6.475021042142596
Training loss: 2.258298397064209 / Valid loss: 6.567639641534715
Training loss: 3.083057403564453 / Valid loss: 6.604821341378348
Training loss: 1.8518853187561035 / Valid loss: 6.5130471433912005

Epoch: 22
Training loss: 1.4417192935943604 / Valid loss: 6.4694782188960485
Training loss: 2.098475456237793 / Valid loss: 6.565907305762881
Training loss: 2.0414884090423584 / Valid loss: 6.555673826308477
Training loss: 3.0321521759033203 / Valid loss: 6.4645792166392
Training loss: 2.2096567153930664 / Valid loss: 6.495615500495547

Epoch: 23
Training loss: 1.361891746520996 / Valid loss: 6.763978558494931
Training loss: 2.314321994781494 / Valid loss: 6.533464581625802
Training loss: 2.430299997329712 / Valid loss: 6.544172137124198
Training loss: 2.0501749515533447 / Valid loss: 7.370088963281541
Training loss: 1.7826251983642578 / Valid loss: 6.693786766415551

Epoch: 24
Training loss: 1.2383769750595093 / Valid loss: 6.518737899689447
Training loss: 2.0862698554992676 / Valid loss: 6.581405317215692
Training loss: 2.5822339057922363 / Valid loss: 6.477531315031506
Training loss: 1.8542532920837402 / Valid loss: 6.564948236374628
Training loss: 2.4593024253845215 / Valid loss: 6.658187185014997

Epoch: 25
Training loss: 1.6738502979278564 / Valid loss: 6.529616626103719
Training loss: 1.5185956954956055 / Valid loss: 6.601922391709827
Training loss: 2.2921547889709473 / Valid loss: 6.689744129635039
Training loss: 1.631066918373108 / Valid loss: 6.639335711797078
Training loss: 1.8912229537963867 / Valid loss: 6.558245154789516

Epoch: 26
Training loss: 1.2759780883789062 / Valid loss: 6.498341619400751
Training loss: 1.158696174621582 / Valid loss: 6.630854347773961
Training loss: 1.655249834060669 / Valid loss: 6.7559194746471585
Training loss: 1.3757429122924805 / Valid loss: 6.695946003141858
Training loss: 1.5937541723251343 / Valid loss: 6.710852350507464

Epoch: 27
Training loss: 1.8504505157470703 / Valid loss: 6.675551244190761
Training loss: 1.3728864192962646 / Valid loss: 6.6888348034449985
Training loss: 2.0032455921173096 / Valid loss: 6.83025582631429
Training loss: 1.617192268371582 / Valid loss: 6.8068348521278015
Training loss: 2.4539294242858887 / Valid loss: 7.082649285452707

Epoch: 28
Training loss: 1.3465940952301025 / Valid loss: 6.764922482626779
Training loss: 1.3588058948516846 / Valid loss: 6.5775680859883625
Training loss: 1.365817904472351 / Valid loss: 6.677980300358364
Training loss: 2.0059609413146973 / Valid loss: 6.94279670715332
Training loss: 1.587989091873169 / Valid loss: 6.914843423025949

Epoch: 29
Training loss: 1.2557193040847778 / Valid loss: 7.095035117013114
Training loss: 1.2145493030548096 / Valid loss: 7.115902210417248
Training loss: 1.7656834125518799 / Valid loss: 6.882766310373942
Training loss: 1.7301480770111084 / Valid loss: 6.717208630698067

Epoch: 30
Training loss: 1.3783451318740845 / Valid loss: 6.719236793972197
Training loss: 1.4497861862182617 / Valid loss: 6.9050541060311454
Training loss: 1.4424954652786255 / Valid loss: 7.7554993947347
Training loss: 1.2763142585754395 / Valid loss: 7.205339922223772
Training loss: 1.6343954801559448 / Valid loss: 8.132530244191488

Epoch: 31
Training loss: 1.6275553703308105 / Valid loss: 6.697470249448504
Training loss: 1.3524636030197144 / Valid loss: 7.171695781889416
Training loss: 1.2041947841644287 / Valid loss: 6.990986472084408
Training loss: 1.6384449005126953 / Valid loss: 6.7393633797055195
Training loss: 1.8073277473449707 / Valid loss: 6.735028975350517

Epoch: 32
Training loss: 0.9895585775375366 / Valid loss: 6.86195003872826
Training loss: 1.2699096202850342 / Valid loss: 6.920426264263335
Training loss: 1.2023615837097168 / Valid loss: 7.108317879268101
Training loss: 1.544065237045288 / Valid loss: 6.763061185110183
Training loss: 1.3890339136123657 / Valid loss: 6.81089246840704

Epoch: 33
Training loss: 0.8974554538726807 / Valid loss: 7.073499956585112
Training loss: 1.1307834386825562 / Valid loss: 6.763486221858433
Training loss: 1.5730431079864502 / Valid loss: 6.803459271930513
Training loss: 1.2185678482055664 / Valid loss: 6.848325552259173
Training loss: 1.565704107284546 / Valid loss: 6.833561895007179

Epoch: 34
Training loss: 0.9949551820755005 / Valid loss: 6.721922729128883
Training loss: 2.112081527709961 / Valid loss: 6.9273134776524135
Training loss: 1.0611205101013184 / Valid loss: 6.887289921442668
Training loss: 1.5711288452148438 / Valid loss: 6.89513190133231
Training loss: 1.397879958152771 / Valid loss: 7.128171598343622

Epoch: 35
Training loss: 1.6793653964996338 / Valid loss: 7.026979891459147
Training loss: 1.3386512994766235 / Valid loss: 7.267780033747355
Training loss: 1.60349440574646 / Valid loss: 6.761610033398583
Training loss: 1.4568772315979004 / Valid loss: 6.836489831833612
Training loss: 1.2902909517288208 / Valid loss: 6.842689555031913

Epoch: 36
Training loss: 1.448194980621338 / Valid loss: 6.767998768034436
Training loss: 1.086112380027771 / Valid loss: 7.316782647087461
Training loss: 1.2233712673187256 / Valid loss: 6.799339303516207
Training loss: 1.4734611511230469 / Valid loss: 6.992565538769677
Training loss: 1.2265136241912842 / Valid loss: 7.1259105591546925

Epoch: 37
Training loss: 1.6677509546279907 / Valid loss: 6.863951624007452
Training loss: 1.029252529144287 / Valid loss: 6.875108260200137
Training loss: 1.242973804473877 / Valid loss: 6.881247484116328
Training loss: 1.0067834854125977 / Valid loss: 7.022786710375831
Training loss: 1.4341918230056763 / Valid loss: 6.967230369931175

Epoch: 38
Training loss: 1.189232349395752 / Valid loss: 7.149541241782052
Training loss: 1.5329698324203491 / Valid loss: 6.922761185963949
Training loss: 1.2551205158233643 / Valid loss: 7.073573612031483
Training loss: 1.5399324893951416 / Valid loss: 6.8928584961664106
Training loss: 1.420736312866211 / Valid loss: 7.338531239827474

Epoch: 39
Training loss: 1.1662254333496094 / Valid loss: 6.954379631224133
Training loss: 1.2068912982940674 / Valid loss: 6.923326017743065
Training loss: 1.5547657012939453 / Valid loss: 7.273401660010928
Training loss: 1.182309865951538 / Valid loss: 6.827539752778553
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2300): 5.4399934609731035
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.74432945251465 / Valid loss: 16.968025570824032
Model is saved in epoch 0, overall batch: 0
Training loss: 13.701087951660156 / Valid loss: 12.826457936423166
Model is saved in epoch 0, overall batch: 100
Training loss: 7.878472805023193 / Valid loss: 8.202859624226887
Model is saved in epoch 0, overall batch: 200
Training loss: 5.817796230316162 / Valid loss: 6.27990471976144
Model is saved in epoch 0, overall batch: 300
Training loss: 6.332569599151611 / Valid loss: 5.833682305472237
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.919895172119141 / Valid loss: 5.642791761670794
Model is saved in epoch 1, overall batch: 500
Training loss: 4.545330047607422 / Valid loss: 5.962502082188924
Training loss: 4.809267997741699 / Valid loss: 5.986106250399635
Training loss: 3.420835018157959 / Valid loss: 5.963797607876006
Training loss: 4.34904670715332 / Valid loss: 6.105996077401297

Epoch: 2
Training loss: 3.3294239044189453 / Valid loss: 6.220286258061727
Training loss: 2.5788192749023438 / Valid loss: 6.30185436067127
Training loss: 2.9801242351531982 / Valid loss: 6.593513597760881
Training loss: 2.7438549995422363 / Valid loss: 6.402285743895031
Training loss: 2.220188617706299 / Valid loss: 6.205838603065128

Epoch: 3
Training loss: 2.431872844696045 / Valid loss: 6.408762623014904
Training loss: 1.8434715270996094 / Valid loss: 6.582267961047944
Training loss: 2.492351770401001 / Valid loss: 6.617960239592053
Training loss: 2.039708137512207 / Valid loss: 6.851360845565796
Training loss: 2.3334579467773438 / Valid loss: 6.663057926722935

Epoch: 4
Training loss: 2.0153863430023193 / Valid loss: 6.880312102181571
Training loss: 1.5823500156402588 / Valid loss: 7.026554988679432
Training loss: 1.186007022857666 / Valid loss: 7.00678977512178
Training loss: 1.4208521842956543 / Valid loss: 7.086355023157029
Training loss: 3.6347150802612305 / Valid loss: 7.131528127761114

Epoch: 5
Training loss: 1.0998377799987793 / Valid loss: 6.991752099990845
Training loss: 1.8629214763641357 / Valid loss: 7.124749431155977
Training loss: 2.421034336090088 / Valid loss: 7.110149338131859
Training loss: 2.508528709411621 / Valid loss: 7.098301540102278
Training loss: 1.5644551515579224 / Valid loss: 7.303036576225644

Epoch: 6
Training loss: 1.2766255140304565 / Valid loss: 7.166155292874291
Training loss: 0.99883633852005 / Valid loss: 7.212969993409656
Training loss: 0.9394252300262451 / Valid loss: 7.181059667042324
Training loss: 1.181306004524231 / Valid loss: 7.112848935808454
Training loss: 1.116051197052002 / Valid loss: 7.305092130388532

Epoch: 7
Training loss: 1.1550812721252441 / Valid loss: 7.162121518452962
Training loss: 2.2114574909210205 / Valid loss: 7.242837746938069
Training loss: 0.9910638332366943 / Valid loss: 7.375455093383789
Training loss: 1.1664555072784424 / Valid loss: 7.499991798400879
Training loss: 1.514666199684143 / Valid loss: 7.169129825773693

Epoch: 8
Training loss: 1.145763635635376 / Valid loss: 7.183999247778029
Training loss: 1.7593793869018555 / Valid loss: 7.209628740946452
Training loss: 1.1164613962173462 / Valid loss: 7.110520698910668
Training loss: 1.6282241344451904 / Valid loss: 7.164680671691895
Training loss: 1.3540337085723877 / Valid loss: 7.249289517175583

Epoch: 9
Training loss: 0.6700859069824219 / Valid loss: 7.051249295189267
Training loss: 1.9760987758636475 / Valid loss: 7.179453690846761
Training loss: 1.3580741882324219 / Valid loss: 7.040864390418643
Training loss: 1.4016999006271362 / Valid loss: 7.1773831639971055

Epoch: 10
Training loss: 1.4331350326538086 / Valid loss: 7.117575018746512
Training loss: 0.5113962888717651 / Valid loss: 7.114682161240351
Training loss: 1.0017439126968384 / Valid loss: 7.070617414656139
Training loss: 1.184169054031372 / Valid loss: 7.38400757199242
Training loss: 0.9126732349395752 / Valid loss: 7.2260769162859235

Epoch: 11
Training loss: 0.8695289492607117 / Valid loss: 7.3252020336332775
Training loss: 0.5794805884361267 / Valid loss: 7.307385390145438
Training loss: 0.9046198129653931 / Valid loss: 7.332969379425049
Training loss: 1.3399097919464111 / Valid loss: 7.133309850238619
Training loss: 0.9985376000404358 / Valid loss: 7.180304549989247

Epoch: 12
Training loss: 0.9697147607803345 / Valid loss: 7.161573991321382
Training loss: 0.7094082832336426 / Valid loss: 7.060044942583357
Training loss: 0.7195202112197876 / Valid loss: 7.053859967277163
Training loss: 0.8043950796127319 / Valid loss: 7.412471764428275
Training loss: 0.7865859270095825 / Valid loss: 7.283807023366292

Epoch: 13
Training loss: 0.8316107392311096 / Valid loss: 7.201490565708705
Training loss: 0.6673004627227783 / Valid loss: 7.221190943036761
Training loss: 0.43477290868759155 / Valid loss: 7.169692225683303
Training loss: 1.2724087238311768 / Valid loss: 7.435022894541422
Training loss: 1.2401840686798096 / Valid loss: 7.3797113327752974

Epoch: 14
Training loss: 0.4819481670856476 / Valid loss: 7.127510899589176
Training loss: 0.7574387788772583 / Valid loss: 7.022613448188419
Training loss: 0.8425259590148926 / Valid loss: 7.215544119335356
Training loss: 0.7029695510864258 / Valid loss: 7.288876874106271
Training loss: 0.49064508080482483 / Valid loss: 7.249659887949625

Epoch: 15
Training loss: 0.4991290867328644 / Valid loss: 7.014254445121402
Training loss: 0.8905322551727295 / Valid loss: 7.042204152970087
Training loss: 0.7386603355407715 / Valid loss: 7.114594223385765
Training loss: 1.0118741989135742 / Valid loss: 7.230151532945179
Training loss: 0.49666088819503784 / Valid loss: 7.245916371118454

Epoch: 16
Training loss: 0.5509888529777527 / Valid loss: 7.188521403358096
Training loss: 0.9352505207061768 / Valid loss: 7.18548195702689
Training loss: 0.6093798875808716 / Valid loss: 7.342370986938477
Training loss: 0.8584973216056824 / Valid loss: 7.131528000604539
Training loss: 0.8274602890014648 / Valid loss: 7.220179580506825

Epoch: 17
Training loss: 1.081323266029358 / Valid loss: 7.144816232862927
Training loss: 0.9391546249389648 / Valid loss: 7.204686532701765
Training loss: 0.6071968674659729 / Valid loss: 7.0941671371459964
Training loss: 0.48700612783432007 / Valid loss: 7.304567961465745
Training loss: 0.6189998388290405 / Valid loss: 7.052287183489118

Epoch: 18
Training loss: 0.5036875009536743 / Valid loss: 7.165780448913575
Training loss: 0.9651045799255371 / Valid loss: 7.115851547604516
Training loss: 0.7407228350639343 / Valid loss: 7.158321257999965
Training loss: 0.8342374563217163 / Valid loss: 7.137241159166608
Training loss: 0.6104909777641296 / Valid loss: 7.057097943623861

Epoch: 19
Training loss: 0.7024731040000916 / Valid loss: 7.094606008983794
Training loss: 0.5713059902191162 / Valid loss: 7.203668294634138
Training loss: 0.973243236541748 / Valid loss: 7.166614550635928
Training loss: 0.7578760981559753 / Valid loss: 7.191526932943435

Epoch: 20
Training loss: 0.4453262686729431 / Valid loss: 7.224525011153448
Training loss: 1.4462051391601562 / Valid loss: 6.937106913612002
Training loss: 0.5335083603858948 / Valid loss: 7.239910393669492
Training loss: 0.7116813659667969 / Valid loss: 7.012922291528611
Training loss: 0.5192513465881348 / Valid loss: 7.203845405578614

Epoch: 21
Training loss: 1.285367488861084 / Valid loss: 6.974642204103016
Training loss: 1.0581923723220825 / Valid loss: 7.045699782598586
Training loss: 0.49007999897003174 / Valid loss: 7.093754750206357
Training loss: 0.3966016471385956 / Valid loss: 7.1894971438816615
Training loss: 0.3161604404449463 / Valid loss: 7.236710893540155

Epoch: 22
Training loss: 0.4234430193901062 / Valid loss: 7.1426367691585
Training loss: 0.8249987363815308 / Valid loss: 7.2045179457891555
Training loss: 1.0331339836120605 / Valid loss: 7.128481113343012
Training loss: 0.5117921829223633 / Valid loss: 7.134317974817185
Training loss: 0.47359365224838257 / Valid loss: 7.130488413856143

Epoch: 23
Training loss: 0.7805944085121155 / Valid loss: 7.113730675833565
Training loss: 0.7490265965461731 / Valid loss: 7.161032835642497
Training loss: 0.565778374671936 / Valid loss: 7.0022653943016415
Training loss: 0.3871240019798279 / Valid loss: 7.190570386250814
Training loss: 0.65378737449646 / Valid loss: 7.306506611052013

Epoch: 24
Training loss: 0.3055424988269806 / Valid loss: 7.0400264467511855
Training loss: 0.5795051455497742 / Valid loss: 7.082472008750552
Training loss: 0.4823834300041199 / Valid loss: 7.024530633290609
Training loss: 0.6380587220191956 / Valid loss: 7.115434601193383
Training loss: 1.2533235549926758 / Valid loss: 7.137566598256429

Epoch: 25
Training loss: 0.39550143480300903 / Valid loss: 7.241762824285598
Training loss: 0.39360859990119934 / Valid loss: 7.0147778556460425
Training loss: 0.6499478816986084 / Valid loss: 7.18029663449242
Training loss: 0.43327999114990234 / Valid loss: 7.118407562800816
Training loss: 0.5099790096282959 / Valid loss: 7.17529803230649

Epoch: 26
Training loss: 0.23791077733039856 / Valid loss: 6.9441566648937405
Training loss: 1.383342981338501 / Valid loss: 7.131459313347226
Training loss: 0.33225053548812866 / Valid loss: 7.051357064928327
Training loss: 0.45333486795425415 / Valid loss: 7.285332507178897
Training loss: 0.4670817255973816 / Valid loss: 7.260355477106003

Epoch: 27
Training loss: 0.24462272226810455 / Valid loss: 7.308085850306919
Training loss: 0.5979281663894653 / Valid loss: 7.401402196430024
Training loss: 0.683452308177948 / Valid loss: 7.216846361614409
Training loss: 0.3847278952598572 / Valid loss: 7.1235215323311945
Training loss: 0.33030951023101807 / Valid loss: 7.038111732119606

Epoch: 28
Training loss: 0.5367685556411743 / Valid loss: 7.158602519262405
Training loss: 0.4163932204246521 / Valid loss: 7.090573637826102
Training loss: 0.4823285937309265 / Valid loss: 7.072137721379598
Training loss: 0.2897070646286011 / Valid loss: 6.932023779551188
Training loss: 0.4235055446624756 / Valid loss: 7.126134182157971

Epoch: 29
Training loss: 0.48443374037742615 / Valid loss: 7.215757478986467
Training loss: 0.575492262840271 / Valid loss: 7.141637720380511
Training loss: 0.497640997171402 / Valid loss: 7.231643851598104
Training loss: 0.726291298866272 / Valid loss: 7.115286649976458

Epoch: 30
Training loss: 0.18860502541065216 / Valid loss: 7.230852149781727
Training loss: 1.006761074066162 / Valid loss: 7.183552787417457
Training loss: 0.6612155437469482 / Valid loss: 7.149371873764765
Training loss: 0.5080017447471619 / Valid loss: 7.082826373690651
Training loss: 0.7293487787246704 / Valid loss: 7.0931773912338985

Epoch: 31
Training loss: 0.3545226454734802 / Valid loss: 7.154468064081101
Training loss: 0.39811497926712036 / Valid loss: 7.152243400755383
Training loss: 0.5994406342506409 / Valid loss: 7.197932370503744
Training loss: 0.2946785092353821 / Valid loss: 7.287171708969843
Training loss: 0.5502701997756958 / Valid loss: 7.461754199436733

Epoch: 32
Training loss: 0.4059062600135803 / Valid loss: 7.239708589372181
Training loss: 0.47160834074020386 / Valid loss: 7.277433504377092
Training loss: 0.45601874589920044 / Valid loss: 7.263698577880859
Training loss: 0.4984944462776184 / Valid loss: 7.10489630926223
Training loss: 0.2980954051017761 / Valid loss: 7.197299144381568

Epoch: 33
Training loss: 0.2699286937713623 / Valid loss: 7.169529029301235
Training loss: 0.41445398330688477 / Valid loss: 7.111472048078265
Training loss: 0.3277558982372284 / Valid loss: 7.129129437037877
Training loss: 0.28265780210494995 / Valid loss: 7.198434039524623
Training loss: 0.27323418855667114 / Valid loss: 7.172622523988996

Epoch: 34
Training loss: 0.39987483620643616 / Valid loss: 7.295120906829834
Training loss: 0.2889772057533264 / Valid loss: 7.167541776384626
Training loss: 0.24835361540317535 / Valid loss: 7.129439485640753
Training loss: 0.33327606320381165 / Valid loss: 7.166113254002163
Training loss: 0.7441483736038208 / Valid loss: 7.089941151936849

Epoch: 35
Training loss: 0.31208503246307373 / Valid loss: 7.119728088378906
Training loss: 0.2497183382511139 / Valid loss: 7.267830344608852
Training loss: 0.27838578820228577 / Valid loss: 7.258755293346587
Training loss: 0.6278088092803955 / Valid loss: 7.243640204838344
Training loss: 0.42221903800964355 / Valid loss: 7.190814594995408

Epoch: 36
Training loss: 0.4870421290397644 / Valid loss: 7.070994050162179
Training loss: 0.49467265605926514 / Valid loss: 7.278970300583612
Training loss: 1.096344232559204 / Valid loss: 7.1353618440173925
Training loss: 0.9371742010116577 / Valid loss: 7.324728666033064
Training loss: 0.5041904449462891 / Valid loss: 7.204359790257045

Epoch: 37
Training loss: 0.36467817425727844 / Valid loss: 7.079918464024861
Training loss: 0.3242100477218628 / Valid loss: 7.195385605948312
Training loss: 0.5617898106575012 / Valid loss: 7.104730683281308
Training loss: 1.185459852218628 / Valid loss: 7.209367084503174
Training loss: 1.0978171825408936 / Valid loss: 7.208616381599789

Epoch: 38
Training loss: 0.34665438532829285 / Valid loss: 7.242190812882923
Training loss: 0.26234355568885803 / Valid loss: 7.156089494341896
Training loss: 0.7550801038742065 / Valid loss: 7.172254984719412
Training loss: 0.43685922026634216 / Valid loss: 7.2515763010297505
Training loss: 0.6196084022521973 / Valid loss: 7.256226721264067

Epoch: 39
Training loss: 0.27097398042678833 / Valid loss: 7.223116824740456
Training loss: 0.6642748117446899 / Valid loss: 7.176181747799828
Training loss: 0.47940123081207275 / Valid loss: 7.077687356585548
Training loss: 0.42211127281188965 / Valid loss: 7.049138350713821
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.559954497927711
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.74432945251465 / Valid loss: 16.968129748389835
Model is saved in epoch 0, overall batch: 0
Training loss: 13.703207015991211 / Valid loss: 12.39322783606393
Model is saved in epoch 0, overall batch: 100
Training loss: 7.544443607330322 / Valid loss: 7.8606634594145275
Model is saved in epoch 0, overall batch: 200
Training loss: 5.282585144042969 / Valid loss: 6.099500933147612
Model is saved in epoch 0, overall batch: 300
Training loss: 6.829627513885498 / Valid loss: 5.838996998469034
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.926822662353516 / Valid loss: 5.650528371901739
Model is saved in epoch 1, overall batch: 500
Training loss: 4.826483249664307 / Valid loss: 5.9733554590316045
Training loss: 4.470418930053711 / Valid loss: 6.03400358018421
Training loss: 3.945619583129883 / Valid loss: 6.066572818301973
Training loss: 4.110616683959961 / Valid loss: 6.113677170163109

Epoch: 2
Training loss: 3.5838191509246826 / Valid loss: 6.166777851468041
Training loss: 2.3117151260375977 / Valid loss: 6.2353435902368455
Training loss: 2.559544801712036 / Valid loss: 6.478437373751686
Training loss: 2.6436901092529297 / Valid loss: 6.402019319080171
Training loss: 1.9452004432678223 / Valid loss: 6.3153213773454935

Epoch: 3
Training loss: 3.138817310333252 / Valid loss: 6.540240401313419
Training loss: 2.337852954864502 / Valid loss: 6.644016919817243
Training loss: 2.505100727081299 / Valid loss: 6.750928497314453
Training loss: 2.058619499206543 / Valid loss: 6.687746474856422
Training loss: 2.4880549907684326 / Valid loss: 6.462046573275612

Epoch: 4
Training loss: 2.1172590255737305 / Valid loss: 6.836069483984084
Training loss: 1.8837049007415771 / Valid loss: 6.862243613742646
Training loss: 1.3708667755126953 / Valid loss: 6.802673739478702
Training loss: 1.8404486179351807 / Valid loss: 6.819467058635893
Training loss: 3.701586961746216 / Valid loss: 7.012198257446289

Epoch: 5
Training loss: 1.2656450271606445 / Valid loss: 6.778393661408197
Training loss: 1.4215341806411743 / Valid loss: 6.870417633510772
Training loss: 2.133108377456665 / Valid loss: 6.827421765100389
Training loss: 2.3764758110046387 / Valid loss: 6.9516372453598745
Training loss: 1.8082808256149292 / Valid loss: 7.081578606650943

Epoch: 6
Training loss: 1.4841547012329102 / Valid loss: 6.899842114675613
Training loss: 1.122295618057251 / Valid loss: 6.901491757801601
Training loss: 1.1889660358428955 / Valid loss: 6.9997352486565
Training loss: 1.6005792617797852 / Valid loss: 7.012067367917015
Training loss: 1.177178144454956 / Valid loss: 7.092744831811814

Epoch: 7
Training loss: 1.2200617790222168 / Valid loss: 6.947842420850481
Training loss: 2.1143691539764404 / Valid loss: 7.138282348996117
Training loss: 0.8325165510177612 / Valid loss: 7.10078318459647
Training loss: 1.5405455827713013 / Valid loss: 7.217983522869292
Training loss: 1.8221747875213623 / Valid loss: 7.139017027900333

Epoch: 8
Training loss: 1.2685973644256592 / Valid loss: 7.146574865068708
Training loss: 1.5522184371948242 / Valid loss: 7.117465668632871
Training loss: 1.3395085334777832 / Valid loss: 6.999616064344134
Training loss: 1.508059024810791 / Valid loss: 6.925021330515544
Training loss: 1.1672396659851074 / Valid loss: 7.067806150799706

Epoch: 9
Training loss: 0.8547692894935608 / Valid loss: 6.956769934154692
Training loss: 2.0851991176605225 / Valid loss: 7.052558533350626
Training loss: 1.4913527965545654 / Valid loss: 6.890024164744786
Training loss: 0.9499760270118713 / Valid loss: 6.952062697637649

Epoch: 10
Training loss: 1.2052826881408691 / Valid loss: 6.962128185090565
Training loss: 0.7196102142333984 / Valid loss: 7.018437353769938
Training loss: 1.0268425941467285 / Valid loss: 6.994163145337786
Training loss: 0.8001355528831482 / Valid loss: 6.8359474817911785
Training loss: 0.6072098612785339 / Valid loss: 6.85869106565203

Epoch: 11
Training loss: 1.0090439319610596 / Valid loss: 7.037754399435861
Training loss: 0.716469407081604 / Valid loss: 7.078037770589193
Training loss: 0.560219407081604 / Valid loss: 7.028440030415853
Training loss: 1.1038198471069336 / Valid loss: 7.003688185555594
Training loss: 1.0067058801651 / Valid loss: 6.963993490309942

Epoch: 12
Training loss: 0.8335266709327698 / Valid loss: 6.906682949974424
Training loss: 0.9910094738006592 / Valid loss: 6.872914804731097
Training loss: 1.081552267074585 / Valid loss: 7.011018060502552
Training loss: 0.9580934047698975 / Valid loss: 7.108364186968122
Training loss: 0.7743744850158691 / Valid loss: 6.983945869264149

Epoch: 13
Training loss: 0.7429616451263428 / Valid loss: 6.998585019792829
Training loss: 0.61011803150177 / Valid loss: 6.939868407022385
Training loss: 0.7945380210876465 / Valid loss: 6.893102450597854
Training loss: 0.9514850378036499 / Valid loss: 7.0551624025617325
Training loss: 1.1993095874786377 / Valid loss: 7.045897797175816

Epoch: 14
Training loss: 0.578635573387146 / Valid loss: 6.937304174332391
Training loss: 1.093762755393982 / Valid loss: 6.814721788678851
Training loss: 0.8027678728103638 / Valid loss: 7.0224460510980515
Training loss: 0.7540866136550903 / Valid loss: 7.070382776714506
Training loss: 0.6824559569358826 / Valid loss: 6.946871648515974

Epoch: 15
Training loss: 0.6657077074050903 / Valid loss: 6.861651354744321
Training loss: 0.5091453790664673 / Valid loss: 6.878674559366136
Training loss: 0.4657455086708069 / Valid loss: 6.867720372336251
Training loss: 0.6479036808013916 / Valid loss: 7.097061236699422
Training loss: 0.9237419962882996 / Valid loss: 7.098951512291318

Epoch: 16
Training loss: 0.4939863383769989 / Valid loss: 6.972758833567301
Training loss: 1.0793540477752686 / Valid loss: 7.006022367023286
Training loss: 0.7109358310699463 / Valid loss: 7.156480961754209
Training loss: 0.7151559591293335 / Valid loss: 7.034635625566755
Training loss: 0.9152621030807495 / Valid loss: 6.963197524206979

Epoch: 17
Training loss: 1.136362910270691 / Valid loss: 6.9307721206120085
Training loss: 0.5914842486381531 / Valid loss: 6.858736646743048
Training loss: 0.7255852818489075 / Valid loss: 6.919395464942569
Training loss: 0.6900678873062134 / Valid loss: 6.934651651836577
Training loss: 0.7227358222007751 / Valid loss: 6.8253406751723515

Epoch: 18
Training loss: 0.765864372253418 / Valid loss: 6.740487591425578
Training loss: 0.9782297611236572 / Valid loss: 6.8926869437808085
Training loss: 1.0323530435562134 / Valid loss: 6.9055319649832585
Training loss: 1.0726263523101807 / Valid loss: 6.801618153708322
Training loss: 0.7456554174423218 / Valid loss: 6.724619129725865

Epoch: 19
Training loss: 0.6817238926887512 / Valid loss: 7.026237537747337
Training loss: 0.6901981830596924 / Valid loss: 6.904281897771926
Training loss: 1.3436425924301147 / Valid loss: 6.95578502473377
Training loss: 0.6933720111846924 / Valid loss: 6.928732999165853

Epoch: 20
Training loss: 0.5433372259140015 / Valid loss: 6.973633221217564
Training loss: 1.1489152908325195 / Valid loss: 6.7852072874705
Training loss: 0.7617028951644897 / Valid loss: 6.903552427746001
Training loss: 0.8721705675125122 / Valid loss: 6.7323802357628235
Training loss: 0.610156774520874 / Valid loss: 6.826741066433135

Epoch: 21
Training loss: 1.1299760341644287 / Valid loss: 6.88320681254069
Training loss: 1.1206296682357788 / Valid loss: 6.774952659152803
Training loss: 0.7009595632553101 / Valid loss: 6.8437555858067105
Training loss: 0.43866419792175293 / Valid loss: 6.8993470736912315
Training loss: 0.3334573805332184 / Valid loss: 6.775929882412865

Epoch: 22
Training loss: 0.5015221834182739 / Valid loss: 6.879101489839099
Training loss: 0.8001072406768799 / Valid loss: 6.770701238087246
Training loss: 1.368606686592102 / Valid loss: 6.7950314362843836
Training loss: 0.5489335060119629 / Valid loss: 6.9204598222460065
Training loss: 0.6500871181488037 / Valid loss: 6.951096051079887

Epoch: 23
Training loss: 0.9018930196762085 / Valid loss: 6.790716716221401
Training loss: 0.6306475400924683 / Valid loss: 6.771864650363014
Training loss: 0.6828603744506836 / Valid loss: 6.753758725665865
Training loss: 0.46050578355789185 / Valid loss: 6.713464936755952
Training loss: 0.8487610816955566 / Valid loss: 6.93843058177403

Epoch: 24
Training loss: 0.48962801694869995 / Valid loss: 6.714910915919712
Training loss: 0.6093672513961792 / Valid loss: 6.697072982788086
Training loss: 0.402322918176651 / Valid loss: 6.795042641957601
Training loss: 0.7945001125335693 / Valid loss: 6.783128924596877
Training loss: 1.2159510850906372 / Valid loss: 6.794465317044939

Epoch: 25
Training loss: 0.6000218391418457 / Valid loss: 6.960180668603806
Training loss: 0.3516554832458496 / Valid loss: 6.819298267364502
Training loss: 0.3214012086391449 / Valid loss: 6.8398661568051295
Training loss: 0.40401551127433777 / Valid loss: 6.943086260841007
Training loss: 0.8086516857147217 / Valid loss: 6.887931705656506

Epoch: 26
Training loss: 0.39308905601501465 / Valid loss: 6.63794895807902
Training loss: 1.2806074619293213 / Valid loss: 6.764972371146793
Training loss: 0.4652373194694519 / Valid loss: 6.7804390203385125
Training loss: 0.41823211312294006 / Valid loss: 6.908950147174654
Training loss: 0.8168313503265381 / Valid loss: 7.080905364808582

Epoch: 27
Training loss: 0.38332700729370117 / Valid loss: 6.899493539901007
Training loss: 0.5388625264167786 / Valid loss: 7.115658819107782
Training loss: 0.7805717587471008 / Valid loss: 6.963531784784227
Training loss: 0.31353694200515747 / Valid loss: 6.960851687476748
Training loss: 0.29849469661712646 / Valid loss: 6.80367291087196

Epoch: 28
Training loss: 0.7568324208259583 / Valid loss: 6.811931721369425
Training loss: 0.5127542614936829 / Valid loss: 6.830962791897002
Training loss: 0.5311248302459717 / Valid loss: 6.784335747219267
Training loss: 0.3673672378063202 / Valid loss: 6.836367043994722
Training loss: 0.5083167552947998 / Valid loss: 6.9585344587053575

Epoch: 29
Training loss: 0.5094007253646851 / Valid loss: 6.94594615754627
Training loss: 0.5843048095703125 / Valid loss: 6.854095016207014
Training loss: 0.7615918517112732 / Valid loss: 6.943096612748645
Training loss: 0.7313312292098999 / Valid loss: 6.804667872474307

Epoch: 30
Training loss: 0.5013649463653564 / Valid loss: 6.9133807545616515
Training loss: 0.913356363773346 / Valid loss: 6.771676515397571
Training loss: 0.7913613319396973 / Valid loss: 6.856303133283343
Training loss: 0.5303515195846558 / Valid loss: 6.783884075709752
Training loss: 0.9524755477905273 / Valid loss: 6.81803039369129

Epoch: 31
Training loss: 0.4238617420196533 / Valid loss: 6.993286757242112
Training loss: 0.4061635732650757 / Valid loss: 6.886598114740281
Training loss: 0.7402017116546631 / Valid loss: 6.880883580162411
Training loss: 0.2914983630180359 / Valid loss: 6.89715127717881
Training loss: 0.6519342660903931 / Valid loss: 6.934063915979294

Epoch: 32
Training loss: 0.4756830036640167 / Valid loss: 6.830568576994397
Training loss: 0.6617171764373779 / Valid loss: 6.887654854002453
Training loss: 0.6574481725692749 / Valid loss: 6.958966800144741
Training loss: 0.8116930723190308 / Valid loss: 6.894156887417748
Training loss: 0.3247474133968353 / Valid loss: 6.842568987891788

Epoch: 33
Training loss: 0.3852583169937134 / Valid loss: 6.89087458110991
Training loss: 0.5529307126998901 / Valid loss: 6.714158662160238
Training loss: 0.36969810724258423 / Valid loss: 6.756750892457508
Training loss: 0.6926481127738953 / Valid loss: 6.840904231298538
Training loss: 0.32077938318252563 / Valid loss: 6.840371163686116

Epoch: 34
Training loss: 0.6420810222625732 / Valid loss: 6.833816423870268
Training loss: 0.3756190538406372 / Valid loss: 6.752809111277262
Training loss: 0.3299032747745514 / Valid loss: 6.7783767541249595
Training loss: 0.33571580052375793 / Valid loss: 6.802943313689459
Training loss: 0.7027975916862488 / Valid loss: 6.718603456588019

Epoch: 35
Training loss: 0.5910583734512329 / Valid loss: 6.71575616200765
Training loss: 0.4676082730293274 / Valid loss: 6.843494499297369
Training loss: 0.32720550894737244 / Valid loss: 6.903311123166765
Training loss: 0.707094669342041 / Valid loss: 6.702362146831694
Training loss: 0.4891711175441742 / Valid loss: 6.863208788917178

Epoch: 36
Training loss: 0.5955634117126465 / Valid loss: 6.682338487534296
Training loss: 0.5130369067192078 / Valid loss: 6.9256363823300315
Training loss: 1.0057317018508911 / Valid loss: 6.698569892701649
Training loss: 0.9294799566268921 / Valid loss: 6.919127187274751
Training loss: 0.41249459981918335 / Valid loss: 6.755196317036947

Epoch: 37
Training loss: 0.30964910984039307 / Valid loss: 6.620914463769822
Training loss: 0.5841237902641296 / Valid loss: 6.752344415301368
Training loss: 0.4140446186065674 / Valid loss: 6.73800143741426
Training loss: 1.3862003087997437 / Valid loss: 6.85871190116519
Training loss: 0.9458202719688416 / Valid loss: 6.9671225820268905

Epoch: 38
Training loss: 0.5025102496147156 / Valid loss: 6.896211562837873
Training loss: 0.5410057306289673 / Valid loss: 6.821352715719314
Training loss: 0.9027124643325806 / Valid loss: 6.745876141956875
Training loss: 0.44726163148880005 / Valid loss: 6.828212052299863
Training loss: 0.9197580814361572 / Valid loss: 6.864306468055362

Epoch: 39
Training loss: 0.45959004759788513 / Valid loss: 6.983738295237223
Training loss: 0.5926836133003235 / Valid loss: 6.933816056024461
Training loss: 0.5620405077934265 / Valid loss: 6.88477292742048
Training loss: 0.4201053977012634 / Valid loss: 6.747240198226202
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.5428800219581245
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.448760986328125 / Valid loss: 15.737742124285017
Model is saved in epoch 0, overall batch: 0
Training loss: 8.795398712158203 / Valid loss: 10.334118756793794
Model is saved in epoch 0, overall batch: 100
Training loss: 4.857907772064209 / Valid loss: 6.878621821176438
Model is saved in epoch 0, overall batch: 200
Training loss: 4.154490947723389 / Valid loss: 6.162181227547782
Model is saved in epoch 0, overall batch: 300
Training loss: 4.547575950622559 / Valid loss: 6.0317652180081325
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 3.8387699127197266 / Valid loss: 5.954685388292585
Model is saved in epoch 1, overall batch: 500
Training loss: 4.361114501953125 / Valid loss: 6.366835226331438
Training loss: 4.5540385246276855 / Valid loss: 6.092624757403419
Training loss: 4.1374640464782715 / Valid loss: 6.083876389548892
Training loss: 4.726019859313965 / Valid loss: 6.04026863461449

Epoch: 2
Training loss: 2.1980910301208496 / Valid loss: 6.148594056992303
Training loss: 2.484157085418701 / Valid loss: 6.55318223862421
Training loss: 2.0412802696228027 / Valid loss: 6.586145982288179
Training loss: 2.319096088409424 / Valid loss: 6.417306668417794
Training loss: 2.795921802520752 / Valid loss: 6.457920778365362

Epoch: 3
Training loss: 1.884460210800171 / Valid loss: 6.56229369753883
Training loss: 3.0367746353149414 / Valid loss: 6.781518005189442
Training loss: 2.8760666847229004 / Valid loss: 6.5208415485563735
Training loss: 2.5790109634399414 / Valid loss: 6.866217458815802
Training loss: 3.1782853603363037 / Valid loss: 6.717215887705485

Epoch: 4
Training loss: 1.2784335613250732 / Valid loss: 6.857428984414963
Training loss: 1.8723385334014893 / Valid loss: 7.002994319370814
Training loss: 1.545300006866455 / Valid loss: 6.899483646665301
Training loss: 1.8077642917633057 / Valid loss: 6.83976712453933
Training loss: 2.144871711730957 / Valid loss: 7.0125527132125125

Epoch: 5
Training loss: 1.1587023735046387 / Valid loss: 6.999511214665004
Training loss: 1.9999371767044067 / Valid loss: 6.886087013426281
Training loss: 2.276913642883301 / Valid loss: 7.166793409983317
Training loss: 1.8698123693466187 / Valid loss: 7.198009840647379
Training loss: 1.4511773586273193 / Valid loss: 7.264252626328242

Epoch: 6
Training loss: 0.7768069505691528 / Valid loss: 7.3149289040338425
Training loss: 1.8593883514404297 / Valid loss: 7.341396502086094
Training loss: 1.2141711711883545 / Valid loss: 7.17579539162772
Training loss: 1.1602962017059326 / Valid loss: 7.428591832660493
Training loss: 1.424504041671753 / Valid loss: 7.404377421878633

Epoch: 7
Training loss: 1.0663200616836548 / Valid loss: 7.375995672316778
Training loss: 0.8691880702972412 / Valid loss: 7.5795298803420295
Training loss: 1.4388667345046997 / Valid loss: 7.496125250770932
Training loss: 1.2229547500610352 / Valid loss: 7.297125312260219
Training loss: 1.106412649154663 / Valid loss: 7.380660808654058

Epoch: 8
Training loss: 1.103425145149231 / Valid loss: 7.350907266707647
Training loss: 1.0981899499893188 / Valid loss: 7.395354080200195
Training loss: 1.102842092514038 / Valid loss: 7.497838251931327
Training loss: 1.028210163116455 / Valid loss: 7.360913571857271
Training loss: 1.951355218887329 / Valid loss: 7.362021966207595

Epoch: 9
Training loss: 1.2832313776016235 / Valid loss: 7.480307149887085
Training loss: 0.9009276628494263 / Valid loss: 7.375753561655681
Training loss: 1.0431005954742432 / Valid loss: 7.469938514346168
Training loss: 2.570435047149658 / Valid loss: 7.561460395086379

Epoch: 10
Training loss: 1.07107675075531 / Valid loss: 7.558645648048038
Training loss: 1.3308390378952026 / Valid loss: 7.343804114205497
Training loss: 0.6928473711013794 / Valid loss: 7.481212434314546
Training loss: 0.6591497659683228 / Valid loss: 7.559833453950428
Training loss: 0.7525424957275391 / Valid loss: 7.375450084322975

Epoch: 11
Training loss: 0.867957592010498 / Valid loss: 7.480240422203427
Training loss: 0.8985975980758667 / Valid loss: 7.43158286412557
Training loss: 1.3199007511138916 / Valid loss: 7.470316641671317
Training loss: 1.020338773727417 / Valid loss: 7.380595166342599
Training loss: 1.8444379568099976 / Valid loss: 7.458039924076625

Epoch: 12
Training loss: 0.6129134893417358 / Valid loss: 7.429353604997908
Training loss: 0.6871697902679443 / Valid loss: 7.316114357539585
Training loss: 0.8854616284370422 / Valid loss: 7.417587352934338
Training loss: 1.1735174655914307 / Valid loss: 7.466257345108759
Training loss: 1.0663018226623535 / Valid loss: 7.3640848159790036

Epoch: 13
Training loss: 1.1600215435028076 / Valid loss: 7.398747226170131
Training loss: 0.6412211656570435 / Valid loss: 7.3224096207391645
Training loss: 0.935207724571228 / Valid loss: 7.347282545907157
Training loss: 0.9330042600631714 / Valid loss: 7.408503105526878
Training loss: 0.9015124440193176 / Valid loss: 7.520912088666644

Epoch: 14
Training loss: 0.7628228664398193 / Valid loss: 7.53602112361363
Training loss: 0.9915259480476379 / Valid loss: 7.389752996535528
Training loss: 0.6621551513671875 / Valid loss: 7.485101758866083
Training loss: 1.0592477321624756 / Valid loss: 7.368987233298165
Training loss: 0.8339943885803223 / Valid loss: 7.525134395417713

Epoch: 15
Training loss: 0.9517706036567688 / Valid loss: 7.489839880807059
Training loss: 0.7557777762413025 / Valid loss: 7.35154854002453
Training loss: 1.2817597389221191 / Valid loss: 7.421953648612613
Training loss: 1.015791416168213 / Valid loss: 7.354858391625541
Training loss: 2.157665252685547 / Valid loss: 7.5405084564572284

Epoch: 16
Training loss: 0.770950436592102 / Valid loss: 7.488406601406279
Training loss: 1.0706779956817627 / Valid loss: 7.396378907703218
Training loss: 0.977404773235321 / Valid loss: 7.365587688627697
Training loss: 1.3162615299224854 / Valid loss: 7.545477024714152
Training loss: 0.8927309513092041 / Valid loss: 7.49842654636928

Epoch: 17
Training loss: 0.6673021912574768 / Valid loss: 7.365172118232364
Training loss: 0.7411050200462341 / Valid loss: 7.50535455431257
Training loss: 0.6476917266845703 / Valid loss: 7.529790717079526
Training loss: 1.1103812456130981 / Valid loss: 7.557209582555862
Training loss: 0.49482831358909607 / Valid loss: 7.515846524919782

Epoch: 18
Training loss: 0.7204346656799316 / Valid loss: 7.258920147305443
Training loss: 0.7534005045890808 / Valid loss: 7.267909608568464
Training loss: 0.711733341217041 / Valid loss: 7.3021132741655626
Training loss: 0.6762040853500366 / Valid loss: 7.455759557088216
Training loss: 0.8043922185897827 / Valid loss: 7.5544868741716655

Epoch: 19
Training loss: 0.6433166265487671 / Valid loss: 7.363277855373564
Training loss: 0.5476953983306885 / Valid loss: 7.565891819908505
Training loss: 0.3759400248527527 / Valid loss: 7.494128912971133
Training loss: 0.608814001083374 / Valid loss: 7.4412002154759

Epoch: 20
Training loss: 0.6267082691192627 / Valid loss: 7.588629708971296
Training loss: 0.561211109161377 / Valid loss: 7.243625377473377
Training loss: 0.8087419271469116 / Valid loss: 7.330663685571579
Training loss: 0.5918982625007629 / Valid loss: 7.512828281947544
Training loss: 0.6429517269134521 / Valid loss: 7.353182140986124

Epoch: 21
Training loss: 0.43457239866256714 / Valid loss: 7.3657901650383355
Training loss: 0.7502707839012146 / Valid loss: 7.464710035778228
Training loss: 0.6103465557098389 / Valid loss: 7.362137115569341
Training loss: 0.47568291425704956 / Valid loss: 7.394699523562477
Training loss: 0.5862425565719604 / Valid loss: 7.39346413839431

Epoch: 22
Training loss: 0.808681845664978 / Valid loss: 7.355399063655308
Training loss: 0.5881801843643188 / Valid loss: 7.213879072098505
Training loss: 0.6211915016174316 / Valid loss: 7.1579687095823745
Training loss: 0.8130762577056885 / Valid loss: 7.309370249793643
Training loss: 0.8040083646774292 / Valid loss: 7.377625619797479

Epoch: 23
Training loss: 0.5490221977233887 / Valid loss: 7.304361057281494
Training loss: 0.5784600973129272 / Valid loss: 7.170027215140206
Training loss: 1.8054066896438599 / Valid loss: 7.370483857109433
Training loss: 0.6951080560684204 / Valid loss: 7.3376126471019925
Training loss: 0.4879794716835022 / Valid loss: 7.471388053894043

Epoch: 24
Training loss: 0.5461485981941223 / Valid loss: 7.192053949265253
Training loss: 0.5879966020584106 / Valid loss: 7.285464595613026
Training loss: 0.8525850772857666 / Valid loss: 7.327164218539283
Training loss: 0.6437826156616211 / Valid loss: 7.3587117149716335
Training loss: 0.6343597769737244 / Valid loss: 7.374356451488676

Epoch: 25
Training loss: 0.8105252981185913 / Valid loss: 7.300261177335467
Training loss: 0.5508577227592468 / Valid loss: 7.252144173213414
Training loss: 0.5803009867668152 / Valid loss: 7.363982999892462
Training loss: 0.7931782603263855 / Valid loss: 7.362205042157854
Training loss: 0.6559293866157532 / Valid loss: 7.282811301095145

Epoch: 26
Training loss: 0.5891096591949463 / Valid loss: 7.254439692270188
Training loss: 0.5694354176521301 / Valid loss: 7.2142103513081866
Training loss: 0.5948366522789001 / Valid loss: 7.314313838595436
Training loss: 0.5118358135223389 / Valid loss: 7.398061879475912
Training loss: 0.5938667058944702 / Valid loss: 7.346452581314813

Epoch: 27
Training loss: 0.629208505153656 / Valid loss: 7.264199443090529
Training loss: 0.47668635845184326 / Valid loss: 7.289469825653803
Training loss: 0.6527495384216309 / Valid loss: 7.3701574370974585
Training loss: 0.4883502125740051 / Valid loss: 7.406509703681582
Training loss: 0.6404697895050049 / Valid loss: 7.353261974879674

Epoch: 28
Training loss: 0.4780304431915283 / Valid loss: 7.177647377195813
Training loss: 0.5360669493675232 / Valid loss: 7.229679984138126
Training loss: 0.8857045769691467 / Valid loss: 7.355479540143694
Training loss: 0.48654741048812866 / Valid loss: 7.287727437700544
Training loss: 0.4332975447177887 / Valid loss: 7.251263116654895

Epoch: 29
Training loss: 0.5503703355789185 / Valid loss: 7.266044480460031
Training loss: 0.48474442958831787 / Valid loss: 7.2653708775838215
Training loss: 0.8222965002059937 / Valid loss: 7.155321811494373
Training loss: 0.5223159193992615 / Valid loss: 7.277486746651785

Epoch: 30
Training loss: 0.6718955039978027 / Valid loss: 7.265111827850342
Training loss: 0.5328627228736877 / Valid loss: 7.341574836912609
Training loss: 0.487415075302124 / Valid loss: 7.148733297983806
Training loss: 0.6113623380661011 / Valid loss: 7.28486404873076
Training loss: 0.7013310790061951 / Valid loss: 7.128824701763334

Epoch: 31
Training loss: 0.8124806880950928 / Valid loss: 7.265695122310094
Training loss: 0.4552473723888397 / Valid loss: 7.3124048732575915
Training loss: 0.5874705910682678 / Valid loss: 7.2925782703218
Training loss: 0.5478851795196533 / Valid loss: 7.348220614024571
Training loss: 0.612449049949646 / Valid loss: 7.317614741552443

Epoch: 32
Training loss: 0.5608398914337158 / Valid loss: 7.226719715481713
Training loss: 0.37876036763191223 / Valid loss: 7.228257070268904
Training loss: 0.48777902126312256 / Valid loss: 7.257968130565825
Training loss: 0.8302129507064819 / Valid loss: 7.294233903430757
Training loss: 0.3204983174800873 / Valid loss: 7.242314824603853

Epoch: 33
Training loss: 0.5750632286071777 / Valid loss: 7.2618542058127264
Training loss: 0.6455756425857544 / Valid loss: 7.391118095034645
Training loss: 0.6021793484687805 / Valid loss: 7.276538692201886
Training loss: 0.4863417446613312 / Valid loss: 7.2452261334373835
Training loss: 0.5398252606391907 / Valid loss: 7.278157892681303

Epoch: 34
Training loss: 0.6351606249809265 / Valid loss: 7.256383196512858
Training loss: 0.34335052967071533 / Valid loss: 7.190791656857445
Training loss: 0.5718402862548828 / Valid loss: 7.25745339620681
Training loss: 0.47502875328063965 / Valid loss: 7.232592451004755
Training loss: 0.47274214029312134 / Valid loss: 7.218878587086995

Epoch: 35
Training loss: 0.41810423135757446 / Valid loss: 7.269768982841855
Training loss: 0.5999933481216431 / Valid loss: 7.206883046740577
Training loss: 0.49166083335876465 / Valid loss: 7.1151200249081565
Training loss: 0.44745802879333496 / Valid loss: 7.239996919177828
Training loss: 0.7331659197807312 / Valid loss: 7.1807870728628975

Epoch: 36
Training loss: 0.5062416791915894 / Valid loss: 7.251865750267392
Training loss: 0.6231710314750671 / Valid loss: 7.243492065157209
Training loss: 0.5460646152496338 / Valid loss: 7.163310859316871
Training loss: 0.4655756950378418 / Valid loss: 7.170845304216657
Training loss: 0.3891140818595886 / Valid loss: 7.241263394128708

Epoch: 37
Training loss: 0.6509530544281006 / Valid loss: 7.238202481042771
Training loss: 0.3775024712085724 / Valid loss: 7.145393323898316
Training loss: 0.910413920879364 / Valid loss: 7.16214812596639
Training loss: 0.478153258562088 / Valid loss: 7.139138562338693
Training loss: 0.3891046941280365 / Valid loss: 7.26813801129659

Epoch: 38
Training loss: 0.5103468894958496 / Valid loss: 7.2856298991612025
Training loss: 0.3037218451499939 / Valid loss: 7.208905269986107
Training loss: 0.5037209987640381 / Valid loss: 7.196125679924375
Training loss: 0.4083157777786255 / Valid loss: 7.130725381487892
Training loss: 0.3602204918861389 / Valid loss: 7.263828722635905

Epoch: 39
Training loss: 0.5433223247528076 / Valid loss: 7.159868335723877
Training loss: 0.700443685054779 / Valid loss: 7.225786490667434
Training loss: 0.4995967149734497 / Valid loss: 7.236855225335984
Training loss: 0.48588594794273376 / Valid loss: 7.213035669780913
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.788892982119606
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.448760986328125 / Valid loss: 15.738069852193197
Model is saved in epoch 0, overall batch: 0
Training loss: 8.732030868530273 / Valid loss: 9.982458732241676
Model is saved in epoch 0, overall batch: 100
Training loss: 5.128015995025635 / Valid loss: 6.50986351285662
Model is saved in epoch 0, overall batch: 200
Training loss: 3.928496837615967 / Valid loss: 5.97737063453311
Model is saved in epoch 0, overall batch: 300
Training loss: 4.785613536834717 / Valid loss: 5.955471556527274
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 3.3143134117126465 / Valid loss: 5.746249144417899
Model is saved in epoch 1, overall batch: 500
Training loss: 4.5697021484375 / Valid loss: 6.213903699602399
Training loss: 5.100259780883789 / Valid loss: 6.08090082123166
Training loss: 4.253655910491943 / Valid loss: 6.022988262630644
Training loss: 4.67922830581665 / Valid loss: 6.066672833760579

Epoch: 2
Training loss: 2.167823314666748 / Valid loss: 6.106042923246111
Training loss: 2.072582721710205 / Valid loss: 6.621854691278367
Training loss: 2.8100838661193848 / Valid loss: 6.490258116949172
Training loss: 2.053375482559204 / Valid loss: 6.410539077577137
Training loss: 2.9639997482299805 / Valid loss: 6.434656615484329

Epoch: 3
Training loss: 1.905282735824585 / Valid loss: 6.4239912055787585
Training loss: 2.160888671875 / Valid loss: 6.692695106778826
Training loss: 2.2511181831359863 / Valid loss: 6.496540896097819
Training loss: 3.2616748809814453 / Valid loss: 6.775029852276757
Training loss: 3.1682116985321045 / Valid loss: 6.672971673238845

Epoch: 4
Training loss: 1.6514196395874023 / Valid loss: 6.889978134064448
Training loss: 1.9564377069473267 / Valid loss: 6.966249297914051
Training loss: 1.2630635499954224 / Valid loss: 6.9071346986861455
Training loss: 1.6353890895843506 / Valid loss: 6.949261803854079
Training loss: 2.1716482639312744 / Valid loss: 7.013501094636463

Epoch: 5
Training loss: 1.1260628700256348 / Valid loss: 7.034815077554612
Training loss: 1.3801050186157227 / Valid loss: 7.005991572425479
Training loss: 1.8741555213928223 / Valid loss: 7.087969893500919
Training loss: 1.9085341691970825 / Valid loss: 7.122933267411732
Training loss: 1.8767309188842773 / Valid loss: 7.073384761810303

Epoch: 6
Training loss: 1.108196496963501 / Valid loss: 7.025383349827358
Training loss: 2.2996859550476074 / Valid loss: 7.143798619224912
Training loss: 1.0288993120193481 / Valid loss: 7.166113176799956
Training loss: 1.2581062316894531 / Valid loss: 7.301532136826288
Training loss: 1.546167254447937 / Valid loss: 7.254866491045271

Epoch: 7
Training loss: 1.3344943523406982 / Valid loss: 7.249020685468401
Training loss: 1.130131721496582 / Valid loss: 7.444823714665004
Training loss: 1.5508077144622803 / Valid loss: 7.395908828008743
Training loss: 1.8949594497680664 / Valid loss: 7.246464011782692
Training loss: 1.5739507675170898 / Valid loss: 7.210943072182792

Epoch: 8
Training loss: 0.9986369609832764 / Valid loss: 7.188880852290562
Training loss: 0.9891020059585571 / Valid loss: 7.17196702048892
Training loss: 0.979092001914978 / Valid loss: 7.2166163489932105
Training loss: 0.6203966736793518 / Valid loss: 7.342424896785191
Training loss: 2.256420135498047 / Valid loss: 7.2729300703321185

Epoch: 9
Training loss: 1.00774085521698 / Valid loss: 7.422165648142497
Training loss: 0.9757622480392456 / Valid loss: 7.288013097218105
Training loss: 1.4343602657318115 / Valid loss: 7.4413378738221665
Training loss: 2.0907235145568848 / Valid loss: 7.358453301021031

Epoch: 10
Training loss: 1.6144641637802124 / Valid loss: 7.301578721545991
Training loss: 1.0055254697799683 / Valid loss: 7.209971023741223
Training loss: 0.72179114818573 / Valid loss: 7.23212030728658
Training loss: 0.7177208662033081 / Valid loss: 7.397435805911109
Training loss: 0.9358530044555664 / Valid loss: 7.361593514397031

Epoch: 11
Training loss: 0.8242725133895874 / Valid loss: 7.512128766377767
Training loss: 0.9796143770217896 / Valid loss: 7.341416878927322
Training loss: 0.9307018518447876 / Valid loss: 7.383322933741979
Training loss: 0.9300851821899414 / Valid loss: 7.345427131652832
Training loss: 2.096162796020508 / Valid loss: 7.301722762698219

Epoch: 12
Training loss: 0.7142853140830994 / Valid loss: 7.372516141619001
Training loss: 0.7950282096862793 / Valid loss: 7.237042922065371
Training loss: 0.9793598651885986 / Valid loss: 7.366765508197603
Training loss: 1.1598063707351685 / Valid loss: 7.378287896655855
Training loss: 0.8501519560813904 / Valid loss: 7.322496614002046

Epoch: 13
Training loss: 1.0938265323638916 / Valid loss: 7.23029222261338
Training loss: 0.731535017490387 / Valid loss: 7.260372170947847
Training loss: 0.8267691135406494 / Valid loss: 7.375913849331083
Training loss: 0.6179215908050537 / Valid loss: 7.358124705723354
Training loss: 0.9264887571334839 / Valid loss: 7.274517890385219

Epoch: 14
Training loss: 0.711382269859314 / Valid loss: 7.3441030865623835
Training loss: 0.9811105728149414 / Valid loss: 7.240846642993746
Training loss: 0.6677743196487427 / Valid loss: 7.3927953311375205
Training loss: 0.927385687828064 / Valid loss: 7.286458651224772
Training loss: 0.8472731709480286 / Valid loss: 7.4223515374319895

Epoch: 15
Training loss: 1.2876209020614624 / Valid loss: 7.426932421184722
Training loss: 0.5697793960571289 / Valid loss: 7.279864106859479
Training loss: 1.9499070644378662 / Valid loss: 7.344353839329311
Training loss: 1.1130480766296387 / Valid loss: 7.276163682483491
Training loss: 2.473437547683716 / Valid loss: 7.453798879895891

Epoch: 16
Training loss: 0.6841765642166138 / Valid loss: 7.315385323479062
Training loss: 1.1914925575256348 / Valid loss: 7.2079999651227675
Training loss: 1.0463192462921143 / Valid loss: 7.288976378667922
Training loss: 1.3335074186325073 / Valid loss: 7.3831890719277515
Training loss: 0.6857855319976807 / Valid loss: 7.373385374886649

Epoch: 17
Training loss: 0.6173280477523804 / Valid loss: 7.205761387234642
Training loss: 0.6670567989349365 / Valid loss: 7.197052297138033
Training loss: 0.6172791719436646 / Valid loss: 7.379423041570754
Training loss: 0.9896843433380127 / Valid loss: 7.438692106519427
Training loss: 0.5997073650360107 / Valid loss: 7.477671346210298

Epoch: 18
Training loss: 0.8573463559150696 / Valid loss: 7.295192795708066
Training loss: 0.8314902782440186 / Valid loss: 7.141117772601899
Training loss: 0.6881555318832397 / Valid loss: 7.369932787758963
Training loss: 0.64985191822052 / Valid loss: 7.362876728602818
Training loss: 0.8614891171455383 / Valid loss: 7.348644619896298

Epoch: 19
Training loss: 0.7326408624649048 / Valid loss: 7.238343577157884
Training loss: 0.6135923266410828 / Valid loss: 7.344015532448179
Training loss: 0.5481317043304443 / Valid loss: 7.336230659484864
Training loss: 0.5523149371147156 / Valid loss: 7.182931166603452

Epoch: 20
Training loss: 0.8780498504638672 / Valid loss: 7.308260763259161
Training loss: 0.6283003687858582 / Valid loss: 7.086600067501976
Training loss: 0.9746077060699463 / Valid loss: 7.208203333900088
Training loss: 0.559203028678894 / Valid loss: 7.355150352205549
Training loss: 0.45754319429397583 / Valid loss: 7.102150449298677

Epoch: 21
Training loss: 0.43223732709884644 / Valid loss: 7.3224254471915105
Training loss: 0.7758286595344543 / Valid loss: 7.303825873420352
Training loss: 0.44938966631889343 / Valid loss: 7.221402608780634
Training loss: 0.6202768087387085 / Valid loss: 7.171271728333973
Training loss: 0.6793991327285767 / Valid loss: 7.361395483925229

Epoch: 22
Training loss: 0.7788894176483154 / Valid loss: 7.240406290690104
Training loss: 0.5356668829917908 / Valid loss: 7.1058381716410315
Training loss: 0.8052983283996582 / Valid loss: 7.100764749163673
Training loss: 0.9703494310379028 / Valid loss: 7.156808884938558
Training loss: 0.7616208791732788 / Valid loss: 7.347239907582601

Epoch: 23
Training loss: 0.5512274503707886 / Valid loss: 7.119232402529035
Training loss: 0.5891202688217163 / Valid loss: 7.029521560668945
Training loss: 1.5118154287338257 / Valid loss: 7.2778043383643745
Training loss: 0.5696700811386108 / Valid loss: 7.241815676007952
Training loss: 0.42461729049682617 / Valid loss: 7.318197927020845

Epoch: 24
Training loss: 0.6593359112739563 / Valid loss: 7.04778775260562
Training loss: 0.5937137007713318 / Valid loss: 7.25614758446103
Training loss: 0.8071039915084839 / Valid loss: 7.087380164010185
Training loss: 0.6280967593193054 / Valid loss: 7.213193711780367
Training loss: 0.8227238655090332 / Valid loss: 7.225047751835414

Epoch: 25
Training loss: 0.9029080867767334 / Valid loss: 7.237884950637818
Training loss: 0.4482448101043701 / Valid loss: 7.16145289057777
Training loss: 0.5572463274002075 / Valid loss: 7.209363782973517
Training loss: 0.8853381872177124 / Valid loss: 7.292361186799549
Training loss: 0.7618217468261719 / Valid loss: 7.07868545168922

Epoch: 26
Training loss: 0.5659929513931274 / Valid loss: 7.09491309438433
Training loss: 0.3699619770050049 / Valid loss: 7.140770460310437
Training loss: 0.7133314609527588 / Valid loss: 7.2388185705457415
Training loss: 0.5230093598365784 / Valid loss: 7.095794491540818
Training loss: 0.6150897741317749 / Valid loss: 7.1473843915121895

Epoch: 27
Training loss: 0.45201781392097473 / Valid loss: 7.143832853862217
Training loss: 0.5262253284454346 / Valid loss: 6.996785801932925
Training loss: 0.5815036296844482 / Valid loss: 6.998639029548282
Training loss: 0.5953099131584167 / Valid loss: 7.187194597153437
Training loss: 0.3797604739665985 / Valid loss: 7.2392750876290455

Epoch: 28
Training loss: 0.6146633625030518 / Valid loss: 7.0363540286109565
Training loss: 0.5728498697280884 / Valid loss: 7.144863183157785
Training loss: 1.238067865371704 / Valid loss: 7.066889024916149
Training loss: 0.5993437767028809 / Valid loss: 7.1531838689531595
Training loss: 0.520740807056427 / Valid loss: 7.112983210881551

Epoch: 29
Training loss: 0.4065859317779541 / Valid loss: 7.114023944309779
Training loss: 0.5538874864578247 / Valid loss: 7.102099028087798
Training loss: 0.5624879002571106 / Valid loss: 7.041274197896322
Training loss: 0.45879262685775757 / Valid loss: 7.0496503966195245

Epoch: 30
Training loss: 1.02530837059021 / Valid loss: 7.073905370348975
Training loss: 0.5587345361709595 / Valid loss: 7.050358213697161
Training loss: 0.48831722140312195 / Valid loss: 6.93188122340611
Training loss: 0.4351840615272522 / Valid loss: 7.030238097054617
Training loss: 0.5063328146934509 / Valid loss: 6.976387850443522

Epoch: 31
Training loss: 0.9016161561012268 / Valid loss: 7.1106956073216026
Training loss: 0.30285724997520447 / Valid loss: 7.209836364927746
Training loss: 0.5246853828430176 / Valid loss: 7.134376766568138
Training loss: 0.5111252069473267 / Valid loss: 7.182052253541492
Training loss: 0.329526424407959 / Valid loss: 7.156474354153588

Epoch: 32
Training loss: 0.45034146308898926 / Valid loss: 7.034959974743071
Training loss: 0.36200302839279175 / Valid loss: 7.063098580496652
Training loss: 0.6208869218826294 / Valid loss: 7.071472815104893
Training loss: 0.9391857981681824 / Valid loss: 7.129809438614618
Training loss: 0.5891504883766174 / Valid loss: 7.079946340833391

Epoch: 33
Training loss: 0.47037649154663086 / Valid loss: 7.026801463535854
Training loss: 0.46417906880378723 / Valid loss: 7.096503253210159
Training loss: 0.7632994651794434 / Valid loss: 7.049698116665795
Training loss: 0.5077027082443237 / Valid loss: 7.027401195253645
Training loss: 0.3472188711166382 / Valid loss: 7.095018818264916

Epoch: 34
Training loss: 0.5092960596084595 / Valid loss: 7.03063680103847
Training loss: 0.32198625802993774 / Valid loss: 7.14586706161499
Training loss: 0.6064193248748779 / Valid loss: 7.036485108875093
Training loss: 0.6879880428314209 / Valid loss: 6.993800612858363
Training loss: 0.40063947439193726 / Valid loss: 6.997705870582944

Epoch: 35
Training loss: 0.39615947008132935 / Valid loss: 7.049218904404413
Training loss: 0.6743810772895813 / Valid loss: 6.9788694745018365
Training loss: 0.7066625356674194 / Valid loss: 6.9036348479134695
Training loss: 0.4506012201309204 / Valid loss: 7.043386370795114
Training loss: 0.7278189659118652 / Valid loss: 7.008890215555827

Epoch: 36
Training loss: 0.5505895614624023 / Valid loss: 7.07335136958531
Training loss: 0.7589244842529297 / Valid loss: 7.01029178074428
Training loss: 0.5218005180358887 / Valid loss: 7.011599191029867
Training loss: 0.3705276846885681 / Valid loss: 7.067968407131377
Training loss: 0.31532612442970276 / Valid loss: 7.003584257761637

Epoch: 37
Training loss: 0.7081388235092163 / Valid loss: 7.074417813618978
Training loss: 0.34564217925071716 / Valid loss: 6.998870731535412
Training loss: 0.9944062232971191 / Valid loss: 6.989699275153024
Training loss: 0.34951478242874146 / Valid loss: 6.984957063765753
Training loss: 0.4396660327911377 / Valid loss: 7.116945972896757

Epoch: 38
Training loss: 0.5563550591468811 / Valid loss: 7.035859489440918
Training loss: 0.6219353079795837 / Valid loss: 7.03034592583066
Training loss: 0.5361130237579346 / Valid loss: 7.005019533066522
Training loss: 0.2752092182636261 / Valid loss: 6.975260319028582
Training loss: 0.5739355087280273 / Valid loss: 7.054572927384149

Epoch: 39
Training loss: 0.6006601452827454 / Valid loss: 6.934843449365525
Training loss: 0.5036587715148926 / Valid loss: 7.05179462886992
Training loss: 0.42665034532546997 / Valid loss: 7.024667240324474
Training loss: 0.2797468304634094 / Valid loss: 7.028275031135196
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.633414075488136
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.259333610534668 / Valid loss: 16.386977095831007
Model is saved in epoch 0, overall batch: 0
Training loss: 7.421402931213379 / Valid loss: 7.294734271367391
Model is saved in epoch 0, overall batch: 100
Training loss: 5.063512802124023 / Valid loss: 7.456066726502918
Training loss: 4.489648818969727 / Valid loss: 7.652603941872006
Training loss: 6.507660865783691 / Valid loss: 7.617288189842587

Epoch: 1
Training loss: 4.853457450866699 / Valid loss: 7.357696228935605
Training loss: 3.3750319480895996 / Valid loss: 7.34837612424578
Training loss: 3.317657470703125 / Valid loss: 7.239775621323359
Model is saved in epoch 1, overall batch: 700
Training loss: 4.547534465789795 / Valid loss: 7.383674035753523
Training loss: 4.821517467498779 / Valid loss: 7.222635069347564
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 1.7411561012268066 / Valid loss: 7.123611690884545
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.4082093238830566 / Valid loss: 7.058926632290794
Model is saved in epoch 2, overall batch: 1100
Training loss: 2.3950462341308594 / Valid loss: 7.2334194819132485
Training loss: 2.3204126358032227 / Valid loss: 7.1026228950137185
Training loss: 2.1996521949768066 / Valid loss: 7.078997539338611

Epoch: 3
Training loss: 1.8472756147384644 / Valid loss: 7.06068126814706
Training loss: 2.327726364135742 / Valid loss: 7.1362818854195735
Training loss: 2.7827653884887695 / Valid loss: 7.1704674743470695
Training loss: 1.9511806964874268 / Valid loss: 7.052367432912191
Model is saved in epoch 3, overall batch: 1800
Training loss: 3.4549245834350586 / Valid loss: 7.161295654660179

Epoch: 4
Training loss: 1.6542669534683228 / Valid loss: 7.148267491658529
Training loss: 1.9183485507965088 / Valid loss: 7.196789269220261
Training loss: 1.9848891496658325 / Valid loss: 7.186891805557978
Training loss: 2.632777690887451 / Valid loss: 7.147887838454474
Training loss: 1.3728902339935303 / Valid loss: 7.277724293300084

Epoch: 5
Training loss: 1.2274720668792725 / Valid loss: 7.231484735579718
Training loss: 1.8925182819366455 / Valid loss: 7.297445301782517
Training loss: 1.1940969228744507 / Valid loss: 7.244021928878057
Training loss: 2.3741962909698486 / Valid loss: 7.278838357471284
Training loss: 2.035041332244873 / Valid loss: 7.264431290399461

Epoch: 6
Training loss: 1.2425270080566406 / Valid loss: 7.342561953408378
Training loss: 1.832422137260437 / Valid loss: 7.291105393001011
Training loss: 2.2173454761505127 / Valid loss: 7.27857050214495
Training loss: 1.1891815662384033 / Valid loss: 7.349356674012684
Training loss: 2.8679633140563965 / Valid loss: 7.310619390578497

Epoch: 7
Training loss: 1.5924818515777588 / Valid loss: 7.389623564765567
Training loss: 1.098783254623413 / Valid loss: 7.369022274017334
Training loss: 1.3942701816558838 / Valid loss: 7.372895172664097
Training loss: 1.1766763925552368 / Valid loss: 7.416247797012329
Training loss: 1.5394418239593506 / Valid loss: 7.49283964520409

Epoch: 8
Training loss: 0.7722517848014832 / Valid loss: 7.458910399391538
Training loss: 1.2936915159225464 / Valid loss: 7.490943100338891
Training loss: 0.9069141745567322 / Valid loss: 7.517860167367118
Training loss: 1.0519990921020508 / Valid loss: 7.562261763073149
Training loss: 0.9688439965248108 / Valid loss: 7.621828051975795

Epoch: 9
Training loss: 0.9563419818878174 / Valid loss: 7.558683109283447
Training loss: 0.9411452412605286 / Valid loss: 7.550787625994001
Training loss: 1.117580771446228 / Valid loss: 7.577686341603597
Training loss: 0.6962872743606567 / Valid loss: 7.502855900355748

Epoch: 10
Training loss: 0.5833444595336914 / Valid loss: 7.535880556560698
Training loss: 0.5962950587272644 / Valid loss: 7.629016535622733
Training loss: 0.6179255843162537 / Valid loss: 7.515890441622052
Training loss: 1.3326702117919922 / Valid loss: 7.5672479947408045
Training loss: 1.1496168375015259 / Valid loss: 7.650594942910331

Epoch: 11
Training loss: 0.6959788799285889 / Valid loss: 7.664989471435547
Training loss: 0.4097755253314972 / Valid loss: 7.629639957064674
Training loss: 0.5550545454025269 / Valid loss: 7.6509999093555265
Training loss: 0.8193585872650146 / Valid loss: 7.622042292640323
Training loss: 0.945412814617157 / Valid loss: 7.718698083786737

Epoch: 12
Training loss: 0.7166229486465454 / Valid loss: 7.681684076218378
Training loss: 0.5068440437316895 / Valid loss: 7.6442746117001485
Training loss: 0.9382292032241821 / Valid loss: 7.713470851807367
Training loss: 0.6928702592849731 / Valid loss: 7.699763684045701
Training loss: 0.6710524559020996 / Valid loss: 7.670187214442662

Epoch: 13
Training loss: 0.9623652696609497 / Valid loss: 7.640221500396729
Training loss: 0.5877181887626648 / Valid loss: 7.649109463464646
Training loss: 0.7075598239898682 / Valid loss: 7.719532421657017
Training loss: 0.4593173861503601 / Valid loss: 7.756970650809151
Training loss: 1.0122019052505493 / Valid loss: 7.672494938260033

Epoch: 14
Training loss: 0.6343217492103577 / Valid loss: 7.708109151749384
Training loss: 0.5755285024642944 / Valid loss: 7.7192162377493725
Training loss: 0.7256033420562744 / Valid loss: 7.747155135018485
Training loss: 0.5976145267486572 / Valid loss: 7.707838685171945
Training loss: 0.7028481364250183 / Valid loss: 7.794702743348621

Epoch: 15
Training loss: 0.8523523807525635 / Valid loss: 7.747870031992594
Training loss: 0.6504839658737183 / Valid loss: 7.749308799562
Training loss: 0.7014446258544922 / Valid loss: 7.716396022978283
Training loss: 0.9422791004180908 / Valid loss: 7.769650890713646
Training loss: 0.3886116147041321 / Valid loss: 7.728266634259906

Epoch: 16
Training loss: 0.7218523025512695 / Valid loss: 7.74188563483102
Training loss: 0.547980785369873 / Valid loss: 7.702582806632632
Training loss: 0.5312973260879517 / Valid loss: 7.786190809522356
Training loss: 0.44078582525253296 / Valid loss: 7.749011639186314
Training loss: 0.8331343531608582 / Valid loss: 7.756561569940477

Epoch: 17
Training loss: 0.5535763502120972 / Valid loss: 7.756860092708043
Training loss: 0.410273015499115 / Valid loss: 7.751201438903808
Training loss: 0.5249017477035522 / Valid loss: 7.772121293204171
Training loss: 0.6253662109375 / Valid loss: 7.744480251130604
Training loss: 0.5640993118286133 / Valid loss: 7.662019425346738

Epoch: 18
Training loss: 0.7832044959068298 / Valid loss: 7.672732598440987
Training loss: 0.6617567539215088 / Valid loss: 7.634000287737165
Training loss: 0.8431117534637451 / Valid loss: 7.759443451109386
Training loss: 0.47186991572380066 / Valid loss: 7.790659214201428
Training loss: 0.5899391174316406 / Valid loss: 7.768643465496245

Epoch: 19
Training loss: 0.8049662113189697 / Valid loss: 7.732694071815128
Training loss: 0.6519738435745239 / Valid loss: 7.687354991549538
Training loss: 0.6507050395011902 / Valid loss: 7.776099790845598
Training loss: 0.934843897819519 / Valid loss: 7.771201892126174

Epoch: 20
Training loss: 0.4637126922607422 / Valid loss: 7.813354510352725
Training loss: 0.6622997522354126 / Valid loss: 7.720173413412912
Training loss: 0.36592912673950195 / Valid loss: 7.711186286381313
Training loss: 0.46823740005493164 / Valid loss: 7.725120290120443
Training loss: 0.46921420097351074 / Valid loss: 7.689217481159028

Epoch: 21
Training loss: 0.38206803798675537 / Valid loss: 7.843234171186174
Training loss: 0.5004764795303345 / Valid loss: 7.849012738182431
Training loss: 0.6883891820907593 / Valid loss: 7.773934082757859
Training loss: 0.5247881412506104 / Valid loss: 7.750081925165086
Training loss: 0.45896899700164795 / Valid loss: 7.769806652977353

Epoch: 22
Training loss: 0.6178790330886841 / Valid loss: 7.749468819300334
Training loss: 0.2715582847595215 / Valid loss: 7.716893586658296
Training loss: 0.8666759729385376 / Valid loss: 7.797267010098412
Training loss: 0.36712783575057983 / Valid loss: 7.773862657092867
Training loss: 0.2966751158237457 / Valid loss: 7.775914423806327

Epoch: 23
Training loss: 0.40706735849380493 / Valid loss: 7.76423126856486
Training loss: 0.30172404646873474 / Valid loss: 7.735501652672177
Training loss: 0.5777668952941895 / Valid loss: 7.669117021560669
Training loss: 0.7599438428878784 / Valid loss: 7.791470264253162
Training loss: 0.584872841835022 / Valid loss: 7.797046375274658

Epoch: 24
Training loss: 0.3471018075942993 / Valid loss: 7.760673450288318
Training loss: 0.4860416054725647 / Valid loss: 7.7607119878133135
Training loss: 0.769312858581543 / Valid loss: 7.75085867927188
Training loss: 0.4224802255630493 / Valid loss: 7.805610456920806
Training loss: 0.47087302803993225 / Valid loss: 7.754316647847493

Epoch: 25
Training loss: 0.5816630125045776 / Valid loss: 7.729251988728842
Training loss: 0.4738662540912628 / Valid loss: 7.789006587437221
Training loss: 0.3066001236438751 / Valid loss: 7.766674831935338
Training loss: 0.49002328515052795 / Valid loss: 7.800380906604585
Training loss: 0.36689573526382446 / Valid loss: 7.821084440322149

Epoch: 26
Training loss: 0.37549492716789246 / Valid loss: 7.7142133667355495
Training loss: 0.40560048818588257 / Valid loss: 7.733352493104481
Training loss: 0.3429441452026367 / Valid loss: 7.746509025210426
Training loss: 0.4019433856010437 / Valid loss: 7.7872209548950195
Training loss: 0.5948649644851685 / Valid loss: 7.773839855194092

Epoch: 27
Training loss: 0.6038488149642944 / Valid loss: 7.759534309023903
Training loss: 0.6449961066246033 / Valid loss: 7.803047243754069
Training loss: 0.4448374807834625 / Valid loss: 7.717813155764625
Training loss: 0.30741769075393677 / Valid loss: 7.8026058424086795
Training loss: 0.40485042333602905 / Valid loss: 7.751014037359329

Epoch: 28
Training loss: 0.3052947223186493 / Valid loss: 7.71564474105835
Training loss: 0.4772845208644867 / Valid loss: 7.7755568141029
Training loss: 0.6415899395942688 / Valid loss: 7.737557461148216
Training loss: 0.5203884840011597 / Valid loss: 7.782589326586042
Training loss: 0.3289738893508911 / Valid loss: 7.76653376079741

Epoch: 29
Training loss: 0.9258175492286682 / Valid loss: 7.736147135779971
Training loss: 0.3086315095424652 / Valid loss: 7.750421900976272
Training loss: 0.6337065696716309 / Valid loss: 7.725694660913376
Training loss: 0.3875366449356079 / Valid loss: 7.76132440112886

Epoch: 30
Training loss: 0.40989089012145996 / Valid loss: 7.765025220598493
Training loss: 0.5053951740264893 / Valid loss: 7.695381532396589
Training loss: 0.7996851205825806 / Valid loss: 7.7585045360383535
Training loss: 0.7004897594451904 / Valid loss: 7.685610525948661
Training loss: 0.14794355630874634 / Valid loss: 7.776988924117315

Epoch: 31
Training loss: 0.3211367726325989 / Valid loss: 7.7260421979995
Training loss: 0.3812273144721985 / Valid loss: 7.686133398328509
Training loss: 0.22909420728683472 / Valid loss: 7.732835519881475
Training loss: 0.34706366062164307 / Valid loss: 7.6959411802746
Training loss: 0.6292916536331177 / Valid loss: 7.724841544741676

Epoch: 32
Training loss: 0.2678816616535187 / Valid loss: 7.708443228403727
Training loss: 0.21805186569690704 / Valid loss: 7.686443719409761
Training loss: 0.31599822640419006 / Valid loss: 7.6864408674694245
Training loss: 0.3327346444129944 / Valid loss: 7.700274285816011
Training loss: 0.4393177926540375 / Valid loss: 7.686028716677711

Epoch: 33
Training loss: 0.7135055065155029 / Valid loss: 7.693426114036924
Training loss: 0.2614678740501404 / Valid loss: 7.744341427939279
Training loss: 0.3329267203807831 / Valid loss: 7.742252676827567
Training loss: 0.26074472069740295 / Valid loss: 7.675437668391637
Training loss: 0.4230120778083801 / Valid loss: 7.69008610135033

Epoch: 34
Training loss: 0.3009459376335144 / Valid loss: 7.735709612710135
Training loss: 0.2835341989994049 / Valid loss: 7.77729488554455
Training loss: 0.6893104314804077 / Valid loss: 7.734128552391415
Training loss: 0.3251539468765259 / Valid loss: 7.673783275059291
Training loss: 0.5468126535415649 / Valid loss: 7.746729378473191

Epoch: 35
Training loss: 0.49405229091644287 / Valid loss: 7.7117414474487305
Training loss: 0.4930240511894226 / Valid loss: 7.728271652403332
Training loss: 0.2576720118522644 / Valid loss: 7.664216636476063
Training loss: 0.20463943481445312 / Valid loss: 7.6529285430908205
Training loss: 0.24624058604240417 / Valid loss: 7.772773878914969

Epoch: 36
Training loss: 0.3930303454399109 / Valid loss: 7.720309525444394
Training loss: 0.34120333194732666 / Valid loss: 7.634539685930524
Training loss: 0.2279122769832611 / Valid loss: 7.66193839708964
Training loss: 0.6164922118186951 / Valid loss: 7.67860703695388
Training loss: 0.2831310033798218 / Valid loss: 7.671414974757603

Epoch: 37
Training loss: 0.3207811117172241 / Valid loss: 7.688708655039469
Training loss: 1.1527056694030762 / Valid loss: 7.678799047924223
Training loss: 0.30674487352371216 / Valid loss: 7.698499225434803
Training loss: 0.44140905141830444 / Valid loss: 7.637709299723308
Training loss: 0.3484949767589569 / Valid loss: 7.6704507736932666

Epoch: 38
Training loss: 0.3271687626838684 / Valid loss: 7.628723762148902
Training loss: 0.17839935421943665 / Valid loss: 7.659501227878389
Training loss: 0.3498941659927368 / Valid loss: 7.686242889222645
Training loss: 0.21842503547668457 / Valid loss: 7.694141837528774
Training loss: 0.39932700991630554 / Valid loss: 7.6856816882178896

Epoch: 39
Training loss: 0.31953608989715576 / Valid loss: 7.628911481584821
Training loss: 0.3881165385246277 / Valid loss: 7.662602588108608
Training loss: 0.550746738910675 / Valid loss: 7.658024029504685
Training loss: 0.1762368381023407 / Valid loss: 7.686026514144171
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1800): 6.927202915009998
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.259333610534668 / Valid loss: 16.385939834231422
Model is saved in epoch 0, overall batch: 0
Training loss: 7.252373218536377 / Valid loss: 7.591402698698498
Model is saved in epoch 0, overall batch: 100
Training loss: 5.23737096786499 / Valid loss: 7.644773914700463
Training loss: 4.503203868865967 / Valid loss: 7.74562828200204
Training loss: 6.097472190856934 / Valid loss: 7.704359853835333

Epoch: 1
Training loss: 4.837756156921387 / Valid loss: 7.430102707090832
Model is saved in epoch 1, overall batch: 500
Training loss: 3.3446929454803467 / Valid loss: 7.43174558367048
Training loss: 3.40881609916687 / Valid loss: 7.263197549184164
Model is saved in epoch 1, overall batch: 700
Training loss: 4.384796142578125 / Valid loss: 7.431910015287853
Training loss: 5.01453971862793 / Valid loss: 7.256502160571871
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 1.754379391670227 / Valid loss: 7.14393812588283
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.3442635536193848 / Valid loss: 7.100853997185117
Model is saved in epoch 2, overall batch: 1100
Training loss: 2.3916707038879395 / Valid loss: 7.2501815341767815
Training loss: 2.3490638732910156 / Valid loss: 7.13268932842073
Training loss: 2.2454710006713867 / Valid loss: 7.083453964051746
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 1.8973710536956787 / Valid loss: 7.055685408910116
Model is saved in epoch 3, overall batch: 1500
Training loss: 2.3536291122436523 / Valid loss: 7.168222863333566
Training loss: 2.899005174636841 / Valid loss: 7.224518605640957
Training loss: 1.912628173828125 / Valid loss: 7.081814516158331
Training loss: 3.3124756813049316 / Valid loss: 7.150453644707089

Epoch: 4
Training loss: 1.581431269645691 / Valid loss: 7.117100306919643
Training loss: 1.8544127941131592 / Valid loss: 7.169473988669259
Training loss: 1.9225026369094849 / Valid loss: 7.184220425287882
Training loss: 2.8285393714904785 / Valid loss: 7.185959216526577
Training loss: 1.4487764835357666 / Valid loss: 7.294784695761544

Epoch: 5
Training loss: 1.3775150775909424 / Valid loss: 7.213582057044619
Training loss: 2.014568567276001 / Valid loss: 7.323452645256406
Training loss: 1.026606798171997 / Valid loss: 7.254944465273902
Training loss: 2.3683106899261475 / Valid loss: 7.268593974340529
Training loss: 2.1220431327819824 / Valid loss: 7.268431007294428

Epoch: 6
Training loss: 1.158233404159546 / Valid loss: 7.334492331459408
Training loss: 1.6292279958724976 / Valid loss: 7.291815987087431
Training loss: 2.2145748138427734 / Valid loss: 7.26947721299671
Training loss: 1.115936517715454 / Valid loss: 7.354025214059012
Training loss: 2.8449490070343018 / Valid loss: 7.262700049082438

Epoch: 7
Training loss: 1.488938570022583 / Valid loss: 7.338129511333647
Training loss: 0.9966207146644592 / Valid loss: 7.376047861008417
Training loss: 1.4107210636138916 / Valid loss: 7.396921911693755
Training loss: 1.3786906003952026 / Valid loss: 7.45283454486302
Training loss: 1.2801525592803955 / Valid loss: 7.473057526633853

Epoch: 8
Training loss: 1.042473316192627 / Valid loss: 7.409027903420585
Training loss: 1.2985676527023315 / Valid loss: 7.417276173546201
Training loss: 0.9350931644439697 / Valid loss: 7.44295992624192
Training loss: 1.122589349746704 / Valid loss: 7.4935279437473845
Training loss: 1.1857001781463623 / Valid loss: 7.523985326857794

Epoch: 9
Training loss: 0.8621349930763245 / Valid loss: 7.485442275092716
Training loss: 1.0252774953842163 / Valid loss: 7.48845252536592
Training loss: 0.7890369892120361 / Valid loss: 7.520670663742792
Training loss: 0.6182724833488464 / Valid loss: 7.499733652387347

Epoch: 10
Training loss: 0.4120250344276428 / Valid loss: 7.5674062410990395
Training loss: 0.6433447599411011 / Valid loss: 7.595100702558245
Training loss: 0.45237457752227783 / Valid loss: 7.49027674084618
Training loss: 1.4594409465789795 / Valid loss: 7.539215383075533
Training loss: 0.9567162990570068 / Valid loss: 7.567287027268183

Epoch: 11
Training loss: 0.685506284236908 / Valid loss: 7.574731690543039
Training loss: 0.4614105820655823 / Valid loss: 7.571998160226005
Training loss: 0.5260586738586426 / Valid loss: 7.608769598461333
Training loss: 0.7565267086029053 / Valid loss: 7.58316790262858
Training loss: 1.0699645280838013 / Valid loss: 7.661766297476632

Epoch: 12
Training loss: 0.7848432064056396 / Valid loss: 7.641270392281669
Training loss: 0.5812451839447021 / Valid loss: 7.587218284606934
Training loss: 1.138641119003296 / Valid loss: 7.70700615474156
Training loss: 0.766156017780304 / Valid loss: 7.6409730638776505
Training loss: 0.6468878388404846 / Valid loss: 7.631494835444859

Epoch: 13
Training loss: 0.9155690670013428 / Valid loss: 7.588308070954822
Training loss: 0.7157909870147705 / Valid loss: 7.630995532444545
Training loss: 0.5242900848388672 / Valid loss: 7.693625668116979
Training loss: 0.4035136103630066 / Valid loss: 7.705456270490374
Training loss: 1.053321123123169 / Valid loss: 7.633903853098551

Epoch: 14
Training loss: 0.8853653073310852 / Valid loss: 7.674412100655692
Training loss: 0.6629244089126587 / Valid loss: 7.660193316141764
Training loss: 0.5388768911361694 / Valid loss: 7.6488186018807545
Training loss: 0.6915408968925476 / Valid loss: 7.624452622731527
Training loss: 0.6788637638092041 / Valid loss: 7.73151121593657

Epoch: 15
Training loss: 0.8442180156707764 / Valid loss: 7.688466308230446
Training loss: 0.5534759163856506 / Valid loss: 7.730913425627209
Training loss: 0.7227962613105774 / Valid loss: 7.713053181057885
Training loss: 0.8476305603981018 / Valid loss: 7.729187638419015
Training loss: 0.5862538814544678 / Valid loss: 7.690870239621117

Epoch: 16
Training loss: 0.7827404737472534 / Valid loss: 7.658340599423363
Training loss: 0.3674689829349518 / Valid loss: 7.651048973628453
Training loss: 0.5423414707183838 / Valid loss: 7.729144387018113
Training loss: 0.44172877073287964 / Valid loss: 7.7015086446489605
Training loss: 0.857509195804596 / Valid loss: 7.6844053540910995

Epoch: 17
Training loss: 0.5701535940170288 / Valid loss: 7.682525271461124
Training loss: 0.339246541261673 / Valid loss: 7.6351565133957635
Training loss: 0.5474088191986084 / Valid loss: 7.694354484194801
Training loss: 0.6329200863838196 / Valid loss: 7.652770637330555
Training loss: 0.46424582600593567 / Valid loss: 7.6144043104989185

Epoch: 18
Training loss: 0.9167977571487427 / Valid loss: 7.636458319709415
Training loss: 0.6015387177467346 / Valid loss: 7.603410929725284
Training loss: 0.9359579086303711 / Valid loss: 7.6913249288286485
Training loss: 0.418607622385025 / Valid loss: 7.7308910369873045
Training loss: 0.610854983329773 / Valid loss: 7.7308801151457285

Epoch: 19
Training loss: 0.8406809568405151 / Valid loss: 7.70784706388201
Training loss: 0.5686765313148499 / Valid loss: 7.6605152311779205
Training loss: 0.8250553011894226 / Valid loss: 7.728544112614223
Training loss: 0.8473042845726013 / Valid loss: 7.70749067578997

Epoch: 20
Training loss: 0.45182618498802185 / Valid loss: 7.722955526624407
Training loss: 0.7737323045730591 / Valid loss: 7.646057197025844
Training loss: 0.44731593132019043 / Valid loss: 7.657916586739677
Training loss: 0.6866109371185303 / Valid loss: 7.753954333350772
Training loss: 0.42390352487564087 / Valid loss: 7.6771890140715096

Epoch: 21
Training loss: 0.4350682199001312 / Valid loss: 7.8375132969447545
Training loss: 0.4301674962043762 / Valid loss: 7.770384075528099
Training loss: 0.6017532348632812 / Valid loss: 7.667503870101202
Training loss: 0.6740768551826477 / Valid loss: 7.655122520810082
Training loss: 0.4581572413444519 / Valid loss: 7.693304134550549

Epoch: 22
Training loss: 0.630089282989502 / Valid loss: 7.736737551007952
Training loss: 0.5437720417976379 / Valid loss: 7.670761003948393
Training loss: 0.9349483847618103 / Valid loss: 7.707387025015695
Training loss: 0.37846821546554565 / Valid loss: 7.694885848817371
Training loss: 0.367053747177124 / Valid loss: 7.6655497596377415

Epoch: 23
Training loss: 0.36209940910339355 / Valid loss: 7.693915839422317
Training loss: 0.3616594076156616 / Valid loss: 7.688939430600121
Training loss: 0.5485801696777344 / Valid loss: 7.602689084552583
Training loss: 0.6840894222259521 / Valid loss: 7.706778907775879
Training loss: 0.5429892539978027 / Valid loss: 7.707777509235201

Epoch: 24
Training loss: 0.29202720522880554 / Valid loss: 7.680475471133278
Training loss: 0.5594267845153809 / Valid loss: 7.7029404458545505
Training loss: 0.7397544980049133 / Valid loss: 7.696308233624413
Training loss: 0.5193742513656616 / Valid loss: 7.75549654733567
Training loss: 0.41431015729904175 / Valid loss: 7.67464386622111

Epoch: 25
Training loss: 0.4688763916492462 / Valid loss: 7.6839897927783785
Training loss: 0.4992485046386719 / Valid loss: 7.735690938858759
Training loss: 0.2931770086288452 / Valid loss: 7.683585988907587
Training loss: 0.3667208254337311 / Valid loss: 7.709155505044119
Training loss: 0.49811282753944397 / Valid loss: 7.69405734198434

Epoch: 26
Training loss: 0.4509601593017578 / Valid loss: 7.672591858818418
Training loss: 0.426550954580307 / Valid loss: 7.730830732981364
Training loss: 0.2681511342525482 / Valid loss: 7.740749434062413
Training loss: 0.42565807700157166 / Valid loss: 7.700776749565488
Training loss: 0.6289275884628296 / Valid loss: 7.692437589736212

Epoch: 27
Training loss: 0.7376934289932251 / Valid loss: 7.712211084365845
Training loss: 0.7826156616210938 / Valid loss: 7.697963900793166
Training loss: 0.4186772108078003 / Valid loss: 7.645804019201369
Training loss: 0.3087539076805115 / Valid loss: 7.741184570675805
Training loss: 0.33597299456596375 / Valid loss: 7.698944623129709

Epoch: 28
Training loss: 0.2752162218093872 / Valid loss: 7.675393075034732
Training loss: 0.5451362729072571 / Valid loss: 7.725919455573672
Training loss: 0.633409857749939 / Valid loss: 7.676008342561268
Training loss: 0.46243733167648315 / Valid loss: 7.732628463563465
Training loss: 0.4101361930370331 / Valid loss: 7.713757310594831

Epoch: 29
Training loss: 0.7189409732818604 / Valid loss: 7.68192944299607
Training loss: 0.30131858587265015 / Valid loss: 7.687726642971947
Training loss: 0.6678038835525513 / Valid loss: 7.699330679575602
Training loss: 0.38694918155670166 / Valid loss: 7.733143406822568

Epoch: 30
Training loss: 0.42250990867614746 / Valid loss: 7.791126764388311
Training loss: 0.5711621642112732 / Valid loss: 7.720078913370768
Training loss: 0.7294062972068787 / Valid loss: 7.745399157206218
Training loss: 0.6203378438949585 / Valid loss: 7.643038100288027
Training loss: 0.22213537991046906 / Valid loss: 7.734165123530796

Epoch: 31
Training loss: 0.4636382758617401 / Valid loss: 7.657734253292992
Training loss: 0.4515085220336914 / Valid loss: 7.709224528358096
Training loss: 0.2860736846923828 / Valid loss: 7.696187359946115
Training loss: 0.3451536297798157 / Valid loss: 7.692850698743547
Training loss: 0.7907519936561584 / Valid loss: 7.713608505612328

Epoch: 32
Training loss: 0.45778459310531616 / Valid loss: 7.712244079226539
Training loss: 0.41905027627944946 / Valid loss: 7.644464819771903
Training loss: 0.3360971212387085 / Valid loss: 7.65884898957752
Training loss: 0.2611235976219177 / Valid loss: 7.71467004049392
Training loss: 0.6183547973632812 / Valid loss: 7.692105347769601

Epoch: 33
Training loss: 0.8168193101882935 / Valid loss: 7.706573890504383
Training loss: 0.3255212903022766 / Valid loss: 7.741437585013253
Training loss: 0.2907058000564575 / Valid loss: 7.7034641470227925
Training loss: 0.37813490629196167 / Valid loss: 7.645056665511358
Training loss: 0.43124616146087646 / Valid loss: 7.637478387923467

Epoch: 34
Training loss: 0.4460601210594177 / Valid loss: 7.700003069923038
Training loss: 0.3315485715866089 / Valid loss: 7.694446345738002
Training loss: 0.7012571096420288 / Valid loss: 7.661834848494757
Training loss: 0.3432731032371521 / Valid loss: 7.632304255167643
Training loss: 0.3841209411621094 / Valid loss: 7.703624988737561

Epoch: 35
Training loss: 0.502434253692627 / Valid loss: 7.683362361363002
Training loss: 0.47210705280303955 / Valid loss: 7.693933818453834
Training loss: 0.2533215880393982 / Valid loss: 7.595882352193197
Training loss: 0.24142193794250488 / Valid loss: 7.621591990334647
Training loss: 0.3927237391471863 / Valid loss: 7.7772402082170755

Epoch: 36
Training loss: 0.476529598236084 / Valid loss: 7.71368769691104
Training loss: 0.41517603397369385 / Valid loss: 7.61738531930106
Training loss: 0.3389992117881775 / Valid loss: 7.645322654360816
Training loss: 0.600695788860321 / Valid loss: 7.6232616833278115
Training loss: 0.3574083745479584 / Valid loss: 7.634440767197382

Epoch: 37
Training loss: 0.4700312614440918 / Valid loss: 7.7100015594845726
Training loss: 1.2730544805526733 / Valid loss: 7.636453106289818
Training loss: 0.3944694995880127 / Valid loss: 7.690707897004627
Training loss: 0.4712972939014435 / Valid loss: 7.616479414985293
Training loss: 0.42666923999786377 / Valid loss: 7.6235624631245935

Epoch: 38
Training loss: 0.3870116174221039 / Valid loss: 7.614204960777646
Training loss: 0.3432047367095947 / Valid loss: 7.632022739592053
Training loss: 0.3982335925102234 / Valid loss: 7.650183707191831
Training loss: 0.3996313214302063 / Valid loss: 7.673926367078509
Training loss: 0.42545202374458313 / Valid loss: 7.68598690032959

Epoch: 39
Training loss: 0.3862805962562561 / Valid loss: 7.663207054138184
Training loss: 0.3730643391609192 / Valid loss: 7.655191126323881
Training loss: 0.4604327380657196 / Valid loss: 7.652069541386195
Training loss: 0.28902557492256165 / Valid loss: 7.67128388995216
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1500): 6.957749845868065
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.74432945251465 / Valid loss: 16.990839376903715
Model is saved in epoch 0, overall batch: 0
Training loss: 18.813312530517578 / Valid loss: 15.719861066909063
Model is saved in epoch 0, overall batch: 100
Training loss: 14.273900985717773 / Valid loss: 14.980305503663562
Model is saved in epoch 0, overall batch: 200
Training loss: 13.989517211914062 / Valid loss: 14.497876003810338
Model is saved in epoch 0, overall batch: 300
Training loss: 16.13970375061035 / Valid loss: 13.832912163507372
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 12.62314224243164 / Valid loss: 13.418391640981039
Model is saved in epoch 1, overall batch: 500
Training loss: 14.444650650024414 / Valid loss: 12.642709950038364
Model is saved in epoch 1, overall batch: 600
Training loss: 10.566679000854492 / Valid loss: 12.061179551624116
Model is saved in epoch 1, overall batch: 700
Training loss: 8.985713005065918 / Valid loss: 11.594688910529728
Model is saved in epoch 1, overall batch: 800
Training loss: 14.171436309814453 / Valid loss: 11.408269509815035
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 8.628375053405762 / Valid loss: 10.899189267839704
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.9038190841674805 / Valid loss: 10.476248073577882
Model is saved in epoch 2, overall batch: 1100
Training loss: 8.903303146362305 / Valid loss: 10.222679138183594
Model is saved in epoch 2, overall batch: 1200
Training loss: 4.07761287689209 / Valid loss: 9.872507304237002
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.890986442565918 / Valid loss: 9.80006348292033
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.747312545776367 / Valid loss: 9.409122353508359
Model is saved in epoch 3, overall batch: 1500
Training loss: 5.609811782836914 / Valid loss: 9.402808230263846
Model is saved in epoch 3, overall batch: 1600
Training loss: 4.346443176269531 / Valid loss: 9.235243125188918
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.2319111824035645 / Valid loss: 8.963190083276658
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.305174350738525 / Valid loss: 8.819133731297084
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 4.024024963378906 / Valid loss: 8.875485638209751
Training loss: 3.5562381744384766 / Valid loss: 8.700787598746164
Model is saved in epoch 4, overall batch: 2100
Training loss: 2.8974218368530273 / Valid loss: 9.051597227369037
Training loss: 2.7730460166931152 / Valid loss: 8.661860547746931
Model is saved in epoch 4, overall batch: 2300
Training loss: 3.276261568069458 / Valid loss: 8.536193983895439
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 1.7267770767211914 / Valid loss: 8.251972034999303
Model is saved in epoch 5, overall batch: 2500
Training loss: 2.258972644805908 / Valid loss: 8.418652330126081
Training loss: 3.373483896255493 / Valid loss: 8.145875808170864
Model is saved in epoch 5, overall batch: 2700
Training loss: 3.064239501953125 / Valid loss: 8.416890412285214
Training loss: 2.0385096073150635 / Valid loss: 8.100297051384336
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 1.5762836933135986 / Valid loss: 8.057634953090123
Model is saved in epoch 6, overall batch: 3000
Training loss: 1.522454023361206 / Valid loss: 7.912473033723377
Model is saved in epoch 6, overall batch: 3100
Training loss: 0.9389065504074097 / Valid loss: 8.032116597039359
Training loss: 1.7566790580749512 / Valid loss: 7.852360834394182
Model is saved in epoch 6, overall batch: 3300
Training loss: 1.4049817323684692 / Valid loss: 8.00399739401681

Epoch: 7
Training loss: 1.6938343048095703 / Valid loss: 7.806478068942115
Model is saved in epoch 7, overall batch: 3500
Training loss: 2.155910015106201 / Valid loss: 7.940120451790946
Training loss: 1.1555659770965576 / Valid loss: 7.708667509896415
Model is saved in epoch 7, overall batch: 3700
Training loss: 1.1622847318649292 / Valid loss: 7.891518719991049
Training loss: 1.555166482925415 / Valid loss: 7.9463343302408855

Epoch: 8
Training loss: 1.2126049995422363 / Valid loss: 7.712676279885429
Training loss: 1.7494028806686401 / Valid loss: 7.772870903923398
Training loss: 1.290571928024292 / Valid loss: 7.666056224278042
Model is saved in epoch 8, overall batch: 4200
Training loss: 1.6771138906478882 / Valid loss: 7.688364971251715
Training loss: 1.0954408645629883 / Valid loss: 7.635402525038947
Model is saved in epoch 8, overall batch: 4400

Epoch: 9
Training loss: 0.7832762002944946 / Valid loss: 7.615385028294154
Model is saved in epoch 9, overall batch: 4500
Training loss: 1.9558049440383911 / Valid loss: 7.604497886839367
Model is saved in epoch 9, overall batch: 4600
Training loss: 1.614858865737915 / Valid loss: 7.532673154558454
Model is saved in epoch 9, overall batch: 4700
Training loss: 0.790798544883728 / Valid loss: 7.586948985145206

Epoch: 10
Training loss: 0.7919823527336121 / Valid loss: 7.5485659644717265
Training loss: 0.8626841902732849 / Valid loss: 7.495163458869571
Model is saved in epoch 10, overall batch: 5000
Training loss: 1.120302438735962 / Valid loss: 7.580841275623866
Training loss: 0.927468478679657 / Valid loss: 7.630618049984887
Training loss: 0.968368411064148 / Valid loss: 7.662728700183687

Epoch: 11
Training loss: 0.8829520344734192 / Valid loss: 7.6285512787955145
Training loss: 0.7617141008377075 / Valid loss: 7.5404040881565635
Training loss: 0.723525881767273 / Valid loss: 7.463328125363304
Model is saved in epoch 11, overall batch: 5600
Training loss: 0.9767261743545532 / Valid loss: 7.566883854638963
Training loss: 1.1643108129501343 / Valid loss: 7.59400680405753

Epoch: 12
Training loss: 0.9180301427841187 / Valid loss: 7.475158596038819
Training loss: 0.8345040678977966 / Valid loss: 7.5380829924628845
Training loss: 0.7884871959686279 / Valid loss: 7.4685365676879885
Training loss: 0.9252773523330688 / Valid loss: 7.602835687001546
Training loss: 0.8961379528045654 / Valid loss: 7.544439933413551

Epoch: 13
Training loss: 0.8130096793174744 / Valid loss: 7.485337525322324
Training loss: 0.5576227903366089 / Valid loss: 7.600084736233666
Training loss: 0.6920883059501648 / Valid loss: 7.455938348316011
Model is saved in epoch 13, overall batch: 6600
Training loss: 0.8710068464279175 / Valid loss: 7.620011125292097
Training loss: 1.2184133529663086 / Valid loss: 7.557222384498233

Epoch: 14
Training loss: 0.6228121519088745 / Valid loss: 7.551608031136649
Training loss: 0.9446000456809998 / Valid loss: 7.4429547945658365
Model is saved in epoch 14, overall batch: 7000
Training loss: 0.7256184220314026 / Valid loss: 7.627997080485026
Training loss: 0.6785268187522888 / Valid loss: 7.5212421871366955
Training loss: 0.4256722629070282 / Valid loss: 7.572247328077044

Epoch: 15
Training loss: 0.5049245953559875 / Valid loss: 7.410635008130755
Model is saved in epoch 15, overall batch: 7400
Training loss: 0.9389246106147766 / Valid loss: 7.4144602957225985
Training loss: 0.8871753215789795 / Valid loss: 7.388565658387684
Model is saved in epoch 15, overall batch: 7600
Training loss: 0.872657299041748 / Valid loss: 7.5172913505917505
Training loss: 0.46819210052490234 / Valid loss: 7.419564101809547

Epoch: 16
Training loss: 0.5496691465377808 / Valid loss: 7.4636959484645296
Training loss: 1.2856229543685913 / Valid loss: 7.531298455737886
Training loss: 0.6966081857681274 / Valid loss: 7.551011117299398
Training loss: 0.7091407775878906 / Valid loss: 7.464497775123233
Training loss: 0.9415431022644043 / Valid loss: 7.434500090281168

Epoch: 17
Training loss: 1.2439026832580566 / Valid loss: 7.360284914289202
Model is saved in epoch 17, overall batch: 8400
Training loss: 1.1442034244537354 / Valid loss: 7.422486813863118
Training loss: 0.8779376745223999 / Valid loss: 7.412794299352736
Training loss: 0.7362772822380066 / Valid loss: 7.545299956912086
Training loss: 0.8588962554931641 / Valid loss: 7.400206629435221

Epoch: 18
Training loss: 0.7391802072525024 / Valid loss: 7.407582550957089
Training loss: 0.7553226351737976 / Valid loss: 7.449095135643368
Training loss: 0.7913364171981812 / Valid loss: 7.4577238037472675
Training loss: 0.6734183430671692 / Valid loss: 7.435183752150762
Training loss: 0.8338404893875122 / Valid loss: 7.441706416720436

Epoch: 19
Training loss: 0.6805326342582703 / Valid loss: 7.420129181089855
Training loss: 0.7954719662666321 / Valid loss: 7.498324762071882
Training loss: 0.9434382319450378 / Valid loss: 7.50538417725336
Training loss: 0.5377823710441589 / Valid loss: 7.517433552514939

Epoch: 20
Training loss: 0.5411249399185181 / Valid loss: 7.520140520731608
Training loss: 1.288891077041626 / Valid loss: 7.354889081773304
Model is saved in epoch 20, overall batch: 9900
Training loss: 0.5347188115119934 / Valid loss: 7.369543034689767
Training loss: 0.5903106927871704 / Valid loss: 7.483390567416237
Training loss: 0.436201274394989 / Valid loss: 7.556688522157215

Epoch: 21
Training loss: 1.249224305152893 / Valid loss: 7.46824970699492
Training loss: 0.7716360092163086 / Valid loss: 7.434505789620536
Training loss: 0.5636555552482605 / Valid loss: 7.5765745571681435
Training loss: 0.43682995438575745 / Valid loss: 7.4853153364998954
Training loss: 0.4332077205181122 / Valid loss: 7.5002570197695775

Epoch: 22
Training loss: 0.49624454975128174 / Valid loss: 7.482817522684733
Training loss: 0.8111604452133179 / Valid loss: 7.454992055892944
Training loss: 0.8119115829467773 / Valid loss: 7.419343961988177
Training loss: 0.6821426749229431 / Valid loss: 7.341489592052642
Model is saved in epoch 22, overall batch: 11100
Training loss: 0.5737651586532593 / Valid loss: 7.505808998289562

Epoch: 23
Training loss: 0.8923112154006958 / Valid loss: 7.484308973948161
Training loss: 0.7887174487113953 / Valid loss: 7.431053384145101
Training loss: 0.625036358833313 / Valid loss: 7.529787581307548
Training loss: 0.4336971044540405 / Valid loss: 7.481791092100598
Training loss: 0.5709677934646606 / Valid loss: 7.525635147094727

Epoch: 24
Training loss: 0.5227223634719849 / Valid loss: 7.450322028568813
Training loss: 0.6808269023895264 / Valid loss: 7.506665461403983
Training loss: 0.5350386500358582 / Valid loss: 7.397324557531448
Training loss: 0.3692931830883026 / Valid loss: 7.459653786250523
Training loss: 1.1281335353851318 / Valid loss: 7.57278342019944

Epoch: 25
Training loss: 0.3195123076438904 / Valid loss: 7.462442021142869
Training loss: 0.43418562412261963 / Valid loss: 7.344118967510405
Training loss: 0.42563873529434204 / Valid loss: 7.489673301151821
Training loss: 0.5214203000068665 / Valid loss: 7.52161222639538
Training loss: 0.5234217643737793 / Valid loss: 7.557514245169504

Epoch: 26
Training loss: 0.36372053623199463 / Valid loss: 7.445266024271647
Training loss: 1.4995970726013184 / Valid loss: 7.534174537658691
Training loss: 0.45029449462890625 / Valid loss: 7.4480772336324055
Training loss: 0.4070250391960144 / Valid loss: 7.363397870744977
Training loss: 0.529305100440979 / Valid loss: 7.434455299377442

Epoch: 27
Training loss: 0.5714131593704224 / Valid loss: 7.37573809396653
Training loss: 0.5351933240890503 / Valid loss: 7.421199003855388
Training loss: 0.7436501979827881 / Valid loss: 7.395827279772077
Training loss: 0.3072514235973358 / Valid loss: 7.436513723645891
Training loss: 0.3949471712112427 / Valid loss: 7.496613870348249

Epoch: 28
Training loss: 0.5007600784301758 / Valid loss: 7.415416476840065
Training loss: 0.6730638742446899 / Valid loss: 7.359137848445347
Training loss: 0.5770184397697449 / Valid loss: 7.393383711860293
Training loss: 0.49562954902648926 / Valid loss: 7.397785105024066
Training loss: 0.49659794569015503 / Valid loss: 7.382977503821963

Epoch: 29
Training loss: 0.45158594846725464 / Valid loss: 7.374274853297642
Training loss: 0.4922773241996765 / Valid loss: 7.451962205341884
Training loss: 0.650726318359375 / Valid loss: 7.470691399347214
Training loss: 0.8569173812866211 / Valid loss: 7.400675460270473

Epoch: 30
Training loss: 0.43595653772354126 / Valid loss: 7.391757874261765
Training loss: 0.7633461356163025 / Valid loss: 7.351234624499367
Training loss: 0.7028955817222595 / Valid loss: 7.424536655062721
Training loss: 0.52046799659729 / Valid loss: 7.33535871959868
Model is saved in epoch 30, overall batch: 15000
Training loss: 1.0629210472106934 / Valid loss: 7.3566318739028205

Epoch: 31
Training loss: 0.47885221242904663 / Valid loss: 7.392964263189406
Training loss: 0.6207704544067383 / Valid loss: 7.415108521779378
Training loss: 0.4553525447845459 / Valid loss: 7.438817977905273
Training loss: 0.4738275110721588 / Valid loss: 7.439741502489363
Training loss: 0.6924597024917603 / Valid loss: 7.620745036715553

Epoch: 32
Training loss: 0.4057348966598511 / Valid loss: 7.440542734236944
Training loss: 0.5216811895370483 / Valid loss: 7.474425306774321
Training loss: 0.6926430463790894 / Valid loss: 7.401492068881081
Training loss: 0.545214056968689 / Valid loss: 7.470178095499675
Training loss: 0.36849793791770935 / Valid loss: 7.395228653862363

Epoch: 33
Training loss: 0.39395153522491455 / Valid loss: 7.37813804717291
Training loss: 0.5615527033805847 / Valid loss: 7.440617316109794
Training loss: 0.5096204876899719 / Valid loss: 7.362418297358921
Training loss: 0.2662809491157532 / Valid loss: 7.36718145098005
Training loss: 0.38131409883499146 / Valid loss: 7.567627584366571

Epoch: 34
Training loss: 0.4387550354003906 / Valid loss: 7.461068530309768
Training loss: 0.4694669246673584 / Valid loss: 7.438691175551641
Training loss: 0.18706607818603516 / Valid loss: 7.4514213516598655
Training loss: 0.37767544388771057 / Valid loss: 7.609166165760585
Training loss: 1.0667245388031006 / Valid loss: 7.48573363167899

Epoch: 35
Training loss: 0.507340669631958 / Valid loss: 7.254406915392194
Model is saved in epoch 35, overall batch: 17200
Training loss: 0.38378143310546875 / Valid loss: 7.431916345868792
Training loss: 0.4092254638671875 / Valid loss: 7.381154410044352
Training loss: 0.6906702518463135 / Valid loss: 7.355726178487142
Training loss: 0.714321494102478 / Valid loss: 7.528925872984386

Epoch: 36
Training loss: 0.5722617506980896 / Valid loss: 7.483573999859038
Training loss: 0.7012848258018494 / Valid loss: 7.420779886699858
Training loss: 0.839893639087677 / Valid loss: 7.389615876334054
Training loss: 0.6870533227920532 / Valid loss: 7.483343546731131
Training loss: 0.22636592388153076 / Valid loss: 7.36722830817813

Epoch: 37
Training loss: 0.27799302339553833 / Valid loss: 7.358601897103446
Training loss: 0.514986515045166 / Valid loss: 7.36119095030285
Training loss: 0.41506558656692505 / Valid loss: 7.350421306065151
Training loss: 0.9099208116531372 / Valid loss: 7.436348579043433
Training loss: 0.8201339840888977 / Valid loss: 7.464227462950207

Epoch: 38
Training loss: 0.3348037898540497 / Valid loss: 7.479400375911168
Training loss: 0.31403008103370667 / Valid loss: 7.301410947527204
Training loss: 0.7826483249664307 / Valid loss: 7.3802891549609955
Training loss: 0.4126150906085968 / Valid loss: 7.4400951930454795
Training loss: 0.8072407245635986 / Valid loss: 7.391406663258871

Epoch: 39
Training loss: 0.37892451882362366 / Valid loss: 7.4352220467158725
Training loss: 0.5185004472732544 / Valid loss: 7.439780076344808
Training loss: 0.42077869176864624 / Valid loss: 7.384612278711228
Training loss: 0.526750922203064 / Valid loss: 7.363243545804705
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 17200): 7.17987882069179
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.74432945251465 / Valid loss: 16.99084669748942
Model is saved in epoch 0, overall batch: 0
Training loss: 18.831626892089844 / Valid loss: 16.288985306876047
Model is saved in epoch 0, overall batch: 100
Training loss: 14.225597381591797 / Valid loss: 15.18314486458188
Model is saved in epoch 0, overall batch: 200
Training loss: 14.179439544677734 / Valid loss: 14.397683570498511
Model is saved in epoch 0, overall batch: 300
Training loss: 15.704951286315918 / Valid loss: 13.693938536871046
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 12.554864883422852 / Valid loss: 13.279092270987375
Model is saved in epoch 1, overall batch: 500
Training loss: 14.29485034942627 / Valid loss: 12.159357134501139
Model is saved in epoch 1, overall batch: 600
Training loss: 10.497272491455078 / Valid loss: 11.800620446886334
Model is saved in epoch 1, overall batch: 700
Training loss: 8.923942565917969 / Valid loss: 11.030059024265833
Model is saved in epoch 1, overall batch: 800
Training loss: 14.232619285583496 / Valid loss: 11.001584448133196
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 8.4553861618042 / Valid loss: 10.620175634111677
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.919839382171631 / Valid loss: 10.137013353620256
Model is saved in epoch 2, overall batch: 1100
Training loss: 9.10477066040039 / Valid loss: 9.86355806986491
Model is saved in epoch 2, overall batch: 1200
Training loss: 4.10219669342041 / Valid loss: 9.505433046250117
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.4981184005737305 / Valid loss: 9.223033155713763
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.878084182739258 / Valid loss: 9.193220601763045
Model is saved in epoch 3, overall batch: 1500
Training loss: 5.464276313781738 / Valid loss: 9.136188606988815
Model is saved in epoch 3, overall batch: 1600
Training loss: 4.149322509765625 / Valid loss: 8.887270773024786
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.404297351837158 / Valid loss: 8.666359215690976
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.01168966293335 / Valid loss: 8.466898268745059
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.9565296173095703 / Valid loss: 8.609400426773798
Training loss: 3.5615758895874023 / Valid loss: 8.235047617412748
Model is saved in epoch 4, overall batch: 2100
Training loss: 2.8559298515319824 / Valid loss: 8.55601704461234
Training loss: 2.880500555038452 / Valid loss: 8.050821672167096
Model is saved in epoch 4, overall batch: 2300
Training loss: 3.186821222305298 / Valid loss: 8.281976886022658

Epoch: 5
Training loss: 1.6747196912765503 / Valid loss: 7.970752520788284
Model is saved in epoch 5, overall batch: 2500
Training loss: 2.278676748275757 / Valid loss: 7.993765980856759
Training loss: 3.6420845985412598 / Valid loss: 7.875860768272763
Model is saved in epoch 5, overall batch: 2700
Training loss: 3.5146749019622803 / Valid loss: 8.073921451114472
Training loss: 2.2706785202026367 / Valid loss: 7.781553186689105
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 1.8876852989196777 / Valid loss: 7.746939359392439
Model is saved in epoch 6, overall batch: 3000
Training loss: 1.7357001304626465 / Valid loss: 7.685674853551955
Model is saved in epoch 6, overall batch: 3100
Training loss: 0.7940698266029358 / Valid loss: 7.820191955566406
Training loss: 1.17197847366333 / Valid loss: 7.741089498429071
Training loss: 1.4604920148849487 / Valid loss: 7.838946215311686

Epoch: 7
Training loss: 1.658324956893921 / Valid loss: 7.582139928000314
Model is saved in epoch 7, overall batch: 3500
Training loss: 2.3687944412231445 / Valid loss: 7.766816588810512
Training loss: 0.9001510143280029 / Valid loss: 7.622357772645496
Training loss: 1.131117820739746 / Valid loss: 7.7712660244533
Training loss: 2.049302101135254 / Valid loss: 7.717305097125825

Epoch: 8
Training loss: 0.9797196388244629 / Valid loss: 7.722776172274635
Training loss: 1.7816922664642334 / Valid loss: 7.632665052868071
Training loss: 1.0781527757644653 / Valid loss: 7.549314208257766
Model is saved in epoch 8, overall batch: 4200
Training loss: 1.4132851362228394 / Valid loss: 7.557406366439093
Training loss: 1.181673288345337 / Valid loss: 7.543992921284267
Model is saved in epoch 8, overall batch: 4400

Epoch: 9
Training loss: 0.832679808139801 / Valid loss: 7.552675692240397
Training loss: 1.6004751920700073 / Valid loss: 7.495048055194673
Model is saved in epoch 9, overall batch: 4600
Training loss: 1.6209694147109985 / Valid loss: 7.442960398537772
Model is saved in epoch 9, overall batch: 4700
Training loss: 1.0518782138824463 / Valid loss: 7.460697333017985

Epoch: 10
Training loss: 0.8384087681770325 / Valid loss: 7.422100285121373
Model is saved in epoch 10, overall batch: 4900
Training loss: 0.8221598267555237 / Valid loss: 7.470055189586821
Training loss: 0.8169636726379395 / Valid loss: 7.417078719820295
Model is saved in epoch 10, overall batch: 5100
Training loss: 0.8965247869491577 / Valid loss: 7.53522394271124
Training loss: 0.7783175706863403 / Valid loss: 7.579924574352446

Epoch: 11
Training loss: 0.8072155714035034 / Valid loss: 7.497147078741165
Training loss: 0.8766748905181885 / Valid loss: 7.563982432229178
Training loss: 0.760600745677948 / Valid loss: 7.561244042714437
Training loss: 1.301759123802185 / Valid loss: 7.552755492074149
Training loss: 1.0816577672958374 / Valid loss: 7.56524897984096

Epoch: 12
Training loss: 0.5798065662384033 / Valid loss: 7.396265379587809
Model is saved in epoch 12, overall batch: 5900
Training loss: 0.8795040845870972 / Valid loss: 7.47292377608163
Training loss: 0.7921026945114136 / Valid loss: 7.385966291881743
Model is saved in epoch 12, overall batch: 6100
Training loss: 0.8571272492408752 / Valid loss: 7.415475609188988
Training loss: 0.8488427400588989 / Valid loss: 7.461823726835705

Epoch: 13
Training loss: 0.8577324151992798 / Valid loss: 7.400651441301618
Training loss: 0.45867615938186646 / Valid loss: 7.289188630240304
Model is saved in epoch 13, overall batch: 6500
Training loss: 0.6700625419616699 / Valid loss: 7.326209399813697
Training loss: 1.3757134675979614 / Valid loss: 7.427212633405413
Training loss: 1.515602707862854 / Valid loss: 7.407925224304199

Epoch: 14
Training loss: 1.0182905197143555 / Valid loss: 7.414419253667195
Training loss: 1.013877272605896 / Valid loss: 7.381845265343076
Training loss: 0.7215063571929932 / Valid loss: 7.484893894195556
Training loss: 0.8212077617645264 / Valid loss: 7.443095316205706
Training loss: 0.44889259338378906 / Valid loss: 7.5771417209080285

Epoch: 15
Training loss: 0.8137260675430298 / Valid loss: 7.36733199982416
Training loss: 0.7088441848754883 / Valid loss: 7.394038318452381
Training loss: 1.0284450054168701 / Valid loss: 7.306001004718599
Training loss: 0.9155628681182861 / Valid loss: 7.447530841827392
Training loss: 0.5726712942123413 / Valid loss: 7.3515142100197926

Epoch: 16
Training loss: 0.805522620677948 / Valid loss: 7.416229030064174
Training loss: 1.288517951965332 / Valid loss: 7.487298288799468
Training loss: 0.7775541543960571 / Valid loss: 7.444373764310565
Training loss: 0.86651211977005 / Valid loss: 7.399488199324835
Training loss: 1.0683948993682861 / Valid loss: 7.333454990386963

Epoch: 17
Training loss: 1.2888262271881104 / Valid loss: 7.311796724228632
Training loss: 1.0530436038970947 / Valid loss: 7.352515175229027
Training loss: 0.7289125919342041 / Valid loss: 7.423583407629104
Training loss: 0.5324078798294067 / Valid loss: 7.469276105789911
Training loss: 0.7256824970245361 / Valid loss: 7.346524865286691

Epoch: 18
Training loss: 0.5107070207595825 / Valid loss: 7.243117518652053
Model is saved in epoch 18, overall batch: 8900
Training loss: 0.913881778717041 / Valid loss: 7.283430285680861
Training loss: 0.7206624746322632 / Valid loss: 7.375772344498407
Training loss: 0.7486413717269897 / Valid loss: 7.2956756682623
Training loss: 0.9818985462188721 / Valid loss: 7.372592812492734

Epoch: 19
Training loss: 0.7883421182632446 / Valid loss: 7.34359689440046
Training loss: 0.8098863363265991 / Valid loss: 7.368300946553548
Training loss: 1.2597402334213257 / Valid loss: 7.399237728118896
Training loss: 0.7006935477256775 / Valid loss: 7.279916922251384

Epoch: 20
Training loss: 0.49998903274536133 / Valid loss: 7.493303571428571
Training loss: 1.1926583051681519 / Valid loss: 7.31697933560326
Training loss: 0.5293252468109131 / Valid loss: 7.35988184156872
Training loss: 0.5561261177062988 / Valid loss: 7.3516120501926965
Training loss: 0.44310280680656433 / Valid loss: 7.454053115844727

Epoch: 21
Training loss: 0.9881063103675842 / Valid loss: 7.364051614488874
Training loss: 1.062317967414856 / Valid loss: 7.350804238092332
Training loss: 0.5968503952026367 / Valid loss: 7.43356679280599
Training loss: 0.4414033889770508 / Valid loss: 7.382183906010219
Training loss: 0.5698893070220947 / Valid loss: 7.386712246849424

Epoch: 22
Training loss: 0.5139263868331909 / Valid loss: 7.374278497695923
Training loss: 0.8063915371894836 / Valid loss: 7.340115356445312
Training loss: 0.9269586801528931 / Valid loss: 7.293531327020554
Training loss: 0.5337738990783691 / Valid loss: 7.3119414556594124
Training loss: 0.5757229924201965 / Valid loss: 7.319860240391322

Epoch: 23
Training loss: 0.8601744174957275 / Valid loss: 7.321306977953229
Training loss: 0.7842580080032349 / Valid loss: 7.267607114428565
Training loss: 0.9513568878173828 / Valid loss: 7.398415293012347
Training loss: 0.3517917990684509 / Valid loss: 7.353854560852051
Training loss: 0.5919569730758667 / Valid loss: 7.287343143281483

Epoch: 24
Training loss: 0.38846585154533386 / Valid loss: 7.259885351998466
Training loss: 0.7409859895706177 / Valid loss: 7.376150903247652
Training loss: 0.561966061592102 / Valid loss: 7.297169907887777
Training loss: 0.38519951701164246 / Valid loss: 7.428612983794439
Training loss: 1.0134186744689941 / Valid loss: 7.449256324768067

Epoch: 25
Training loss: 0.32634371519088745 / Valid loss: 7.381485820951916
Training loss: 0.4036214351654053 / Valid loss: 7.286453996385847
Training loss: 0.27916476130485535 / Valid loss: 7.316627915700277
Training loss: 0.42238983511924744 / Valid loss: 7.421665155319941
Training loss: 0.5646923184394836 / Valid loss: 7.401324431101481

Epoch: 26
Training loss: 0.43234503269195557 / Valid loss: 7.304775551387242
Training loss: 1.3730058670043945 / Valid loss: 7.41540743964059
Training loss: 0.48354166746139526 / Valid loss: 7.386051109858921
Training loss: 0.5706186890602112 / Valid loss: 7.307985314868746
Training loss: 0.41891640424728394 / Valid loss: 7.391875562213716

Epoch: 27
Training loss: 0.42589256167411804 / Valid loss: 7.3177984283083966
Training loss: 0.5265803933143616 / Valid loss: 7.387144061497279
Training loss: 0.6855471730232239 / Valid loss: 7.410168843042283
Training loss: 0.291456937789917 / Valid loss: 7.304197742825463
Training loss: 0.4001946747303009 / Valid loss: 7.360041899908157

Epoch: 28
Training loss: 0.42845800518989563 / Valid loss: 7.309368960062662
Training loss: 0.5909109711647034 / Valid loss: 7.341716353098551
Training loss: 0.6080434918403625 / Valid loss: 7.331568663460867
Training loss: 0.5128519535064697 / Valid loss: 7.282645266396659
Training loss: 0.5949019193649292 / Valid loss: 7.349179227011544

Epoch: 29
Training loss: 0.6020747423171997 / Valid loss: 7.3332623118445985
Training loss: 0.48281222581863403 / Valid loss: 7.305550491242181
Training loss: 0.6638023853302002 / Valid loss: 7.42261746270316
Training loss: 0.5491997599601746 / Valid loss: 7.347465456099737

Epoch: 30
Training loss: 0.4107440710067749 / Valid loss: 7.268945648556664
Training loss: 0.7980325222015381 / Valid loss: 7.225135978062948
Model is saved in epoch 30, overall batch: 14800
Training loss: 0.6835157871246338 / Valid loss: 7.281225149972098
Training loss: 0.590650200843811 / Valid loss: 7.2194022451128275
Model is saved in epoch 30, overall batch: 15000
Training loss: 0.9756487607955933 / Valid loss: 7.316185097467332

Epoch: 31
Training loss: 0.27600961923599243 / Valid loss: 7.342401849655878
Training loss: 0.4564259350299835 / Valid loss: 7.334918594360351
Training loss: 0.5334159135818481 / Valid loss: 7.279858262198312
Training loss: 0.4554738402366638 / Valid loss: 7.342266945611863
Training loss: 0.519690990447998 / Valid loss: 7.4559218679155626

Epoch: 32
Training loss: 0.33977532386779785 / Valid loss: 7.2367741403125585
Training loss: 0.4854479730129242 / Valid loss: 7.2883998507545105
Training loss: 0.7992781400680542 / Valid loss: 7.243461813245501
Training loss: 0.6879101395606995 / Valid loss: 7.330706846146357
Training loss: 0.5469111204147339 / Valid loss: 7.236821288154239

Epoch: 33
Training loss: 0.44717806577682495 / Valid loss: 7.215039194197882
Model is saved in epoch 33, overall batch: 16200
Training loss: 0.36787426471710205 / Valid loss: 7.225575919378372
Training loss: 0.4204300045967102 / Valid loss: 7.191193126496815
Model is saved in epoch 33, overall batch: 16400
Training loss: 0.40037670731544495 / Valid loss: 7.248242410024007
Training loss: 0.34621238708496094 / Valid loss: 7.357177818389166

Epoch: 34
Training loss: 0.5868954062461853 / Valid loss: 7.266822912579491
Training loss: 0.4883018136024475 / Valid loss: 7.176620817184448
Model is saved in epoch 34, overall batch: 16800
Training loss: 0.2965116500854492 / Valid loss: 7.2028784116109215
Training loss: 0.46654871106147766 / Valid loss: 7.326948338463193
Training loss: 0.9575481414794922 / Valid loss: 7.311002799442837

Epoch: 35
Training loss: 0.42298996448516846 / Valid loss: 7.127266452426002
Model is saved in epoch 35, overall batch: 17200
Training loss: 0.30146366357803345 / Valid loss: 7.2326444807506745
Training loss: 0.4718906581401825 / Valid loss: 7.232244773138137
Training loss: 0.7409930229187012 / Valid loss: 7.204897276560465
Training loss: 0.6235971450805664 / Valid loss: 7.28134846006121

Epoch: 36
Training loss: 0.6199628710746765 / Valid loss: 7.259318017959595
Training loss: 0.8076983690261841 / Valid loss: 7.23761263801938
Training loss: 0.5731371641159058 / Valid loss: 7.278589938935779
Training loss: 0.7750130891799927 / Valid loss: 7.377273654937744
Training loss: 0.6474166512489319 / Valid loss: 7.2617073785691035

Epoch: 37
Training loss: 0.22579291462898254 / Valid loss: 7.184353356134324
Training loss: 0.6572978496551514 / Valid loss: 7.244054562704903
Training loss: 0.438504695892334 / Valid loss: 7.234078984033494
Training loss: 1.2685729265213013 / Valid loss: 7.3377005531674335
Training loss: 0.9036164283752441 / Valid loss: 7.279513763246082

Epoch: 38
Training loss: 0.3726503252983093 / Valid loss: 7.362749013446626
Training loss: 0.40589672327041626 / Valid loss: 7.226170953114828
Training loss: 0.6670575737953186 / Valid loss: 7.377524711972192
Training loss: 0.5110044479370117 / Valid loss: 7.365895861671085
Training loss: 1.0046191215515137 / Valid loss: 7.33821040562221

Epoch: 39
Training loss: 0.4140186905860901 / Valid loss: 7.2831008275349935
Training loss: 0.5615882277488708 / Valid loss: 7.401115235828218
Training loss: 0.3789677619934082 / Valid loss: 7.291923761367798
Training loss: 0.5578044652938843 / Valid loss: 7.296081002553304
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 17200): 7.042409374600365
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.448760986328125 / Valid loss: 15.755828294299898
Model is saved in epoch 0, overall batch: 0
Training loss: 14.640989303588867 / Valid loss: 15.568338012695312
Model is saved in epoch 0, overall batch: 100
Training loss: 12.98184585571289 / Valid loss: 14.974400084359305
Model is saved in epoch 0, overall batch: 200
Training loss: 7.4854607582092285 / Valid loss: 13.93549417768206
Model is saved in epoch 0, overall batch: 300
Training loss: 8.612174987792969 / Valid loss: 13.342452607836043
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 8.165114402770996 / Valid loss: 12.521997451782227
Model is saved in epoch 1, overall batch: 500
Training loss: 8.089675903320312 / Valid loss: 12.107566052391416
Model is saved in epoch 1, overall batch: 600
Training loss: 10.353645324707031 / Valid loss: 11.553868566240583
Model is saved in epoch 1, overall batch: 700
Training loss: 7.416875839233398 / Valid loss: 10.840366695040748
Model is saved in epoch 1, overall batch: 800
Training loss: 9.322234153747559 / Valid loss: 10.6791888600304
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.167574405670166 / Valid loss: 10.291166959490095
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.5740561485290527 / Valid loss: 10.176890927269346
Model is saved in epoch 2, overall batch: 1100
Training loss: 4.089329719543457 / Valid loss: 9.72908071336292
Model is saved in epoch 2, overall batch: 1200
Training loss: 3.4161596298217773 / Valid loss: 9.445483407520113
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.763088703155518 / Valid loss: 9.23400731767927
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 1.0491852760314941 / Valid loss: 8.62774061929612
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.9878368377685547 / Valid loss: 8.940056805383591
Training loss: 3.4439055919647217 / Valid loss: 8.642837846846808
Training loss: 3.7845263481140137 / Valid loss: 8.457420907701765
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.293408393859863 / Valid loss: 8.350551400865827
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 1.5573160648345947 / Valid loss: 8.20272732689267
Model is saved in epoch 4, overall batch: 2000
Training loss: 1.944300651550293 / Valid loss: 8.345638107118152
Training loss: 1.0215628147125244 / Valid loss: 8.054503222874233
Model is saved in epoch 4, overall batch: 2200
Training loss: 1.998764157295227 / Valid loss: 7.976649979182652
Model is saved in epoch 4, overall batch: 2300
Training loss: 1.9609906673431396 / Valid loss: 7.977830659775507

Epoch: 5
Training loss: 1.41386878490448 / Valid loss: 7.806595466250465
Model is saved in epoch 5, overall batch: 2500
Training loss: 1.4983540773391724 / Valid loss: 7.671169698806036
Model is saved in epoch 5, overall batch: 2600
Training loss: 1.791452407836914 / Valid loss: 7.968569156101772
Training loss: 1.98711359500885 / Valid loss: 7.708349241529192
Training loss: 1.502676010131836 / Valid loss: 7.798156592959449

Epoch: 6
Training loss: 0.8488779067993164 / Valid loss: 7.693580958956764
Training loss: 1.7322758436203003 / Valid loss: 7.731964640390306
Training loss: 1.061464548110962 / Valid loss: 7.650658275967553
Model is saved in epoch 6, overall batch: 3200
Training loss: 1.123375415802002 / Valid loss: 7.539037545522054
Model is saved in epoch 6, overall batch: 3300
Training loss: 0.9121811389923096 / Valid loss: 7.663518233526321

Epoch: 7
Training loss: 0.8123846054077148 / Valid loss: 7.598454007648287
Training loss: 0.861777663230896 / Valid loss: 7.690055311293829
Training loss: 1.552844762802124 / Valid loss: 7.628376683734712
Training loss: 1.677812099456787 / Valid loss: 7.546411332629976
Training loss: 1.2370773553848267 / Valid loss: 7.563827646346319

Epoch: 8
Training loss: 0.715609073638916 / Valid loss: 7.656736943835304
Training loss: 1.3403980731964111 / Valid loss: 7.466347630818685
Model is saved in epoch 8, overall batch: 4100
Training loss: 0.9408721327781677 / Valid loss: 7.445497658139184
Model is saved in epoch 8, overall batch: 4200
Training loss: 0.9181759357452393 / Valid loss: 7.593054607936314
Training loss: 1.6974852085113525 / Valid loss: 7.647479234422956

Epoch: 9
Training loss: 1.2391927242279053 / Valid loss: 7.572255874815442
Training loss: 1.0821009874343872 / Valid loss: 7.616018386114211
Training loss: 1.2009639739990234 / Valid loss: 7.550031943548293
Training loss: 1.8564705848693848 / Valid loss: 7.588933676765079

Epoch: 10
Training loss: 0.8802971839904785 / Valid loss: 7.441481217883882
Model is saved in epoch 10, overall batch: 4900
Training loss: 1.028684377670288 / Valid loss: 7.409492047627767
Model is saved in epoch 10, overall batch: 5000
Training loss: 0.7316392660140991 / Valid loss: 7.497565228598458
Training loss: 0.6329706311225891 / Valid loss: 7.465177835736956
Training loss: 0.5187495350837708 / Valid loss: 7.397420406341553
Model is saved in epoch 10, overall batch: 5300

Epoch: 11
Training loss: 0.5504922270774841 / Valid loss: 7.435377048310779
Training loss: 0.8948567509651184 / Valid loss: 7.399473081316267
Training loss: 1.020497441291809 / Valid loss: 7.465495618184407
Training loss: 0.8100764751434326 / Valid loss: 7.420417485918318
Training loss: 1.712442398071289 / Valid loss: 7.42870911189488

Epoch: 12
Training loss: 1.0059304237365723 / Valid loss: 7.3646144049508235
Model is saved in epoch 12, overall batch: 5900
Training loss: 0.9569824934005737 / Valid loss: 7.4429885728018625
Training loss: 0.8615061044692993 / Valid loss: 7.434115014757428
Training loss: 1.1193349361419678 / Valid loss: 7.3841484342302595
Training loss: 0.7765688896179199 / Valid loss: 7.411802310035342

Epoch: 13
Training loss: 1.5293781757354736 / Valid loss: 7.400493335723877
Training loss: 0.8138123750686646 / Valid loss: 7.368192148208618
Training loss: 1.0517337322235107 / Valid loss: 7.307039883023217
Model is saved in epoch 13, overall batch: 6600
Training loss: 1.10152006149292 / Valid loss: 7.370122494016375
Training loss: 0.7697432041168213 / Valid loss: 7.518723369780041

Epoch: 14
Training loss: 0.7472348809242249 / Valid loss: 7.44247210820516
Training loss: 1.0111980438232422 / Valid loss: 7.353832115445818
Training loss: 0.7881417274475098 / Valid loss: 7.45315408706665
Training loss: 0.7830971479415894 / Valid loss: 7.382475748516264
Training loss: 0.6676281094551086 / Valid loss: 7.416459914616176

Epoch: 15
Training loss: 0.9201884865760803 / Valid loss: 7.476448249816895
Training loss: 0.4325997829437256 / Valid loss: 7.308268928527832
Training loss: 1.4968596696853638 / Valid loss: 7.481888305573237
Training loss: 1.182433009147644 / Valid loss: 7.3894238335745674
Training loss: 1.864489197731018 / Valid loss: 7.509494781494141

Epoch: 16
Training loss: 0.6215482354164124 / Valid loss: 7.497111529395694
Training loss: 0.8144112229347229 / Valid loss: 7.461249964577811
Training loss: 0.9724026918411255 / Valid loss: 7.374446882520403
Training loss: 0.8496023416519165 / Valid loss: 7.4811985924130395
Training loss: 0.6051892042160034 / Valid loss: 7.568054780505952

Epoch: 17
Training loss: 0.5972740650177002 / Valid loss: 7.47864503406343
Training loss: 0.45162197947502136 / Valid loss: 7.454261784326462
Training loss: 0.6625856161117554 / Valid loss: 7.379203982580275
Training loss: 1.012449026107788 / Valid loss: 7.563739626748221
Training loss: 0.6786490678787231 / Valid loss: 7.540211200714111

Epoch: 18
Training loss: 0.5664093494415283 / Valid loss: 7.430803594135103
Training loss: 1.1059765815734863 / Valid loss: 7.423160139719645
Training loss: 0.819676399230957 / Valid loss: 7.42318757829212
Training loss: 0.7400517463684082 / Valid loss: 7.530981863112677
Training loss: 0.8638308048248291 / Valid loss: 7.510574804033552

Epoch: 19
Training loss: 0.6202202439308167 / Valid loss: 7.361931873503186
Training loss: 0.8066710829734802 / Valid loss: 7.405958804630098
Training loss: 0.6325748562812805 / Valid loss: 7.482612759726388
Training loss: 0.5098080635070801 / Valid loss: 7.463545560836792

Epoch: 20
Training loss: 0.5428905487060547 / Valid loss: 7.4643487476167225
Training loss: 0.767983615398407 / Valid loss: 7.460636393229167
Training loss: 0.6033095717430115 / Valid loss: 7.439183721088228
Training loss: 0.5875081419944763 / Valid loss: 7.5490437053498765
Training loss: 0.7042005062103271 / Valid loss: 7.510264167331514

Epoch: 21
Training loss: 0.40179938077926636 / Valid loss: 7.491609975269863
Training loss: 0.9993691444396973 / Valid loss: 7.554557759421212
Training loss: 0.6994773149490356 / Valid loss: 7.483973548525856
Training loss: 0.5296845436096191 / Valid loss: 7.542304320562454
Training loss: 0.2916136085987091 / Valid loss: 7.479503338677542

Epoch: 22
Training loss: 0.7613041996955872 / Valid loss: 7.399114790416899
Training loss: 0.5511831641197205 / Valid loss: 7.431611428941999
Training loss: 0.6247005462646484 / Valid loss: 7.429928234645298
Training loss: 0.8608971834182739 / Valid loss: 7.449710069383894
Training loss: 0.873705267906189 / Valid loss: 7.491031442369733

Epoch: 23
Training loss: 0.6342173218727112 / Valid loss: 7.556477519444057
Training loss: 0.7894744277000427 / Valid loss: 7.490402040027437
Training loss: 1.6318492889404297 / Valid loss: 7.529435888926188
Training loss: 0.328989177942276 / Valid loss: 7.4384238697233656
Training loss: 0.7835466861724854 / Valid loss: 7.515935271126883

Epoch: 24
Training loss: 0.6582049131393433 / Valid loss: 7.402415302821568
Training loss: 0.7567221522331238 / Valid loss: 7.530572082882836
Training loss: 0.7115928530693054 / Valid loss: 7.552879478817895
Training loss: 0.884096086025238 / Valid loss: 7.616359365554083
Training loss: 0.5832324624061584 / Valid loss: 7.503468817756289

Epoch: 25
Training loss: 0.9519184827804565 / Valid loss: 7.457821332840693
Training loss: 0.48818856477737427 / Valid loss: 7.403796125593639
Training loss: 0.6591991186141968 / Valid loss: 7.50108509971982
Training loss: 0.6985414028167725 / Valid loss: 7.474258125396002
Training loss: 0.8104392886161804 / Valid loss: 7.438130703426543

Epoch: 26
Training loss: 0.6702878475189209 / Valid loss: 7.430459626515707
Training loss: 0.5223394632339478 / Valid loss: 7.498866950897943
Training loss: 0.6302234530448914 / Valid loss: 7.458349981762114
Training loss: 0.44287097454071045 / Valid loss: 7.385103543599446
Training loss: 0.49280887842178345 / Valid loss: 7.440775144667852

Epoch: 27
Training loss: 0.41445106267929077 / Valid loss: 7.456673499516079
Training loss: 0.6173829436302185 / Valid loss: 7.4230446565718875
Training loss: 0.7474552989006042 / Valid loss: 7.448702607836042
Training loss: 0.6349411606788635 / Valid loss: 7.525565601530529
Training loss: 0.5064843893051147 / Valid loss: 7.580069305783226

Epoch: 28
Training loss: 0.3474036753177643 / Valid loss: 7.386364764258975
Training loss: 0.602755606174469 / Valid loss: 7.4254689761570525
Training loss: 0.9993484020233154 / Valid loss: 7.411183393569219
Training loss: 0.6935032606124878 / Valid loss: 7.5887566066923595
Training loss: 0.45889437198638916 / Valid loss: 7.453301427477882

Epoch: 29
Training loss: 0.3673039674758911 / Valid loss: 7.450011030832926
Training loss: 0.4623680114746094 / Valid loss: 7.478902530670166
Training loss: 0.7132611870765686 / Valid loss: 7.37766782669794
Training loss: 0.47351962327957153 / Valid loss: 7.435649572099958

Epoch: 30
Training loss: 0.959864616394043 / Valid loss: 7.522715632120768
Training loss: 0.3353503942489624 / Valid loss: 7.374475955963135
Training loss: 0.6179208755493164 / Valid loss: 7.42019671031407
Training loss: 0.5885894298553467 / Valid loss: 7.440409792037237
Training loss: 0.606908917427063 / Valid loss: 7.43721703574771

Epoch: 31
Training loss: 0.553551197052002 / Valid loss: 7.454611909957159
Training loss: 0.44301241636276245 / Valid loss: 7.520831839243571
Training loss: 0.5751352310180664 / Valid loss: 7.515913295745849
Training loss: 0.6191964745521545 / Valid loss: 7.530311770666213
Training loss: 0.6918274164199829 / Valid loss: 7.473823038736979

Epoch: 32
Training loss: 0.41767939925193787 / Valid loss: 7.450106098538353
Training loss: 0.4312949776649475 / Valid loss: 7.396897474924724
Training loss: 0.5102776288986206 / Valid loss: 7.4578821590968545
Training loss: 0.6553682088851929 / Valid loss: 7.437263420649937
Training loss: 0.46506553888320923 / Valid loss: 7.467449942089263

Epoch: 33
Training loss: 0.5407454371452332 / Valid loss: 7.466475545792353
Training loss: 0.5091583132743835 / Valid loss: 7.371409942990257
Training loss: 0.4723089337348938 / Valid loss: 7.401373590741839
Training loss: 0.3350413739681244 / Valid loss: 7.39154836563837
Training loss: 0.40256941318511963 / Valid loss: 7.481323614574614

Epoch: 34
Training loss: 0.8058267831802368 / Valid loss: 7.452356511070615
Training loss: 0.524679958820343 / Valid loss: 7.4683432806105845
Training loss: 0.5000497102737427 / Valid loss: 7.542462026505243
Training loss: 0.5542179346084595 / Valid loss: 7.445631613050188
Training loss: 0.5682398080825806 / Valid loss: 7.410158629644485

Epoch: 35
Training loss: 0.3177195191383362 / Valid loss: 7.495339361826579
Training loss: 0.7698888778686523 / Valid loss: 7.449950747262864
Training loss: 0.32612159848213196 / Valid loss: 7.457579771677653
Training loss: 0.49305206537246704 / Valid loss: 7.456566283816383
Training loss: 0.622362494468689 / Valid loss: 7.450109490894136

Epoch: 36
Training loss: 0.4604973793029785 / Valid loss: 7.495308281126476
Training loss: 0.7746292352676392 / Valid loss: 7.4917388416471935
Training loss: 0.4960635304450989 / Valid loss: 7.4606878961835585
Training loss: 0.4275989532470703 / Valid loss: 7.426255834670294
Training loss: 0.5824722051620483 / Valid loss: 7.438082102366856

Epoch: 37
Training loss: 0.6602683663368225 / Valid loss: 7.384690929594494
Training loss: 0.5282235741615295 / Valid loss: 7.399690641675677
Training loss: 0.9839301109313965 / Valid loss: 7.475968270074754
Training loss: 0.5707838535308838 / Valid loss: 7.5119252749851775
Training loss: 0.4539848566055298 / Valid loss: 7.465158296766735

Epoch: 38
Training loss: 0.5670403242111206 / Valid loss: 7.506451220739455
Training loss: 0.4736078679561615 / Valid loss: 7.504983075459799
Training loss: 0.3976733684539795 / Valid loss: 7.445305288405645
Training loss: 0.4356740415096283 / Valid loss: 7.443646176656087
Training loss: 0.46288102865219116 / Valid loss: 7.440501453762963

Epoch: 39
Training loss: 0.5145346522331238 / Valid loss: 7.369197536650158
Training loss: 0.6736892461776733 / Valid loss: 7.41676576705206
Training loss: 0.4296230971813202 / Valid loss: 7.4583257221040276
Training loss: 0.40257346630096436 / Valid loss: 7.488493556068057
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 6600): 7.26724921635219
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.448760986328125 / Valid loss: 15.75584637778146
Model is saved in epoch 0, overall batch: 0
Training loss: 14.737918853759766 / Valid loss: 15.057877204531716
Model is saved in epoch 0, overall batch: 100
Training loss: 12.537369728088379 / Valid loss: 14.07050240834554
Model is saved in epoch 0, overall batch: 200
Training loss: 7.55405330657959 / Valid loss: 12.913335241590227
Model is saved in epoch 0, overall batch: 300
Training loss: 8.44123649597168 / Valid loss: 12.550536360059466
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 8.089468002319336 / Valid loss: 11.902106357756114
Model is saved in epoch 1, overall batch: 500
Training loss: 7.958566665649414 / Valid loss: 11.553427473704021
Model is saved in epoch 1, overall batch: 600
Training loss: 10.28841781616211 / Valid loss: 11.057983389354888
Model is saved in epoch 1, overall batch: 700
Training loss: 7.527253150939941 / Valid loss: 10.356444322495234
Model is saved in epoch 1, overall batch: 800
Training loss: 9.29715347290039 / Valid loss: 10.34384286063058
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.4019012451171875 / Valid loss: 9.938729218074254
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.620502471923828 / Valid loss: 10.05354442142305
Training loss: 3.8330576419830322 / Valid loss: 9.526149826958067
Model is saved in epoch 2, overall batch: 1200
Training loss: 3.501479148864746 / Valid loss: 9.227757776351202
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.44625186920166 / Valid loss: 9.008448505401612
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 1.0202643871307373 / Valid loss: 8.621201878502255
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.16199254989624 / Valid loss: 8.744977546873546
Training loss: 3.5062389373779297 / Valid loss: 8.517400142124721
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.4729912281036377 / Valid loss: 8.376419135502406
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.133014678955078 / Valid loss: 8.188170605614072
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 1.5386786460876465 / Valid loss: 7.959190804617745
Model is saved in epoch 4, overall batch: 2000
Training loss: 2.213742733001709 / Valid loss: 8.143073572431291
Training loss: 1.454099416732788 / Valid loss: 7.904809404554821
Model is saved in epoch 4, overall batch: 2200
Training loss: 1.5870953798294067 / Valid loss: 7.786114524659657
Model is saved in epoch 4, overall batch: 2300
Training loss: 2.0640149116516113 / Valid loss: 7.971593152909052

Epoch: 5
Training loss: 1.6698780059814453 / Valid loss: 7.659234732673282
Model is saved in epoch 5, overall batch: 2500
Training loss: 1.6605994701385498 / Valid loss: 7.60953479948498
Model is saved in epoch 5, overall batch: 2600
Training loss: 1.8461227416992188 / Valid loss: 7.841337444668724
Training loss: 1.9122140407562256 / Valid loss: 7.580955042157854
Model is saved in epoch 5, overall batch: 2800
Training loss: 1.5425517559051514 / Valid loss: 7.672609329223633

Epoch: 6
Training loss: 1.2488322257995605 / Valid loss: 7.667149051030477
Training loss: 1.6631022691726685 / Valid loss: 7.76002428418114
Training loss: 1.0774022340774536 / Valid loss: 7.661640812101818
Training loss: 1.237837314605713 / Valid loss: 7.5587875411624
Model is saved in epoch 6, overall batch: 3300
Training loss: 1.1662893295288086 / Valid loss: 7.682441849935622

Epoch: 7
Training loss: 1.1423976421356201 / Valid loss: 7.603296309425717
Training loss: 1.0074191093444824 / Valid loss: 7.655310508183071
Training loss: 1.3168882131576538 / Valid loss: 7.617132293610346
Training loss: 1.6176934242248535 / Valid loss: 7.550825037275042
Model is saved in epoch 7, overall batch: 3800
Training loss: 1.3835238218307495 / Valid loss: 7.590712722142538

Epoch: 8
Training loss: 0.7520751357078552 / Valid loss: 7.626644175393241
Training loss: 1.138840913772583 / Valid loss: 7.520151774088542
Model is saved in epoch 8, overall batch: 4100
Training loss: 1.3668849468231201 / Valid loss: 7.470030194237118
Model is saved in epoch 8, overall batch: 4200
Training loss: 0.8311131000518799 / Valid loss: 7.687594549996512
Training loss: 1.9351043701171875 / Valid loss: 7.692228993915376

Epoch: 9
Training loss: 1.0833070278167725 / Valid loss: 7.641354156675793
Training loss: 0.7999159097671509 / Valid loss: 7.605667286827451
Training loss: 1.28350031375885 / Valid loss: 7.553921624592372
Training loss: 1.9030299186706543 / Valid loss: 7.548568157922654

Epoch: 10
Training loss: 1.2302191257476807 / Valid loss: 7.488048126584007
Training loss: 0.9946445822715759 / Valid loss: 7.412957009815035
Model is saved in epoch 10, overall batch: 5000
Training loss: 0.9631102085113525 / Valid loss: 7.490693056015742
Training loss: 0.5807852745056152 / Valid loss: 7.4617423284621465
Training loss: 0.7079815864562988 / Valid loss: 7.414944535210019

Epoch: 11
Training loss: 0.7419611811637878 / Valid loss: 7.535822473253522
Training loss: 0.8004035949707031 / Valid loss: 7.439049477804275
Training loss: 0.7574272155761719 / Valid loss: 7.506036604018439
Training loss: 0.8878387212753296 / Valid loss: 7.475006662096296
Training loss: 1.5416183471679688 / Valid loss: 7.528102407001314

Epoch: 12
Training loss: 1.0182244777679443 / Valid loss: 7.504428786323184
Training loss: 0.7055384516716003 / Valid loss: 7.510315191178095
Training loss: 0.9046033620834351 / Valid loss: 7.457561847141811
Training loss: 1.1943225860595703 / Valid loss: 7.405448718298049
Model is saved in epoch 12, overall batch: 6200
Training loss: 0.6774576902389526 / Valid loss: 7.434450662703741

Epoch: 13
Training loss: 1.3755913972854614 / Valid loss: 7.438538435527256
Training loss: 0.4867752194404602 / Valid loss: 7.476312971115112
Training loss: 0.8928563594818115 / Valid loss: 7.379758862086705
Model is saved in epoch 13, overall batch: 6600
Training loss: 0.8813516497612 / Valid loss: 7.3886743159521195
Training loss: 0.9280059337615967 / Valid loss: 7.555620368321737

Epoch: 14
Training loss: 0.8028314113616943 / Valid loss: 7.465344483511789
Training loss: 0.9504320621490479 / Valid loss: 7.356498818170457
Model is saved in epoch 14, overall batch: 7000
Training loss: 0.801060676574707 / Valid loss: 7.558049624306815
Training loss: 0.7638828754425049 / Valid loss: 7.484453582763672
Training loss: 0.8405215740203857 / Valid loss: 7.576749837966192

Epoch: 15
Training loss: 1.2157578468322754 / Valid loss: 7.550788911183675
Training loss: 0.5837110280990601 / Valid loss: 7.416664927346366
Training loss: 1.5903764963150024 / Valid loss: 7.5702264195396785
Training loss: 1.248429775238037 / Valid loss: 7.47580083211263
Training loss: 2.2048916816711426 / Valid loss: 7.569943736848377

Epoch: 16
Training loss: 0.5713289380073547 / Valid loss: 7.50237256912958
Training loss: 1.0764299631118774 / Valid loss: 7.530097416469029
Training loss: 0.9889892935752869 / Valid loss: 7.455888684590658
Training loss: 1.020202398300171 / Valid loss: 7.482259718577067
Training loss: 0.5990705490112305 / Valid loss: 7.550889165060861

Epoch: 17
Training loss: 0.5061784386634827 / Valid loss: 7.512176141284761
Training loss: 0.5052586197853088 / Valid loss: 7.450355389004662
Training loss: 0.6599475145339966 / Valid loss: 7.330040205092657
Model is saved in epoch 17, overall batch: 8600
Training loss: 0.9784784317016602 / Valid loss: 7.5102964265005925
Training loss: 0.5507460236549377 / Valid loss: 7.557898476010277

Epoch: 18
Training loss: 0.45648902654647827 / Valid loss: 7.356379234223139
Training loss: 0.9509012699127197 / Valid loss: 7.3905087743486675
Training loss: 1.0242595672607422 / Valid loss: 7.469505282810756
Training loss: 0.9483603239059448 / Valid loss: 7.515606398809524
Training loss: 0.739147424697876 / Valid loss: 7.533271235511417

Epoch: 19
Training loss: 0.7502555251121521 / Valid loss: 7.406407004310971
Training loss: 0.80506831407547 / Valid loss: 7.48123068582444
Training loss: 0.6037185192108154 / Valid loss: 7.5260497592744375
Training loss: 0.5453940033912659 / Valid loss: 7.4631551765260244

Epoch: 20
Training loss: 0.681473970413208 / Valid loss: 7.495067805335635
Training loss: 0.7936848402023315 / Valid loss: 7.428982970828102
Training loss: 0.9162890911102295 / Valid loss: 7.448539584023612
Training loss: 0.4785116910934448 / Valid loss: 7.480044001624698
Training loss: 0.7740766406059265 / Valid loss: 7.463104888371059

Epoch: 21
Training loss: 0.4185551106929779 / Valid loss: 7.383739877882458
Training loss: 0.9916212558746338 / Valid loss: 7.438077081952776
Training loss: 0.5434485077857971 / Valid loss: 7.413112020492553
Training loss: 0.5016398429870605 / Valid loss: 7.457987140473866
Training loss: 0.5079532861709595 / Valid loss: 7.486548553194319

Epoch: 22
Training loss: 0.6916759014129639 / Valid loss: 7.394617621103922
Training loss: 0.6541372537612915 / Valid loss: 7.398795695531936
Training loss: 0.5740613341331482 / Valid loss: 7.345435828254336
Training loss: 0.9060907363891602 / Valid loss: 7.465242404029483
Training loss: 0.8776551485061646 / Valid loss: 7.512634647460211

Epoch: 23
Training loss: 0.5303666591644287 / Valid loss: 7.442654827662877
Training loss: 0.7502445578575134 / Valid loss: 7.381754593622117
Training loss: 1.6266807317733765 / Valid loss: 7.445061424800328
Training loss: 0.39265012741088867 / Valid loss: 7.397654833112444
Training loss: 0.7915050983428955 / Valid loss: 7.427040109180269

Epoch: 24
Training loss: 0.5590491890907288 / Valid loss: 7.3856063524882
Training loss: 0.7955516576766968 / Valid loss: 7.46687836874099
Training loss: 0.8089651465415955 / Valid loss: 7.475307864234561
Training loss: 0.8091350793838501 / Valid loss: 7.499871167682466
Training loss: 0.37212198972702026 / Valid loss: 7.382844200588408

Epoch: 25
Training loss: 1.086276888847351 / Valid loss: 7.426580701555524
Training loss: 0.6133488416671753 / Valid loss: 7.343278591973441
Training loss: 0.46093547344207764 / Valid loss: 7.498794896262033
Training loss: 0.674167275428772 / Valid loss: 7.458425946462722
Training loss: 0.590978741645813 / Valid loss: 7.465636870974586

Epoch: 26
Training loss: 0.4526475965976715 / Valid loss: 7.421724496568952
Training loss: 0.5886101126670837 / Valid loss: 7.475681086948939
Training loss: 0.7041161060333252 / Valid loss: 7.510128652481805
Training loss: 0.7604525089263916 / Valid loss: 7.43676567985898
Training loss: 0.56682288646698 / Valid loss: 7.4359547138214115

Epoch: 27
Training loss: 0.5879223942756653 / Valid loss: 7.442163156327747
Training loss: 0.8672037124633789 / Valid loss: 7.38019316082909
Training loss: 0.8172745704650879 / Valid loss: 7.387359342120942
Training loss: 0.6104134321212769 / Valid loss: 7.468100456964402
Training loss: 0.5507558584213257 / Valid loss: 7.511754358382452

Epoch: 28
Training loss: 0.47046172618865967 / Valid loss: 7.315947864169166
Model is saved in epoch 28, overall batch: 13800
Training loss: 0.6553917527198792 / Valid loss: 7.3872829482668925
Training loss: 0.8965569734573364 / Valid loss: 7.373568514415196
Training loss: 0.6188223361968994 / Valid loss: 7.475629665738061
Training loss: 0.4705997109413147 / Valid loss: 7.43561931337629

Epoch: 29
Training loss: 0.28906434774398804 / Valid loss: 7.404552455175491
Training loss: 0.4497503936290741 / Valid loss: 7.427613412766229
Training loss: 0.6155023574829102 / Valid loss: 7.345812774839855
Training loss: 0.5067501664161682 / Valid loss: 7.381057848249163

Epoch: 30
Training loss: 1.0032397508621216 / Valid loss: 7.443844418298631
Training loss: 0.44234299659729004 / Valid loss: 7.322410379137311
Training loss: 0.4455938935279846 / Valid loss: 7.451893443153018
Training loss: 0.5235931873321533 / Valid loss: 7.435036627451579
Training loss: 0.6428765654563904 / Valid loss: 7.413975420452299

Epoch: 31
Training loss: 0.6683690547943115 / Valid loss: 7.46158390045166
Training loss: 0.5491783618927002 / Valid loss: 7.531250086284819
Training loss: 0.6261506676673889 / Valid loss: 7.46820098786127
Training loss: 0.720280647277832 / Valid loss: 7.502906858353388
Training loss: 0.3592876195907593 / Valid loss: 7.4662189937773205

Epoch: 32
Training loss: 0.44899430871009827 / Valid loss: 7.402147751762754
Training loss: 0.4184098243713379 / Valid loss: 7.363402911594936
Training loss: 0.6278913617134094 / Valid loss: 7.404894774300711
Training loss: 0.6942326426506042 / Valid loss: 7.481622359866187
Training loss: 0.2931448221206665 / Valid loss: 7.509423251379104

Epoch: 33
Training loss: 0.400479257106781 / Valid loss: 7.5129244531903945
Training loss: 0.38862818479537964 / Valid loss: 7.363006800696963
Training loss: 0.6534056067466736 / Valid loss: 7.425321494965326
Training loss: 0.4790055751800537 / Valid loss: 7.330549317314511
Training loss: 0.5481287240982056 / Valid loss: 7.454016542434692

Epoch: 34
Training loss: 0.6679244041442871 / Valid loss: 7.459957917531331
Training loss: 0.34568148851394653 / Valid loss: 7.4343926747639975
Training loss: 0.4734842777252197 / Valid loss: 7.438052722385952
Training loss: 0.5574307441711426 / Valid loss: 7.406387896764846
Training loss: 0.5207390785217285 / Valid loss: 7.347919028145927

Epoch: 35
Training loss: 0.36659613251686096 / Valid loss: 7.512340354919433
Training loss: 0.6413993835449219 / Valid loss: 7.42628706296285
Training loss: 0.47037091851234436 / Valid loss: 7.390344742366246
Training loss: 0.49281153082847595 / Valid loss: 7.4967186019534156
Training loss: 0.8388869166374207 / Valid loss: 7.468693104244414

Epoch: 36
Training loss: 0.5716972947120667 / Valid loss: 7.52291852406093
Training loss: 0.6784458160400391 / Valid loss: 7.448395066034227
Training loss: 0.4607928395271301 / Valid loss: 7.4372398421877906
Training loss: 0.42306897044181824 / Valid loss: 7.443667836416335
Training loss: 0.4198320508003235 / Valid loss: 7.383024349666777

Epoch: 37
Training loss: 0.4592721462249756 / Valid loss: 7.398559356871106
Training loss: 0.39667999744415283 / Valid loss: 7.4128448395502
Training loss: 1.0405709743499756 / Valid loss: 7.401538778486706
Training loss: 0.5547699332237244 / Valid loss: 7.458637546357655
Training loss: 0.5818096399307251 / Valid loss: 7.444848006112235

Epoch: 38
Training loss: 0.33857041597366333 / Valid loss: 7.440283534640358
Training loss: 0.3810879588127136 / Valid loss: 7.426797726040795
Training loss: 0.44782063364982605 / Valid loss: 7.396462799253918
Training loss: 0.4359782934188843 / Valid loss: 7.425983406248546
Training loss: 0.4054519832134247 / Valid loss: 7.402384222121466

Epoch: 39
Training loss: 0.524728536605835 / Valid loss: 7.334305045718239
Training loss: 0.43650299310684204 / Valid loss: 7.359092190152123
Training loss: 0.4379752576351166 / Valid loss: 7.391157563527425
Training loss: 0.467207670211792 / Valid loss: 7.3889414469401045
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 13800): 7.246909813653855
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.259333610534668 / Valid loss: 16.436905479431154
Model is saved in epoch 0, overall batch: 0
Training loss: 10.583904266357422 / Valid loss: 12.476065245128813
Model is saved in epoch 0, overall batch: 100
Training loss: 7.961256980895996 / Valid loss: 9.482079891931443
Model is saved in epoch 0, overall batch: 200
Training loss: 5.057451248168945 / Valid loss: 8.026790407725743
Model is saved in epoch 0, overall batch: 300
Training loss: 6.724361896514893 / Valid loss: 7.211808204650879
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.667150974273682 / Valid loss: 6.681170797348022
Model is saved in epoch 1, overall batch: 500
Training loss: 3.5264933109283447 / Valid loss: 6.554639580136254
Model is saved in epoch 1, overall batch: 600
Training loss: 3.104177713394165 / Valid loss: 6.51564987046378
Model is saved in epoch 1, overall batch: 700
Training loss: 4.722572326660156 / Valid loss: 6.5657128288632345
Training loss: 4.609307289123535 / Valid loss: 6.52449247723534

Epoch: 2
Training loss: 1.6928937435150146 / Valid loss: 6.450498106366112
Model is saved in epoch 2, overall batch: 1000
Training loss: 2.4217796325683594 / Valid loss: 6.439197685605004
Model is saved in epoch 2, overall batch: 1100
Training loss: 2.3816728591918945 / Valid loss: 6.521092769077846
Training loss: 2.060384750366211 / Valid loss: 6.55475476582845
Training loss: 2.0224766731262207 / Valid loss: 6.559338823954264

Epoch: 3
Training loss: 1.7326637506484985 / Valid loss: 6.637975088755289
Training loss: 2.0319113731384277 / Valid loss: 6.698233990442185
Training loss: 2.015317916870117 / Valid loss: 6.744591953640892
Training loss: 1.6101502180099487 / Valid loss: 6.709371907370431
Training loss: 2.759378433227539 / Valid loss: 6.744099653334844

Epoch: 4
Training loss: 1.4133527278900146 / Valid loss: 6.811341026851109
Training loss: 1.1638035774230957 / Valid loss: 6.81017362957909
Training loss: 1.496971607208252 / Valid loss: 6.830389000120617
Training loss: 2.1787424087524414 / Valid loss: 6.84852313768296
Training loss: 1.1622207164764404 / Valid loss: 6.9123467944917225

Epoch: 5
Training loss: 0.9926975965499878 / Valid loss: 6.85536516734532
Training loss: 1.707214117050171 / Valid loss: 6.921812511625744
Training loss: 0.8500037789344788 / Valid loss: 6.9371763093130925
Training loss: 2.072002410888672 / Valid loss: 6.923926789419991
Training loss: 1.8312392234802246 / Valid loss: 6.94576004573277

Epoch: 6
Training loss: 1.0631799697875977 / Valid loss: 6.976050322396414
Training loss: 1.5683706998825073 / Valid loss: 6.998438776107061
Training loss: 1.7070600986480713 / Valid loss: 6.991637976964315
Training loss: 0.9281859397888184 / Valid loss: 7.010049838111514
Training loss: 2.4039227962493896 / Valid loss: 7.015189829326811

Epoch: 7
Training loss: 1.5232799053192139 / Valid loss: 7.046936198643276
Training loss: 0.9921600222587585 / Valid loss: 7.038925232206072
Training loss: 1.214705228805542 / Valid loss: 7.020871875399635
Training loss: 1.0480133295059204 / Valid loss: 7.101932200931367
Training loss: 1.187280535697937 / Valid loss: 7.160618064517067

Epoch: 8
Training loss: 0.9705322980880737 / Valid loss: 7.075502913338798
Training loss: 1.278000831604004 / Valid loss: 7.07277322042556
Training loss: 1.0187506675720215 / Valid loss: 7.150269045148577
Training loss: 0.7726355791091919 / Valid loss: 7.173735355195545
Training loss: 0.9304389953613281 / Valid loss: 7.185388751257033

Epoch: 9
Training loss: 0.7946221828460693 / Valid loss: 7.174974832080659
Training loss: 0.8440307378768921 / Valid loss: 7.1572276297069735
Training loss: 0.8433363437652588 / Valid loss: 7.169175011771066
Training loss: 0.7073385715484619 / Valid loss: 7.1679322742280505

Epoch: 10
Training loss: 0.6022152304649353 / Valid loss: 7.180351389022101
Training loss: 0.8141109943389893 / Valid loss: 7.2261058126177105
Training loss: 0.5069828033447266 / Valid loss: 7.153348532177153
Training loss: 1.2120211124420166 / Valid loss: 7.1700016929989765
Training loss: 1.1289124488830566 / Valid loss: 7.27726214726766

Epoch: 11
Training loss: 0.4130842089653015 / Valid loss: 7.244447635468982
Training loss: 0.4896865487098694 / Valid loss: 7.226484625680106
Training loss: 0.5887352228164673 / Valid loss: 7.265805753072103
Training loss: 0.7974143028259277 / Valid loss: 7.328313841138567
Training loss: 0.9268289804458618 / Valid loss: 7.3506631033761165

Epoch: 12
Training loss: 0.6621772050857544 / Valid loss: 7.330463159651983
Training loss: 0.4787498414516449 / Valid loss: 7.295042255946568
Training loss: 1.0453157424926758 / Valid loss: 7.297692521413167
Training loss: 0.6433141827583313 / Valid loss: 7.241549914223807
Training loss: 0.5675365924835205 / Valid loss: 7.29284029006958

Epoch: 13
Training loss: 0.8279665112495422 / Valid loss: 7.234921512149629
Training loss: 0.49397528171539307 / Valid loss: 7.276764760698591
Training loss: 0.6599913239479065 / Valid loss: 7.290771534329369
Training loss: 0.6070254445075989 / Valid loss: 7.365136198770433
Training loss: 0.9213343858718872 / Valid loss: 7.244299647921608

Epoch: 14
Training loss: 0.5520321130752563 / Valid loss: 7.281297524770101
Training loss: 0.7248287200927734 / Valid loss: 7.322723016284761
Training loss: 0.5309895873069763 / Valid loss: 7.267925884610131
Training loss: 0.5952274799346924 / Valid loss: 7.279589911869594
Training loss: 0.4993707239627838 / Valid loss: 7.296491114298503

Epoch: 15
Training loss: 0.816809892654419 / Valid loss: 7.275265107836042
Training loss: 0.5723860263824463 / Valid loss: 7.266113462902251
Training loss: 0.7498440146446228 / Valid loss: 7.300572676885696
Training loss: 0.8886563777923584 / Valid loss: 7.282962317693801
Training loss: 0.4535568952560425 / Valid loss: 7.29015288125901

Epoch: 16
Training loss: 0.7301639318466187 / Valid loss: 7.278346788315546
Training loss: 0.5392200946807861 / Valid loss: 7.197331485294161
Training loss: 0.36855101585388184 / Valid loss: 7.348465419950939
Training loss: 0.7335914373397827 / Valid loss: 7.305287965138754
Training loss: 0.9859453439712524 / Valid loss: 7.309608854566302

Epoch: 17
Training loss: 0.4717618227005005 / Valid loss: 7.3409377915518625
Training loss: 0.3658949136734009 / Valid loss: 7.246523766290574
Training loss: 0.5974735617637634 / Valid loss: 7.292544001624698
Training loss: 0.5677967071533203 / Valid loss: 7.3023453712463375
Training loss: 0.5461571216583252 / Valid loss: 7.2674207051595054

Epoch: 18
Training loss: 0.9139352440834045 / Valid loss: 7.279985600426084
Training loss: 0.8848928213119507 / Valid loss: 7.182481661297026
Training loss: 1.0542651414871216 / Valid loss: 7.356153665270124
Training loss: 0.48131412267684937 / Valid loss: 7.346963673546201
Training loss: 0.49668776988983154 / Valid loss: 7.404044364747547

Epoch: 19
Training loss: 0.8469362854957581 / Valid loss: 7.291139461880639
Training loss: 0.5463723540306091 / Valid loss: 7.274751726786295
Training loss: 0.686600923538208 / Valid loss: 7.288263929457892
Training loss: 0.5909897089004517 / Valid loss: 7.237085405985514

Epoch: 20
Training loss: 0.4175325334072113 / Valid loss: 7.33582421711513
Training loss: 0.7027217149734497 / Valid loss: 7.2645632335117885
Training loss: 0.36259084939956665 / Valid loss: 7.256797840481712
Training loss: 0.5916412472724915 / Valid loss: 7.367568442935036
Training loss: 0.5327692031860352 / Valid loss: 7.333056808653332

Epoch: 21
Training loss: 0.40402233600616455 / Valid loss: 7.40628551301502
Training loss: 0.5796432495117188 / Valid loss: 7.402671800340925
Training loss: 0.6699527502059937 / Valid loss: 7.329097675141834
Training loss: 0.5203951597213745 / Valid loss: 7.312902282533192
Training loss: 0.7198763489723206 / Valid loss: 7.309983412424724

Epoch: 22
Training loss: 0.6566165089607239 / Valid loss: 7.295826521373931
Training loss: 0.30725085735321045 / Valid loss: 7.290370468866257
Training loss: 0.8293355107307434 / Valid loss: 7.3163436526343935
Training loss: 0.41335052251815796 / Valid loss: 7.342873541514079
Training loss: 0.2924519181251526 / Valid loss: 7.306664262499128

Epoch: 23
Training loss: 0.3827943503856659 / Valid loss: 7.380224050794329
Training loss: 0.4141172170639038 / Valid loss: 7.270516152608963
Training loss: 0.4269741177558899 / Valid loss: 7.257697691236223
Training loss: 0.8419672846794128 / Valid loss: 7.343157772790818
Training loss: 0.6030680537223816 / Valid loss: 7.322918283371698

Epoch: 24
Training loss: 0.4194483160972595 / Valid loss: 7.28816111428397
Training loss: 0.5826210975646973 / Valid loss: 7.338075846717471
Training loss: 0.7761521339416504 / Valid loss: 7.375681298119681
Training loss: 0.5722935199737549 / Valid loss: 7.383566206977481
Training loss: 0.33844444155693054 / Valid loss: 7.3303875060308545

Epoch: 25
Training loss: 0.6875352263450623 / Valid loss: 7.31365186600458
Training loss: 0.4739527702331543 / Valid loss: 7.395649478549049
Training loss: 0.48539528250694275 / Valid loss: 7.369262813386463
Training loss: 0.4678881764411926 / Valid loss: 7.404558136349633
Training loss: 0.33509543538093567 / Valid loss: 7.375525451841808

Epoch: 26
Training loss: 0.5529685020446777 / Valid loss: 7.289110279083252
Training loss: 0.5464569330215454 / Valid loss: 7.285093897864932
Training loss: 0.32807648181915283 / Valid loss: 7.283765720185779
Training loss: 0.4939515292644501 / Valid loss: 7.367041930698213
Training loss: 0.5653769969940186 / Valid loss: 7.332253279004778

Epoch: 27
Training loss: 0.7121444344520569 / Valid loss: 7.333265876770019
Training loss: 0.7753736972808838 / Valid loss: 7.354595025380452
Training loss: 0.36457568407058716 / Valid loss: 7.305389694940477
Training loss: 0.32133862376213074 / Valid loss: 7.42280665352231
Training loss: 0.3427685499191284 / Valid loss: 7.325152710505894

Epoch: 28
Training loss: 0.3339731991291046 / Valid loss: 7.318345029013497
Training loss: 0.46971091628074646 / Valid loss: 7.3114409855433875
Training loss: 0.6644999980926514 / Valid loss: 7.331276230585008
Training loss: 0.47611814737319946 / Valid loss: 7.275014332362583
Training loss: 0.26410943269729614 / Valid loss: 7.340085547310966

Epoch: 29
Training loss: 0.8931013345718384 / Valid loss: 7.3609720139276416
Training loss: 0.3778102993965149 / Valid loss: 7.269038018726167
Training loss: 0.6590383052825928 / Valid loss: 7.2585253851754326
Training loss: 0.43478158116340637 / Valid loss: 7.330653422219413

Epoch: 30
Training loss: 0.5488946437835693 / Valid loss: 7.357066199893043
Training loss: 0.5055277943611145 / Valid loss: 7.264397816430955
Training loss: 0.6373031139373779 / Valid loss: 7.3165526253836495
Training loss: 0.6065207719802856 / Valid loss: 7.243006558645339
Training loss: 0.2629101872444153 / Valid loss: 7.318955993652343

Epoch: 31
Training loss: 0.4058927893638611 / Valid loss: 7.25639929544358
Training loss: 0.5222413539886475 / Valid loss: 7.367827810559954
Training loss: 0.3862406611442566 / Valid loss: 7.284749843960717
Training loss: 0.4236290752887726 / Valid loss: 7.246531307129633
Training loss: 0.7679797410964966 / Valid loss: 7.228753646214803

Epoch: 32
Training loss: 0.2879610061645508 / Valid loss: 7.260425980885824
Training loss: 0.3335328996181488 / Valid loss: 7.290442689259847
Training loss: 0.348163902759552 / Valid loss: 7.238560163407099
Training loss: 0.2836152911186218 / Valid loss: 7.402064087277367
Training loss: 0.4169127941131592 / Valid loss: 7.314486331031436

Epoch: 33
Training loss: 0.802971363067627 / Valid loss: 7.3128098805745445
Training loss: 0.3245324492454529 / Valid loss: 7.371828165508452
Training loss: 0.3780151605606079 / Valid loss: 7.381107916150774
Training loss: 0.2773491144180298 / Valid loss: 7.279934892200289
Training loss: 0.4199182987213135 / Valid loss: 7.275991862160819

Epoch: 34
Training loss: 0.3392408490180969 / Valid loss: 7.337896760304769
Training loss: 0.3961614966392517 / Valid loss: 7.389041339783441
Training loss: 0.5262469053268433 / Valid loss: 7.313006648563204
Training loss: 0.4691562354564667 / Valid loss: 7.275066393897647
Training loss: 0.2448652684688568 / Valid loss: 7.334789435068767

Epoch: 35
Training loss: 0.37531477212905884 / Valid loss: 7.266919349488758
Training loss: 0.53251051902771 / Valid loss: 7.355471647353399
Training loss: 0.3338656425476074 / Valid loss: 7.203627297991798
Training loss: 0.39001309871673584 / Valid loss: 7.265278430212112
Training loss: 0.4403781294822693 / Valid loss: 7.330691610063825

Epoch: 36
Training loss: 0.28611278533935547 / Valid loss: 7.327607354663667
Training loss: 0.38231009244918823 / Valid loss: 7.242505346025739
Training loss: 0.20041704177856445 / Valid loss: 7.288973871866862
Training loss: 0.46084487438201904 / Valid loss: 7.365712279365177
Training loss: 0.33459198474884033 / Valid loss: 7.239995534079416

Epoch: 37
Training loss: 0.2744831442832947 / Valid loss: 7.310352329980759
Training loss: 1.104201078414917 / Valid loss: 7.219737450281779
Training loss: 0.2967261075973511 / Valid loss: 7.3072849500746955
Training loss: 0.29562926292419434 / Valid loss: 7.2497311864580425
Training loss: 0.33639976382255554 / Valid loss: 7.246192030679612

Epoch: 38
Training loss: 0.38099443912506104 / Valid loss: 7.237668255397252
Training loss: 0.23837095499038696 / Valid loss: 7.3698309103647865
Training loss: 0.3278180956840515 / Valid loss: 7.338975811004639
Training loss: 0.27765539288520813 / Valid loss: 7.3542499950953895
Training loss: 0.4152221083641052 / Valid loss: 7.321505800882975

Epoch: 39
Training loss: 0.2596171498298645 / Valid loss: 7.274940277281262
Training loss: 0.29174354672431946 / Valid loss: 7.275822040012905
Training loss: 0.4930036664009094 / Valid loss: 7.229749625069754
Training loss: 0.34782522916793823 / Valid loss: 7.322011039370582
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1100): 6.318531277066185
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.259333610534668 / Valid loss: 16.436931510198683
Model is saved in epoch 0, overall batch: 0
Training loss: 10.55910873413086 / Valid loss: 12.559696288335891
Model is saved in epoch 0, overall batch: 100
Training loss: 7.69476318359375 / Valid loss: 9.710477206820533
Model is saved in epoch 0, overall batch: 200
Training loss: 5.055356025695801 / Valid loss: 8.307625820523217
Model is saved in epoch 0, overall batch: 300
Training loss: 6.6656646728515625 / Valid loss: 7.471647416977655
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.912644386291504 / Valid loss: 6.893995305470058
Model is saved in epoch 1, overall batch: 500
Training loss: 3.5492398738861084 / Valid loss: 6.776600737798781
Model is saved in epoch 1, overall batch: 600
Training loss: 3.266611099243164 / Valid loss: 6.731328396570115
Model is saved in epoch 1, overall batch: 700
Training loss: 4.537032127380371 / Valid loss: 6.76856392451695
Training loss: 5.0542778968811035 / Valid loss: 6.681146315165928
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 1.8616279363632202 / Valid loss: 6.5767250219980875
Model is saved in epoch 2, overall batch: 1000
Training loss: 2.6925668716430664 / Valid loss: 6.527767331259591
Model is saved in epoch 2, overall batch: 1100
Training loss: 2.6317338943481445 / Valid loss: 6.6356269586653935
Training loss: 2.146195411682129 / Valid loss: 6.60885150319054
Training loss: 2.1934127807617188 / Valid loss: 6.591149870554606

Epoch: 3
Training loss: 1.8310699462890625 / Valid loss: 6.662931321916126
Training loss: 1.8571051359176636 / Valid loss: 6.72282643091111
Training loss: 2.1909542083740234 / Valid loss: 6.774879264831543
Training loss: 1.6725554466247559 / Valid loss: 6.736156499953497
Training loss: 2.788022994995117 / Valid loss: 6.787861812682379

Epoch: 4
Training loss: 1.278605341911316 / Valid loss: 6.850436537606376
Training loss: 1.2134437561035156 / Valid loss: 6.852302814665295
Training loss: 1.5663071870803833 / Valid loss: 6.907328646523612
Training loss: 2.078458309173584 / Valid loss: 6.845736487706502
Training loss: 1.2909221649169922 / Valid loss: 6.950761717841739

Epoch: 5
Training loss: 1.0636080503463745 / Valid loss: 6.896305220467704
Training loss: 1.6794649362564087 / Valid loss: 6.969808823721749
Training loss: 0.8767246007919312 / Valid loss: 6.966428011939639
Training loss: 2.1318960189819336 / Valid loss: 6.964324919382731
Training loss: 1.8672924041748047 / Valid loss: 6.996164921351841

Epoch: 6
Training loss: 1.0375899076461792 / Valid loss: 7.008498441605341
Training loss: 1.5037100315093994 / Valid loss: 7.01035240264166
Training loss: 1.8657132387161255 / Valid loss: 6.996644978296189
Training loss: 0.8603520393371582 / Valid loss: 7.046283492587862
Training loss: 2.3347930908203125 / Valid loss: 7.016848296210879

Epoch: 7
Training loss: 1.3721925020217896 / Valid loss: 7.06554506392706
Training loss: 0.986170768737793 / Valid loss: 7.117975157783145
Training loss: 1.1828559637069702 / Valid loss: 7.063952529998053
Training loss: 1.1226692199707031 / Valid loss: 7.160091023218064
Training loss: 1.1675277948379517 / Valid loss: 7.1649225825355165

Epoch: 8
Training loss: 0.9261729121208191 / Valid loss: 7.112845364071074
Training loss: 1.0935463905334473 / Valid loss: 7.132117117018927
Training loss: 0.9568603038787842 / Valid loss: 7.173615673610143
Training loss: 0.7886291742324829 / Valid loss: 7.212529872712635
Training loss: 0.890593945980072 / Valid loss: 7.231016783487229

Epoch: 9
Training loss: 0.8581349849700928 / Valid loss: 7.211231504167829
Training loss: 1.025399923324585 / Valid loss: 7.165897373926072
Training loss: 0.9439840912818909 / Valid loss: 7.20237653823126
Training loss: 0.7679101228713989 / Valid loss: 7.197457881200881

Epoch: 10
Training loss: 0.5489166975021362 / Valid loss: 7.187888045538039
Training loss: 0.8544583320617676 / Valid loss: 7.243854577200754
Training loss: 0.607170045375824 / Valid loss: 7.204941322689964
Training loss: 1.4196813106536865 / Valid loss: 7.2189778782072525
Training loss: 1.0705626010894775 / Valid loss: 7.267145878928048

Epoch: 11
Training loss: 0.4508916139602661 / Valid loss: 7.209912204742432
Training loss: 0.5824398994445801 / Valid loss: 7.178145029431298
Training loss: 0.7174937725067139 / Valid loss: 7.265987101055327
Training loss: 0.9286391735076904 / Valid loss: 7.303922185443697
Training loss: 0.9276168942451477 / Valid loss: 7.317586253938221

Epoch: 12
Training loss: 0.8302741646766663 / Valid loss: 7.317189500445411
Training loss: 0.5743091106414795 / Valid loss: 7.28968620300293
Training loss: 1.209841251373291 / Valid loss: 7.262543355850946
Training loss: 0.7506230473518372 / Valid loss: 7.297825291043236
Training loss: 0.6823257803916931 / Valid loss: 7.282238819485619

Epoch: 13
Training loss: 0.7885076403617859 / Valid loss: 7.245861316862561
Training loss: 0.5310457944869995 / Valid loss: 7.262969802674793
Training loss: 0.6598827838897705 / Valid loss: 7.27577870686849
Training loss: 0.3969913721084595 / Valid loss: 7.352455552419027
Training loss: 1.0411317348480225 / Valid loss: 7.236147517249698

Epoch: 14
Training loss: 0.5351141691207886 / Valid loss: 7.286918326786586
Training loss: 0.4915393888950348 / Valid loss: 7.307342284066337
Training loss: 0.5027128458023071 / Valid loss: 7.222992020561581
Training loss: 0.5075052976608276 / Valid loss: 7.240397857484363
Training loss: 0.5038460493087769 / Valid loss: 7.318848369235084

Epoch: 15
Training loss: 0.8311719298362732 / Valid loss: 7.247895499638148
Training loss: 0.6603149175643921 / Valid loss: 7.233958003634498
Training loss: 0.6797078847885132 / Valid loss: 7.288612270355225
Training loss: 0.8648062944412231 / Valid loss: 7.285961042131697
Training loss: 0.3774726688861847 / Valid loss: 7.301177419934954

Epoch: 16
Training loss: 0.8200879096984863 / Valid loss: 7.247452676863897
Training loss: 0.4987896680831909 / Valid loss: 7.200410238901774
Training loss: 0.4034675657749176 / Valid loss: 7.335156354450044
Training loss: 0.7079350352287292 / Valid loss: 7.285301912398565
Training loss: 0.9751113653182983 / Valid loss: 7.224130671364921

Epoch: 17
Training loss: 0.6682010889053345 / Valid loss: 7.309297379993257
Training loss: 0.2953472435474396 / Valid loss: 7.229058565412249
Training loss: 0.5492063760757446 / Valid loss: 7.251453685760498
Training loss: 0.7123609781265259 / Valid loss: 7.264785625821069
Training loss: 0.6261863708496094 / Valid loss: 7.228932158152262

Epoch: 18
Training loss: 0.8382657766342163 / Valid loss: 7.237353429340181
Training loss: 0.9307494163513184 / Valid loss: 7.159516284579323
Training loss: 0.9636867642402649 / Valid loss: 7.296061901819138
Training loss: 0.5151832103729248 / Valid loss: 7.329008007049561
Training loss: 0.5636674165725708 / Valid loss: 7.3740932055882045

Epoch: 19
Training loss: 1.0036580562591553 / Valid loss: 7.269804046267555
Training loss: 0.5529712438583374 / Valid loss: 7.243688719613211
Training loss: 0.7149418592453003 / Valid loss: 7.281024937402634
Training loss: 0.5260089635848999 / Valid loss: 7.230206503186907

Epoch: 20
Training loss: 0.5269458293914795 / Valid loss: 7.34538627124968
Training loss: 0.7680504322052002 / Valid loss: 7.213775046666464
Training loss: 0.37960612773895264 / Valid loss: 7.207490875607445
Training loss: 0.5633543729782104 / Valid loss: 7.286861347016834
Training loss: 0.6064206957817078 / Valid loss: 7.244138222648984

Epoch: 21
Training loss: 0.263042151927948 / Valid loss: 7.3279116880326045
Training loss: 0.5096392631530762 / Valid loss: 7.3138096809387205
Training loss: 0.7267584800720215 / Valid loss: 7.2599958964756555
Training loss: 0.5506165027618408 / Valid loss: 7.29160985038394
Training loss: 0.6699703931808472 / Valid loss: 7.265755839574904

Epoch: 22
Training loss: 0.5362145900726318 / Valid loss: 7.245069998786563
Training loss: 0.30801764130592346 / Valid loss: 7.238492089226132
Training loss: 0.83952796459198 / Valid loss: 7.221647489638555
Training loss: 0.4483410716056824 / Valid loss: 7.265749459039597
Training loss: 0.4088422656059265 / Valid loss: 7.241180978502546

Epoch: 23
Training loss: 0.3335721492767334 / Valid loss: 7.278582068852016
Training loss: 0.414154976606369 / Valid loss: 7.205933466411772
Training loss: 0.4672565460205078 / Valid loss: 7.194144053686233
Training loss: 0.6747460961341858 / Valid loss: 7.307330058869861
Training loss: 0.4978693127632141 / Valid loss: 7.289403978983561

Epoch: 24
Training loss: 0.4905688166618347 / Valid loss: 7.240662842705136
Training loss: 0.58234703540802 / Valid loss: 7.305638236091251
Training loss: 0.7470823526382446 / Valid loss: 7.33387344678243
Training loss: 0.6250166893005371 / Valid loss: 7.299047579084124
Training loss: 0.3829178810119629 / Valid loss: 7.266499328613281

Epoch: 25
Training loss: 0.6164097785949707 / Valid loss: 7.197815858750117
Training loss: 0.5058256387710571 / Valid loss: 7.3142066637674965
Training loss: 0.43801650404930115 / Valid loss: 7.311049924577985
Training loss: 0.47468122839927673 / Valid loss: 7.339879090445383
Training loss: 0.4202306866645813 / Valid loss: 7.336421603248233

Epoch: 26
Training loss: 0.5557188987731934 / Valid loss: 7.264964012872605
Training loss: 0.46796777844429016 / Valid loss: 7.222869773138137
Training loss: 0.33875811100006104 / Valid loss: 7.216403847648984
Training loss: 0.4276920557022095 / Valid loss: 7.27244565827506
Training loss: 0.5122987031936646 / Valid loss: 7.235443782806397

Epoch: 27
Training loss: 0.6906232833862305 / Valid loss: 7.210344105675107
Training loss: 0.7656534910202026 / Valid loss: 7.215576498849051
Training loss: 0.3312072157859802 / Valid loss: 7.165633442288353
Training loss: 0.2639773488044739 / Valid loss: 7.286332548232306
Training loss: 0.3016557991504669 / Valid loss: 7.208890206473214

Epoch: 28
Training loss: 0.38533157110214233 / Valid loss: 7.2430652868180045
Training loss: 0.44699785113334656 / Valid loss: 7.219482826051258
Training loss: 0.6502010822296143 / Valid loss: 7.284504279636201
Training loss: 0.4480707049369812 / Valid loss: 7.218942701248896
Training loss: 0.29181426763534546 / Valid loss: 7.241718560173398

Epoch: 29
Training loss: 0.9792288541793823 / Valid loss: 7.254140495118641
Training loss: 0.2524127960205078 / Valid loss: 7.201409008389428
Training loss: 0.6503776907920837 / Valid loss: 7.197626599811373
Training loss: 0.5537070035934448 / Valid loss: 7.2885011854625885

Epoch: 30
Training loss: 0.5046490430831909 / Valid loss: 7.316193707784017
Training loss: 0.47793176770210266 / Valid loss: 7.193452766963413
Training loss: 0.6363704204559326 / Valid loss: 7.259482261112758
Training loss: 0.6020787954330444 / Valid loss: 7.215859102067493
Training loss: 0.2759796380996704 / Valid loss: 7.243326423281715

Epoch: 31
Training loss: 0.4007290005683899 / Valid loss: 7.192653960273379
Training loss: 0.5471673011779785 / Valid loss: 7.248988950820197
Training loss: 0.3384259343147278 / Valid loss: 7.2059912045796715
Training loss: 0.38368868827819824 / Valid loss: 7.15116575331915
Training loss: 0.7534776926040649 / Valid loss: 7.199753747667585

Epoch: 32
Training loss: 0.3010733425617218 / Valid loss: 7.1681725093296595
Training loss: 0.328837513923645 / Valid loss: 7.204812503996349
Training loss: 0.3146568238735199 / Valid loss: 7.200760836828323
Training loss: 0.2575887441635132 / Valid loss: 7.3179152307056246
Training loss: 0.4589908719062805 / Valid loss: 7.253590240932646

Epoch: 33
Training loss: 0.66187584400177 / Valid loss: 7.21573250180199
Training loss: 0.25904417037963867 / Valid loss: 7.291640000116257
Training loss: 0.3270636200904846 / Valid loss: 7.283612128666469
Training loss: 0.25814545154571533 / Valid loss: 7.163583242325556
Training loss: 0.5642447471618652 / Valid loss: 7.210322611672538

Epoch: 34
Training loss: 0.2928004562854767 / Valid loss: 7.263837759835379
Training loss: 0.41808485984802246 / Valid loss: 7.31375512168521
Training loss: 0.5826818943023682 / Valid loss: 7.213515436081659
Training loss: 0.5479110479354858 / Valid loss: 7.183956795647031
Training loss: 0.2823723554611206 / Valid loss: 7.26795267377581

Epoch: 35
Training loss: 0.3592037558555603 / Valid loss: 7.19498398190453
Training loss: 0.39449864625930786 / Valid loss: 7.260765765962146
Training loss: 0.32721030712127686 / Valid loss: 7.1016159284682505
Training loss: 0.28111499547958374 / Valid loss: 7.142049262637184
Training loss: 0.4264049232006073 / Valid loss: 7.19553295090085

Epoch: 36
Training loss: 0.3083057701587677 / Valid loss: 7.2039657343001595
Training loss: 0.31499651074409485 / Valid loss: 7.138035068057833
Training loss: 0.20686793327331543 / Valid loss: 7.205733331044515
Training loss: 0.3865346610546112 / Valid loss: 7.255422678447905
Training loss: 0.3656767010688782 / Valid loss: 7.179043604078747

Epoch: 37
Training loss: 0.2783750891685486 / Valid loss: 7.224633975256057
Training loss: 1.053532361984253 / Valid loss: 7.152630701519194
Training loss: 0.3383406400680542 / Valid loss: 7.214035261245001
Training loss: 0.3502580523490906 / Valid loss: 7.149258431934175
Training loss: 0.44439685344696045 / Valid loss: 7.150732426416306

Epoch: 38
Training loss: 0.4064718186855316 / Valid loss: 7.1322897456941154
Training loss: 0.19152326881885529 / Valid loss: 7.265412548610143
Training loss: 0.46468231081962585 / Valid loss: 7.236129819779169
Training loss: 0.21891692280769348 / Valid loss: 7.225634443192255
Training loss: 0.3839791417121887 / Valid loss: 7.206739902496338

Epoch: 39
Training loss: 0.30140817165374756 / Valid loss: 7.179327869415284
Training loss: 0.2967921495437622 / Valid loss: 7.2053521020071845
Training loss: 0.6166531443595886 / Valid loss: 7.121372924532209
Training loss: 0.35881519317626953 / Valid loss: 7.25263154620216
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1100): 6.41183200336638
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.837162017822266 / Valid loss: 15.278275898524694
Model is saved in epoch 0, overall batch: 0
Training loss: 5.220392227172852 / Valid loss: 7.828187306722005
Model is saved in epoch 0, overall batch: 100
Training loss: 4.614802360534668 / Valid loss: 6.533045884541103
Model is saved in epoch 0, overall batch: 200
Training loss: 7.094908714294434 / Valid loss: 5.943607189541772
Model is saved in epoch 0, overall batch: 300
Training loss: 6.126688003540039 / Valid loss: 5.806771148954119
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.353774070739746 / Valid loss: 5.696808117911929
Model is saved in epoch 1, overall batch: 500
Training loss: 4.544833183288574 / Valid loss: 5.620947976339431
Model is saved in epoch 1, overall batch: 600
Training loss: 5.1407270431518555 / Valid loss: 5.5910748163859045
Model is saved in epoch 1, overall batch: 700
Training loss: 5.23374080657959 / Valid loss: 5.597980662754604
Training loss: 4.959288120269775 / Valid loss: 5.574686050415039
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.556849479675293 / Valid loss: 5.619413732347034
Training loss: 6.288552284240723 / Valid loss: 5.63654354867481
Training loss: 4.750277996063232 / Valid loss: 5.627563651402792
Training loss: 6.519655227661133 / Valid loss: 5.614048092705863
Training loss: 4.4150261878967285 / Valid loss: 5.54403490566072
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 3.8142452239990234 / Valid loss: 5.549830797740391
Training loss: 4.09561014175415 / Valid loss: 5.637673373449417
Training loss: 5.598890781402588 / Valid loss: 5.544549083709716
Training loss: 3.3288722038269043 / Valid loss: 5.5963041146596275
Training loss: 5.782506942749023 / Valid loss: 5.57953546614874

Epoch: 4
Training loss: 5.578970432281494 / Valid loss: 5.648937847500756
Training loss: 4.710021495819092 / Valid loss: 5.72748453276498
Training loss: 4.706234931945801 / Valid loss: 5.720544544855754
Training loss: 5.106228351593018 / Valid loss: 5.7241539342062815
Training loss: 3.7800700664520264 / Valid loss: 5.683205518268403

Epoch: 5
Training loss: 6.189901828765869 / Valid loss: 5.627976655960083
Training loss: 5.276583194732666 / Valid loss: 5.757366750353858
Training loss: 4.502431869506836 / Valid loss: 5.652087577184042
Training loss: 4.081334114074707 / Valid loss: 5.754167704355149
Training loss: 3.619175434112549 / Valid loss: 5.688865591230846

Epoch: 6
Training loss: 7.604041576385498 / Valid loss: 5.827529114768619
Training loss: 4.412910461425781 / Valid loss: 5.9396217346191404
Training loss: 3.8069498538970947 / Valid loss: 5.793403132756551
Training loss: 4.3780035972595215 / Valid loss: 5.883842014131092
Training loss: 4.804666519165039 / Valid loss: 5.888160523914156

Epoch: 7
Training loss: 4.861610412597656 / Valid loss: 5.798321810222808
Training loss: 4.2804975509643555 / Valid loss: 6.011219812574841
Training loss: 4.582839012145996 / Valid loss: 5.8603581542060486
Training loss: 5.217390060424805 / Valid loss: 5.933433478219168
Training loss: 4.0308074951171875 / Valid loss: 5.865613362902686

Epoch: 8
Training loss: 2.978114128112793 / Valid loss: 6.054315083367484
Training loss: 2.56524920463562 / Valid loss: 5.987492534092494
Training loss: 4.999377250671387 / Valid loss: 6.00810196967352
Training loss: 5.868736267089844 / Valid loss: 6.054757694970994
Training loss: 4.604429244995117 / Valid loss: 5.948786233720325

Epoch: 9
Training loss: 3.3059730529785156 / Valid loss: 5.990330832345145
Training loss: 4.928725242614746 / Valid loss: 5.9273925917489185
Training loss: 6.0457763671875 / Valid loss: 5.911477252415248
Training loss: 4.606113433837891 / Valid loss: 5.983702959333147

Epoch: 10
Training loss: 3.696871757507324 / Valid loss: 6.026373742875599
Training loss: 4.701446533203125 / Valid loss: 6.142410809653146
Training loss: 3.0516114234924316 / Valid loss: 6.099901251565843
Training loss: 3.296008586883545 / Valid loss: 6.110843756085351
Training loss: 3.784219741821289 / Valid loss: 6.120627053578695

Epoch: 11
Training loss: 2.777834892272949 / Valid loss: 6.080820564996628
Training loss: 3.6341707706451416 / Valid loss: 6.425314578555879
Training loss: 4.343212604522705 / Valid loss: 6.155010536738804
Training loss: 3.3364171981811523 / Valid loss: 6.095467780885242
Training loss: 3.4487123489379883 / Valid loss: 6.147239637374878

Epoch: 12
Training loss: 2.900473117828369 / Valid loss: 6.190355566569737
Training loss: 3.0765790939331055 / Valid loss: 6.2592661085582915
Training loss: 4.193549156188965 / Valid loss: 6.300632774262201
Training loss: 4.162607192993164 / Valid loss: 6.374150330679758
Training loss: 4.236857891082764 / Valid loss: 6.430488109588623

Epoch: 13
Training loss: 2.748725652694702 / Valid loss: 6.344200731459118
Training loss: 2.3179068565368652 / Valid loss: 6.584972161338443
Training loss: 4.081613540649414 / Valid loss: 6.352835618881953
Training loss: 3.5283424854278564 / Valid loss: 6.253992262340727
Training loss: 5.369858741760254 / Valid loss: 6.629874374752953

Epoch: 14
Training loss: 3.803812026977539 / Valid loss: 6.365315024058024
Training loss: 3.157043933868408 / Valid loss: 6.440950756981259
Training loss: 4.0840373039245605 / Valid loss: 6.317860151472546
Training loss: 2.9314098358154297 / Valid loss: 6.3686284156072706
Training loss: 4.402039051055908 / Valid loss: 6.406946268535796

Epoch: 15
Training loss: 2.7788238525390625 / Valid loss: 6.379336105074201
Training loss: 3.322819709777832 / Valid loss: 6.404091399056571
Training loss: 2.8768303394317627 / Valid loss: 6.393023043587094
Training loss: 3.7036681175231934 / Valid loss: 6.557851493926275
Training loss: 2.630862236022949 / Valid loss: 6.531751185371762

Epoch: 16
Training loss: 2.1653056144714355 / Valid loss: 6.45057423001244
Training loss: 2.4321515560150146 / Valid loss: 6.375600040526617
Training loss: 2.868467330932617 / Valid loss: 6.6586385931287495
Training loss: 4.112655162811279 / Valid loss: 6.370766001655942
Training loss: 3.2801461219787598 / Valid loss: 6.365341835930234

Epoch: 17
Training loss: 2.7668304443359375 / Valid loss: 6.4918376763661705
Training loss: 3.4355661869049072 / Valid loss: 6.405659262339274
Training loss: 2.636476993560791 / Valid loss: 6.454760640008109
Training loss: 3.3665194511413574 / Valid loss: 6.6118697166442875
Training loss: 2.5620675086975098 / Valid loss: 6.481954538254511

Epoch: 18
Training loss: 1.8992063999176025 / Valid loss: 6.3581834202721
Training loss: 3.0122532844543457 / Valid loss: 6.464468038649786
Training loss: 2.455310344696045 / Valid loss: 6.343556669780186
Training loss: 3.5593645572662354 / Valid loss: 6.458861553101313
Training loss: 3.811570405960083 / Valid loss: 6.446905660629272

Epoch: 19
Training loss: 2.8619964122772217 / Valid loss: 6.742872606004988
Training loss: 2.8291711807250977 / Valid loss: 6.4918276423499695
Training loss: 2.84952449798584 / Valid loss: 6.50658248038519
Training loss: 2.581334352493286 / Valid loss: 6.419793029058547

Epoch: 20
Training loss: 2.315227508544922 / Valid loss: 6.4834177221570695
Training loss: 2.0554261207580566 / Valid loss: 6.521936468850999
Training loss: 2.633113384246826 / Valid loss: 6.612780291693551
Training loss: 2.6047301292419434 / Valid loss: 6.545739855085101
Training loss: 3.250464916229248 / Valid loss: 6.536496453058152

Epoch: 21
Training loss: 2.5708720684051514 / Valid loss: 6.794168783369519
Training loss: 2.0450422763824463 / Valid loss: 6.658193106878372
Training loss: 2.0585620403289795 / Valid loss: 6.6021565982273644
Training loss: 2.5342814922332764 / Valid loss: 6.6758326734815325
Training loss: 3.426398754119873 / Valid loss: 6.7222680069151375

Epoch: 22
Training loss: 2.6728732585906982 / Valid loss: 6.554464108603341
Training loss: 1.7534689903259277 / Valid loss: 6.515180163156419
Training loss: 1.8076969385147095 / Valid loss: 6.63672410420009
Training loss: 3.0790083408355713 / Valid loss: 6.751296252296084
Training loss: 2.316678285598755 / Valid loss: 6.631135970070249

Epoch: 23
Training loss: 2.2234973907470703 / Valid loss: 6.850265641439528
Training loss: 2.667018413543701 / Valid loss: 6.621845781235468
Training loss: 2.936716318130493 / Valid loss: 6.598957792917887
Training loss: 3.367244243621826 / Valid loss: 6.864020951588949
Training loss: 3.3124332427978516 / Valid loss: 6.597983893894014

Epoch: 24
Training loss: 3.3489980697631836 / Valid loss: 6.7125696772620795
Training loss: 2.496105194091797 / Valid loss: 6.5833848930540535
Training loss: 2.0826785564422607 / Valid loss: 6.739263711656843
Training loss: 2.9548606872558594 / Valid loss: 6.7044389474959605
Training loss: 3.366668462753296 / Valid loss: 6.7427838461739675

Epoch: 25
Training loss: 1.5484092235565186 / Valid loss: 6.890424394607544
Training loss: 1.6881096363067627 / Valid loss: 6.611271322341192
Training loss: 3.1771206855773926 / Valid loss: 6.689552170889718
Training loss: 2.084249496459961 / Valid loss: 6.7111607324509395
Training loss: 2.226624011993408 / Valid loss: 6.735666783650716

Epoch: 26
Training loss: 1.9508622884750366 / Valid loss: 6.853197093237014
Training loss: 2.682239532470703 / Valid loss: 6.896807275499616
Training loss: 2.361522674560547 / Valid loss: 6.856617584682646
Training loss: 2.2372937202453613 / Valid loss: 6.970826235271636
Training loss: 2.269771099090576 / Valid loss: 6.755932703472319

Epoch: 27
Training loss: 1.9267468452453613 / Valid loss: 6.720092784790766
Training loss: 1.718837022781372 / Valid loss: 6.623372595650809
Training loss: 2.0676326751708984 / Valid loss: 6.678005045936221
Training loss: 1.8433690071105957 / Valid loss: 6.886990345092046
Training loss: 1.8946152925491333 / Valid loss: 6.734717675617763

Epoch: 28
Training loss: 1.7365700006484985 / Valid loss: 6.803142699741182
Training loss: 2.2824158668518066 / Valid loss: 6.8295205797467915
Training loss: 1.535973310470581 / Valid loss: 6.720227284658523
Training loss: 1.7656422853469849 / Valid loss: 6.897679428827195
Training loss: 1.5865732431411743 / Valid loss: 6.748520642235166

Epoch: 29
Training loss: 2.4582736492156982 / Valid loss: 6.784504972185407
Training loss: 1.968271255493164 / Valid loss: 6.782597777957008
Training loss: 1.487624168395996 / Valid loss: 6.640175145012992
Training loss: 2.0784058570861816 / Valid loss: 6.616441899254209

Epoch: 30
Training loss: 1.2398313283920288 / Valid loss: 6.7077480361575175
Training loss: 1.221197485923767 / Valid loss: 6.714909480866932
Training loss: 1.6691923141479492 / Valid loss: 6.7893610886165074
Training loss: 1.8318992853164673 / Valid loss: 6.750659252348401
Training loss: 2.015369176864624 / Valid loss: 6.7513463565281455

Epoch: 31
Training loss: 2.3009300231933594 / Valid loss: 6.714651984260196
Training loss: 1.16608726978302 / Valid loss: 6.782297781535557
Training loss: 1.888723611831665 / Valid loss: 6.79812076205299
Training loss: 1.8381954431533813 / Valid loss: 6.707774979727609
Training loss: 1.6649727821350098 / Valid loss: 6.687518131165278

Epoch: 32
Training loss: 1.3177266120910645 / Valid loss: 6.722842872710455
Training loss: 1.7174485921859741 / Valid loss: 6.641975650333223
Training loss: 1.350371241569519 / Valid loss: 6.872809092203776
Training loss: 1.1605887413024902 / Valid loss: 6.7072492099943615
Training loss: 2.0257322788238525 / Valid loss: 6.8380734171186175

Epoch: 33
Training loss: 2.040482759475708 / Valid loss: 7.033499931153797
Training loss: 1.9569554328918457 / Valid loss: 7.067677917934599
Training loss: 1.8667092323303223 / Valid loss: 6.824632095155262
Training loss: 1.9305460453033447 / Valid loss: 6.981481147947766
Training loss: 1.8462533950805664 / Valid loss: 6.789756193615141

Epoch: 34
Training loss: 1.9623836278915405 / Valid loss: 6.779409485771542
Training loss: 2.1082887649536133 / Valid loss: 6.676126484643845
Training loss: 1.2580862045288086 / Valid loss: 6.761429259890602
Training loss: 1.5345746278762817 / Valid loss: 6.745234718776884
Training loss: 1.7359001636505127 / Valid loss: 6.915047491164435

Epoch: 35
Training loss: 2.3046905994415283 / Valid loss: 6.720174896149408
Training loss: 1.5290769338607788 / Valid loss: 6.726409539722261
Training loss: 1.8055176734924316 / Valid loss: 6.8851121811639695
Training loss: 1.6121420860290527 / Valid loss: 6.8754821845463345
Training loss: 1.8148958683013916 / Valid loss: 6.7677401974087665

Epoch: 36
Training loss: 1.7438024282455444 / Valid loss: 6.7356549149467835
Training loss: 2.4927477836608887 / Valid loss: 6.726809796832857
Training loss: 2.0061278343200684 / Valid loss: 6.859071922302246
Training loss: 1.2712130546569824 / Valid loss: 6.747278980981736
Training loss: 2.06705904006958 / Valid loss: 7.068961611248198

Epoch: 37
Training loss: 1.7931632995605469 / Valid loss: 6.773156454449608
Training loss: 1.505996823310852 / Valid loss: 6.958202316647484
Training loss: 1.5807976722717285 / Valid loss: 6.793681657881963
Training loss: 2.025851249694824 / Valid loss: 6.700558169682821
Training loss: 2.193423271179199 / Valid loss: 6.897459699994042

Epoch: 38
Training loss: 1.2288992404937744 / Valid loss: 6.933962576729911
Training loss: 1.811704158782959 / Valid loss: 6.979053143092564
Training loss: 1.6953924894332886 / Valid loss: 6.881341834295363
Training loss: 1.5412602424621582 / Valid loss: 6.817397228876749
Training loss: 1.6877806186676025 / Valid loss: 6.896295125143869

Epoch: 39
Training loss: 1.1904165744781494 / Valid loss: 6.909520072028751
Training loss: 1.2881288528442383 / Valid loss: 6.804500924973261
Training loss: 1.0442862510681152 / Valid loss: 6.830529276529948
Training loss: 1.848346471786499 / Valid loss: 6.858663711093721
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1400): 5.419725370407105
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.837162017822266 / Valid loss: 15.27827588944208
Model is saved in epoch 0, overall batch: 0
Training loss: 5.221534729003906 / Valid loss: 7.819145316169376
Model is saved in epoch 0, overall batch: 100
Training loss: 4.6318511962890625 / Valid loss: 6.50560630162557
Model is saved in epoch 0, overall batch: 200
Training loss: 6.853342533111572 / Valid loss: 5.997372684024629
Model is saved in epoch 0, overall batch: 300
Training loss: 6.127419948577881 / Valid loss: 5.8350031920841765
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.470029830932617 / Valid loss: 5.696478409994216
Model is saved in epoch 1, overall batch: 500
Training loss: 4.443192481994629 / Valid loss: 5.6016305151439845
Model is saved in epoch 1, overall batch: 600
Training loss: 5.33552360534668 / Valid loss: 5.610146204630534
Training loss: 5.4848833084106445 / Valid loss: 5.625793438866025
Training loss: 4.791634559631348 / Valid loss: 5.654992394220262

Epoch: 2
Training loss: 6.663965225219727 / Valid loss: 5.628261330014183
Training loss: 6.256107330322266 / Valid loss: 5.64380373273577
Training loss: 4.9091901779174805 / Valid loss: 5.595854350498745
Model is saved in epoch 2, overall batch: 1200
Training loss: 6.0588297843933105 / Valid loss: 5.611802500770206
Training loss: 4.382164001464844 / Valid loss: 5.538769456318446
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 3.768853187561035 / Valid loss: 5.55205675760905
Training loss: 4.225557327270508 / Valid loss: 5.667231571106684
Training loss: 5.525142192840576 / Valid loss: 5.579689459573655
Training loss: 3.220621347427368 / Valid loss: 5.666664420990717
Training loss: 5.320400238037109 / Valid loss: 5.609546661376953

Epoch: 4
Training loss: 5.6871657371521 / Valid loss: 5.6562362489246185
Training loss: 4.5676655769348145 / Valid loss: 5.743133647101266
Training loss: 4.944106101989746 / Valid loss: 5.743464499428159
Training loss: 5.097833633422852 / Valid loss: 5.694757992880685
Training loss: 4.2988786697387695 / Valid loss: 5.681320085979643

Epoch: 5
Training loss: 6.697812557220459 / Valid loss: 5.641124975113642
Training loss: 4.936735153198242 / Valid loss: 5.713119702112107
Training loss: 4.418243408203125 / Valid loss: 5.715273153214228
Training loss: 4.267518520355225 / Valid loss: 5.780401005063738
Training loss: 3.643307685852051 / Valid loss: 5.717283759798322

Epoch: 6
Training loss: 7.387979030609131 / Valid loss: 5.889283439091273
Training loss: 4.483123779296875 / Valid loss: 5.82630615234375
Training loss: 4.002373695373535 / Valid loss: 5.832138649622599
Training loss: 3.874378204345703 / Valid loss: 5.880956388655163
Training loss: 4.665679454803467 / Valid loss: 5.886078968502226

Epoch: 7
Training loss: 4.940365791320801 / Valid loss: 5.762271822066534
Training loss: 3.7824981212615967 / Valid loss: 5.933978144327799
Training loss: 4.829026699066162 / Valid loss: 5.825121227900187
Training loss: 5.600563049316406 / Valid loss: 6.000624554497855
Training loss: 3.886378765106201 / Valid loss: 5.816749000549317

Epoch: 8
Training loss: 3.086852550506592 / Valid loss: 5.977976047425043
Training loss: 2.8345541954040527 / Valid loss: 5.995354529789516
Training loss: 4.558419227600098 / Valid loss: 6.012753684180123
Training loss: 6.233065605163574 / Valid loss: 6.119677752540225
Training loss: 4.4474897384643555 / Valid loss: 5.938409896123977

Epoch: 9
Training loss: 3.2160019874572754 / Valid loss: 5.967349204562959
Training loss: 5.075382232666016 / Valid loss: 5.991754366102673
Training loss: 6.295709133148193 / Valid loss: 5.952397017251878
Training loss: 4.687097549438477 / Valid loss: 6.028604852585565

Epoch: 10
Training loss: 3.516814708709717 / Valid loss: 6.015554057984125
Training loss: 5.160768508911133 / Valid loss: 6.010179033733549
Training loss: 2.997086763381958 / Valid loss: 6.094900085812523
Training loss: 3.2463927268981934 / Valid loss: 6.126066941306704
Training loss: 4.098748207092285 / Valid loss: 6.0648516745794385

Epoch: 11
Training loss: 2.9021053314208984 / Valid loss: 6.053155061176845
Training loss: 3.5709519386291504 / Valid loss: 6.341799795059931
Training loss: 4.756691932678223 / Valid loss: 6.089967473347982
Training loss: 3.0282936096191406 / Valid loss: 6.066175833202544
Training loss: 3.355605363845825 / Valid loss: 6.1921333312988285

Epoch: 12
Training loss: 2.7539901733398438 / Valid loss: 6.11584003993443
Training loss: 2.971978187561035 / Valid loss: 6.166161248797462
Training loss: 4.395700931549072 / Valid loss: 6.23774436541966
Training loss: 4.551423072814941 / Valid loss: 6.328032865978423
Training loss: 3.9393868446350098 / Valid loss: 6.215734643027896

Epoch: 13
Training loss: 2.8396501541137695 / Valid loss: 6.301796849568685
Training loss: 2.4191393852233887 / Valid loss: 6.583065146491641
Training loss: 4.443625450134277 / Valid loss: 6.439234624590193
Training loss: 3.1859517097473145 / Valid loss: 6.28510258311317
Training loss: 4.222951889038086 / Valid loss: 6.5219536327180405

Epoch: 14
Training loss: 3.772968292236328 / Valid loss: 6.301652442841303
Training loss: 3.233886241912842 / Valid loss: 6.462132226853144
Training loss: 4.829395294189453 / Valid loss: 6.440784822191511
Training loss: 2.995242118835449 / Valid loss: 6.319668018250239
Training loss: 4.801290512084961 / Valid loss: 6.41840793518793

Epoch: 15
Training loss: 2.60835337638855 / Valid loss: 6.377284322466169
Training loss: 3.863574981689453 / Valid loss: 6.423524089086623
Training loss: 3.057933807373047 / Valid loss: 6.444073054904029
Training loss: 3.065351724624634 / Valid loss: 6.466821579706101
Training loss: 2.7927017211914062 / Valid loss: 6.46833917754037

Epoch: 16
Training loss: 2.6614837646484375 / Valid loss: 6.418203185853504
Training loss: 2.6075618267059326 / Valid loss: 6.41412398474557
Training loss: 2.722071886062622 / Valid loss: 6.547009054819743
Training loss: 3.559192419052124 / Valid loss: 6.492341797692435
Training loss: 3.651386260986328 / Valid loss: 6.452758870806012

Epoch: 17
Training loss: 2.6963400840759277 / Valid loss: 6.531026781172979
Training loss: 2.735873222351074 / Valid loss: 6.436191422598703
Training loss: 2.960740566253662 / Valid loss: 6.4614446730840776
Training loss: 3.6778979301452637 / Valid loss: 6.607392851511637
Training loss: 2.497438907623291 / Valid loss: 6.478383157366798

Epoch: 18
Training loss: 2.261073350906372 / Valid loss: 6.563376762753442
Training loss: 3.029677629470825 / Valid loss: 6.489194561186291
Training loss: 2.3412983417510986 / Valid loss: 6.4993250392732165
Training loss: 3.097707509994507 / Valid loss: 6.5519940943945025
Training loss: 3.451571464538574 / Valid loss: 6.5747765404837475

Epoch: 19
Training loss: 2.6279382705688477 / Valid loss: 6.772661011559623
Training loss: 3.08215069770813 / Valid loss: 6.533610657283238
Training loss: 2.403763771057129 / Valid loss: 6.60777519771031
Training loss: 1.9990192651748657 / Valid loss: 6.48681823185512

Epoch: 20
Training loss: 2.0649352073669434 / Valid loss: 6.478408245813279
Training loss: 2.195641040802002 / Valid loss: 6.531895355951218
Training loss: 2.247663974761963 / Valid loss: 6.800663434891474
Training loss: 2.308317184448242 / Valid loss: 6.657424745105562
Training loss: 3.371093273162842 / Valid loss: 6.513464509873163

Epoch: 21
Training loss: 2.3539185523986816 / Valid loss: 6.591872887384324
Training loss: 2.1967897415161133 / Valid loss: 6.671855395180838
Training loss: 1.9365036487579346 / Valid loss: 6.557404627118792
Training loss: 2.860743999481201 / Valid loss: 6.730589714504424
Training loss: 2.964510917663574 / Valid loss: 6.636037399655296

Epoch: 22
Training loss: 3.0095462799072266 / Valid loss: 6.540019562130882
Training loss: 1.7638022899627686 / Valid loss: 6.485462202344622
Training loss: 1.8642292022705078 / Valid loss: 6.677077327455793
Training loss: 2.9085326194763184 / Valid loss: 6.6065847737448555
Training loss: 2.661754608154297 / Valid loss: 6.626424909773327

Epoch: 23
Training loss: 2.4990363121032715 / Valid loss: 6.602350241797311
Training loss: 2.5506834983825684 / Valid loss: 6.588493115561349
Training loss: 2.4928154945373535 / Valid loss: 6.670846321469262
Training loss: 2.9195966720581055 / Valid loss: 6.872378926050096
Training loss: 3.001675605773926 / Valid loss: 6.609687718890962

Epoch: 24
Training loss: 3.4032087326049805 / Valid loss: 6.729148928324381
Training loss: 2.2882792949676514 / Valid loss: 6.680225013551258
Training loss: 2.580036163330078 / Valid loss: 6.620394588652111
Training loss: 2.7542948722839355 / Valid loss: 6.820804196312314
Training loss: 3.9021899700164795 / Valid loss: 6.649527676900228

Epoch: 25
Training loss: 2.0024075508117676 / Valid loss: 6.766730503808884
Training loss: 1.6178808212280273 / Valid loss: 6.664491246995472
Training loss: 3.0601649284362793 / Valid loss: 6.796108373006185
Training loss: 2.079181671142578 / Valid loss: 6.73106564113072
Training loss: 2.6940903663635254 / Valid loss: 6.807207068942842

Epoch: 26
Training loss: 2.6183338165283203 / Valid loss: 6.999659243084135
Training loss: 3.0376405715942383 / Valid loss: 6.798325107211158
Training loss: 2.245487689971924 / Valid loss: 6.890801043737502
Training loss: 2.4004859924316406 / Valid loss: 6.775471376237415
Training loss: 1.8238261938095093 / Valid loss: 6.655689398447673

Epoch: 27
Training loss: 1.9262042045593262 / Valid loss: 6.7384283701578775
Training loss: 1.6404502391815186 / Valid loss: 6.672076931453886
Training loss: 1.4394420385360718 / Valid loss: 6.768217286609468
Training loss: 1.7464823722839355 / Valid loss: 6.794784119015648
Training loss: 1.794144630432129 / Valid loss: 6.704315966651553

Epoch: 28
Training loss: 1.4540845155715942 / Valid loss: 7.07413945879255
Training loss: 2.0341668128967285 / Valid loss: 6.946659163066319
Training loss: 1.571348786354065 / Valid loss: 6.863580506188529
Training loss: 1.3185302019119263 / Valid loss: 6.983425907861619
Training loss: 1.8018910884857178 / Valid loss: 6.823267584755307

Epoch: 29
Training loss: 2.2364277839660645 / Valid loss: 7.050778529757545
Training loss: 1.8523623943328857 / Valid loss: 6.88612475622268
Training loss: 1.9274494647979736 / Valid loss: 6.865301536378406
Training loss: 2.4638538360595703 / Valid loss: 6.79993386495681

Epoch: 30
Training loss: 1.4461424350738525 / Valid loss: 6.776527965636481
Training loss: 1.4432216882705688 / Valid loss: 6.729027927489508
Training loss: 1.3406426906585693 / Valid loss: 6.798382293610346
Training loss: 1.9527475833892822 / Valid loss: 6.7710141045706616
Training loss: 2.554736614227295 / Valid loss: 6.788300975163778

Epoch: 31
Training loss: 1.9934606552124023 / Valid loss: 6.787646529788063
Training loss: 1.2296279668807983 / Valid loss: 6.845158881232852
Training loss: 1.714743971824646 / Valid loss: 6.756919370378767
Training loss: 1.5204753875732422 / Valid loss: 6.839310957136608
Training loss: 1.6224589347839355 / Valid loss: 6.701958956037249

Epoch: 32
Training loss: 1.345677137374878 / Valid loss: 6.836194742293585
Training loss: 1.315589427947998 / Valid loss: 6.733848371959868
Training loss: 1.2448227405548096 / Valid loss: 7.053470588865734
Training loss: 1.2336416244506836 / Valid loss: 6.878612468356178
Training loss: 2.3485817909240723 / Valid loss: 6.9256220726739794

Epoch: 33
Training loss: 1.8844884634017944 / Valid loss: 6.864845387140909
Training loss: 1.7462732791900635 / Valid loss: 6.908345737911406
Training loss: 1.548670768737793 / Valid loss: 6.820565789086478
Training loss: 1.7137380838394165 / Valid loss: 6.9928762799217585
Training loss: 1.8830838203430176 / Valid loss: 6.809553813934326

Epoch: 34
Training loss: 1.9750583171844482 / Valid loss: 6.881219600495838
Training loss: 2.161107063293457 / Valid loss: 6.842830251512074
Training loss: 1.468839406967163 / Valid loss: 6.7961353483654205
Training loss: 1.9071650505065918 / Valid loss: 6.907340099698021
Training loss: 1.898047924041748 / Valid loss: 6.828075785863967

Epoch: 35
Training loss: 1.7533113956451416 / Valid loss: 6.82207940419515
Training loss: 1.508134126663208 / Valid loss: 6.820003632136753
Training loss: 1.5997563600540161 / Valid loss: 6.735665893554687
Training loss: 1.8071088790893555 / Valid loss: 6.817158724012829
Training loss: 1.4351434707641602 / Valid loss: 6.934026030131749

Epoch: 36
Training loss: 2.386627674102783 / Valid loss: 6.820510755266462
Training loss: 2.10746431350708 / Valid loss: 6.932413959503174
Training loss: 1.8122563362121582 / Valid loss: 6.920272509256999
Training loss: 1.4805711507797241 / Valid loss: 6.935239058449155
Training loss: 2.024651050567627 / Valid loss: 7.31495536622547

Epoch: 37
Training loss: 1.8152086734771729 / Valid loss: 6.9987026350838795
Training loss: 1.1895921230316162 / Valid loss: 7.04553770337786
Training loss: 1.6038217544555664 / Valid loss: 6.968612834385463
Training loss: 2.2652640342712402 / Valid loss: 6.881406588781448
Training loss: 2.121025800704956 / Valid loss: 6.98082510857355

Epoch: 38
Training loss: 1.266377329826355 / Valid loss: 6.925466237749372
Training loss: 2.0696094036102295 / Valid loss: 7.002367296673003
Training loss: 1.5451358556747437 / Valid loss: 6.968585845402309
Training loss: 1.357852816581726 / Valid loss: 6.881085404895601
Training loss: 1.9241299629211426 / Valid loss: 7.077431342715308

Epoch: 39
Training loss: 1.2666141986846924 / Valid loss: 7.21404930750529
Training loss: 1.4851067066192627 / Valid loss: 6.908290717715309
Training loss: 0.9385549426078796 / Valid loss: 6.976858856564476
Training loss: 1.8407098054885864 / Valid loss: 6.806990557625181
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1400): 5.412232460294451
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.974810600280762 / Valid loss: 16.553642518179757
Model is saved in epoch 0, overall batch: 0
Training loss: 3.2214431762695312 / Valid loss: 7.385508682614281
Model is saved in epoch 0, overall batch: 100
Training loss: 5.790338039398193 / Valid loss: 6.743573136556716
Model is saved in epoch 0, overall batch: 200
Training loss: 4.923066139221191 / Valid loss: 6.666937442052932
Model is saved in epoch 0, overall batch: 300
Training loss: 4.181571960449219 / Valid loss: 6.615882482982817
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.024608612060547 / Valid loss: 6.445354298182896
Model is saved in epoch 1, overall batch: 500
Training loss: 5.29262113571167 / Valid loss: 6.4008284863971525
Model is saved in epoch 1, overall batch: 600
Training loss: 4.664851188659668 / Valid loss: 6.687797877902076
Training loss: 4.5195770263671875 / Valid loss: 6.59180238814581
Training loss: 5.885019302368164 / Valid loss: 5.951872192110334
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.853878974914551 / Valid loss: 6.286695848192488
Training loss: 4.378852844238281 / Valid loss: 6.171266953150432
Training loss: 4.700272560119629 / Valid loss: 5.963468917210897
Training loss: 6.09489631652832 / Valid loss: 6.172077029091971
Training loss: 5.727889060974121 / Valid loss: 6.26577270144508

Epoch: 3
Training loss: 5.238607883453369 / Valid loss: 5.957097303299677
Training loss: 4.081147193908691 / Valid loss: 5.945383042380923
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.5741167068481445 / Valid loss: 6.348569754191807
Training loss: 5.027029037475586 / Valid loss: 5.859715691066923
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.396420001983643 / Valid loss: 6.156999883197603

Epoch: 4
Training loss: 3.4581689834594727 / Valid loss: 5.997040716807048
Training loss: 4.1341376304626465 / Valid loss: 6.102419219698224
Training loss: 4.143847942352295 / Valid loss: 6.138813640957787
Training loss: 4.890590667724609 / Valid loss: 5.786032674426124
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.495059967041016 / Valid loss: 6.011568700699579

Epoch: 5
Training loss: 5.952596187591553 / Valid loss: 5.908335381462461
Training loss: 5.918523788452148 / Valid loss: 5.868268959862846
Training loss: 5.839385509490967 / Valid loss: 5.805882878530593
Training loss: 3.346085548400879 / Valid loss: 5.701661380132039
Model is saved in epoch 5, overall batch: 2800
Training loss: 6.591358661651611 / Valid loss: 5.960373823983328

Epoch: 6
Training loss: 5.0807952880859375 / Valid loss: 5.614453667686099
Model is saved in epoch 6, overall batch: 3000
Training loss: 2.988102912902832 / Valid loss: 5.895857438587007
Training loss: 5.066937446594238 / Valid loss: 5.759974740800404
Training loss: 4.460421562194824 / Valid loss: 5.842493545441401
Training loss: 4.836494445800781 / Valid loss: 5.787394582657587

Epoch: 7
Training loss: 4.301815986633301 / Valid loss: 5.728195135934012
Training loss: 3.1954002380371094 / Valid loss: 5.7148876439957395
Training loss: 3.626802921295166 / Valid loss: 5.654050631750198
Training loss: 5.1939287185668945 / Valid loss: 5.694851439339774
Training loss: 4.3237690925598145 / Valid loss: 5.770399729410808

Epoch: 8
Training loss: 3.589414596557617 / Valid loss: 5.671610764094761
Training loss: 4.088677406311035 / Valid loss: 6.025671052932739
Training loss: 3.4896063804626465 / Valid loss: 5.684690975007557
Training loss: 4.477606773376465 / Valid loss: 5.80233439717974
Training loss: 4.124122619628906 / Valid loss: 6.060026990799677

Epoch: 9
Training loss: 3.3595290184020996 / Valid loss: 5.804760340281895
Training loss: 4.615509986877441 / Valid loss: 5.755874433971587
Training loss: 5.148487091064453 / Valid loss: 5.788068110602243
Training loss: 4.00901460647583 / Valid loss: 5.771978303364345

Epoch: 10
Training loss: 3.9796314239501953 / Valid loss: 5.923343617575509
Training loss: 3.242478609085083 / Valid loss: 5.791216916129702
Training loss: 4.113953590393066 / Valid loss: 5.835962302344186
Training loss: 3.673666477203369 / Valid loss: 5.824796565373739
Training loss: 4.9831624031066895 / Valid loss: 6.03645841053554

Epoch: 11
Training loss: 4.3004889488220215 / Valid loss: 5.76075702394758
Training loss: 5.468070030212402 / Valid loss: 5.952799345198132
Training loss: 4.419939041137695 / Valid loss: 5.877624266488212
Training loss: 4.594923973083496 / Valid loss: 5.980498972393217
Training loss: 4.720625877380371 / Valid loss: 5.853038488115583

Epoch: 12
Training loss: 3.8670454025268555 / Valid loss: 6.101079650152297
Training loss: 4.5813446044921875 / Valid loss: 6.042883941105434
Training loss: 4.431128025054932 / Valid loss: 5.980861943108695
Training loss: 4.23965311050415 / Valid loss: 5.837023076556978
Training loss: 5.144822597503662 / Valid loss: 6.137276445116315

Epoch: 13
Training loss: 3.9080939292907715 / Valid loss: 5.855675726845151
Training loss: 5.297342300415039 / Valid loss: 6.161423369816371
Training loss: 2.9293994903564453 / Valid loss: 6.339580431438628
Training loss: 3.8623874187469482 / Valid loss: 6.017865748632522
Training loss: 4.195286750793457 / Valid loss: 5.919154183069865

Epoch: 14
Training loss: 3.3968029022216797 / Valid loss: 5.990416504087902
Training loss: 3.7022907733917236 / Valid loss: 5.944853648685274
Training loss: 3.280748128890991 / Valid loss: 6.2430021195184615
Training loss: 3.9376955032348633 / Valid loss: 6.021887343270438
Training loss: 3.4605281352996826 / Valid loss: 6.035109824226016

Epoch: 15
Training loss: 3.740410327911377 / Valid loss: 6.031165302367437
Training loss: 3.5758450031280518 / Valid loss: 6.107483080455235
Training loss: 3.9146690368652344 / Valid loss: 6.05469475927807
Training loss: 4.252986907958984 / Valid loss: 6.025684343065534
Training loss: 3.329566478729248 / Valid loss: 6.031488679704212

Epoch: 16
Training loss: 4.984180450439453 / Valid loss: 6.1936598346346905
Training loss: 3.5108444690704346 / Valid loss: 6.2492795807974675
Training loss: 2.7098679542541504 / Valid loss: 6.103070792697725
Training loss: 3.1939473152160645 / Valid loss: 6.086962418329148
Training loss: 4.682498931884766 / Valid loss: 6.073685552960351

Epoch: 17
Training loss: 2.939906597137451 / Valid loss: 6.129984403791882
Training loss: 3.5271050930023193 / Valid loss: 6.079217738196963
Training loss: 2.7505998611450195 / Valid loss: 6.118839756647746
Training loss: 4.065694808959961 / Valid loss: 6.12553208214896
Training loss: 4.027407646179199 / Valid loss: 6.329261296136039

Epoch: 18
Training loss: 2.479475736618042 / Valid loss: 6.398823461078462
Training loss: 3.645756959915161 / Valid loss: 6.12011871564956
Training loss: 2.8539397716522217 / Valid loss: 6.33006858371553
Training loss: 4.018857955932617 / Valid loss: 6.085340102513631
Training loss: 3.9842896461486816 / Valid loss: 6.399512829099383

Epoch: 19
Training loss: 3.863011598587036 / Valid loss: 6.229334663209461
Training loss: 2.9629368782043457 / Valid loss: 6.208879236947922
Training loss: 3.0738515853881836 / Valid loss: 6.195145577476138
Training loss: 3.2910938262939453 / Valid loss: 6.449064683914185

Epoch: 20
Training loss: 2.803424835205078 / Valid loss: 6.1566371168409075
Training loss: 3.000673294067383 / Valid loss: 6.240767022541591
Training loss: 2.8587677478790283 / Valid loss: 6.25852560769944
Training loss: 4.387332439422607 / Valid loss: 6.201708907172794
Training loss: 2.8734688758850098 / Valid loss: 6.170851310094197

Epoch: 21
Training loss: 2.5950326919555664 / Valid loss: 6.218191719055175
Training loss: 2.5170059204101562 / Valid loss: 6.726378281911214
Training loss: 3.5435266494750977 / Valid loss: 6.585398210797991
Training loss: 3.1937716007232666 / Valid loss: 6.2114981901077995
Training loss: 3.5894060134887695 / Valid loss: 6.382895278930664

Epoch: 22
Training loss: 2.885366916656494 / Valid loss: 6.328711595989409
Training loss: 2.467966079711914 / Valid loss: 6.276598094758533
Training loss: 3.6580734252929688 / Valid loss: 6.262741661071777
Training loss: 2.4015421867370605 / Valid loss: 6.282021601994832
Training loss: 4.2640380859375 / Valid loss: 6.522566397984822

Epoch: 23
Training loss: 3.3411953449249268 / Valid loss: 6.213523342495873
Training loss: 3.1022934913635254 / Valid loss: 6.5699216161455425
Training loss: 3.360084056854248 / Valid loss: 6.311510898953393
Training loss: 4.608236312866211 / Valid loss: 6.420699623652867
Training loss: 3.20692777633667 / Valid loss: 6.664783309754871

Epoch: 24
Training loss: 3.120830535888672 / Valid loss: 6.375630669366746
Training loss: 3.4407639503479004 / Valid loss: 6.410198518208095
Training loss: 2.303253412246704 / Valid loss: 6.516656478246053
Training loss: 3.526202917098999 / Valid loss: 6.6265239261445545
Training loss: 3.0691640377044678 / Valid loss: 6.487921687534877

Epoch: 25
Training loss: 2.786410093307495 / Valid loss: 6.389730299086798
Training loss: 2.106461524963379 / Valid loss: 6.560401330675398
Training loss: 2.3897705078125 / Valid loss: 6.551687633423578
Training loss: 2.9279370307922363 / Valid loss: 6.319638608750843
Training loss: 2.9908924102783203 / Valid loss: 6.695761519386655

Epoch: 26
Training loss: 3.334284543991089 / Valid loss: 6.299938905806768
Training loss: 3.0410842895507812 / Valid loss: 6.658866598492577
Training loss: 2.7274012565612793 / Valid loss: 6.421281183333624
Training loss: 2.3691389560699463 / Valid loss: 6.71884312856765
Training loss: 3.4505434036254883 / Valid loss: 6.624010145096552

Epoch: 27
Training loss: 3.2040555477142334 / Valid loss: 6.777088228861491
Training loss: 2.2217812538146973 / Valid loss: 6.675918379284087
Training loss: 3.5239343643188477 / Valid loss: 6.587946426300776
Training loss: 3.805065870285034 / Valid loss: 6.477174965540568
Training loss: 1.5685460567474365 / Valid loss: 6.419995139894032

Epoch: 28
Training loss: 2.2081096172332764 / Valid loss: 6.4273276646931965
Training loss: 1.7254959344863892 / Valid loss: 6.497591286613828
Training loss: 2.5841431617736816 / Valid loss: 6.568358593895322
Training loss: 1.813298225402832 / Valid loss: 6.630928934188116
Training loss: 2.715667247772217 / Valid loss: 6.8061595666976205

Epoch: 29
Training loss: 2.862896203994751 / Valid loss: 6.776058757872809
Training loss: 2.3229196071624756 / Valid loss: 6.497763626916068
Training loss: 2.622469902038574 / Valid loss: 6.439820362272717
Training loss: 3.3053672313690186 / Valid loss: 6.449719669705345

Epoch: 30
Training loss: 2.1967520713806152 / Valid loss: 6.470728102184477
Training loss: 2.4420809745788574 / Valid loss: 6.952707844688779
Training loss: 2.1023788452148438 / Valid loss: 6.57662615776062
Training loss: 3.0095713138580322 / Valid loss: 6.625410159428914
Training loss: 2.7874767780303955 / Valid loss: 6.694046556381952

Epoch: 31
Training loss: 2.022855281829834 / Valid loss: 6.48120912597293
Training loss: 5.1003875732421875 / Valid loss: 6.7360841819218225
Training loss: 2.967989921569824 / Valid loss: 6.628574832280477
Training loss: 2.354170560836792 / Valid loss: 6.803379549298968
Training loss: 2.4161741733551025 / Valid loss: 6.460262966156006

Epoch: 32
Training loss: 1.8430330753326416 / Valid loss: 6.5139365423293345
Training loss: 2.031937599182129 / Valid loss: 6.600886419841221
Training loss: 2.755856513977051 / Valid loss: 6.556011599586124
Training loss: 2.922417163848877 / Valid loss: 6.438843016397385
Training loss: 2.895946502685547 / Valid loss: 6.842738103866577

Epoch: 33
Training loss: 2.241623640060425 / Valid loss: 6.7809306962149485
Training loss: 2.677640914916992 / Valid loss: 6.503568538029989
Training loss: 2.449221611022949 / Valid loss: 6.573141188848586
Training loss: 2.622309684753418 / Valid loss: 6.794988604954311
Training loss: 1.7302216291427612 / Valid loss: 6.6429859706333705

Epoch: 34
Training loss: 2.490842342376709 / Valid loss: 6.6038714726765955
Training loss: 2.545659065246582 / Valid loss: 6.496912538437616
Training loss: 1.9720865488052368 / Valid loss: 6.698706826709565
Training loss: 2.4153199195861816 / Valid loss: 7.33279428027925
Training loss: 1.889317274093628 / Valid loss: 6.821252768380301

Epoch: 35
Training loss: 2.0991616249084473 / Valid loss: 6.548335920061384
Training loss: 2.0352749824523926 / Valid loss: 6.730895006088984
Training loss: 2.2469468116760254 / Valid loss: 6.9234883490062895
Training loss: 1.999588966369629 / Valid loss: 6.627125136057535
Training loss: 2.358476161956787 / Valid loss: 6.645311301095145

Epoch: 36
Training loss: 1.813423752784729 / Valid loss: 6.584770434243339
Training loss: 1.892151117324829 / Valid loss: 6.512325152896699
Training loss: 1.9783308506011963 / Valid loss: 7.047653152829125
Training loss: 2.8674333095550537 / Valid loss: 6.541419758115496
Training loss: 2.1900272369384766 / Valid loss: 6.5537486825670515

Epoch: 37
Training loss: 2.1372599601745605 / Valid loss: 6.737428867249262
Training loss: 1.5681581497192383 / Valid loss: 6.934134796687535
Training loss: 1.616969108581543 / Valid loss: 6.624740491594587
Training loss: 1.8574286699295044 / Valid loss: 6.58140824181693
Training loss: 2.0525407791137695 / Valid loss: 6.53359682900565

Epoch: 38
Training loss: 1.7180547714233398 / Valid loss: 6.576241477330526
Training loss: 2.13122296333313 / Valid loss: 6.756957926068988
Training loss: 1.9578428268432617 / Valid loss: 6.506206012907482
Training loss: 2.4261698722839355 / Valid loss: 6.768434081758771
Training loss: 2.1801259517669678 / Valid loss: 6.923895082019624

Epoch: 39
Training loss: 2.0990889072418213 / Valid loss: 6.623250825064523
Training loss: 1.7356362342834473 / Valid loss: 6.741289025261288
Training loss: 1.8948050737380981 / Valid loss: 6.586906730561029
Training loss: 2.245029926300049 / Valid loss: 6.750729065849668
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 3000): 5.509328431174868
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.974810600280762 / Valid loss: 16.55364252726237
Model is saved in epoch 0, overall batch: 0
Training loss: 3.2214441299438477 / Valid loss: 7.385514595395043
Model is saved in epoch 0, overall batch: 100
Training loss: 5.790334224700928 / Valid loss: 6.743579151516869
Model is saved in epoch 0, overall batch: 200
Training loss: 4.935799598693848 / Valid loss: 6.710210679826282
Model is saved in epoch 0, overall batch: 300
Training loss: 4.129919052124023 / Valid loss: 6.740663907641456

Epoch: 1
Training loss: 6.013772010803223 / Valid loss: 6.510670341764178
Model is saved in epoch 1, overall batch: 500
Training loss: 5.3237714767456055 / Valid loss: 6.437555914833432
Model is saved in epoch 1, overall batch: 600
Training loss: 4.550409317016602 / Valid loss: 6.827863366263253
Training loss: 4.281014919281006 / Valid loss: 6.576507107416789
Training loss: 5.693778991699219 / Valid loss: 6.050260718663534
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.501856327056885 / Valid loss: 6.348745686667306
Training loss: 4.627789497375488 / Valid loss: 6.25421503384908
Training loss: 4.641681671142578 / Valid loss: 6.055735774267287
Training loss: 6.055310249328613 / Valid loss: 6.34668520064581
Training loss: 5.655203819274902 / Valid loss: 6.232457444781349

Epoch: 3
Training loss: 5.143023490905762 / Valid loss: 5.885758981250581
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.7367892265319824 / Valid loss: 5.7789337067377
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.430977821350098 / Valid loss: 6.173400479271298
Training loss: 4.8591837882995605 / Valid loss: 5.826839376631237
Training loss: 4.620282173156738 / Valid loss: 6.213226029986427

Epoch: 4
Training loss: 3.6183478832244873 / Valid loss: 6.005642906824748
Training loss: 4.063469409942627 / Valid loss: 6.041423111870175
Training loss: 4.244951248168945 / Valid loss: 6.205229470843361
Training loss: 4.883378982543945 / Valid loss: 5.767042294002715
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.249691009521484 / Valid loss: 5.9423238459087555

Epoch: 5
Training loss: 6.436344146728516 / Valid loss: 5.731090953236534
Model is saved in epoch 5, overall batch: 2500
Training loss: 5.974180221557617 / Valid loss: 5.919757327579316
Training loss: 5.716156959533691 / Valid loss: 5.846293908073789
Training loss: 3.6279282569885254 / Valid loss: 5.676224649520147
Model is saved in epoch 5, overall batch: 2800
Training loss: 6.350156307220459 / Valid loss: 6.017956924438477

Epoch: 6
Training loss: 5.1934990882873535 / Valid loss: 5.670412426903134
Model is saved in epoch 6, overall batch: 3000
Training loss: 3.035461902618408 / Valid loss: 5.821675414130802
Training loss: 5.250075817108154 / Valid loss: 5.771674267450968
Training loss: 4.4760565757751465 / Valid loss: 5.718154257819766
Training loss: 5.0244035720825195 / Valid loss: 5.643113753909156
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 4.04318904876709 / Valid loss: 5.7088421435583205
Training loss: 3.305774211883545 / Valid loss: 5.69240593001956
Training loss: 3.5747878551483154 / Valid loss: 5.636669744764055
Model is saved in epoch 7, overall batch: 3700
Training loss: 4.806318283081055 / Valid loss: 5.6766489869072325
Training loss: 4.052477836608887 / Valid loss: 5.7699444929758705

Epoch: 8
Training loss: 3.7428596019744873 / Valid loss: 5.666786055337815
Training loss: 4.1322832107543945 / Valid loss: 5.958032353719076
Training loss: 3.357832193374634 / Valid loss: 5.7025400797526045
Training loss: 4.772866249084473 / Valid loss: 5.707534980773926
Training loss: 3.87314510345459 / Valid loss: 6.046615428016299

Epoch: 9
Training loss: 3.3314414024353027 / Valid loss: 5.750546773274739
Training loss: 4.99544095993042 / Valid loss: 5.757270815258934
Training loss: 5.031781196594238 / Valid loss: 5.757027980259487
Training loss: 3.7719571590423584 / Valid loss: 5.889632817677089

Epoch: 10
Training loss: 4.072352409362793 / Valid loss: 6.087170462381272
Training loss: 3.1025390625 / Valid loss: 5.734895465487526
Training loss: 4.368669033050537 / Valid loss: 5.814650474275862
Training loss: 4.202473163604736 / Valid loss: 5.868179096494402
Training loss: 5.556859493255615 / Valid loss: 6.085848501750401

Epoch: 11
Training loss: 4.206772804260254 / Valid loss: 5.744100166502453
Training loss: 4.968628883361816 / Valid loss: 5.951858561379569
Training loss: 4.773936748504639 / Valid loss: 5.876391776402792
Training loss: 4.461423873901367 / Valid loss: 6.211732700892857
Training loss: 3.999394655227661 / Valid loss: 5.888288861229306

Epoch: 12
Training loss: 3.2258448600769043 / Valid loss: 6.18540445509411
Training loss: 4.62492561340332 / Valid loss: 5.984893201646351
Training loss: 4.48555326461792 / Valid loss: 6.048895640600295
Training loss: 4.162057876586914 / Valid loss: 5.807413142068046
Training loss: 5.717647552490234 / Valid loss: 6.148829126358033

Epoch: 13
Training loss: 3.8048977851867676 / Valid loss: 5.821450887407575
Training loss: 5.134483814239502 / Valid loss: 6.072062135878063
Training loss: 2.8388357162475586 / Valid loss: 6.4764899753388905
Training loss: 3.744290828704834 / Valid loss: 6.027212642488026
Training loss: 4.096716403961182 / Valid loss: 6.055734423228673

Epoch: 14
Training loss: 3.341485023498535 / Valid loss: 6.02696498462132
Training loss: 3.305919647216797 / Valid loss: 6.020335531234741
Training loss: 3.3007588386535645 / Valid loss: 6.310342205138434
Training loss: 4.022200584411621 / Valid loss: 5.969180143447149
Training loss: 3.801363945007324 / Valid loss: 5.982715086709885

Epoch: 15
Training loss: 3.7755792140960693 / Valid loss: 6.060534538541521
Training loss: 3.4115538597106934 / Valid loss: 6.117125933510916
Training loss: 3.4414901733398438 / Valid loss: 6.055035668327695
Training loss: 4.363877296447754 / Valid loss: 6.0584468728020076
Training loss: 3.437499761581421 / Valid loss: 5.971172289621262

Epoch: 16
Training loss: 4.640663146972656 / Valid loss: 6.309064774286179
Training loss: 2.8904247283935547 / Valid loss: 6.1758029120309015
Training loss: 3.3106682300567627 / Valid loss: 6.058796837216332
Training loss: 3.515601396560669 / Valid loss: 6.119879184450422
Training loss: 4.703901290893555 / Valid loss: 5.983171771821522

Epoch: 17
Training loss: 2.4825189113616943 / Valid loss: 6.092668344860985
Training loss: 3.6716771125793457 / Valid loss: 6.144682455062866
Training loss: 3.049227714538574 / Valid loss: 6.073500883011591
Training loss: 3.723529815673828 / Valid loss: 6.109929125649589
Training loss: 4.011979103088379 / Valid loss: 6.245641013554164

Epoch: 18
Training loss: 2.37473201751709 / Valid loss: 6.188846340633574
Training loss: 3.4171929359436035 / Valid loss: 6.188236597606114
Training loss: 2.7666208744049072 / Valid loss: 6.209190922691708
Training loss: 4.008566856384277 / Valid loss: 6.105794429779053
Training loss: 3.885791301727295 / Valid loss: 6.343369470323835

Epoch: 19
Training loss: 3.5938100814819336 / Valid loss: 6.186319871175857
Training loss: 2.533677101135254 / Valid loss: 6.207563566026233
Training loss: 3.3540236949920654 / Valid loss: 6.118360934938703
Training loss: 3.7851366996765137 / Valid loss: 6.434136463346936

Epoch: 20
Training loss: 2.4102864265441895 / Valid loss: 6.162451408022926
Training loss: 2.8217315673828125 / Valid loss: 6.213693275905791
Training loss: 2.534731864929199 / Valid loss: 6.283081343060448
Training loss: 3.902496814727783 / Valid loss: 6.273911971137637
Training loss: 2.672341823577881 / Valid loss: 6.252216595695132

Epoch: 21
Training loss: 2.7717907428741455 / Valid loss: 6.201144749777658
Training loss: 2.925837516784668 / Valid loss: 6.997657158261254
Training loss: 3.0099833011627197 / Valid loss: 6.642218576158796
Training loss: 3.197936534881592 / Valid loss: 6.179661873408726
Training loss: 3.884507179260254 / Valid loss: 6.334736528850737

Epoch: 22
Training loss: 2.5387189388275146 / Valid loss: 6.4624659538269045
Training loss: 2.6154019832611084 / Valid loss: 6.2192282971881685
Training loss: 3.548948049545288 / Valid loss: 6.2935136181967595
Training loss: 2.407926559448242 / Valid loss: 6.323210080464681
Training loss: 4.2734222412109375 / Valid loss: 6.558399248123169

Epoch: 23
Training loss: 3.2751412391662598 / Valid loss: 6.234893930526007
Training loss: 3.118985652923584 / Valid loss: 6.372609994525001
Training loss: 3.640219211578369 / Valid loss: 6.310194192613874
Training loss: 4.1959381103515625 / Valid loss: 6.83912596929641
Training loss: 2.7100577354431152 / Valid loss: 6.501697776431129

Epoch: 24
Training loss: 3.317873477935791 / Valid loss: 6.349273479552496
Training loss: 3.6594319343566895 / Valid loss: 6.498836058662051
Training loss: 2.495022773742676 / Valid loss: 6.426514366694859
Training loss: 3.108886957168579 / Valid loss: 7.0003630002339685
Training loss: 2.907276153564453 / Valid loss: 6.499943803605579

Epoch: 25
Training loss: 2.9124627113342285 / Valid loss: 6.4177403790610175
Training loss: 2.1426265239715576 / Valid loss: 6.6332709448678155
Training loss: 2.369536876678467 / Valid loss: 6.60075094586327
Training loss: 3.325507640838623 / Valid loss: 6.289075265611921
Training loss: 3.4916553497314453 / Valid loss: 6.559149042765299

Epoch: 26
Training loss: 4.022364616394043 / Valid loss: 6.275354862213135
Training loss: 3.5916965007781982 / Valid loss: 6.441546403794062
Training loss: 3.3348217010498047 / Valid loss: 6.460360517955961
Training loss: 2.3291687965393066 / Valid loss: 6.8446387949444
Training loss: 3.52108097076416 / Valid loss: 6.53389915057591

Epoch: 27
Training loss: 3.11186146736145 / Valid loss: 6.502554575602214
Training loss: 2.214629650115967 / Valid loss: 6.828051762353806
Training loss: 3.2878286838531494 / Valid loss: 6.655855923607236
Training loss: 3.354280948638916 / Valid loss: 6.368792872201829
Training loss: 1.8029568195343018 / Valid loss: 6.381844388870966

Epoch: 28
Training loss: 1.987394094467163 / Valid loss: 6.459728247778756
Training loss: 2.5295968055725098 / Valid loss: 6.501981617155529
Training loss: 2.637159824371338 / Valid loss: 6.594294037137713
Training loss: 2.241542339324951 / Valid loss: 6.768674577985491
Training loss: 2.6883952617645264 / Valid loss: 6.586687131155105

Epoch: 29
Training loss: 2.7513391971588135 / Valid loss: 6.657126653762091
Training loss: 2.6401827335357666 / Valid loss: 6.467727997189477
Training loss: 2.536942481994629 / Valid loss: 6.437376885187058
Training loss: 3.9235262870788574 / Valid loss: 6.539895184834799

Epoch: 30
Training loss: 1.918013572692871 / Valid loss: 6.693446182069325
Training loss: 2.9028444290161133 / Valid loss: 7.521914836338588
Training loss: 2.295083522796631 / Valid loss: 6.540091314769927
Training loss: 3.02069354057312 / Valid loss: 6.5206433841160365
Training loss: 3.123236656188965 / Valid loss: 6.6513374828156975

Epoch: 31
Training loss: 2.274129867553711 / Valid loss: 6.556060825075422
Training loss: 4.620698928833008 / Valid loss: 6.655083367938087
Training loss: 3.0612330436706543 / Valid loss: 6.904260231199719
Training loss: 1.9488115310668945 / Valid loss: 6.951440293448312
Training loss: 2.3907978534698486 / Valid loss: 6.583872829164777

Epoch: 32
Training loss: 1.8940215110778809 / Valid loss: 6.730206534976051
Training loss: 2.1204638481140137 / Valid loss: 6.527466083708264
Training loss: 3.1486499309539795 / Valid loss: 6.640826193491618
Training loss: 2.529238224029541 / Valid loss: 6.544845989772251
Training loss: 2.9827418327331543 / Valid loss: 6.776532248088292

Epoch: 33
Training loss: 2.472479820251465 / Valid loss: 6.528155719666254
Training loss: 2.9690732955932617 / Valid loss: 6.560597699029105
Training loss: 2.1743862628936768 / Valid loss: 6.625594786235264
Training loss: 2.585236072540283 / Valid loss: 6.726515715462821
Training loss: 2.2094194889068604 / Valid loss: 6.492033940269834

Epoch: 34
Training loss: 2.727627992630005 / Valid loss: 6.63497410047622
Training loss: 2.446214437484741 / Valid loss: 6.538660707927885
Training loss: 2.210397243499756 / Valid loss: 6.603345087596348
Training loss: 2.323847532272339 / Valid loss: 6.978242756071545
Training loss: 2.1678755283355713 / Valid loss: 6.705130440848214

Epoch: 35
Training loss: 2.2690036296844482 / Valid loss: 6.649529947553362
Training loss: 1.6622953414916992 / Valid loss: 6.870330592564174
Training loss: 2.8383021354675293 / Valid loss: 7.265134779612223
Training loss: 1.6933194398880005 / Valid loss: 6.704385051273165
Training loss: 2.031052589416504 / Valid loss: 6.55991941179548

Epoch: 36
Training loss: 2.119006395339966 / Valid loss: 6.725104895092192
Training loss: 2.11799955368042 / Valid loss: 6.95433600289481
Training loss: 2.081117868423462 / Valid loss: 7.1216693787347705
Training loss: 3.065727710723877 / Valid loss: 6.539203484853108
Training loss: 2.517076015472412 / Valid loss: 6.533212171282087

Epoch: 37
Training loss: 2.467585563659668 / Valid loss: 6.593153063456217
Training loss: 1.4124538898468018 / Valid loss: 6.6411374455406555
Training loss: 1.9675633907318115 / Valid loss: 6.963228080386207
Training loss: 2.3304367065429688 / Valid loss: 6.5493869236537385
Training loss: 2.5792698860168457 / Valid loss: 6.636145460037958

Epoch: 38
Training loss: 1.9784834384918213 / Valid loss: 6.597678143637521
Training loss: 2.270082712173462 / Valid loss: 6.795576781318301
Training loss: 2.40594744682312 / Valid loss: 6.575382266725812
Training loss: 1.938258171081543 / Valid loss: 6.764650730859666
Training loss: 2.286708354949951 / Valid loss: 6.832113304592315

Epoch: 39
Training loss: 3.1244029998779297 / Valid loss: 6.670992224557059
Training loss: 1.6407966613769531 / Valid loss: 6.6873870350065685
Training loss: 1.974543809890747 / Valid loss: 6.5292907896496
Training loss: 2.621394634246826 / Valid loss: 6.716328348432269
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 3700): 5.503731691269648
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.614124298095703 / Valid loss: 15.779084723336355
Model is saved in epoch 0, overall batch: 0
Training loss: 6.43978214263916 / Valid loss: 6.185112324215117
Model is saved in epoch 0, overall batch: 100
Training loss: 6.64673376083374 / Valid loss: 5.872817384629022
Model is saved in epoch 0, overall batch: 200
Training loss: 6.055812835693359 / Valid loss: 5.677532080241612
Model is saved in epoch 0, overall batch: 300
Training loss: 8.421233177185059 / Valid loss: 5.707274066834223

Epoch: 1
Training loss: 5.175351142883301 / Valid loss: 5.7989003908066525
Training loss: 5.763352394104004 / Valid loss: 5.7243059044792535
Training loss: 7.4598002433776855 / Valid loss: 5.508290236336844
Model is saved in epoch 1, overall batch: 700
Training loss: 5.976564407348633 / Valid loss: 5.539868091401599
Training loss: 5.404888153076172 / Valid loss: 5.709366226196289

Epoch: 2
Training loss: 5.236649513244629 / Valid loss: 5.788992123376755
Training loss: 4.405208110809326 / Valid loss: 5.579549948374431
Training loss: 5.217757225036621 / Valid loss: 5.539224926630656
Training loss: 5.461146354675293 / Valid loss: 5.591076687404088
Training loss: 5.6224212646484375 / Valid loss: 5.669248835245768

Epoch: 3
Training loss: 3.725843906402588 / Valid loss: 5.574965415682112
Training loss: 4.8676276206970215 / Valid loss: 5.588290521076748
Training loss: 4.003915786743164 / Valid loss: 5.754215921674456
Training loss: 5.112907886505127 / Valid loss: 6.067142833982195
Training loss: 4.606826305389404 / Valid loss: 5.685289192199707

Epoch: 4
Training loss: 3.3286404609680176 / Valid loss: 5.974575465066092
Training loss: 4.212878227233887 / Valid loss: 5.578689275469099
Training loss: 4.002946376800537 / Valid loss: 5.67892552103315
Training loss: 5.149818420410156 / Valid loss: 5.62960456439427
Training loss: 6.401076316833496 / Valid loss: 5.707573895227341

Epoch: 5
Training loss: 3.7431671619415283 / Valid loss: 5.7299758843013215
Training loss: 3.748915910720825 / Valid loss: 5.869471895127069
Training loss: 5.835734844207764 / Valid loss: 5.630681019737607
Training loss: 5.389091491699219 / Valid loss: 5.729549909773327
Training loss: 6.53006649017334 / Valid loss: 6.125496787116641

Epoch: 6
Training loss: 3.9760866165161133 / Valid loss: 5.735123125712077
Training loss: 3.428696632385254 / Valid loss: 5.93684541384379
Training loss: 4.1447343826293945 / Valid loss: 5.757276135399228
Training loss: 3.79541015625 / Valid loss: 5.7352118446713405
Training loss: 3.121812105178833 / Valid loss: 5.7167665504273915

Epoch: 7
Training loss: 2.591675281524658 / Valid loss: 5.841269924527123
Training loss: 3.339311361312866 / Valid loss: 5.976536476044428
Training loss: 3.965353488922119 / Valid loss: 5.946779723394485
Training loss: 4.804716110229492 / Valid loss: 5.828203323909214
Training loss: 4.636925220489502 / Valid loss: 6.337212206068493

Epoch: 8
Training loss: 3.5161900520324707 / Valid loss: 5.787668845767066
Training loss: 3.209627628326416 / Valid loss: 6.133031765619914
Training loss: 4.491207122802734 / Valid loss: 5.928529732567924
Training loss: 3.749563217163086 / Valid loss: 5.892548713229951
Training loss: 3.6405093669891357 / Valid loss: 6.153955629893711

Epoch: 9
Training loss: 3.4920921325683594 / Valid loss: 5.915371352150327
Training loss: 3.6313891410827637 / Valid loss: 5.965941529046922
Training loss: 4.075352668762207 / Valid loss: 6.010658616111392
Training loss: 2.467341899871826 / Valid loss: 6.985461116972424

Epoch: 10
Training loss: 3.4624202251434326 / Valid loss: 5.938456315086001
Training loss: 3.354161024093628 / Valid loss: 5.9763847260248095
Training loss: 3.6267123222351074 / Valid loss: 6.026763023648943
Training loss: 3.671818494796753 / Valid loss: 6.111876165299188
Training loss: 3.298711061477661 / Valid loss: 6.161117326645624

Epoch: 11
Training loss: 3.6073999404907227 / Valid loss: 6.067795362926665
Training loss: 3.4288101196289062 / Valid loss: 6.083330585843041
Training loss: 3.3500542640686035 / Valid loss: 6.241954699016753
Training loss: 3.7912628650665283 / Valid loss: 6.07950561841329
Training loss: 3.3492798805236816 / Valid loss: 6.179457689466931

Epoch: 12
Training loss: 3.5036096572875977 / Valid loss: 6.210352150599162
Training loss: 2.841182231903076 / Valid loss: 6.339152724402291
Training loss: 4.100679397583008 / Valid loss: 6.207998727616809
Training loss: 2.7907683849334717 / Valid loss: 6.8366588047572545
Training loss: 2.753324031829834 / Valid loss: 6.345100221179781

Epoch: 13
Training loss: 2.4825539588928223 / Valid loss: 6.258953317006429
Training loss: 2.998323440551758 / Valid loss: 6.186182428541637
Training loss: 2.341442108154297 / Valid loss: 6.335324905032203
Training loss: 2.673330783843994 / Valid loss: 6.221412526993524
Training loss: 2.689366579055786 / Valid loss: 6.303516867047264

Epoch: 14
Training loss: 2.1227798461914062 / Valid loss: 6.366530836196173
Training loss: 2.8330273628234863 / Valid loss: 6.237487297966367
Training loss: 4.572192668914795 / Valid loss: 6.564355641319638
Training loss: 2.9672489166259766 / Valid loss: 6.425509979611351
Training loss: 3.020162343978882 / Valid loss: 6.360920036406744

Epoch: 15
Training loss: 2.217289924621582 / Valid loss: 6.375525615328834
Training loss: 3.624298572540283 / Valid loss: 6.366323366619292
Training loss: 2.7933449745178223 / Valid loss: 6.23594449815296
Training loss: 2.9395084381103516 / Valid loss: 6.414660544622512
Training loss: 2.7775964736938477 / Valid loss: 6.480213455926805

Epoch: 16
Training loss: 2.995630979537964 / Valid loss: 6.345381280354091
Training loss: 2.538764238357544 / Valid loss: 6.406259059906006
Training loss: 2.167606830596924 / Valid loss: 6.35146819069272
Training loss: 2.791558265686035 / Valid loss: 6.555303087688627
Training loss: 2.5907809734344482 / Valid loss: 6.475345012119838

Epoch: 17
Training loss: 2.5569186210632324 / Valid loss: 6.307219507580712
Training loss: 2.647228717803955 / Valid loss: 6.672702727999006
Training loss: 2.0784473419189453 / Valid loss: 6.805798766726539
Training loss: 4.958789825439453 / Valid loss: 6.482676728566488
Training loss: 2.5939345359802246 / Valid loss: 6.525450815473284

Epoch: 18
Training loss: 2.8161532878875732 / Valid loss: 6.519868632725307
Training loss: 2.067505121231079 / Valid loss: 6.771933024270194
Training loss: 2.597166061401367 / Valid loss: 6.474693257468087
Training loss: 2.159065008163452 / Valid loss: 6.532814082645235
Training loss: 2.772414207458496 / Valid loss: 6.696206433432443

Epoch: 19
Training loss: 1.633622407913208 / Valid loss: 6.97665338970366
Training loss: 2.3247880935668945 / Valid loss: 6.65236633845738
Training loss: 2.9075891971588135 / Valid loss: 6.77331797963097
Training loss: 2.1066503524780273 / Valid loss: 6.613123062678746

Epoch: 20
Training loss: 1.9734082221984863 / Valid loss: 6.455910625911894
Training loss: 1.6653897762298584 / Valid loss: 6.6907467297145296
Training loss: 1.8931751251220703 / Valid loss: 6.8038427352905275
Training loss: 1.9357068538665771 / Valid loss: 6.580056787672497
Training loss: 1.5059311389923096 / Valid loss: 6.842430337270101

Epoch: 21
Training loss: 1.9064886569976807 / Valid loss: 6.605584196817308
Training loss: 2.462337017059326 / Valid loss: 6.989993267967588
Training loss: 2.5381157398223877 / Valid loss: 6.921696567535401
Training loss: 3.1730337142944336 / Valid loss: 6.659145155407134
Training loss: 2.225001096725464 / Valid loss: 6.757792032332647

Epoch: 22
Training loss: 1.6581569910049438 / Valid loss: 6.642442208244687
Training loss: 1.8218538761138916 / Valid loss: 6.60078227860587
Training loss: 2.6675782203674316 / Valid loss: 6.875515415554955
Training loss: 2.1939713954925537 / Valid loss: 6.971019504183815
Training loss: 2.172210216522217 / Valid loss: 6.838611434754871

Epoch: 23
Training loss: 1.4317742586135864 / Valid loss: 6.539927932194301
Training loss: 2.4210216999053955 / Valid loss: 8.06532244001116
Training loss: 1.9862432479858398 / Valid loss: 6.84508770988101
Training loss: 1.4156901836395264 / Valid loss: 7.124142506009057
Training loss: 2.0043742656707764 / Valid loss: 6.970907497406006

Epoch: 24
Training loss: 1.5224015712738037 / Valid loss: 6.849871099562872
Training loss: 2.1606264114379883 / Valid loss: 7.209681669871013
Training loss: 2.4594554901123047 / Valid loss: 7.223549788338797
Training loss: 1.3756026029586792 / Valid loss: 6.694535961605253
Training loss: 1.4478967189788818 / Valid loss: 7.892288171677363

Epoch: 25
Training loss: 1.6379947662353516 / Valid loss: 6.81755139941261
Training loss: 1.4235649108886719 / Valid loss: 6.797801971435547
Training loss: 2.088960647583008 / Valid loss: 6.811376583008539
Training loss: 1.9162349700927734 / Valid loss: 6.878440829685756
Training loss: 1.9843848943710327 / Valid loss: 6.634847845349993

Epoch: 26
Training loss: 1.1771152019500732 / Valid loss: 6.7465062413896835
Training loss: 1.949805498123169 / Valid loss: 7.018674632481166
Training loss: 1.329363226890564 / Valid loss: 7.1475668725513275
Training loss: 1.6294986009597778 / Valid loss: 6.99995630809239
Training loss: 1.6502182483673096 / Valid loss: 6.793686371757871

Epoch: 27
Training loss: 1.755964756011963 / Valid loss: 6.6786092599233
Training loss: 1.402139663696289 / Valid loss: 7.306380512600853
Training loss: 1.7851911783218384 / Valid loss: 8.419425814492362
Training loss: 1.5168850421905518 / Valid loss: 7.020218826475597
Training loss: 2.571504592895508 / Valid loss: 7.19087134315854

Epoch: 28
Training loss: 1.6953034400939941 / Valid loss: 6.790463542938232
Training loss: 1.3934255838394165 / Valid loss: 7.547784868876139
Training loss: 1.6630239486694336 / Valid loss: 6.9297332718258815
Training loss: 2.580141544342041 / Valid loss: 7.8789514042082285
Training loss: 1.6124812364578247 / Valid loss: 6.725113087608701

Epoch: 29
Training loss: 1.5131388902664185 / Valid loss: 7.361294106074742
Training loss: 1.6226117610931396 / Valid loss: 8.268034185682025
Training loss: 1.6592984199523926 / Valid loss: 6.910700843447731
Training loss: 1.6921930313110352 / Valid loss: 6.85062080564953

Epoch: 30
Training loss: 0.9882962107658386 / Valid loss: 6.930355121975853
Training loss: 1.4663782119750977 / Valid loss: 6.8189543542407804
Training loss: 1.7721712589263916 / Valid loss: 7.0004806791033065
Training loss: 1.2909748554229736 / Valid loss: 6.80087856565203
Training loss: 1.2949354648590088 / Valid loss: 6.984749551046463

Epoch: 31
Training loss: 1.489856481552124 / Valid loss: 6.839187894548688
Training loss: 1.1899139881134033 / Valid loss: 6.7913610549200145
Training loss: 1.0107181072235107 / Valid loss: 6.954664993286133
Training loss: 1.7101655006408691 / Valid loss: 6.805517296564012
Training loss: 1.6931235790252686 / Valid loss: 8.164817669278099

Epoch: 32
Training loss: 1.6088266372680664 / Valid loss: 6.958923435211181
Training loss: 1.3429524898529053 / Valid loss: 6.729458241235642
Training loss: 1.2177481651306152 / Valid loss: 6.847101161593483
Training loss: 1.1478886604309082 / Valid loss: 6.891232095445905
Training loss: 1.2916179895401 / Valid loss: 6.82253452028547

Epoch: 33
Training loss: 0.8639665246009827 / Valid loss: 6.83045813696725
Training loss: 1.0486314296722412 / Valid loss: 7.107445058368501
Training loss: 1.5884159803390503 / Valid loss: 7.126123755318778
Training loss: 1.4410150051116943 / Valid loss: 6.844544628688268
Training loss: 1.2318873405456543 / Valid loss: 6.89687594913301

Epoch: 34
Training loss: 1.3455052375793457 / Valid loss: 6.959550067356655
Training loss: 2.12746000289917 / Valid loss: 7.8348319008236835
Training loss: 0.9512336850166321 / Valid loss: 6.7366525786263605
Training loss: 1.308968186378479 / Valid loss: 6.781425044650123
Training loss: 1.284740686416626 / Valid loss: 6.818888945806594

Epoch: 35
Training loss: 1.286137342453003 / Valid loss: 7.187448369889032
Training loss: 1.2035491466522217 / Valid loss: 6.84754726319086
Training loss: 1.6719835996627808 / Valid loss: 7.215241175606137
Training loss: 1.2422583103179932 / Valid loss: 8.858951168968565
Training loss: 1.6715872287750244 / Valid loss: 7.299318531581334

Epoch: 36
Training loss: 1.6585649251937866 / Valid loss: 6.828417914254325
Training loss: 1.070083737373352 / Valid loss: 7.071046286537534
Training loss: 0.8626769781112671 / Valid loss: 6.84786498660133
Training loss: 1.4603487253189087 / Valid loss: 7.453446093059721
Training loss: 1.6437058448791504 / Valid loss: 7.131788626171294

Epoch: 37
Training loss: 1.6046926975250244 / Valid loss: 7.033143851870582
Training loss: 1.349705457687378 / Valid loss: 6.859331065132505
Training loss: 1.2778962850570679 / Valid loss: 7.005917058672224
Training loss: 1.2343618869781494 / Valid loss: 6.979228065127418
Training loss: 1.30274498462677 / Valid loss: 6.957296280633836

Epoch: 38
Training loss: 1.325932502746582 / Valid loss: 6.894140956515358
Training loss: 1.42079758644104 / Valid loss: 8.303472400846935
Training loss: 1.104600429534912 / Valid loss: 7.519546163649786
Training loss: 1.5386762619018555 / Valid loss: 7.111515808105469
Training loss: 1.4210834503173828 / Valid loss: 7.917688932872954

Epoch: 39
Training loss: 1.1929876804351807 / Valid loss: 6.9531021072751
Training loss: 1.1784520149230957 / Valid loss: 6.985084599540347
Training loss: 1.0971804857254028 / Valid loss: 6.933935878390358
Training loss: 0.8144872784614563 / Valid loss: 6.840630722045899
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 700): 5.33451890491304
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.614124298095703 / Valid loss: 15.779084687005906
Model is saved in epoch 0, overall batch: 0
Training loss: 6.442082405090332 / Valid loss: 6.185961655208042
Model is saved in epoch 0, overall batch: 100
Training loss: 6.660813808441162 / Valid loss: 5.845704071862357
Model is saved in epoch 0, overall batch: 200
Training loss: 6.057681083679199 / Valid loss: 5.676577079863775
Model is saved in epoch 0, overall batch: 300
Training loss: 8.487798690795898 / Valid loss: 5.703388820375715

Epoch: 1
Training loss: 5.178082466125488 / Valid loss: 5.787368170420328
Training loss: 5.787713050842285 / Valid loss: 5.726603078842163
Training loss: 7.478495121002197 / Valid loss: 5.508422756195069
Model is saved in epoch 1, overall batch: 700
Training loss: 6.022063255310059 / Valid loss: 5.540717086337861
Training loss: 5.379602432250977 / Valid loss: 5.730309654417492

Epoch: 2
Training loss: 5.221841335296631 / Valid loss: 5.769547700881958
Training loss: 4.389341354370117 / Valid loss: 5.571454683939616
Training loss: 5.154569625854492 / Valid loss: 5.532578504653204
Training loss: 5.4307098388671875 / Valid loss: 5.599657133647374
Training loss: 5.628597259521484 / Valid loss: 5.641856590906779

Epoch: 3
Training loss: 3.737752914428711 / Valid loss: 5.560989164170765
Training loss: 4.824573516845703 / Valid loss: 5.589524623325893
Training loss: 4.068887710571289 / Valid loss: 5.785007336026147
Training loss: 5.09434700012207 / Valid loss: 6.028690086092268
Training loss: 4.651166915893555 / Valid loss: 5.6497298422313875

Epoch: 4
Training loss: 3.287266731262207 / Valid loss: 5.932080400557745
Training loss: 4.210696697235107 / Valid loss: 5.5636513210478284
Training loss: 4.019786357879639 / Valid loss: 5.678433111735752
Training loss: 5.155420303344727 / Valid loss: 5.635522783370245
Training loss: 6.517248153686523 / Valid loss: 5.7017753918965655

Epoch: 5
Training loss: 3.6978890895843506 / Valid loss: 5.749985208965483
Training loss: 3.7427003383636475 / Valid loss: 5.891082507088071
Training loss: 5.883145809173584 / Valid loss: 5.637126105172293
Training loss: 5.360844612121582 / Valid loss: 5.743366847719465
Training loss: 6.644677639007568 / Valid loss: 6.091191403071085

Epoch: 6
Training loss: 3.987973690032959 / Valid loss: 5.6875084854307625
Training loss: 3.446343183517456 / Valid loss: 5.925414257957822
Training loss: 4.226875305175781 / Valid loss: 5.734277382351103
Training loss: 3.734708070755005 / Valid loss: 5.705799077806018
Training loss: 3.1547043323516846 / Valid loss: 5.714654865719023

Epoch: 7
Training loss: 2.632686138153076 / Valid loss: 5.822085989089239
Training loss: 3.327805995941162 / Valid loss: 5.956474395025344
Training loss: 4.067770957946777 / Valid loss: 5.920148574738276
Training loss: 4.819311141967773 / Valid loss: 5.791680733362834
Training loss: 4.672924041748047 / Valid loss: 6.242694534574237

Epoch: 8
Training loss: 3.6445624828338623 / Valid loss: 5.809137793949672
Training loss: 3.1335606575012207 / Valid loss: 6.180592823028564
Training loss: 4.4550957679748535 / Valid loss: 5.907524245125907
Training loss: 3.95896315574646 / Valid loss: 5.889462155387515
Training loss: 3.6607494354248047 / Valid loss: 6.129573903764997

Epoch: 9
Training loss: 3.5620675086975098 / Valid loss: 5.911054170699346
Training loss: 3.5691328048706055 / Valid loss: 5.981692856834048
Training loss: 4.113585472106934 / Valid loss: 5.979149807067144
Training loss: 2.5416572093963623 / Valid loss: 7.135577683221726

Epoch: 10
Training loss: 3.3828959465026855 / Valid loss: 5.897204281034924
Training loss: 3.2927944660186768 / Valid loss: 5.985071577344622
Training loss: 3.6395654678344727 / Valid loss: 6.018493034726098
Training loss: 3.6386475563049316 / Valid loss: 6.096959406988962
Training loss: 3.356217861175537 / Valid loss: 6.102783589136033

Epoch: 11
Training loss: 3.5428690910339355 / Valid loss: 6.037855720520019
Training loss: 3.4665021896362305 / Valid loss: 6.106395669210524
Training loss: 3.2264721393585205 / Valid loss: 6.203878811427525
Training loss: 3.797244071960449 / Valid loss: 6.046400038401286
Training loss: 3.3498377799987793 / Valid loss: 6.221954277583531

Epoch: 12
Training loss: 3.5586941242218018 / Valid loss: 6.18423114504133
Training loss: 2.803933620452881 / Valid loss: 6.214327569234939
Training loss: 4.024382591247559 / Valid loss: 6.172465887523833
Training loss: 2.8324832916259766 / Valid loss: 6.548515056428455
Training loss: 2.8046369552612305 / Valid loss: 6.3048141615731375

Epoch: 13
Training loss: 2.5251529216766357 / Valid loss: 6.214787374223982
Training loss: 2.8239779472351074 / Valid loss: 6.173654179345994
Training loss: 2.2225990295410156 / Valid loss: 6.337091974985032
Training loss: 2.5634279251098633 / Valid loss: 6.25768848827907
Training loss: 2.8511743545532227 / Valid loss: 6.3302836486271445

Epoch: 14
Training loss: 2.079718828201294 / Valid loss: 6.377236032485962
Training loss: 2.783881187438965 / Valid loss: 6.272305295580909
Training loss: 4.452258110046387 / Valid loss: 6.448329943702334
Training loss: 2.940242290496826 / Valid loss: 6.529491456349691
Training loss: 2.99790096282959 / Valid loss: 6.303071696417672

Epoch: 15
Training loss: 2.2088894844055176 / Valid loss: 6.403941340673537
Training loss: 3.7251110076904297 / Valid loss: 6.343284250441052
Training loss: 2.7718687057495117 / Valid loss: 6.259781147184826
Training loss: 3.0295746326446533 / Valid loss: 6.454854912984938
Training loss: 2.888629913330078 / Valid loss: 6.46247635341826

Epoch: 16
Training loss: 2.811025619506836 / Valid loss: 6.338307201294672
Training loss: 2.6061456203460693 / Valid loss: 6.3505778358096165
Training loss: 2.3446102142333984 / Valid loss: 6.353092629568917
Training loss: 2.625725746154785 / Valid loss: 6.574103298641386
Training loss: 2.4596991539001465 / Valid loss: 6.59136989684332

Epoch: 17
Training loss: 2.390563488006592 / Valid loss: 6.2796014785766605
Training loss: 2.525815486907959 / Valid loss: 6.5590032123384026
Training loss: 2.0383098125457764 / Valid loss: 6.954764917918614
Training loss: 4.545018196105957 / Valid loss: 6.4387801806131995
Training loss: 2.441120147705078 / Valid loss: 6.567154698144822

Epoch: 18
Training loss: 2.910931348800659 / Valid loss: 6.628771868206206
Training loss: 2.152198076248169 / Valid loss: 6.780308024088542
Training loss: 2.662567377090454 / Valid loss: 6.453483826773507
Training loss: 2.2270214557647705 / Valid loss: 6.524771451950073
Training loss: 2.768065929412842 / Valid loss: 6.864777851104736

Epoch: 19
Training loss: 1.7242422103881836 / Valid loss: 6.980991853986468
Training loss: 2.3888063430786133 / Valid loss: 6.739207681020101
Training loss: 2.8648667335510254 / Valid loss: 6.944048718043736
Training loss: 2.0505752563476562 / Valid loss: 6.554138283502488

Epoch: 20
Training loss: 2.132719039916992 / Valid loss: 6.447362586430141
Training loss: 1.6558265686035156 / Valid loss: 6.672232695988247
Training loss: 2.0525505542755127 / Valid loss: 6.820019363221668
Training loss: 2.0287814140319824 / Valid loss: 6.553051687422252
Training loss: 1.4244394302368164 / Valid loss: 6.986006273542132

Epoch: 21
Training loss: 1.730201244354248 / Valid loss: 6.53720805985587
Training loss: 2.389955759048462 / Valid loss: 6.712886739912487
Training loss: 2.5465853214263916 / Valid loss: 6.825119795118059
Training loss: 3.2289302349090576 / Valid loss: 6.646420165470668
Training loss: 2.2819461822509766 / Valid loss: 6.893081378936768

Epoch: 22
Training loss: 1.6268179416656494 / Valid loss: 6.611580548967634
Training loss: 1.7338289022445679 / Valid loss: 6.687706479572115
Training loss: 2.56514048576355 / Valid loss: 6.769655191330683
Training loss: 2.1132187843322754 / Valid loss: 6.937290686652774
Training loss: 1.9655299186706543 / Valid loss: 6.966159034910656

Epoch: 23
Training loss: 1.6109384298324585 / Valid loss: 6.579461583637056
Training loss: 2.4715921878814697 / Valid loss: 7.5190992900303435
Training loss: 2.067795753479004 / Valid loss: 6.932891813913981
Training loss: 1.400069236755371 / Valid loss: 6.958797146025158
Training loss: 1.8181571960449219 / Valid loss: 6.698703638712565

Epoch: 24
Training loss: 1.6953788995742798 / Valid loss: 6.971325365702311
Training loss: 2.0722475051879883 / Valid loss: 7.299983515058245
Training loss: 2.3565657138824463 / Valid loss: 6.884292657034738
Training loss: 1.2940267324447632 / Valid loss: 6.701062713350568
Training loss: 1.4293920993804932 / Valid loss: 7.541987528119768

Epoch: 25
Training loss: 1.600019097328186 / Valid loss: 6.710732192084903
Training loss: 1.4042613506317139 / Valid loss: 6.9959702400934125
Training loss: 2.089505195617676 / Valid loss: 6.8858870551699685
Training loss: 1.9833526611328125 / Valid loss: 6.919363257998512
Training loss: 2.1025986671447754 / Valid loss: 6.628335557665143

Epoch: 26
Training loss: 1.1501997709274292 / Valid loss: 6.683227323350453
Training loss: 1.8167204856872559 / Valid loss: 6.915174988337926
Training loss: 1.1879457235336304 / Valid loss: 7.259567247118269
Training loss: 1.5974607467651367 / Valid loss: 6.9518808728172665
Training loss: 1.3920788764953613 / Valid loss: 6.773114608582996

Epoch: 27
Training loss: 1.6770641803741455 / Valid loss: 6.649453490121024
Training loss: 1.3878183364868164 / Valid loss: 7.3032321339561825
Training loss: 1.6845959424972534 / Valid loss: 8.119095511663527
Training loss: 1.546111822128296 / Valid loss: 7.104289999462309
Training loss: 2.495614528656006 / Valid loss: 7.1215500740777875

Epoch: 28
Training loss: 1.6391165256500244 / Valid loss: 6.781766768864223
Training loss: 1.350848913192749 / Valid loss: 7.3520994458879745
Training loss: 1.6039674282073975 / Valid loss: 6.876836549668085
Training loss: 2.5655155181884766 / Valid loss: 7.826194340842111
Training loss: 1.652632474899292 / Valid loss: 6.812644931248256

Epoch: 29
Training loss: 1.420108675956726 / Valid loss: 7.172238181886219
Training loss: 1.6367411613464355 / Valid loss: 8.832832036699568
Training loss: 1.6234304904937744 / Valid loss: 7.3254289581662135
Training loss: 1.7439570426940918 / Valid loss: 6.740257901237125

Epoch: 30
Training loss: 1.0188384056091309 / Valid loss: 7.038058458055769
Training loss: 1.4726974964141846 / Valid loss: 7.00005734761556
Training loss: 1.704420566558838 / Valid loss: 6.880491263525826
Training loss: 1.1742608547210693 / Valid loss: 6.796170053027925
Training loss: 1.3291583061218262 / Valid loss: 6.965622481845674

Epoch: 31
Training loss: 1.559417486190796 / Valid loss: 6.79778071812221
Training loss: 1.0947320461273193 / Valid loss: 6.824648816244943
Training loss: 1.038090705871582 / Valid loss: 6.872549402146112
Training loss: 1.6714229583740234 / Valid loss: 6.838570581163679
Training loss: 1.6669175624847412 / Valid loss: 7.722964804513114

Epoch: 32
Training loss: 1.4157042503356934 / Valid loss: 6.922225257328578
Training loss: 1.2075566053390503 / Valid loss: 6.754807124819075
Training loss: 1.2277066707611084 / Valid loss: 6.904766425632295
Training loss: 1.175577163696289 / Valid loss: 6.834327143714542
Training loss: 1.279412031173706 / Valid loss: 6.894474869682675

Epoch: 33
Training loss: 0.9078219532966614 / Valid loss: 6.817358734494164
Training loss: 1.1459394693374634 / Valid loss: 6.9428657577151345
Training loss: 1.5868574380874634 / Valid loss: 6.868244311923072
Training loss: 1.507765293121338 / Valid loss: 6.883584367661249
Training loss: 1.3768653869628906 / Valid loss: 6.883099646795364

Epoch: 34
Training loss: 1.3252184391021729 / Valid loss: 6.800747667040143
Training loss: 2.092879295349121 / Valid loss: 7.930749393644787
Training loss: 1.0723540782928467 / Valid loss: 6.754523427145822
Training loss: 1.3202598094940186 / Valid loss: 6.78514701298305
Training loss: 1.394156813621521 / Valid loss: 6.843467335473924

Epoch: 35
Training loss: 1.330712914466858 / Valid loss: 7.429466774350121
Training loss: 1.15692937374115 / Valid loss: 7.007388042268299
Training loss: 1.544060230255127 / Valid loss: 7.03312170391991
Training loss: 1.3470038175582886 / Valid loss: 8.809689798809233
Training loss: 1.635777235031128 / Valid loss: 7.347241569700695

Epoch: 36
Training loss: 1.6036593914031982 / Valid loss: 6.865480765842256
Training loss: 1.145463466644287 / Valid loss: 7.108269423530215
Training loss: 0.7843372821807861 / Valid loss: 6.877354640052432
Training loss: 1.5325121879577637 / Valid loss: 7.514243153163365
Training loss: 1.5438754558563232 / Valid loss: 6.966975430079869

Epoch: 37
Training loss: 1.7020289897918701 / Valid loss: 6.949432350340343
Training loss: 1.3616989850997925 / Valid loss: 6.90302605401902
Training loss: 1.3438727855682373 / Valid loss: 7.041926120576404
Training loss: 0.9968979358673096 / Valid loss: 6.863938794817243
Training loss: 1.2064779996871948 / Valid loss: 6.920930381048294

Epoch: 38
Training loss: 1.3830249309539795 / Valid loss: 6.929312597002302
Training loss: 1.4059624671936035 / Valid loss: 7.72205354145595
Training loss: 1.1477986574172974 / Valid loss: 7.367531908126105
Training loss: 1.546088695526123 / Valid loss: 6.94858223143078
Training loss: 1.6030529737472534 / Valid loss: 7.746486713772728

Epoch: 39
Training loss: 1.069867730140686 / Valid loss: 6.903044110252743
Training loss: 1.170082926750183 / Valid loss: 6.866041478656587
Training loss: 1.1597646474838257 / Valid loss: 6.8603994641985215
Training loss: 0.8748548626899719 / Valid loss: 6.780198324294317
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 700): 5.334303281420753
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.837162017822266 / Valid loss: 15.324073673429943
Model is saved in epoch 0, overall batch: 0
Training loss: 9.19436264038086 / Valid loss: 13.22190689813523
Model is saved in epoch 0, overall batch: 100
Training loss: 10.10901927947998 / Valid loss: 12.294306232815696
Model is saved in epoch 0, overall batch: 200
Training loss: 15.420944213867188 / Valid loss: 11.432432124728248
Model is saved in epoch 0, overall batch: 300
Training loss: 10.838432312011719 / Valid loss: 10.757288369678315
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 9.101236343383789 / Valid loss: 10.18679491224743
Model is saved in epoch 1, overall batch: 500
Training loss: 7.949270725250244 / Valid loss: 9.569213653746106
Model is saved in epoch 1, overall batch: 600
Training loss: 6.926806926727295 / Valid loss: 9.274591863723028
Model is saved in epoch 1, overall batch: 700
Training loss: 7.2429890632629395 / Valid loss: 8.788934403374082
Model is saved in epoch 1, overall batch: 800
Training loss: 6.4093523025512695 / Valid loss: 8.36056931813558
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 8.721491813659668 / Valid loss: 8.2433184442066
Model is saved in epoch 2, overall batch: 1000
Training loss: 8.69894790649414 / Valid loss: 7.841567089444115
Model is saved in epoch 2, overall batch: 1100
Training loss: 7.820150375366211 / Valid loss: 7.510541902269636
Model is saved in epoch 2, overall batch: 1200
Training loss: 8.659130096435547 / Valid loss: 7.187456144605364
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.622818470001221 / Valid loss: 6.884347418376378
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.183680534362793 / Valid loss: 6.914553937457857
Training loss: 4.508092880249023 / Valid loss: 6.712235566547939
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.799285411834717 / Valid loss: 6.581652654920306
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.0015411376953125 / Valid loss: 6.436768059503464
Model is saved in epoch 3, overall batch: 1800
Training loss: 6.04927921295166 / Valid loss: 6.301085594722203
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 6.809843063354492 / Valid loss: 6.390356170563471
Training loss: 4.927591323852539 / Valid loss: 6.21402230716887
Model is saved in epoch 4, overall batch: 2100
Training loss: 5.186846733093262 / Valid loss: 6.232677820750645
Training loss: 5.932415008544922 / Valid loss: 6.126803300494239
Model is saved in epoch 4, overall batch: 2300
Training loss: 4.514595985412598 / Valid loss: 6.052310914085025
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 7.514176368713379 / Valid loss: 6.038333313805716
Model is saved in epoch 5, overall batch: 2500
Training loss: 6.392370700836182 / Valid loss: 5.998830243519374
Model is saved in epoch 5, overall batch: 2600
Training loss: 5.748220443725586 / Valid loss: 5.980207431884039
Model is saved in epoch 5, overall batch: 2700
Training loss: 4.908628463745117 / Valid loss: 5.871122905186245
Model is saved in epoch 5, overall batch: 2800
Training loss: 3.757138252258301 / Valid loss: 5.8772239594232465

Epoch: 6
Training loss: 10.38902759552002 / Valid loss: 5.854356688544864
Model is saved in epoch 6, overall batch: 3000
Training loss: 5.213415145874023 / Valid loss: 5.835838983172462
Model is saved in epoch 6, overall batch: 3100
Training loss: 5.348294734954834 / Valid loss: 5.862376678557623
Training loss: 5.410418510437012 / Valid loss: 5.826483865011306
Model is saved in epoch 6, overall batch: 3300
Training loss: 5.7824811935424805 / Valid loss: 5.816179534367152
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 6.879613876342773 / Valid loss: 5.803109109969366
Model is saved in epoch 7, overall batch: 3500
Training loss: 5.462737083435059 / Valid loss: 5.80751781463623
Training loss: 5.518723011016846 / Valid loss: 5.7899017379397435
Model is saved in epoch 7, overall batch: 3700
Training loss: 6.056833744049072 / Valid loss: 5.752718326023647
Model is saved in epoch 7, overall batch: 3800
Training loss: 4.37252140045166 / Valid loss: 5.754542003359114

Epoch: 8
Training loss: 4.558428764343262 / Valid loss: 5.806838087808519
Training loss: 3.728111743927002 / Valid loss: 5.778475759142921
Training loss: 6.115395545959473 / Valid loss: 5.762566714059739
Training loss: 6.724968433380127 / Valid loss: 5.715239504405431
Model is saved in epoch 8, overall batch: 4300
Training loss: 5.952537536621094 / Valid loss: 5.758576974414644

Epoch: 9
Training loss: 4.2423505783081055 / Valid loss: 5.733153763271513
Training loss: 7.814561367034912 / Valid loss: 5.768266743705386
Training loss: 6.471599102020264 / Valid loss: 5.721740218571254
Training loss: 5.602276802062988 / Valid loss: 5.70411358106704
Model is saved in epoch 9, overall batch: 4800

Epoch: 10
Training loss: 5.139850616455078 / Valid loss: 5.698258424940564
Model is saved in epoch 10, overall batch: 4900
Training loss: 4.565548896789551 / Valid loss: 5.733519374756586
Training loss: 3.841604232788086 / Valid loss: 5.743995573407128
Training loss: 4.110796928405762 / Valid loss: 5.751579761505127
Training loss: 4.449056625366211 / Valid loss: 5.769448511941093

Epoch: 11
Training loss: 3.4952454566955566 / Valid loss: 5.767785192671276
Training loss: 6.125340938568115 / Valid loss: 5.7610830897376655
Training loss: 7.453166961669922 / Valid loss: 5.777061012813023
Training loss: 3.2775583267211914 / Valid loss: 5.718370825903756
Training loss: 4.533167839050293 / Valid loss: 5.74591684568496

Epoch: 12
Training loss: 4.282937526702881 / Valid loss: 5.7634296894073485
Training loss: 5.086686611175537 / Valid loss: 5.7690245696476525
Training loss: 7.738694190979004 / Valid loss: 5.801684690657116
Training loss: 6.720814228057861 / Valid loss: 5.802797219866798
Training loss: 4.920591354370117 / Valid loss: 5.7406133265722366

Epoch: 13
Training loss: 3.5405988693237305 / Valid loss: 5.744317431676955
Training loss: 3.902862787246704 / Valid loss: 5.789098934900193
Training loss: 5.904706954956055 / Valid loss: 5.781564748854864
Training loss: 4.369678020477295 / Valid loss: 5.7711438519614084
Training loss: 4.667814254760742 / Valid loss: 5.788317941483998

Epoch: 14
Training loss: 5.306430816650391 / Valid loss: 5.764196675164359
Training loss: 4.8248820304870605 / Valid loss: 5.806788049425397
Training loss: 4.564420700073242 / Valid loss: 5.777452464330764
Training loss: 4.1852521896362305 / Valid loss: 5.768677922657558
Training loss: 6.441444396972656 / Valid loss: 5.822391866502308

Epoch: 15
Training loss: 4.380308151245117 / Valid loss: 5.761760357448033
Training loss: 5.423174858093262 / Valid loss: 5.800867223739624
Training loss: 4.960232734680176 / Valid loss: 5.879218253635225
Training loss: 5.461972236633301 / Valid loss: 5.817228714625041
Training loss: 4.731366157531738 / Valid loss: 5.794682468686785

Epoch: 16
Training loss: 3.255443811416626 / Valid loss: 5.759277323314122
Training loss: 4.616212368011475 / Valid loss: 5.753770823705764
Training loss: 3.8817009925842285 / Valid loss: 5.841003336225238
Training loss: 4.859289169311523 / Valid loss: 5.85901974269322
Training loss: 5.375888824462891 / Valid loss: 5.792716709772746

Epoch: 17
Training loss: 5.6089019775390625 / Valid loss: 5.863855895541963
Training loss: 4.005648612976074 / Valid loss: 5.784652925672985
Training loss: 5.479033946990967 / Valid loss: 5.796669919150216
Training loss: 5.234950065612793 / Valid loss: 5.820806353432792
Training loss: 4.455345153808594 / Valid loss: 5.940867723737444

Epoch: 18
Training loss: 2.8916125297546387 / Valid loss: 5.828214874721708
Training loss: 4.976597785949707 / Valid loss: 5.873559365953718
Training loss: 4.869490623474121 / Valid loss: 5.843520643597557
Training loss: 4.367429733276367 / Valid loss: 5.870031088874454
Training loss: 4.251065254211426 / Valid loss: 5.867760188238961

Epoch: 19
Training loss: 5.106612205505371 / Valid loss: 5.9603294803982685
Training loss: 5.026766300201416 / Valid loss: 5.909421489352272
Training loss: 4.240074634552002 / Valid loss: 5.9434617882683165
Training loss: 4.647636413574219 / Valid loss: 5.905190749395461

Epoch: 20
Training loss: 4.18752384185791 / Valid loss: 5.940364292689732
Training loss: 3.532384157180786 / Valid loss: 5.875804036004203
Training loss: 4.537381172180176 / Valid loss: 5.9543968791053405
Training loss: 5.159034729003906 / Valid loss: 5.927488567715599
Training loss: 5.86702299118042 / Valid loss: 5.90289903595334

Epoch: 21
Training loss: 4.033718585968018 / Valid loss: 5.9162168729872935
Training loss: 3.087702751159668 / Valid loss: 5.900689304442633
Training loss: 3.460482597351074 / Valid loss: 5.933691864921933
Training loss: 4.287537097930908 / Valid loss: 6.006863087699527
Training loss: 3.9441027641296387 / Valid loss: 5.990197302046276

Epoch: 22
Training loss: 3.982959270477295 / Valid loss: 5.970916793459938
Training loss: 4.259367942810059 / Valid loss: 6.051884358269827
Training loss: 4.0901947021484375 / Valid loss: 6.041273546218872
Training loss: 5.643535137176514 / Valid loss: 6.079549521491641
Training loss: 4.495580673217773 / Valid loss: 6.028632098152524

Epoch: 23
Training loss: 4.295351028442383 / Valid loss: 6.051576287405831
Training loss: 4.51650333404541 / Valid loss: 6.073710291726249
Training loss: 5.392812728881836 / Valid loss: 6.080260363079253
Training loss: 5.0474700927734375 / Valid loss: 6.124147610437302
Training loss: 5.550943374633789 / Valid loss: 6.062128691446214

Epoch: 24
Training loss: 5.697122573852539 / Valid loss: 6.042373947870164
Training loss: 4.310014724731445 / Valid loss: 6.055247418085734
Training loss: 4.294339179992676 / Valid loss: 6.024003735042753
Training loss: 5.413270950317383 / Valid loss: 6.0773396537417455
Training loss: 4.905623435974121 / Valid loss: 6.056556887853713

Epoch: 25
Training loss: 3.096627950668335 / Valid loss: 6.195903221766154
Training loss: 3.2414064407348633 / Valid loss: 6.206232683999198
Training loss: 5.114660739898682 / Valid loss: 6.121312745412191
Training loss: 3.6108920574188232 / Valid loss: 6.061825373059228
Training loss: 4.90484619140625 / Valid loss: 6.182307025364467

Epoch: 26
Training loss: 5.108614921569824 / Valid loss: 6.194082786923363
Training loss: 4.86319637298584 / Valid loss: 6.115865528015863
Training loss: 3.4457507133483887 / Valid loss: 6.052618819191342
Training loss: 3.763964891433716 / Valid loss: 6.074544927052089
Training loss: 2.816528797149658 / Valid loss: 6.092517469042823

Epoch: 27
Training loss: 3.359340190887451 / Valid loss: 6.101878259295509
Training loss: 3.291414737701416 / Valid loss: 6.051787857782273
Training loss: 3.0485191345214844 / Valid loss: 6.108252602531796
Training loss: 4.0978803634643555 / Valid loss: 6.163784376780192
Training loss: 2.9396424293518066 / Valid loss: 6.155419140770322

Epoch: 28
Training loss: 3.5227839946746826 / Valid loss: 6.201102586019607
Training loss: 3.5199015140533447 / Valid loss: 6.175143378121512
Training loss: 2.704632043838501 / Valid loss: 6.148474877221243
Training loss: 2.267332077026367 / Valid loss: 6.144241124107724
Training loss: 3.072408437728882 / Valid loss: 6.1399605932689845

Epoch: 29
Training loss: 3.419395923614502 / Valid loss: 6.205787454332624
Training loss: 3.5162007808685303 / Valid loss: 6.246059762863886
Training loss: 4.361059188842773 / Valid loss: 6.232665520622617
Training loss: 4.050942897796631 / Valid loss: 6.2743236973172145

Epoch: 30
Training loss: 3.533754348754883 / Valid loss: 6.210707934697469
Training loss: 3.2101974487304688 / Valid loss: 6.227521959940592
Training loss: 3.4181180000305176 / Valid loss: 6.275275514239357
Training loss: 2.8604745864868164 / Valid loss: 6.2618219216664635
Training loss: 3.916038751602173 / Valid loss: 6.2567200751531695

Epoch: 31
Training loss: 4.158259391784668 / Valid loss: 6.224623698279971
Training loss: 2.4307093620300293 / Valid loss: 6.207892383847918
Training loss: 4.053572654724121 / Valid loss: 6.202673099154517
Training loss: 2.663536310195923 / Valid loss: 6.221652255739484
Training loss: 3.6328916549682617 / Valid loss: 6.195139285496303

Epoch: 32
Training loss: 2.81829833984375 / Valid loss: 6.238781145640782
Training loss: 2.360619068145752 / Valid loss: 6.2643566358657115
Training loss: 3.3300578594207764 / Valid loss: 6.314563281195504
Training loss: 3.35730242729187 / Valid loss: 6.353664307367234
Training loss: 3.8022923469543457 / Valid loss: 6.329159436907087

Epoch: 33
Training loss: 3.1502013206481934 / Valid loss: 6.3820153259095695
Training loss: 3.9324209690093994 / Valid loss: 6.389161109924316
Training loss: 3.566800594329834 / Valid loss: 6.271350136257353
Training loss: 3.2456107139587402 / Valid loss: 6.314194758733113
Training loss: 3.9528164863586426 / Valid loss: 6.281051433654058

Epoch: 34
Training loss: 3.4323558807373047 / Valid loss: 6.34271833556039
Training loss: 3.9486985206604004 / Valid loss: 6.344534406207857
Training loss: 2.3072574138641357 / Valid loss: 6.319443984258743
Training loss: 2.7621140480041504 / Valid loss: 6.349118559701102
Training loss: 3.2724573612213135 / Valid loss: 6.3046631585984

Epoch: 35
Training loss: 2.8925864696502686 / Valid loss: 6.387923249744234
Training loss: 2.779982566833496 / Valid loss: 6.401744161333356
Training loss: 3.406684637069702 / Valid loss: 6.336680182956514
Training loss: 3.5044002532958984 / Valid loss: 6.419884820211501
Training loss: 2.9811339378356934 / Valid loss: 6.373654224759057

Epoch: 36
Training loss: 3.2466187477111816 / Valid loss: 6.3453293141864595
Training loss: 4.851409912109375 / Valid loss: 6.47047823270162
Training loss: 2.667349338531494 / Valid loss: 6.358810161408924
Training loss: 2.43414044380188 / Valid loss: 6.365766600200108
Training loss: 3.735109329223633 / Valid loss: 6.49354190826416

Epoch: 37
Training loss: 2.8992629051208496 / Valid loss: 6.49436992917742
Training loss: 2.6811537742614746 / Valid loss: 6.474438036055792
Training loss: 2.4527812004089355 / Valid loss: 6.4651307060605
Training loss: 4.823068618774414 / Valid loss: 6.397647110621135
Training loss: 4.106616973876953 / Valid loss: 6.398211211249942

Epoch: 38
Training loss: 2.1495633125305176 / Valid loss: 6.57217165629069
Training loss: 3.151918888092041 / Valid loss: 6.494780181703113
Training loss: 2.4960732460021973 / Valid loss: 6.390523295175462
Training loss: 3.1822965145111084 / Valid loss: 6.501755287533714
Training loss: 2.4628005027770996 / Valid loss: 6.506336768468221

Epoch: 39
Training loss: 1.6103085279464722 / Valid loss: 6.607154387519473
Training loss: 2.8088135719299316 / Valid loss: 6.4574006670997255
Training loss: 3.3566668033599854 / Valid loss: 6.461766429174514
Training loss: 3.0227293968200684 / Valid loss: 6.422138459341866
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 4900): 5.510149930772327
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.837162017822266 / Valid loss: 15.324073637099493
Model is saved in epoch 0, overall batch: 0
Training loss: 9.19436264038086 / Valid loss: 13.221907438550677
Model is saved in epoch 0, overall batch: 100
Training loss: 10.109148025512695 / Valid loss: 12.294309507097516
Model is saved in epoch 0, overall batch: 200
Training loss: 15.422040939331055 / Valid loss: 11.43251618430728
Model is saved in epoch 0, overall batch: 300
Training loss: 10.84260368347168 / Valid loss: 10.75644252413795
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 9.107320785522461 / Valid loss: 10.194311455317907
Model is saved in epoch 1, overall batch: 500
Training loss: 7.95086669921875 / Valid loss: 9.573898746853782
Model is saved in epoch 1, overall batch: 600
Training loss: 6.924193859100342 / Valid loss: 9.287402057647705
Model is saved in epoch 1, overall batch: 700
Training loss: 7.238700866699219 / Valid loss: 8.79285174324399
Model is saved in epoch 1, overall batch: 800
Training loss: 6.385772705078125 / Valid loss: 8.356423618679955
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 8.718920707702637 / Valid loss: 8.260407179877872
Model is saved in epoch 2, overall batch: 1000
Training loss: 8.675165176391602 / Valid loss: 7.843988109770275
Model is saved in epoch 2, overall batch: 1100
Training loss: 7.798285007476807 / Valid loss: 7.514533878508068
Model is saved in epoch 2, overall batch: 1200
Training loss: 8.673935890197754 / Valid loss: 7.191874465488252
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.597003936767578 / Valid loss: 6.893896513893491
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.168773651123047 / Valid loss: 6.9148334049043205
Training loss: 4.536783218383789 / Valid loss: 6.709164051782517
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.814208984375 / Valid loss: 6.580883139655704
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.033742427825928 / Valid loss: 6.4383902277265275
Model is saved in epoch 3, overall batch: 1800
Training loss: 6.032520294189453 / Valid loss: 6.312608616692679
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 6.821277141571045 / Valid loss: 6.38153746241615
Training loss: 4.963433265686035 / Valid loss: 6.2413740135374525
Model is saved in epoch 4, overall batch: 2100
Training loss: 5.1844329833984375 / Valid loss: 6.2608922004699705
Training loss: 5.942577362060547 / Valid loss: 6.1332681860242575
Model is saved in epoch 4, overall batch: 2300
Training loss: 4.531643390655518 / Valid loss: 6.074742278598603
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 7.45535945892334 / Valid loss: 6.063166003000169
Model is saved in epoch 5, overall batch: 2500
Training loss: 6.281803607940674 / Valid loss: 6.026393027532668
Model is saved in epoch 5, overall batch: 2600
Training loss: 5.791292667388916 / Valid loss: 6.0046630132766
Model is saved in epoch 5, overall batch: 2700
Training loss: 4.790297985076904 / Valid loss: 5.884086622510638
Model is saved in epoch 5, overall batch: 2800
Training loss: 3.890636920928955 / Valid loss: 5.888156009855725

Epoch: 6
Training loss: 10.254716873168945 / Valid loss: 5.872583981922695
Model is saved in epoch 6, overall batch: 3000
Training loss: 5.122994422912598 / Valid loss: 5.85643538747515
Model is saved in epoch 6, overall batch: 3100
Training loss: 5.426646709442139 / Valid loss: 5.884282786505563
Training loss: 5.239382743835449 / Valid loss: 5.833440676189604
Model is saved in epoch 6, overall batch: 3300
Training loss: 5.788313388824463 / Valid loss: 5.838237026759557

Epoch: 7
Training loss: 6.843592643737793 / Valid loss: 5.827810287475586
Model is saved in epoch 7, overall batch: 3500
Training loss: 5.524658203125 / Valid loss: 5.825720871062506
Model is saved in epoch 7, overall batch: 3600
Training loss: 5.49564266204834 / Valid loss: 5.810802480152675
Model is saved in epoch 7, overall batch: 3700
Training loss: 5.9596734046936035 / Valid loss: 5.766402721405029
Model is saved in epoch 7, overall batch: 3800
Training loss: 4.410182952880859 / Valid loss: 5.7737301758357455

Epoch: 8
Training loss: 4.43757963180542 / Valid loss: 5.818626515070597
Training loss: 3.787642002105713 / Valid loss: 5.797967256818499
Training loss: 5.966350078582764 / Valid loss: 5.787673959277925
Training loss: 6.865384101867676 / Valid loss: 5.725072415669759
Model is saved in epoch 8, overall batch: 4300
Training loss: 6.089057922363281 / Valid loss: 5.773672221955799

Epoch: 9
Training loss: 4.340111255645752 / Valid loss: 5.745435047149658
Training loss: 7.784948825836182 / Valid loss: 5.7712107930864605
Training loss: 6.450427055358887 / Valid loss: 5.72223432858785
Model is saved in epoch 9, overall batch: 4700
Training loss: 5.668809413909912 / Valid loss: 5.714342423847744
Model is saved in epoch 9, overall batch: 4800

Epoch: 10
Training loss: 5.044167995452881 / Valid loss: 5.710800577345348
Model is saved in epoch 10, overall batch: 4900
Training loss: 4.565871238708496 / Valid loss: 5.744639607838222
Training loss: 3.8904571533203125 / Valid loss: 5.746195829482305
Training loss: 3.9890170097351074 / Valid loss: 5.750908390680949
Training loss: 4.395656585693359 / Valid loss: 5.773992347717285

Epoch: 11
Training loss: 3.671891689300537 / Valid loss: 5.77849082719712
Training loss: 5.978133201599121 / Valid loss: 5.759939541135515
Training loss: 7.476212978363037 / Valid loss: 5.789121350787934
Training loss: 3.1806530952453613 / Valid loss: 5.722432754153297
Training loss: 4.574840545654297 / Valid loss: 5.755371084667387

Epoch: 12
Training loss: 4.409760475158691 / Valid loss: 5.762107749212356
Training loss: 5.054782390594482 / Valid loss: 5.786588705153692
Training loss: 7.752046585083008 / Valid loss: 5.808431468691144
Training loss: 6.616949558258057 / Valid loss: 5.799004752295358
Training loss: 4.870627403259277 / Valid loss: 5.739292810076759

Epoch: 13
Training loss: 3.590700626373291 / Valid loss: 5.740600872039795
Training loss: 3.898397445678711 / Valid loss: 5.776692694709414
Training loss: 5.9605278968811035 / Valid loss: 5.789680617196219
Training loss: 4.304422378540039 / Valid loss: 5.778642565863473
Training loss: 4.652461528778076 / Valid loss: 5.790229443141392

Epoch: 14
Training loss: 5.229622840881348 / Valid loss: 5.761844914300101
Training loss: 4.983638763427734 / Valid loss: 5.8186730225880945
Training loss: 4.523270606994629 / Valid loss: 5.768100089118594
Training loss: 4.200037479400635 / Valid loss: 5.758705804461525
Training loss: 6.6139421463012695 / Valid loss: 5.812742330914452

Epoch: 15
Training loss: 4.475536346435547 / Valid loss: 5.761129424685524
Training loss: 5.402241230010986 / Valid loss: 5.783962921869187
Training loss: 4.772302150726318 / Valid loss: 5.868851525442941
Training loss: 5.474637985229492 / Valid loss: 5.820981245949155
Training loss: 4.687992572784424 / Valid loss: 5.811819285438174

Epoch: 16
Training loss: 3.1872570514678955 / Valid loss: 5.7735364005679175
Training loss: 4.656075954437256 / Valid loss: 5.747683908825829
Training loss: 3.7526614665985107 / Valid loss: 5.866829874402001
Training loss: 4.7986884117126465 / Valid loss: 5.887740394047328
Training loss: 5.225988864898682 / Valid loss: 5.796114842096965

Epoch: 17
Training loss: 5.701791763305664 / Valid loss: 5.86696723756336
Training loss: 3.983860492706299 / Valid loss: 5.7929362864721385
Training loss: 5.447882652282715 / Valid loss: 5.800476512454805
Training loss: 5.109716415405273 / Valid loss: 5.818472894032796
Training loss: 4.24940824508667 / Valid loss: 5.962016655149914

Epoch: 18
Training loss: 2.7785606384277344 / Valid loss: 5.838129488627116
Training loss: 4.895273208618164 / Valid loss: 5.882677981967018
Training loss: 4.997366905212402 / Valid loss: 5.8527587822505405
Training loss: 4.286368370056152 / Valid loss: 5.8728994959876655
Training loss: 4.48650598526001 / Valid loss: 5.870301307950701

Epoch: 19
Training loss: 4.94793701171875 / Valid loss: 5.955930096762521
Training loss: 5.157111167907715 / Valid loss: 5.906385383151826
Training loss: 4.448537826538086 / Valid loss: 5.949360375177292
Training loss: 4.8117780685424805 / Valid loss: 5.908358533041818

Epoch: 20
Training loss: 3.944209337234497 / Valid loss: 5.947578607286726
Training loss: 3.3063549995422363 / Valid loss: 5.881880987258184
Training loss: 4.624167442321777 / Valid loss: 5.9446950526464555
Training loss: 5.213094711303711 / Valid loss: 5.938688548405965
Training loss: 5.847250938415527 / Valid loss: 5.902780925659906

Epoch: 21
Training loss: 3.935654401779175 / Valid loss: 5.916940600531442
Training loss: 2.9604709148406982 / Valid loss: 5.880683408464704
Training loss: 3.506791830062866 / Valid loss: 5.9268577166966026
Training loss: 4.561221599578857 / Valid loss: 6.031626596904936
Training loss: 3.8880550861358643 / Valid loss: 5.995397451945713

Epoch: 22
Training loss: 3.9256591796875 / Valid loss: 5.990752024877639
Training loss: 4.377205848693848 / Valid loss: 6.072097065335228
Training loss: 4.137416839599609 / Valid loss: 6.031701830455235
Training loss: 5.799577713012695 / Valid loss: 6.06935791742234
Training loss: 4.1363725662231445 / Valid loss: 6.025778747740246

Epoch: 23
Training loss: 4.309638500213623 / Valid loss: 6.0578165440332326
Training loss: 4.632501602172852 / Valid loss: 6.09760661806379
Training loss: 5.357460975646973 / Valid loss: 6.09915223802839
Training loss: 5.1320061683654785 / Valid loss: 6.1465389456067765
Training loss: 5.493762016296387 / Valid loss: 6.060596563702538

Epoch: 24
Training loss: 5.642712593078613 / Valid loss: 6.078100374766758
Training loss: 4.292126178741455 / Valid loss: 6.072221535728091
Training loss: 4.3766069412231445 / Valid loss: 6.007804995491391
Training loss: 5.063336372375488 / Valid loss: 6.094116760435559
Training loss: 5.12562370300293 / Valid loss: 6.071883676165626

Epoch: 25
Training loss: 3.272878885269165 / Valid loss: 6.226807542074294
Training loss: 3.1328511238098145 / Valid loss: 6.220869909014021
Training loss: 5.259116172790527 / Valid loss: 6.122603934151786
Training loss: 3.6984169483184814 / Valid loss: 6.068738154002598
Training loss: 4.707665920257568 / Valid loss: 6.206404370353336

Epoch: 26
Training loss: 5.096412658691406 / Valid loss: 6.219032498768398
Training loss: 4.908990859985352 / Valid loss: 6.148190078281221
Training loss: 3.3813071250915527 / Valid loss: 6.075915729431879
Training loss: 3.891958236694336 / Valid loss: 6.0871718156905406
Training loss: 2.9763364791870117 / Valid loss: 6.113700803120931

Epoch: 27
Training loss: 3.402953624725342 / Valid loss: 6.131832279477801
Training loss: 3.3171873092651367 / Valid loss: 6.042971624646868
Training loss: 3.0459744930267334 / Valid loss: 6.096641962868826
Training loss: 4.309691429138184 / Valid loss: 6.169131290344965
Training loss: 2.9733283519744873 / Valid loss: 6.148838554109846

Epoch: 28
Training loss: 3.5937652587890625 / Valid loss: 6.178231988634382
Training loss: 3.738650321960449 / Valid loss: 6.146921341759818
Training loss: 2.8100225925445557 / Valid loss: 6.169795290629069
Training loss: 2.4219751358032227 / Valid loss: 6.15526491800944
Training loss: 2.944575071334839 / Valid loss: 6.129949147360666

Epoch: 29
Training loss: 3.392183780670166 / Valid loss: 6.198685387202672
Training loss: 3.5597333908081055 / Valid loss: 6.246008827572777
Training loss: 4.383123397827148 / Valid loss: 6.234873669488089
Training loss: 4.145818710327148 / Valid loss: 6.269711172013055

Epoch: 30
Training loss: 3.5499308109283447 / Valid loss: 6.2142900921049575
Training loss: 3.2750277519226074 / Valid loss: 6.237259812582106
Training loss: 3.304678440093994 / Valid loss: 6.266677493140811
Training loss: 2.887767791748047 / Valid loss: 6.236584792818342
Training loss: 3.9352970123291016 / Valid loss: 6.265516437802996

Epoch: 31
Training loss: 4.208026885986328 / Valid loss: 6.189940354937598
Training loss: 2.422816514968872 / Valid loss: 6.1748566127958755
Training loss: 3.9704277515411377 / Valid loss: 6.197135162353516
Training loss: 2.710174560546875 / Valid loss: 6.203627899714879
Training loss: 3.719393253326416 / Valid loss: 6.186576171148391

Epoch: 32
Training loss: 2.7920989990234375 / Valid loss: 6.221997592562721
Training loss: 2.4238834381103516 / Valid loss: 6.245764659699939
Training loss: 3.410983085632324 / Valid loss: 6.294405380884807
Training loss: 3.400393009185791 / Valid loss: 6.326845577784947
Training loss: 3.6120684146881104 / Valid loss: 6.316175762812296

Epoch: 33
Training loss: 3.4643630981445312 / Valid loss: 6.4081104959760395
Training loss: 4.088418960571289 / Valid loss: 6.4001357351030626
Training loss: 3.655935764312744 / Valid loss: 6.276902480352493
Training loss: 3.260158061981201 / Valid loss: 6.319300728752499
Training loss: 3.8739614486694336 / Valid loss: 6.275757351375762

Epoch: 34
Training loss: 3.646967887878418 / Valid loss: 6.367062207630703
Training loss: 3.962940216064453 / Valid loss: 6.30205328805106
Training loss: 2.249948024749756 / Valid loss: 6.30570821080889
Training loss: 2.9302797317504883 / Valid loss: 6.338089908872332
Training loss: 3.1965534687042236 / Valid loss: 6.3265095211210705

Epoch: 35
Training loss: 2.8464772701263428 / Valid loss: 6.37430483727228
Training loss: 2.7489089965820312 / Valid loss: 6.418981016249884
Training loss: 3.2829337120056152 / Valid loss: 6.352306170690627
Training loss: 3.388737440109253 / Valid loss: 6.428452182951427
Training loss: 2.9813218116760254 / Valid loss: 6.376620440256028

Epoch: 36
Training loss: 2.9534857273101807 / Valid loss: 6.324731231871105
Training loss: 4.702754497528076 / Valid loss: 6.427718709764027
Training loss: 2.733097553253174 / Valid loss: 6.34054692586263
Training loss: 2.561511754989624 / Valid loss: 6.356301650546846
Training loss: 3.8396964073181152 / Valid loss: 6.468998082478842

Epoch: 37
Training loss: 2.9283835887908936 / Valid loss: 6.433915833064488
Training loss: 2.664803981781006 / Valid loss: 6.447712257930211
Training loss: 2.5101051330566406 / Valid loss: 6.434596418199085
Training loss: 4.734152317047119 / Valid loss: 6.403213646298363
Training loss: 4.010556697845459 / Valid loss: 6.391326908838181

Epoch: 38
Training loss: 2.2605504989624023 / Valid loss: 6.5607495103563584
Training loss: 2.9544339179992676 / Valid loss: 6.467800907861619
Training loss: 2.4523086547851562 / Valid loss: 6.383781103860764
Training loss: 3.2069525718688965 / Valid loss: 6.478861490885417
Training loss: 2.5984389781951904 / Valid loss: 6.477471097310384

Epoch: 39
Training loss: 1.657096266746521 / Valid loss: 6.603954803375971
Training loss: 2.63850736618042 / Valid loss: 6.446407576969691
Training loss: 3.3644189834594727 / Valid loss: 6.436348245257423
Training loss: 3.0917038917541504 / Valid loss: 6.414983136313302
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 4900): 5.5267484029134115
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.974810600280762 / Valid loss: 16.661306408473422
Model is saved in epoch 0, overall batch: 0
Training loss: 8.355022430419922 / Valid loss: 14.134223365783692
Model is saved in epoch 0, overall batch: 100
Training loss: 9.981088638305664 / Valid loss: 12.504613399505615
Model is saved in epoch 0, overall batch: 200
Training loss: 8.817472457885742 / Valid loss: 11.635444854554676
Model is saved in epoch 0, overall batch: 300
Training loss: 7.110782146453857 / Valid loss: 10.596894068945021
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 9.991365432739258 / Valid loss: 9.974089077540807
Model is saved in epoch 1, overall batch: 500
Training loss: 6.834090232849121 / Valid loss: 9.52256380262829
Model is saved in epoch 1, overall batch: 600
Training loss: 6.536222457885742 / Valid loss: 9.258265549795968
Model is saved in epoch 1, overall batch: 700
Training loss: 4.981109142303467 / Valid loss: 8.986477602095832
Model is saved in epoch 1, overall batch: 800
Training loss: 7.57994270324707 / Valid loss: 8.002961531139555
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.282113075256348 / Valid loss: 8.008218538193475
Training loss: 5.465083599090576 / Valid loss: 7.645565319061279
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.568007946014404 / Valid loss: 7.514113171895345
Model is saved in epoch 2, overall batch: 1200
Training loss: 7.052048683166504 / Valid loss: 7.508908848535447
Model is saved in epoch 2, overall batch: 1300
Training loss: 6.041375637054443 / Valid loss: 7.348440547216506
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 6.329966068267822 / Valid loss: 7.062363840284801
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.6787519454956055 / Valid loss: 6.901731418427967
Model is saved in epoch 3, overall batch: 1600
Training loss: 7.757177352905273 / Valid loss: 6.855110554468064
Model is saved in epoch 3, overall batch: 1700
Training loss: 5.231598854064941 / Valid loss: 6.755240603855678
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.032174587249756 / Valid loss: 6.906361162094843

Epoch: 4
Training loss: 3.736682415008545 / Valid loss: 6.758214332943871
Training loss: 4.130289554595947 / Valid loss: 6.878790735063099
Training loss: 4.875184059143066 / Valid loss: 6.88675271215893
Training loss: 6.104864120483398 / Valid loss: 6.675078464689709
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.887497901916504 / Valid loss: 6.633909491130284
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 6.971824645996094 / Valid loss: 6.545562612442743
Model is saved in epoch 5, overall batch: 2500
Training loss: 7.028324604034424 / Valid loss: 6.59799128032866
Training loss: 6.19699764251709 / Valid loss: 6.569227157320295
Training loss: 3.8403167724609375 / Valid loss: 6.4909016722724555
Model is saved in epoch 5, overall batch: 2800
Training loss: 6.726925373077393 / Valid loss: 6.556985142117455

Epoch: 6
Training loss: 5.043555736541748 / Valid loss: 6.370464458919707
Model is saved in epoch 6, overall batch: 3000
Training loss: 3.415797710418701 / Valid loss: 6.458265790485201
Training loss: 5.719287395477295 / Valid loss: 6.547039981115432
Training loss: 4.344595909118652 / Valid loss: 6.472599876494635
Training loss: 5.826605319976807 / Valid loss: 6.434791739781698

Epoch: 7
Training loss: 4.420452117919922 / Valid loss: 6.414699193409511
Training loss: 3.6500205993652344 / Valid loss: 6.335723806562878
Model is saved in epoch 7, overall batch: 3600
Training loss: 3.6997716426849365 / Valid loss: 6.231689993540446
Model is saved in epoch 7, overall batch: 3700
Training loss: 5.971949100494385 / Valid loss: 6.344682221185593
Training loss: 4.737114429473877 / Valid loss: 6.453710394813901

Epoch: 8
Training loss: 4.16495943069458 / Valid loss: 6.3070095402853825
Training loss: 5.022822856903076 / Valid loss: 6.490209740684146
Training loss: 4.2417097091674805 / Valid loss: 6.17072430792309
Model is saved in epoch 8, overall batch: 4200
Training loss: 5.245993614196777 / Valid loss: 6.203102154958816
Training loss: 6.383859157562256 / Valid loss: 6.364832165127709

Epoch: 9
Training loss: 3.8616299629211426 / Valid loss: 6.4248635269346694
Training loss: 5.811215400695801 / Valid loss: 6.2784668082282655
Training loss: 6.297665596008301 / Valid loss: 6.29713500567845
Training loss: 4.664426803588867 / Valid loss: 6.283009170350574

Epoch: 10
Training loss: 4.921909332275391 / Valid loss: 6.272496316546485
Training loss: 3.4000062942504883 / Valid loss: 6.147777216775077
Model is saved in epoch 10, overall batch: 5000
Training loss: 5.376988887786865 / Valid loss: 6.148756022680374
Training loss: 4.896510124206543 / Valid loss: 6.351917096546718
Training loss: 6.75662088394165 / Valid loss: 6.324049811136155

Epoch: 11
Training loss: 5.584421157836914 / Valid loss: 6.242567641394479
Training loss: 5.776676177978516 / Valid loss: 6.1887637161073235
Training loss: 5.927063941955566 / Valid loss: 6.303397267205375
Training loss: 5.5895280838012695 / Valid loss: 6.180705245335897
Training loss: 5.090886116027832 / Valid loss: 6.114024475642613
Model is saved in epoch 11, overall batch: 5800

Epoch: 12
Training loss: 4.409725666046143 / Valid loss: 6.322702525910877
Training loss: 6.068522930145264 / Valid loss: 6.195234864098685
Training loss: 5.395336627960205 / Valid loss: 6.247353126889183
Training loss: 4.667446136474609 / Valid loss: 6.090490073249454
Model is saved in epoch 12, overall batch: 6200
Training loss: 5.002225875854492 / Valid loss: 6.238895509356544

Epoch: 13
Training loss: 4.232743263244629 / Valid loss: 5.934437865302677
Model is saved in epoch 13, overall batch: 6400
Training loss: 6.576798439025879 / Valid loss: 6.099127074650355
Training loss: 3.2480154037475586 / Valid loss: 6.125695373898461
Training loss: 4.082297325134277 / Valid loss: 6.114976819356283
Training loss: 4.872472286224365 / Valid loss: 6.251589797791981

Epoch: 14
Training loss: 4.256016731262207 / Valid loss: 6.011774237950643
Training loss: 4.542508602142334 / Valid loss: 6.115533297402518
Training loss: 4.44193172454834 / Valid loss: 6.256850512822469
Training loss: 4.843562126159668 / Valid loss: 6.0692953177860804
Training loss: 3.630459785461426 / Valid loss: 6.0535659517560685

Epoch: 15
Training loss: 4.926118850708008 / Valid loss: 6.126245852879116
Training loss: 5.3838419914245605 / Valid loss: 6.060071459270659
Training loss: 4.941718101501465 / Valid loss: 6.0412281740279425
Training loss: 4.6144938468933105 / Valid loss: 6.1109564508710585
Training loss: 4.637479782104492 / Valid loss: 6.023639722097488

Epoch: 16
Training loss: 6.16854190826416 / Valid loss: 6.0133145173390705
Training loss: 4.6689910888671875 / Valid loss: 6.225595403852917
Training loss: 4.437319755554199 / Valid loss: 6.110241678782872
Training loss: 4.666962623596191 / Valid loss: 6.137385817936488
Training loss: 4.887510299682617 / Valid loss: 6.0283299264453705

Epoch: 17
Training loss: 5.003348350524902 / Valid loss: 6.1132745356786815
Training loss: 3.370349884033203 / Valid loss: 5.9954140209016344
Training loss: 4.902135372161865 / Valid loss: 6.313225246611095
Training loss: 4.290127754211426 / Valid loss: 5.931262515840077
Model is saved in epoch 17, overall batch: 8700
Training loss: 5.270650863647461 / Valid loss: 6.06700313431876

Epoch: 18
Training loss: 3.252821922302246 / Valid loss: 6.129641730444772
Training loss: 4.987515449523926 / Valid loss: 6.027324020294916
Training loss: 3.84346079826355 / Valid loss: 6.092647275470552
Training loss: 4.3782172203063965 / Valid loss: 6.110148123332432
Training loss: 5.304892063140869 / Valid loss: 6.079244150434222

Epoch: 19
Training loss: 5.8245697021484375 / Valid loss: 5.952058426539103
Training loss: 3.496870279312134 / Valid loss: 5.973136969975063
Training loss: 3.400068759918213 / Valid loss: 5.91664571080889
Model is saved in epoch 19, overall batch: 9600
Training loss: 4.243488311767578 / Valid loss: 6.089627894901094

Epoch: 20
Training loss: 3.557837963104248 / Valid loss: 6.097326626096453
Training loss: 4.091840744018555 / Valid loss: 6.023678361801874
Training loss: 4.153747081756592 / Valid loss: 5.950516335169474
Training loss: 4.621753692626953 / Valid loss: 5.905663889930362
Model is saved in epoch 20, overall batch: 10100
Training loss: 4.0331597328186035 / Valid loss: 5.969703238351005

Epoch: 21
Training loss: 3.8981709480285645 / Valid loss: 5.943077228182838
Training loss: 3.8063905239105225 / Valid loss: 6.334829298655192
Training loss: 3.756434440612793 / Valid loss: 6.206858550934564
Training loss: 4.079286098480225 / Valid loss: 6.058968235197521
Training loss: 3.689406156539917 / Valid loss: 6.201295666467576

Epoch: 22
Training loss: 3.700655221939087 / Valid loss: 6.231264972686768
Training loss: 3.563276529312134 / Valid loss: 6.091128256207421
Training loss: 6.104656219482422 / Valid loss: 6.009235668182373
Training loss: 3.0105507373809814 / Valid loss: 5.966728285380772
Training loss: 4.601498126983643 / Valid loss: 6.09388739267985

Epoch: 23
Training loss: 4.865365028381348 / Valid loss: 6.073138784226917
Training loss: 4.234341621398926 / Valid loss: 6.214892637161982
Training loss: 4.975221633911133 / Valid loss: 6.103957725706555
Training loss: 5.838359832763672 / Valid loss: 6.11244154430571
Training loss: 4.588014602661133 / Valid loss: 6.160080996013823

Epoch: 24
Training loss: 3.9265949726104736 / Valid loss: 6.0462132181440085
Training loss: 4.723886489868164 / Valid loss: 6.151002631868635
Training loss: 2.720278263092041 / Valid loss: 6.073655632564
Training loss: 5.413627624511719 / Valid loss: 6.094405315035865
Training loss: 3.5748448371887207 / Valid loss: 6.090354174659366

Epoch: 25
Training loss: 4.55255126953125 / Valid loss: 6.0389129729498
Training loss: 3.223287582397461 / Valid loss: 6.214264302026658
Training loss: 3.5329482555389404 / Valid loss: 6.019281339645386
Training loss: 3.4059529304504395 / Valid loss: 6.073044263748896
Training loss: 3.592613935470581 / Valid loss: 6.103545243399484

Epoch: 26
Training loss: 4.666426658630371 / Valid loss: 5.905262919834682
Model is saved in epoch 26, overall batch: 12800
Training loss: 4.264007091522217 / Valid loss: 6.175768697829474
Training loss: 3.826366662979126 / Valid loss: 5.95466768628075
Training loss: 4.656630992889404 / Valid loss: 6.44556584812346
Training loss: 4.265838146209717 / Valid loss: 6.263090011051723

Epoch: 27
Training loss: 4.644655227661133 / Valid loss: 6.34203600202288
Training loss: 3.6241812705993652 / Valid loss: 6.304779729389009
Training loss: 4.781742572784424 / Valid loss: 6.106022258031936
Training loss: 5.197226524353027 / Valid loss: 5.943328271593367
Training loss: 2.5843963623046875 / Valid loss: 5.996688068480719

Epoch: 28
Training loss: 2.8118977546691895 / Valid loss: 6.045946162087577
Training loss: 3.6234097480773926 / Valid loss: 5.9885917867933
Training loss: 3.0956690311431885 / Valid loss: 6.063849308377221
Training loss: 2.60206937789917 / Valid loss: 6.140359161013649
Training loss: 2.9480538368225098 / Valid loss: 6.121418880280994

Epoch: 29
Training loss: 3.8872146606445312 / Valid loss: 6.074940953935895
Training loss: 3.4649581909179688 / Valid loss: 5.93826356615339
Training loss: 2.3443050384521484 / Valid loss: 6.062479670842489
Training loss: 4.665385723114014 / Valid loss: 6.050399537313552

Epoch: 30
Training loss: 2.786705255508423 / Valid loss: 6.106495780036563
Training loss: 3.5101258754730225 / Valid loss: 6.462578421547299
Training loss: 3.5499753952026367 / Valid loss: 6.168406500135149
Training loss: 4.80052375793457 / Valid loss: 6.012654415766398
Training loss: 3.7457141876220703 / Valid loss: 6.38631953284854

Epoch: 31
Training loss: 3.2329113483428955 / Valid loss: 6.129206850415184
Training loss: 5.6359148025512695 / Valid loss: 6.157378201257615
Training loss: 4.345579147338867 / Valid loss: 6.284460167657762
Training loss: 3.1303634643554688 / Valid loss: 6.433969924563454
Training loss: 4.837131023406982 / Valid loss: 5.997882307143438

Epoch: 32
Training loss: 2.7223353385925293 / Valid loss: 6.083523809342157
Training loss: 3.1281750202178955 / Valid loss: 6.046147773379372
Training loss: 4.3858842849731445 / Valid loss: 5.986805048443022
Training loss: 2.828016519546509 / Valid loss: 6.311544109526134
Training loss: 3.849630355834961 / Valid loss: 6.332616835548764

Epoch: 33
Training loss: 3.3800268173217773 / Valid loss: 6.126901086171468
Training loss: 4.951086044311523 / Valid loss: 6.029569321586973
Training loss: 3.236379623413086 / Valid loss: 6.263234374636696
Training loss: 3.406621217727661 / Valid loss: 6.250813649949573
Training loss: 2.59757137298584 / Valid loss: 6.123427025477091

Epoch: 34
Training loss: 3.097106456756592 / Valid loss: 6.040790487471081
Training loss: 3.9529988765716553 / Valid loss: 6.030740145274571
Training loss: 2.7205114364624023 / Valid loss: 6.126203641437349
Training loss: 3.0250842571258545 / Valid loss: 6.357022900808425
Training loss: 2.4804799556732178 / Valid loss: 6.182332981200445

Epoch: 35
Training loss: 3.691746950149536 / Valid loss: 6.090480325335548
Training loss: 2.767862319946289 / Valid loss: 6.508650804701306
Training loss: 3.239717483520508 / Valid loss: 6.646485076631818
Training loss: 2.910919189453125 / Valid loss: 6.268728267578852
Training loss: 2.9991984367370605 / Valid loss: 6.120152864002046

Epoch: 36
Training loss: 3.1161608695983887 / Valid loss: 6.178786591121129
Training loss: 3.1614439487457275 / Valid loss: 6.120173372541156
Training loss: 2.8705530166625977 / Valid loss: 6.404733687355405
Training loss: 4.310754776000977 / Valid loss: 6.060855136598859
Training loss: 4.036733150482178 / Valid loss: 6.179334088734218

Epoch: 37
Training loss: 3.0602869987487793 / Valid loss: 6.174499266488212
Training loss: 2.6620163917541504 / Valid loss: 6.166200985227312
Training loss: 2.7365264892578125 / Valid loss: 6.227849905831473
Training loss: 3.8947649002075195 / Valid loss: 6.032173733484178
Training loss: 4.0137529373168945 / Valid loss: 6.204102561587379

Epoch: 38
Training loss: 2.725968599319458 / Valid loss: 6.052104473114014
Training loss: 3.6282382011413574 / Valid loss: 6.222870379402524
Training loss: 3.0282769203186035 / Valid loss: 6.189103339967273
Training loss: 2.8106460571289062 / Valid loss: 6.258806989306495
Training loss: 3.7231857776641846 / Valid loss: 6.411503401256743

Epoch: 39
Training loss: 3.789891004562378 / Valid loss: 6.146640654972622
Training loss: 2.7807838916778564 / Valid loss: 6.266944753556024
Training loss: 3.0416722297668457 / Valid loss: 6.1456239427839
Training loss: 3.3138394355773926 / Valid loss: 6.230389722188314
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 12800): 5.760482424781436
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.974810600280762 / Valid loss: 16.661306408473422
Model is saved in epoch 0, overall batch: 0
Training loss: 8.355022430419922 / Valid loss: 14.134223774501256
Model is saved in epoch 0, overall batch: 100
Training loss: 9.981088638305664 / Valid loss: 12.504614194234213
Model is saved in epoch 0, overall batch: 200
Training loss: 8.817474365234375 / Valid loss: 11.635446752820696
Model is saved in epoch 0, overall batch: 300
Training loss: 7.110784530639648 / Valid loss: 10.59689603078933
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 9.991369247436523 / Valid loss: 9.974091902233305
Model is saved in epoch 1, overall batch: 500
Training loss: 6.8340935707092285 / Valid loss: 9.522566981542678
Model is saved in epoch 1, overall batch: 600
Training loss: 6.536225318908691 / Valid loss: 9.25826990490868
Model is saved in epoch 1, overall batch: 700
Training loss: 4.981111526489258 / Valid loss: 8.986482443128313
Model is saved in epoch 1, overall batch: 800
Training loss: 7.579948425292969 / Valid loss: 8.002966217767625
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.282114505767822 / Valid loss: 8.008223529089065
Training loss: 5.464908599853516 / Valid loss: 7.650487550099691
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.567383766174316 / Valid loss: 7.522957497551328
Model is saved in epoch 2, overall batch: 1200
Training loss: 7.049678802490234 / Valid loss: 7.500233827318464
Model is saved in epoch 2, overall batch: 1300
Training loss: 6.040199279785156 / Valid loss: 7.352042477471488
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 6.340805530548096 / Valid loss: 7.056234060014997
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.674283981323242 / Valid loss: 6.893666446776617
Model is saved in epoch 3, overall batch: 1600
Training loss: 7.768270492553711 / Valid loss: 6.840054666428339
Model is saved in epoch 3, overall batch: 1700
Training loss: 5.242100715637207 / Valid loss: 6.731943927492414
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.031867980957031 / Valid loss: 6.886562351953415

Epoch: 4
Training loss: 3.766486644744873 / Valid loss: 6.730292690367926
Model is saved in epoch 4, overall batch: 2000
Training loss: 4.15360164642334 / Valid loss: 6.818678540275211
Training loss: 4.907664775848389 / Valid loss: 6.823754539943876
Training loss: 6.151419639587402 / Valid loss: 6.649682390122186
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.854930877685547 / Valid loss: 6.653994287763323

Epoch: 5
Training loss: 6.914450168609619 / Valid loss: 6.561267977669125
Model is saved in epoch 5, overall batch: 2500
Training loss: 6.970232963562012 / Valid loss: 6.592410028548468
Training loss: 6.225648403167725 / Valid loss: 6.6169544742220925
Training loss: 3.84173321723938 / Valid loss: 6.51785626411438
Model is saved in epoch 5, overall batch: 2800
Training loss: 6.755731105804443 / Valid loss: 6.579728140149798

Epoch: 6
Training loss: 5.107076644897461 / Valid loss: 6.393754479998634
Model is saved in epoch 6, overall batch: 3000
Training loss: 3.4357895851135254 / Valid loss: 6.477643755504063
Training loss: 5.775252342224121 / Valid loss: 6.633951943261283
Training loss: 4.345074653625488 / Valid loss: 6.483910151890346
Training loss: 5.784680366516113 / Valid loss: 6.462446278617495

Epoch: 7
Training loss: 4.488906383514404 / Valid loss: 6.374853697277251
Model is saved in epoch 7, overall batch: 3500
Training loss: 3.632035732269287 / Valid loss: 6.355024794169835
Model is saved in epoch 7, overall batch: 3600
Training loss: 3.676954984664917 / Valid loss: 6.227232220059349
Model is saved in epoch 7, overall batch: 3700
Training loss: 6.005670547485352 / Valid loss: 6.327841016224452
Training loss: 4.758101463317871 / Valid loss: 6.468053751900083

Epoch: 8
Training loss: 4.139797210693359 / Valid loss: 6.34364964848473
Training loss: 5.045654296875 / Valid loss: 6.498730137234642
Training loss: 4.330074310302734 / Valid loss: 6.203031769252958
Model is saved in epoch 8, overall batch: 4200
Training loss: 5.237455368041992 / Valid loss: 6.231829361688523
Training loss: 6.379477500915527 / Valid loss: 6.392854452133179

Epoch: 9
Training loss: 3.977357864379883 / Valid loss: 6.437191949571882
Training loss: 5.8272881507873535 / Valid loss: 6.350555535725185
Training loss: 6.185785293579102 / Valid loss: 6.303838879721505
Training loss: 4.74549674987793 / Valid loss: 6.269250890186854

Epoch: 10
Training loss: 4.983661651611328 / Valid loss: 6.239034139542353
Training loss: 3.3562726974487305 / Valid loss: 6.160196828842163
Model is saved in epoch 10, overall batch: 5000
Training loss: 5.330868721008301 / Valid loss: 6.153552888688587
Model is saved in epoch 10, overall batch: 5100
Training loss: 4.893062591552734 / Valid loss: 6.305988148280552
Training loss: 6.673222541809082 / Valid loss: 6.317766255424136

Epoch: 11
Training loss: 5.63560676574707 / Valid loss: 6.251067581630888
Training loss: 5.811335563659668 / Valid loss: 6.131784647987002
Model is saved in epoch 11, overall batch: 5500
Training loss: 5.878659248352051 / Valid loss: 6.2938212916964575
Training loss: 5.534021854400635 / Valid loss: 6.170455569312686
Training loss: 5.068185806274414 / Valid loss: 6.130293248948597
Model is saved in epoch 11, overall batch: 5800

Epoch: 12
Training loss: 4.467143535614014 / Valid loss: 6.283504181816465
Training loss: 6.061159133911133 / Valid loss: 6.2204212393079485
Training loss: 5.448855876922607 / Valid loss: 6.203695506141299
Training loss: 4.665627479553223 / Valid loss: 6.009623554774693
Model is saved in epoch 12, overall batch: 6200
Training loss: 4.9174652099609375 / Valid loss: 6.214176030386062

Epoch: 13
Training loss: 4.0872344970703125 / Valid loss: 5.911568721135457
Model is saved in epoch 13, overall batch: 6400
Training loss: 6.65079402923584 / Valid loss: 6.122128718239921
Training loss: 3.2062063217163086 / Valid loss: 6.140925114495413
Training loss: 4.004050254821777 / Valid loss: 6.136580757867723
Training loss: 4.903668403625488 / Valid loss: 6.255842129389445

Epoch: 14
Training loss: 4.205507278442383 / Valid loss: 5.991865015029907
Training loss: 4.490575790405273 / Valid loss: 6.140697624569848
Training loss: 4.440090656280518 / Valid loss: 6.314421324502854
Training loss: 4.7486090660095215 / Valid loss: 6.0939540091015045
Training loss: 3.7438111305236816 / Valid loss: 6.052262217657907

Epoch: 15
Training loss: 4.76347541809082 / Valid loss: 6.12103020804269
Training loss: 5.399858474731445 / Valid loss: 6.057419465837024
Training loss: 4.923226356506348 / Valid loss: 6.047027099700201
Training loss: 4.6307268142700195 / Valid loss: 6.079063974108014
Training loss: 4.607449054718018 / Valid loss: 5.984668720336187

Epoch: 16
Training loss: 6.059967994689941 / Valid loss: 6.012220598402477
Training loss: 4.602598667144775 / Valid loss: 6.22100601877485
Training loss: 4.478918075561523 / Valid loss: 6.069250772112892
Training loss: 4.692582607269287 / Valid loss: 6.128962843758719
Training loss: 4.912362098693848 / Valid loss: 6.076859110877628

Epoch: 17
Training loss: 5.0148210525512695 / Valid loss: 6.138915320805141
Training loss: 3.3597071170806885 / Valid loss: 5.995643901824951
Training loss: 4.834105968475342 / Valid loss: 6.32681881813776
Training loss: 4.325222015380859 / Valid loss: 5.9036943798973445
Model is saved in epoch 17, overall batch: 8700
Training loss: 5.197037696838379 / Valid loss: 6.09656613667806

Epoch: 18
Training loss: 3.288954734802246 / Valid loss: 6.102457430249169
Training loss: 5.007477283477783 / Valid loss: 5.979579873312087
Training loss: 3.7208056449890137 / Valid loss: 6.094563786188761
Training loss: 4.312443256378174 / Valid loss: 6.0722679546901155
Training loss: 5.441250801086426 / Valid loss: 6.111910320463634

Epoch: 19
Training loss: 5.776551246643066 / Valid loss: 5.959562655857631
Training loss: 3.2879722118377686 / Valid loss: 5.96534956296285
Training loss: 3.3537497520446777 / Valid loss: 5.920692802610851
Training loss: 4.233455181121826 / Valid loss: 6.10544632048834

Epoch: 20
Training loss: 3.4963366985321045 / Valid loss: 6.054213651021322
Training loss: 3.970785140991211 / Valid loss: 6.036443215324765
Training loss: 4.12176513671875 / Valid loss: 5.939557016463507
Training loss: 4.80368185043335 / Valid loss: 5.888983874093919
Model is saved in epoch 20, overall batch: 10100
Training loss: 3.9738008975982666 / Valid loss: 5.960934579940069

Epoch: 21
Training loss: 3.985238552093506 / Valid loss: 5.9544560250781835
Training loss: 3.8786733150482178 / Valid loss: 6.354591271990821
Training loss: 3.874192476272583 / Valid loss: 6.17720460437593
Training loss: 3.9698128700256348 / Valid loss: 6.0888440154847645
Training loss: 3.7939929962158203 / Valid loss: 6.176025674456642

Epoch: 22
Training loss: 3.712902307510376 / Valid loss: 6.172193781534831
Training loss: 3.5232696533203125 / Valid loss: 6.063184595108032
Training loss: 6.01768159866333 / Valid loss: 6.002794599533081
Training loss: 3.1124649047851562 / Valid loss: 5.936146797452654
Training loss: 4.674991607666016 / Valid loss: 6.024642022450765

Epoch: 23
Training loss: 4.7759318351745605 / Valid loss: 6.0796897820064
Training loss: 4.294351577758789 / Valid loss: 6.184735722768874
Training loss: 4.888796806335449 / Valid loss: 6.090330811909267
Training loss: 5.869382858276367 / Valid loss: 6.127146341687157
Training loss: 4.623785972595215 / Valid loss: 6.136396314984276

Epoch: 24
Training loss: 3.945186138153076 / Valid loss: 6.027392185302007
Training loss: 4.665998935699463 / Valid loss: 6.195389827092488
Training loss: 2.7899858951568604 / Valid loss: 6.062684165863764
Training loss: 5.477010726928711 / Valid loss: 6.074565803437006
Training loss: 3.5207717418670654 / Valid loss: 6.0527463663192025

Epoch: 25
Training loss: 4.5496110916137695 / Valid loss: 6.031338487352643
Training loss: 3.2435832023620605 / Valid loss: 6.246160466330392
Training loss: 3.6424336433410645 / Valid loss: 6.015137574786231
Training loss: 3.459836959838867 / Valid loss: 6.081459583554949
Training loss: 3.630068302154541 / Valid loss: 6.124070187977382

Epoch: 26
Training loss: 4.675729274749756 / Valid loss: 5.913407602764312
Training loss: 4.166506767272949 / Valid loss: 6.141394220079694
Training loss: 3.822566270828247 / Valid loss: 5.924954355330694
Training loss: 4.372908592224121 / Valid loss: 6.465718911942981
Training loss: 4.29970121383667 / Valid loss: 6.2040088199433825

Epoch: 27
Training loss: 4.564637184143066 / Valid loss: 6.351073859986805
Training loss: 3.5332915782928467 / Valid loss: 6.262737122036162
Training loss: 4.737530708312988 / Valid loss: 6.0957039401644755
Training loss: 5.401008605957031 / Valid loss: 5.954675404230754
Training loss: 2.645899772644043 / Valid loss: 5.94976916086106

Epoch: 28
Training loss: 2.719111204147339 / Valid loss: 6.0138125941866925
Training loss: 3.5574898719787598 / Valid loss: 5.954040827069964
Training loss: 3.2077407836914062 / Valid loss: 6.053255355925787
Training loss: 2.620556354522705 / Valid loss: 6.096973396482921
Training loss: 2.9949207305908203 / Valid loss: 6.149590140297299

Epoch: 29
Training loss: 3.7486777305603027 / Valid loss: 6.075425241107032
Training loss: 3.4405806064605713 / Valid loss: 5.910093872887748
Training loss: 2.3622045516967773 / Valid loss: 6.0727451778593515
Training loss: 4.830934047698975 / Valid loss: 6.05744895480928

Epoch: 30
Training loss: 2.793630599975586 / Valid loss: 6.116165049870809
Training loss: 3.4583899974823 / Valid loss: 6.42601121266683
Training loss: 3.4654548168182373 / Valid loss: 6.215474857602801
Training loss: 4.744440078735352 / Valid loss: 6.0205551011221745
Training loss: 3.7461657524108887 / Valid loss: 6.33675696509225

Epoch: 31
Training loss: 3.3417325019836426 / Valid loss: 6.113249553952898
Training loss: 5.691563606262207 / Valid loss: 6.176271300088792
Training loss: 4.149293899536133 / Valid loss: 6.334819952646892
Training loss: 3.1897525787353516 / Valid loss: 6.439699695223854
Training loss: 4.75330114364624 / Valid loss: 6.005126971290225

Epoch: 32
Training loss: 2.7188401222229004 / Valid loss: 6.061927992956979
Training loss: 3.074702739715576 / Valid loss: 6.021972049985613
Training loss: 4.371864318847656 / Valid loss: 5.967160570053827
Training loss: 2.8671462535858154 / Valid loss: 6.327484644026983
Training loss: 3.8144075870513916 / Valid loss: 6.3422607285635815

Epoch: 33
Training loss: 3.2492728233337402 / Valid loss: 6.156758596783592
Training loss: 4.875982284545898 / Valid loss: 6.029503586178734
Training loss: 3.244842052459717 / Valid loss: 6.213821613220942
Training loss: 3.5793280601501465 / Valid loss: 6.281114907491775
Training loss: 2.555729866027832 / Valid loss: 6.104095343181065

Epoch: 34
Training loss: 3.2462971210479736 / Valid loss: 6.067495105380104
Training loss: 4.051392555236816 / Valid loss: 6.005307697114491
Training loss: 2.6227967739105225 / Valid loss: 6.117479674021403
Training loss: 3.1802945137023926 / Valid loss: 6.339101255507696
Training loss: 2.4786829948425293 / Valid loss: 6.207060393832979

Epoch: 35
Training loss: 3.67307448387146 / Valid loss: 6.1219015507471
Training loss: 2.8544397354125977 / Valid loss: 6.504794504528954
Training loss: 3.1995694637298584 / Valid loss: 6.6211090065184095
Training loss: 2.8586788177490234 / Valid loss: 6.227178457805088
Training loss: 3.123889923095703 / Valid loss: 6.12255855060759

Epoch: 36
Training loss: 3.27030086517334 / Valid loss: 6.201051219304403
Training loss: 3.1189732551574707 / Valid loss: 6.136887831914993
Training loss: 2.8238024711608887 / Valid loss: 6.392031483423143
Training loss: 4.098538398742676 / Valid loss: 6.066019891557239
Training loss: 3.921877384185791 / Valid loss: 6.148404328028361

Epoch: 37
Training loss: 3.086336851119995 / Valid loss: 6.154407414935884
Training loss: 2.77462100982666 / Valid loss: 6.15928829738072
Training loss: 2.675623893737793 / Valid loss: 6.218178172338576
Training loss: 3.8228330612182617 / Valid loss: 6.052732826414562
Training loss: 3.7117807865142822 / Valid loss: 6.212524963560559

Epoch: 38
Training loss: 2.552557945251465 / Valid loss: 6.039018724078224
Training loss: 3.6348845958709717 / Valid loss: 6.235682773590088
Training loss: 2.9354608058929443 / Valid loss: 6.168933861596244
Training loss: 2.6804604530334473 / Valid loss: 6.275583237693423
Training loss: 3.5311713218688965 / Valid loss: 6.484489009493873

Epoch: 39
Training loss: 3.792348861694336 / Valid loss: 6.123265638805571
Training loss: 2.747034788131714 / Valid loss: 6.230018770127069
Training loss: 3.0184056758880615 / Valid loss: 6.1289136001041955
Training loss: 3.498056411743164 / Valid loss: 6.207799000967117
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 10100): 5.715501501446679
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.614124298095703 / Valid loss: 16.367621131170363
Model is saved in epoch 0, overall batch: 0
Training loss: 9.704437255859375 / Valid loss: 8.886051691146124
Model is saved in epoch 0, overall batch: 100
Training loss: 7.413427352905273 / Valid loss: 6.9626156602587015
Model is saved in epoch 0, overall batch: 200
Training loss: 6.300765037536621 / Valid loss: 6.197278531392415
Model is saved in epoch 0, overall batch: 300
Training loss: 8.802066802978516 / Valid loss: 6.02739751906622
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.791377067565918 / Valid loss: 5.955524810155233
Model is saved in epoch 1, overall batch: 500
Training loss: 6.090208053588867 / Valid loss: 5.87060820715768
Model is saved in epoch 1, overall batch: 600
Training loss: 7.6506853103637695 / Valid loss: 5.828908209573655
Model is saved in epoch 1, overall batch: 700
Training loss: 5.796271324157715 / Valid loss: 5.773598107837495
Model is saved in epoch 1, overall batch: 800
Training loss: 5.896729946136475 / Valid loss: 5.829637386685326

Epoch: 2
Training loss: 5.724773406982422 / Valid loss: 5.778531419663202
Training loss: 5.001583576202393 / Valid loss: 5.7461166745140435
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.601018905639648 / Valid loss: 5.766274774642218
Training loss: 6.744325160980225 / Valid loss: 5.772047047387986
Training loss: 5.910353183746338 / Valid loss: 5.815295566831316

Epoch: 3
Training loss: 4.078302383422852 / Valid loss: 5.77471079145159
Training loss: 5.298784255981445 / Valid loss: 5.720315056755429
Model is saved in epoch 3, overall batch: 1600
Training loss: 4.494434356689453 / Valid loss: 5.6946932951609295
Model is saved in epoch 3, overall batch: 1700
Training loss: 5.798088073730469 / Valid loss: 5.73084188642956
Training loss: 4.860529899597168 / Valid loss: 5.699193588892618

Epoch: 4
Training loss: 3.691286087036133 / Valid loss: 5.724833402179536
Training loss: 4.644388198852539 / Valid loss: 5.659775568190075
Model is saved in epoch 4, overall batch: 2100
Training loss: 4.595715522766113 / Valid loss: 5.619948023841495
Model is saved in epoch 4, overall batch: 2200
Training loss: 5.859700679779053 / Valid loss: 5.6640429655710856
Training loss: 7.499014854431152 / Valid loss: 5.702313756942749

Epoch: 5
Training loss: 4.565852165222168 / Valid loss: 5.702422191983177
Training loss: 4.577946186065674 / Valid loss: 5.6660071849823
Training loss: 6.220345973968506 / Valid loss: 5.678571276437669
Training loss: 6.159444808959961 / Valid loss: 5.6903174763634095
Training loss: 8.127423286437988 / Valid loss: 5.783917885734922

Epoch: 6
Training loss: 4.85772705078125 / Valid loss: 5.658921580087571
Training loss: 3.7803142070770264 / Valid loss: 5.661135351090204
Training loss: 5.470653057098389 / Valid loss: 5.620415975933983
Training loss: 4.231034278869629 / Valid loss: 5.625665680567423
Training loss: 3.723985433578491 / Valid loss: 5.625127708344232

Epoch: 7
Training loss: 3.1644673347473145 / Valid loss: 5.635040694191343
Training loss: 4.669065475463867 / Valid loss: 5.625703654970441
Training loss: 4.651971817016602 / Valid loss: 5.646488507588704
Training loss: 5.635735511779785 / Valid loss: 5.542659938903082
Model is saved in epoch 7, overall batch: 3800
Training loss: 5.768196105957031 / Valid loss: 5.62607707977295

Epoch: 8
Training loss: 5.061463356018066 / Valid loss: 5.561728404817127
Training loss: 4.166346549987793 / Valid loss: 5.631455618994576
Training loss: 6.564955711364746 / Valid loss: 5.58849591981797
Training loss: 5.497773170471191 / Valid loss: 5.563379998434158
Training loss: 4.987276077270508 / Valid loss: 5.587784344809396

Epoch: 9
Training loss: 4.111711502075195 / Valid loss: 5.610495038259597
Training loss: 4.501335144042969 / Valid loss: 5.536525928406489
Model is saved in epoch 9, overall batch: 4600
Training loss: 5.01436710357666 / Valid loss: 5.569535482497442
Training loss: 3.1463489532470703 / Valid loss: 5.61298550651187

Epoch: 10
Training loss: 3.5867061614990234 / Valid loss: 5.549043260301862
Training loss: 4.042591094970703 / Valid loss: 5.596961770738874
Training loss: 4.884124755859375 / Valid loss: 5.522488909675961
Model is saved in epoch 10, overall batch: 5100
Training loss: 5.0706586837768555 / Valid loss: 5.559805220649356
Training loss: 5.004022598266602 / Valid loss: 5.558681188310896

Epoch: 11
Training loss: 5.939829349517822 / Valid loss: 5.54515233039856
Training loss: 4.501356601715088 / Valid loss: 5.558157157897949
Training loss: 5.25816011428833 / Valid loss: 5.546514781316121
Training loss: 4.4630632400512695 / Valid loss: 5.511955622264317
Model is saved in epoch 11, overall batch: 5700
Training loss: 4.40573787689209 / Valid loss: 5.572727394104004

Epoch: 12
Training loss: 6.537014961242676 / Valid loss: 5.535916989190238
Training loss: 4.350788116455078 / Valid loss: 5.58115952355521
Training loss: 5.569911956787109 / Valid loss: 5.524846923918951
Training loss: 4.8536577224731445 / Valid loss: 5.507670375279018
Model is saved in epoch 12, overall batch: 6200
Training loss: 4.544045448303223 / Valid loss: 5.515124311901274

Epoch: 13
Training loss: 4.696887493133545 / Valid loss: 5.522644288199288
Training loss: 4.906340599060059 / Valid loss: 5.560928051812308
Training loss: 5.228302001953125 / Valid loss: 5.569691630772182
Training loss: 3.3959662914276123 / Valid loss: 5.546360020410447
Training loss: 4.834464073181152 / Valid loss: 5.5178204422905335

Epoch: 14
Training loss: 4.943047523498535 / Valid loss: 5.560030087970552
Training loss: 3.3967385292053223 / Valid loss: 5.529810065314884
Training loss: 5.9315924644470215 / Valid loss: 5.511075712385631
Training loss: 5.026376247406006 / Valid loss: 5.525332439513433
Training loss: 4.02043342590332 / Valid loss: 5.548241901397705

Epoch: 15
Training loss: 3.503018856048584 / Valid loss: 5.5252143587384905
Training loss: 6.043283462524414 / Valid loss: 5.523497772216797
Training loss: 4.788600921630859 / Valid loss: 5.525901347114926
Training loss: 4.784276962280273 / Valid loss: 5.510543205624535
Training loss: 4.010771751403809 / Valid loss: 5.520798540115356

Epoch: 16
Training loss: 5.97348690032959 / Valid loss: 5.54907063529605
Training loss: 4.672165870666504 / Valid loss: 5.5054605688367575
Model is saved in epoch 16, overall batch: 8000
Training loss: 3.334502696990967 / Valid loss: 5.580471077419463
Training loss: 5.348100662231445 / Valid loss: 5.5055107321058
Training loss: 4.3495283126831055 / Valid loss: 5.563783675148374

Epoch: 17
Training loss: 4.372962474822998 / Valid loss: 5.538816756293887
Training loss: 4.574574947357178 / Valid loss: 5.506497476214454
Training loss: 2.909627914428711 / Valid loss: 5.524771038691203
Training loss: 7.019026756286621 / Valid loss: 5.532116154261998
Training loss: 3.8242106437683105 / Valid loss: 5.523101695378622

Epoch: 18
Training loss: 4.091366767883301 / Valid loss: 5.546959382011777
Training loss: 4.067999839782715 / Valid loss: 5.516549110412598
Training loss: 4.629030227661133 / Valid loss: 5.524311983017695
Training loss: 4.654656887054443 / Valid loss: 5.519311571121216
Training loss: 6.540454387664795 / Valid loss: 5.538881265549433

Epoch: 19
Training loss: 4.454305171966553 / Valid loss: 5.5368254616147
Training loss: 3.889078140258789 / Valid loss: 5.5314994653066
Training loss: 5.495434761047363 / Valid loss: 5.565485625039964
Training loss: 4.793923854827881 / Valid loss: 5.5235049974350705

Epoch: 20
Training loss: 3.784285545349121 / Valid loss: 5.585701760791597
Training loss: 3.7370543479919434 / Valid loss: 5.547539963041033
Training loss: 4.032604217529297 / Valid loss: 5.5611699717385426
Training loss: 4.793642044067383 / Valid loss: 5.524654050100417
Training loss: 2.820270538330078 / Valid loss: 5.550802723566691

Epoch: 21
Training loss: 4.660737991333008 / Valid loss: 5.561869173958188
Training loss: 4.325296401977539 / Valid loss: 5.56267888886588
Training loss: 5.358591079711914 / Valid loss: 5.539960974738712
Training loss: 6.089583873748779 / Valid loss: 5.615233423596337
Training loss: 3.8044967651367188 / Valid loss: 5.54997147832598

Epoch: 22
Training loss: 2.863041877746582 / Valid loss: 5.544132972898938
Training loss: 4.211613655090332 / Valid loss: 5.52694775717599
Training loss: 4.668875217437744 / Valid loss: 5.528884971709479
Training loss: 5.322997093200684 / Valid loss: 5.538501135508219
Training loss: 3.685183525085449 / Valid loss: 5.562001241956438

Epoch: 23
Training loss: 3.6772866249084473 / Valid loss: 5.594476886022658
Training loss: 4.654555320739746 / Valid loss: 5.611338565463112
Training loss: 4.059589385986328 / Valid loss: 5.581397637866792
Training loss: 3.982088565826416 / Valid loss: 5.5709195341382705
Training loss: 4.263842582702637 / Valid loss: 5.582692959195092

Epoch: 24
Training loss: 3.285284996032715 / Valid loss: 5.57204631850833
Training loss: 5.495289325714111 / Valid loss: 5.568689659663609
Training loss: 4.550150394439697 / Valid loss: 5.575269036065965
Training loss: 3.578601121902466 / Valid loss: 5.585661953971499
Training loss: 4.196140289306641 / Valid loss: 5.56713699840364

Epoch: 25
Training loss: 3.537621259689331 / Valid loss: 5.578330312456403
Training loss: 3.411402702331543 / Valid loss: 5.607042498815627
Training loss: 5.400557041168213 / Valid loss: 5.60434878894261
Training loss: 3.245258331298828 / Valid loss: 5.590241543451945
Training loss: 4.424234390258789 / Valid loss: 5.588477096103486

Epoch: 26
Training loss: 3.220470905303955 / Valid loss: 5.587755155563355
Training loss: 3.1823348999023438 / Valid loss: 5.5788523378826325
Training loss: 3.2601513862609863 / Valid loss: 5.602734588441395
Training loss: 4.027618408203125 / Valid loss: 5.582781921114241
Training loss: 3.0323941707611084 / Valid loss: 5.581079126539684

Epoch: 27
Training loss: 4.001002311706543 / Valid loss: 5.603381018411546
Training loss: 2.7480690479278564 / Valid loss: 5.586807403110322
Training loss: 3.691671371459961 / Valid loss: 5.597693043663388
Training loss: 4.584314346313477 / Valid loss: 5.621367706571307
Training loss: 5.799715995788574 / Valid loss: 5.622042086010888

Epoch: 28
Training loss: 4.07145357131958 / Valid loss: 5.606624467032296
Training loss: 3.7730886936187744 / Valid loss: 5.597204855510166
Training loss: 4.547636985778809 / Valid loss: 5.608918271745954
Training loss: 5.592430591583252 / Valid loss: 5.610486734481085
Training loss: 3.329676628112793 / Valid loss: 5.593582262311663

Epoch: 29
Training loss: 3.90004825592041 / Valid loss: 5.6187465917496455
Training loss: 3.1114485263824463 / Valid loss: 5.627957816351028
Training loss: 5.02491569519043 / Valid loss: 5.635713967822847
Training loss: 3.6223208904266357 / Valid loss: 5.644591508592878

Epoch: 30
Training loss: 4.249076843261719 / Valid loss: 5.616141135351999
Training loss: 4.217125415802002 / Valid loss: 5.661429950169155
Training loss: 4.248227596282959 / Valid loss: 5.68195355279105
Training loss: 3.971268653869629 / Valid loss: 5.629883545920962
Training loss: 3.0091400146484375 / Valid loss: 5.640635756083897

Epoch: 31
Training loss: 4.301997661590576 / Valid loss: 5.63597591036842
Training loss: 3.8836541175842285 / Valid loss: 5.63033097357977
Training loss: 3.490715265274048 / Valid loss: 5.658475578398932
Training loss: 3.9796345233917236 / Valid loss: 5.627786182221913
Training loss: 3.471123695373535 / Valid loss: 5.720663011641729

Epoch: 32
Training loss: 3.114670753479004 / Valid loss: 5.642578209014166
Training loss: 3.46020245552063 / Valid loss: 5.728382723672049
Training loss: 2.822118043899536 / Valid loss: 5.651862253461565
Training loss: 4.4512104988098145 / Valid loss: 5.637570095062256
Training loss: 2.9811410903930664 / Valid loss: 5.673344734736851

Epoch: 33
Training loss: 2.7671995162963867 / Valid loss: 5.65659825915382
Training loss: 3.9260873794555664 / Valid loss: 5.671430871600196
Training loss: 3.212343215942383 / Valid loss: 5.662264814830961
Training loss: 3.060067892074585 / Valid loss: 5.678709888458252
Training loss: 3.572335720062256 / Valid loss: 5.704290317353748

Epoch: 34
Training loss: 3.4414262771606445 / Valid loss: 5.672675963810512
Training loss: 4.54063606262207 / Valid loss: 5.680142482121785
Training loss: 3.962071657180786 / Valid loss: 5.68708640280224
Training loss: 4.554821491241455 / Valid loss: 5.712236726851691
Training loss: 2.1387991905212402 / Valid loss: 5.669630216416858

Epoch: 35
Training loss: 3.4312825202941895 / Valid loss: 5.694444476990473
Training loss: 2.704144239425659 / Valid loss: 5.680069541931152
Training loss: 3.2583680152893066 / Valid loss: 5.684878601346697
Training loss: 3.682600259780884 / Valid loss: 5.749970501945132
Training loss: 2.8132143020629883 / Valid loss: 5.707342129661924

Epoch: 36
Training loss: 4.2659382820129395 / Valid loss: 5.683257336843582
Training loss: 3.1498656272888184 / Valid loss: 5.745424872352963
Training loss: 3.497001886367798 / Valid loss: 5.737524048487345
Training loss: 3.265533447265625 / Valid loss: 5.731333405630929
Training loss: 3.5167362689971924 / Valid loss: 5.709963287625994

Epoch: 37
Training loss: 3.4109694957733154 / Valid loss: 5.724431591942197
Training loss: 2.7718186378479004 / Valid loss: 5.710775720505487
Training loss: 3.922283411026001 / Valid loss: 5.7185663291386195
Training loss: 3.640641212463379 / Valid loss: 5.726714136486962
Training loss: 2.8162331581115723 / Valid loss: 5.697383119946434

Epoch: 38
Training loss: 3.2742676734924316 / Valid loss: 5.728596423921131
Training loss: 3.557875394821167 / Valid loss: 5.7290999185471305
Training loss: 3.673628807067871 / Valid loss: 5.78557160014198
Training loss: 3.200075626373291 / Valid loss: 5.745821814309983
Training loss: 4.049845218658447 / Valid loss: 5.8036184492565335

Epoch: 39
Training loss: 2.954041004180908 / Valid loss: 5.742526849110921
Training loss: 3.990504026412964 / Valid loss: 5.774639131909325
Training loss: 3.2124314308166504 / Valid loss: 5.783866085324969
Training loss: 2.868361711502075 / Valid loss: 5.787275293895177
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 8000): 5.362525858197894
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.614124298095703 / Valid loss: 16.36762111300514
Model is saved in epoch 0, overall batch: 0
Training loss: 9.704438209533691 / Valid loss: 8.886052322387695
Model is saved in epoch 0, overall batch: 100
Training loss: 7.413429260253906 / Valid loss: 6.962616623015631
Model is saved in epoch 0, overall batch: 200
Training loss: 6.3007659912109375 / Valid loss: 6.197279555456979
Model is saved in epoch 0, overall batch: 300
Training loss: 8.802068710327148 / Valid loss: 6.027398270652408
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.79140043258667 / Valid loss: 5.955540443602063
Model is saved in epoch 1, overall batch: 500
Training loss: 6.090313911437988 / Valid loss: 5.870630012239729
Model is saved in epoch 1, overall batch: 600
Training loss: 7.650876522064209 / Valid loss: 5.82893046878633
Model is saved in epoch 1, overall batch: 700
Training loss: 5.79640007019043 / Valid loss: 5.773602739969889
Model is saved in epoch 1, overall batch: 800
Training loss: 5.896632194519043 / Valid loss: 5.829702754247756

Epoch: 2
Training loss: 5.7249016761779785 / Valid loss: 5.778550257001604
Training loss: 5.0010986328125 / Valid loss: 5.746194142387027
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.60150146484375 / Valid loss: 5.766417040143694
Training loss: 6.743947982788086 / Valid loss: 5.771948026475452
Training loss: 5.910796642303467 / Valid loss: 5.814483317874727

Epoch: 3
Training loss: 4.077479362487793 / Valid loss: 5.774650362559727
Training loss: 5.299042701721191 / Valid loss: 5.7219741049267
Model is saved in epoch 3, overall batch: 1600
Training loss: 4.495156764984131 / Valid loss: 5.694748251778739
Model is saved in epoch 3, overall batch: 1700
Training loss: 5.796494483947754 / Valid loss: 5.73055362701416
Training loss: 4.860478401184082 / Valid loss: 5.699273157119751

Epoch: 4
Training loss: 3.690345287322998 / Valid loss: 5.726005910691761
Training loss: 4.6420979499816895 / Valid loss: 5.659230257215954
Model is saved in epoch 4, overall batch: 2100
Training loss: 4.594174861907959 / Valid loss: 5.619910231090727
Model is saved in epoch 4, overall batch: 2200
Training loss: 5.8592119216918945 / Valid loss: 5.6632511411394395
Training loss: 7.503003120422363 / Valid loss: 5.7024975118182955

Epoch: 5
Training loss: 4.566441535949707 / Valid loss: 5.701968004589989
Training loss: 4.578308582305908 / Valid loss: 5.666414996555873
Training loss: 6.217241287231445 / Valid loss: 5.680261023839315
Training loss: 6.155149459838867 / Valid loss: 5.691580856414069
Training loss: 8.125568389892578 / Valid loss: 5.7819370178949265

Epoch: 6
Training loss: 4.8535566329956055 / Valid loss: 5.66189082236517
Training loss: 3.7787299156188965 / Valid loss: 5.659898694356283
Training loss: 5.468436241149902 / Valid loss: 5.619086944489252
Model is saved in epoch 6, overall batch: 3200
Training loss: 4.233851909637451 / Valid loss: 5.625128816422962
Training loss: 3.7126822471618652 / Valid loss: 5.625337879998344

Epoch: 7
Training loss: 3.164727210998535 / Valid loss: 5.636143589019776
Training loss: 4.6710381507873535 / Valid loss: 5.626064881824312
Training loss: 4.650623321533203 / Valid loss: 5.6440684046064105
Training loss: 5.6330766677856445 / Valid loss: 5.541305750892276
Model is saved in epoch 7, overall batch: 3800
Training loss: 5.774487018585205 / Valid loss: 5.624242607752482

Epoch: 8
Training loss: 5.04703426361084 / Valid loss: 5.560847055344355
Training loss: 4.174530506134033 / Valid loss: 5.631520096460978
Training loss: 6.559050559997559 / Valid loss: 5.5882810115814205
Training loss: 5.490533828735352 / Valid loss: 5.562652892158145
Training loss: 4.986878395080566 / Valid loss: 5.589869642257691

Epoch: 9
Training loss: 4.111263751983643 / Valid loss: 5.611234553654989
Training loss: 4.500513076782227 / Valid loss: 5.535943782897222
Model is saved in epoch 9, overall batch: 4600
Training loss: 5.00834321975708 / Valid loss: 5.571313499269031
Training loss: 3.1495954990386963 / Valid loss: 5.6129855519249325

Epoch: 10
Training loss: 3.596938133239746 / Valid loss: 5.550363011587233
Training loss: 4.035421371459961 / Valid loss: 5.595342300051734
Training loss: 4.881843566894531 / Valid loss: 5.521225127719697
Model is saved in epoch 10, overall batch: 5100
Training loss: 5.072714328765869 / Valid loss: 5.55788430032276
Training loss: 5.00306510925293 / Valid loss: 5.5612567447480705

Epoch: 11
Training loss: 5.936638832092285 / Valid loss: 5.544211800893148
Training loss: 4.510313987731934 / Valid loss: 5.556514290400914
Training loss: 5.256905555725098 / Valid loss: 5.544866282599313
Training loss: 4.450594902038574 / Valid loss: 5.511162953149705
Model is saved in epoch 11, overall batch: 5700
Training loss: 4.393567085266113 / Valid loss: 5.570010387329829

Epoch: 12
Training loss: 6.520820617675781 / Valid loss: 5.534172110330491
Training loss: 4.342085838317871 / Valid loss: 5.581983695711409
Training loss: 5.5485148429870605 / Valid loss: 5.525779849007016
Training loss: 4.866657257080078 / Valid loss: 5.507686492374965
Model is saved in epoch 12, overall batch: 6200
Training loss: 4.546547889709473 / Valid loss: 5.51721632594154

Epoch: 13
Training loss: 4.703135967254639 / Valid loss: 5.52467356863476
Training loss: 4.898614406585693 / Valid loss: 5.5604897930508566
Training loss: 5.238001823425293 / Valid loss: 5.570528688884917
Training loss: 3.410370349884033 / Valid loss: 5.547864353089105
Training loss: 4.850953102111816 / Valid loss: 5.519899745214553

Epoch: 14
Training loss: 4.934115886688232 / Valid loss: 5.558838335673014
Training loss: 3.3958072662353516 / Valid loss: 5.529512736910865
Training loss: 5.939626216888428 / Valid loss: 5.509506123406546
Training loss: 5.0095415115356445 / Valid loss: 5.527758185068766
Training loss: 4.014622688293457 / Valid loss: 5.546415138244629

Epoch: 15
Training loss: 3.504934072494507 / Valid loss: 5.5236808799562
Training loss: 6.065691947937012 / Valid loss: 5.522909000941685
Training loss: 4.799507141113281 / Valid loss: 5.5285293510981965
Training loss: 4.779238700866699 / Valid loss: 5.512022527058919
Training loss: 4.000895023345947 / Valid loss: 5.519636497043428

Epoch: 16
Training loss: 5.981413841247559 / Valid loss: 5.550120201564971
Training loss: 4.655679702758789 / Valid loss: 5.505309173039028
Model is saved in epoch 16, overall batch: 8000
Training loss: 3.3316519260406494 / Valid loss: 5.581776521319434
Training loss: 5.32647180557251 / Valid loss: 5.509825313658942
Training loss: 4.354537010192871 / Valid loss: 5.562756418046497

Epoch: 17
Training loss: 4.373406410217285 / Valid loss: 5.537845820472354
Training loss: 4.5778303146362305 / Valid loss: 5.506772938228789
Training loss: 2.8888022899627686 / Valid loss: 5.523929847989764
Training loss: 7.010053634643555 / Valid loss: 5.53357332093375
Training loss: 3.8261189460754395 / Valid loss: 5.525102519989014

Epoch: 18
Training loss: 4.088883876800537 / Valid loss: 5.544416498002552
Training loss: 4.071689605712891 / Valid loss: 5.518311273484003
Training loss: 4.6068854331970215 / Valid loss: 5.525278273082915
Training loss: 4.670169353485107 / Valid loss: 5.519730574744088
Training loss: 6.533172607421875 / Valid loss: 5.537970697312128

Epoch: 19
Training loss: 4.453875541687012 / Valid loss: 5.536338065919422
Training loss: 3.8701679706573486 / Valid loss: 5.532404770169939
Training loss: 5.515655994415283 / Valid loss: 5.563924405688331
Training loss: 4.785444736480713 / Valid loss: 5.524133536929176

Epoch: 20
Training loss: 3.774202823638916 / Valid loss: 5.587467336654663
Training loss: 3.735748767852783 / Valid loss: 5.54902316956293
Training loss: 4.016408443450928 / Valid loss: 5.561683947699411
Training loss: 4.796361446380615 / Valid loss: 5.527741407212757
Training loss: 2.828087091445923 / Valid loss: 5.551094815844581

Epoch: 21
Training loss: 4.663309097290039 / Valid loss: 5.561662008648827
Training loss: 4.324254989624023 / Valid loss: 5.561102528799148
Training loss: 5.335227012634277 / Valid loss: 5.538784401757376
Training loss: 6.093991756439209 / Valid loss: 5.616740746725173
Training loss: 3.7955944538116455 / Valid loss: 5.550303565888178

Epoch: 22
Training loss: 2.8654215335845947 / Valid loss: 5.5454697472708565
Training loss: 4.199416160583496 / Valid loss: 5.527750037965321
Training loss: 4.69124698638916 / Valid loss: 5.529704641160511
Training loss: 5.313359260559082 / Valid loss: 5.53807559240432
Training loss: 3.670820713043213 / Valid loss: 5.561111495608375

Epoch: 23
Training loss: 3.6862006187438965 / Valid loss: 5.591688405899775
Training loss: 4.662375450134277 / Valid loss: 5.612295872824532
Training loss: 4.064492225646973 / Valid loss: 5.5785907382056825
Training loss: 3.988727569580078 / Valid loss: 5.568586413065592
Training loss: 4.275688171386719 / Valid loss: 5.583173990249634

Epoch: 24
Training loss: 3.290574073791504 / Valid loss: 5.573155834561303
Training loss: 5.488359451293945 / Valid loss: 5.568155520302909
Training loss: 4.536005020141602 / Valid loss: 5.577257281257992
Training loss: 3.5659475326538086 / Valid loss: 5.584482883271717
Training loss: 4.16574764251709 / Valid loss: 5.567369402022589

Epoch: 25
Training loss: 3.5621724128723145 / Valid loss: 5.576312199093047
Training loss: 3.3950045108795166 / Valid loss: 5.602899038223994
Training loss: 5.404333591461182 / Valid loss: 5.602594988686698
Training loss: 3.2398390769958496 / Valid loss: 5.589482975006104
Training loss: 4.408609390258789 / Valid loss: 5.587796778905959

Epoch: 26
Training loss: 3.2376010417938232 / Valid loss: 5.5825099241165885
Training loss: 3.1627612113952637 / Valid loss: 5.578890834535871
Training loss: 3.2800803184509277 / Valid loss: 5.603062913531349
Training loss: 4.026804447174072 / Valid loss: 5.580877192815145
Training loss: 3.0463955402374268 / Valid loss: 5.57992567107791

Epoch: 27
Training loss: 3.997349262237549 / Valid loss: 5.6029816854567756
Training loss: 2.745896816253662 / Valid loss: 5.588305073692685
Training loss: 3.693397283554077 / Valid loss: 5.595681921641032
Training loss: 4.594969749450684 / Valid loss: 5.619394960857573
Training loss: 5.806933403015137 / Valid loss: 5.617761137371971

Epoch: 28
Training loss: 4.055698394775391 / Valid loss: 5.606351375579834
Training loss: 3.7743654251098633 / Valid loss: 5.596066222872053
Training loss: 4.55661153793335 / Valid loss: 5.605612543651036
Training loss: 5.6043291091918945 / Valid loss: 5.609643082391648
Training loss: 3.307708740234375 / Valid loss: 5.593402953374953

Epoch: 29
Training loss: 3.8995251655578613 / Valid loss: 5.614008220036824
Training loss: 3.097193956375122 / Valid loss: 5.628609791256133
Training loss: 5.024351119995117 / Valid loss: 5.635340059371222
Training loss: 3.6004443168640137 / Valid loss: 5.647969059717088

Epoch: 30
Training loss: 4.236734390258789 / Valid loss: 5.613836436044602
Training loss: 4.229837894439697 / Valid loss: 5.661410256794521
Training loss: 4.2371954917907715 / Valid loss: 5.682673297609601
Training loss: 3.966007709503174 / Valid loss: 5.629060574940273
Training loss: 3.003906488418579 / Valid loss: 5.641370130720593

Epoch: 31
Training loss: 4.318474769592285 / Valid loss: 5.633211660385132
Training loss: 3.8708887100219727 / Valid loss: 5.630119623456682
Training loss: 3.488112449645996 / Valid loss: 5.657234991164435
Training loss: 3.9762957096099854 / Valid loss: 5.63180506115868
Training loss: 3.471971273422241 / Valid loss: 5.7199195203327

Epoch: 32
Training loss: 3.1045901775360107 / Valid loss: 5.643951824733189
Training loss: 3.498323678970337 / Valid loss: 5.7282480580466135
Training loss: 2.8541510105133057 / Valid loss: 5.6516972882407055
Training loss: 4.435588359832764 / Valid loss: 5.63858668690636
Training loss: 2.994690418243408 / Valid loss: 5.677743455341884

Epoch: 33
Training loss: 2.742771625518799 / Valid loss: 5.658571270533971
Training loss: 3.950151205062866 / Valid loss: 5.668974817366827
Training loss: 3.1559832096099854 / Valid loss: 5.66252597173055
Training loss: 3.0479650497436523 / Valid loss: 5.67912567229498
Training loss: 3.5767910480499268 / Valid loss: 5.705048840386527

Epoch: 34
Training loss: 3.4652915000915527 / Valid loss: 5.674253179913475
Training loss: 4.52957820892334 / Valid loss: 5.681476334163121
Training loss: 3.939652919769287 / Valid loss: 5.683874961308071
Training loss: 4.529078006744385 / Valid loss: 5.712232691901071
Training loss: 2.1427695751190186 / Valid loss: 5.666815537498111

Epoch: 35
Training loss: 3.4116501808166504 / Valid loss: 5.693646521795364
Training loss: 2.7087740898132324 / Valid loss: 5.6777096952710835
Training loss: 3.2412939071655273 / Valid loss: 5.684850075131371
Training loss: 3.6590635776519775 / Valid loss: 5.744915923618135
Training loss: 2.81583833694458 / Valid loss: 5.707180395580473

Epoch: 36
Training loss: 4.270602226257324 / Valid loss: 5.680146471659342
Training loss: 3.1483888626098633 / Valid loss: 5.74426002956572
Training loss: 3.523894786834717 / Valid loss: 5.7361660548618865
Training loss: 3.254976272583008 / Valid loss: 5.733525426047189
Training loss: 3.5148394107818604 / Valid loss: 5.707997808002291

Epoch: 37
Training loss: 3.447507858276367 / Valid loss: 5.726542388825189
Training loss: 2.756459951400757 / Valid loss: 5.706444411050706
Training loss: 3.93084454536438 / Valid loss: 5.717223959877377
Training loss: 3.6545639038085938 / Valid loss: 5.722769069671631
Training loss: 2.822634220123291 / Valid loss: 5.694640207290649

Epoch: 38
Training loss: 3.3116824626922607 / Valid loss: 5.728696543829781
Training loss: 3.573986530303955 / Valid loss: 5.729957190014067
Training loss: 3.6471643447875977 / Valid loss: 5.784622537522089
Training loss: 3.1745033264160156 / Valid loss: 5.7452496119907925
Training loss: 4.008693695068359 / Valid loss: 5.7956832545144215

Epoch: 39
Training loss: 2.959132194519043 / Valid loss: 5.740570611045474
Training loss: 4.021573066711426 / Valid loss: 5.770766160601661
Training loss: 3.219092845916748 / Valid loss: 5.780377090544928
Training loss: 2.8280348777770996 / Valid loss: 5.786515192758469
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 8000): 5.362054279872349
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.74432945251465 / Valid loss: 16.93612883431571
Model is saved in epoch 0, overall batch: 0
Training loss: 10.355813980102539 / Valid loss: 9.493440046764555
Model is saved in epoch 0, overall batch: 100
Training loss: 6.782283306121826 / Valid loss: 6.712387952350435
Model is saved in epoch 0, overall batch: 200
Training loss: 4.879397869110107 / Valid loss: 5.812719524474371
Model is saved in epoch 0, overall batch: 300
Training loss: 5.637808799743652 / Valid loss: 5.652969680513654
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.358053207397461 / Valid loss: 5.549949600583031
Model is saved in epoch 1, overall batch: 500
Training loss: 5.823020935058594 / Valid loss: 5.659411369051252
Training loss: 4.920243263244629 / Valid loss: 5.820711721692766
Training loss: 4.5992751121521 / Valid loss: 5.918815508342925
Training loss: 4.816232204437256 / Valid loss: 6.218687463942028

Epoch: 2
Training loss: 4.555686950683594 / Valid loss: 5.962214858191354
Training loss: 3.233529567718506 / Valid loss: 5.867048990158808
Training loss: 3.873351573944092 / Valid loss: 6.096067548933483
Training loss: 4.230205059051514 / Valid loss: 6.020088883808681
Training loss: 3.1788785457611084 / Valid loss: 5.829805801028297

Epoch: 3
Training loss: 3.319302797317505 / Valid loss: 5.862662242707752
Training loss: 3.254704713821411 / Valid loss: 6.094779409681048
Training loss: 3.619163751602173 / Valid loss: 6.230919683547247
Training loss: 2.3796446323394775 / Valid loss: 6.20236154964992
Training loss: 3.104916572570801 / Valid loss: 6.094674964178176

Epoch: 4
Training loss: 2.5584816932678223 / Valid loss: 6.196993058068411
Training loss: 2.926743507385254 / Valid loss: 6.302179182143439
Training loss: 2.038360595703125 / Valid loss: 6.489133982431321
Training loss: 2.475682020187378 / Valid loss: 6.511549836113339
Training loss: 3.9669575691223145 / Valid loss: 6.289422607421875

Epoch: 5
Training loss: 1.7256108522415161 / Valid loss: 6.435374977475121
Training loss: 2.3933897018432617 / Valid loss: 6.442124707358224
Training loss: 3.115813732147217 / Valid loss: 6.56668978645688
Training loss: 2.945387840270996 / Valid loss: 6.5930574553353445
Training loss: 2.3365511894226074 / Valid loss: 6.838924176352364

Epoch: 6
Training loss: 2.6323084831237793 / Valid loss: 6.8113538378760925
Training loss: 2.134772300720215 / Valid loss: 6.735959607078915
Training loss: 1.6636039018630981 / Valid loss: 6.883197632290068
Training loss: 1.7989577054977417 / Valid loss: 6.780085715793428
Training loss: 2.4541945457458496 / Valid loss: 6.955224632081531

Epoch: 7
Training loss: 2.2112865447998047 / Valid loss: 6.769608833676293
Training loss: 2.551942825317383 / Valid loss: 6.8719676471891855
Training loss: 1.1531856060028076 / Valid loss: 6.849570315224784
Training loss: 1.8848295211791992 / Valid loss: 6.909536997477214
Training loss: 2.9605274200439453 / Valid loss: 7.156803081149147

Epoch: 8
Training loss: 1.8810157775878906 / Valid loss: 7.039067182086763
Training loss: 2.232675075531006 / Valid loss: 6.838784590221587
Training loss: 1.8832483291625977 / Valid loss: 6.910046945299421
Training loss: 1.997575044631958 / Valid loss: 7.737990674518404
Training loss: 1.5860347747802734 / Valid loss: 6.780267263594128

Epoch: 9
Training loss: 0.96696537733078 / Valid loss: 6.991377203805106
Training loss: 2.3949339389801025 / Valid loss: 7.008846800667899
Training loss: 2.3315303325653076 / Valid loss: 6.817493870144799
Training loss: 1.7943549156188965 / Valid loss: 6.842913629895165

Epoch: 10
Training loss: 1.1009999513626099 / Valid loss: 6.880444413139706
Training loss: 1.1308590173721313 / Valid loss: 7.0224058650788805
Training loss: 1.2431268692016602 / Valid loss: 7.038748264312744
Training loss: 1.5713255405426025 / Valid loss: 7.058135196140834
Training loss: 0.7864406108856201 / Valid loss: 6.93761678877331

Epoch: 11
Training loss: 1.2757413387298584 / Valid loss: 7.336800579797654
Training loss: 0.7957825660705566 / Valid loss: 7.001320997873942
Training loss: 0.6874170303344727 / Valid loss: 7.09907564889817
Training loss: 1.368927240371704 / Valid loss: 7.000155203683036
Training loss: 1.3404338359832764 / Valid loss: 6.954926781427293

Epoch: 12
Training loss: 1.0929160118103027 / Valid loss: 7.040744581676665
Training loss: 0.7342498302459717 / Valid loss: 6.991756611778623
Training loss: 1.226722002029419 / Valid loss: 7.078245276496524
Training loss: 1.0473108291625977 / Valid loss: 7.070914804367792
Training loss: 1.1381080150604248 / Valid loss: 7.099402182442802

Epoch: 13
Training loss: 0.978043794631958 / Valid loss: 7.021544851575579
Training loss: 1.2784030437469482 / Valid loss: 7.030944102151054
Training loss: 0.6151285171508789 / Valid loss: 7.036056386856806
Training loss: 1.150350570678711 / Valid loss: 7.300206023170834
Training loss: 1.3655321598052979 / Valid loss: 7.021917352222261

Epoch: 14
Training loss: 1.046849250793457 / Valid loss: 7.083632414681571
Training loss: 1.412926435470581 / Valid loss: 7.066711957114084
Training loss: 0.9435579776763916 / Valid loss: 7.213844131288075
Training loss: 1.3119555711746216 / Valid loss: 6.975164940243675
Training loss: 1.2387423515319824 / Valid loss: 7.269417998904274

Epoch: 15
Training loss: 0.758969783782959 / Valid loss: 7.314710340045747
Training loss: 1.2479990720748901 / Valid loss: 6.9629885264805385
Training loss: 0.9743800163269043 / Valid loss: 7.063955593109131
Training loss: 0.8802205324172974 / Valid loss: 7.201879601251512
Training loss: 1.2288562059402466 / Valid loss: 7.002638376326788

Epoch: 16
Training loss: 0.8868635892868042 / Valid loss: 7.156458891005743
Training loss: 1.1871687173843384 / Valid loss: 7.251289074761527
Training loss: 0.8054847717285156 / Valid loss: 7.14366676012675
Training loss: 0.9456114768981934 / Valid loss: 7.060482034229097
Training loss: 0.8122463226318359 / Valid loss: 7.0756703513009205

Epoch: 17
Training loss: 1.6617389917373657 / Valid loss: 7.1044137591407415
Training loss: 0.817846953868866 / Valid loss: 6.980641523996989
Training loss: 1.1511952877044678 / Valid loss: 7.133402919769287
Training loss: 1.4509403705596924 / Valid loss: 7.172527058919271
Training loss: 1.0448263883590698 / Valid loss: 7.100492986043294

Epoch: 18
Training loss: 0.6004354953765869 / Valid loss: 7.072037976128715
Training loss: 0.8831487894058228 / Valid loss: 7.314843918028332
Training loss: 1.198651909828186 / Valid loss: 7.34074449085054
Training loss: 1.0055873394012451 / Valid loss: 7.230946826934814
Training loss: 1.040992021560669 / Valid loss: 7.062592660813104

Epoch: 19
Training loss: 0.9072222709655762 / Valid loss: 7.134840579259963
Training loss: 0.7130460739135742 / Valid loss: 7.170089535486131
Training loss: 1.3971525430679321 / Valid loss: 7.1644934608822775
Training loss: 0.8611938953399658 / Valid loss: 7.1218793505714055

Epoch: 20
Training loss: 0.8737019300460815 / Valid loss: 7.166892796471005
Training loss: 1.3407124280929565 / Valid loss: 7.267886357080369
Training loss: 0.6221746206283569 / Valid loss: 7.080052430289133
Training loss: 1.1362088918685913 / Valid loss: 7.0592659586951845
Training loss: 0.6503563523292542 / Valid loss: 7.14419405346825

Epoch: 21
Training loss: 1.1190197467803955 / Valid loss: 7.241127036866688
Training loss: 1.0688695907592773 / Valid loss: 7.037449373517718
Training loss: 0.5280499458312988 / Valid loss: 7.32554867154076
Training loss: 0.5444976091384888 / Valid loss: 7.319012010665167
Training loss: 0.809282660484314 / Valid loss: 7.219712634313674

Epoch: 22
Training loss: 0.666685938835144 / Valid loss: 7.078988565717425
Training loss: 0.8849331736564636 / Valid loss: 7.084515644255139
Training loss: 1.5127761363983154 / Valid loss: 7.1595516114007856
Training loss: 0.6037926077842712 / Valid loss: 7.151645140420823
Training loss: 0.9748907089233398 / Valid loss: 7.2236709367661245

Epoch: 23
Training loss: 0.867840051651001 / Valid loss: 7.07431667418707
Training loss: 0.9093174934387207 / Valid loss: 7.059517687842959
Training loss: 0.7757971286773682 / Valid loss: 7.215163971128918
Training loss: 0.717643141746521 / Valid loss: 7.10459483464559
Training loss: 0.7864611148834229 / Valid loss: 7.083343056270055

Epoch: 24
Training loss: 0.5853210091590881 / Valid loss: 7.19789917355492
Training loss: 0.9362950921058655 / Valid loss: 7.006123520079113
Training loss: 0.6997455954551697 / Valid loss: 7.238759058997744
Training loss: 0.7046229839324951 / Valid loss: 7.186659118107387
Training loss: 1.3656033277511597 / Valid loss: 7.1288130851019

Epoch: 25
Training loss: 0.5058494210243225 / Valid loss: 7.393037864140102
Training loss: 0.6040845513343811 / Valid loss: 7.059923585255941
Training loss: 0.6978315711021423 / Valid loss: 6.990762176967802
Training loss: 0.6196988821029663 / Valid loss: 7.1589643614632745
Training loss: 0.8060933351516724 / Valid loss: 7.055159954797654

Epoch: 26
Training loss: 0.4078463017940521 / Valid loss: 7.008391421181815
Training loss: 1.2117576599121094 / Valid loss: 7.169539551507859
Training loss: 0.41107138991355896 / Valid loss: 7.119411311830793
Training loss: 0.7454692125320435 / Valid loss: 7.156960244405838
Training loss: 0.7067652344703674 / Valid loss: 7.273851789746966

Epoch: 27
Training loss: 0.6308614015579224 / Valid loss: 7.0984004156930105
Training loss: 0.5785681009292603 / Valid loss: 7.08466929481143
Training loss: 0.8973068594932556 / Valid loss: 7.282860347202846
Training loss: 0.6995844841003418 / Valid loss: 7.16630665915353
Training loss: 0.5295498371124268 / Valid loss: 7.262424409957159

Epoch: 28
Training loss: 1.0006301403045654 / Valid loss: 7.506333146776472
Training loss: 0.6858968138694763 / Valid loss: 7.238722337995257
Training loss: 1.0850822925567627 / Valid loss: 7.278126834687733
Training loss: 0.4840328097343445 / Valid loss: 7.038091850280762
Training loss: 0.5112250447273254 / Valid loss: 7.1777479898361936

Epoch: 29
Training loss: 0.7517773509025574 / Valid loss: 7.103624044145857
Training loss: 0.5662846565246582 / Valid loss: 7.121165350505284
Training loss: 0.6398941278457642 / Valid loss: 7.284867768060593
Training loss: 0.8872009515762329 / Valid loss: 7.0937117576599125

Epoch: 30
Training loss: 0.5758281946182251 / Valid loss: 7.253803907121931
Training loss: 1.0329434871673584 / Valid loss: 7.142222820009504
Training loss: 0.7611639499664307 / Valid loss: 7.251019600459507
Training loss: 0.6404635906219482 / Valid loss: 7.31495554787772
Training loss: 1.1268795728683472 / Valid loss: 7.219462676275344

Epoch: 31
Training loss: 0.6094150543212891 / Valid loss: 7.133619022369385
Training loss: 0.4057748317718506 / Valid loss: 7.230488827115013
Training loss: 0.7521984577178955 / Valid loss: 7.19892977305821
Training loss: 0.48310816287994385 / Valid loss: 7.2010794957478845
Training loss: 0.7513020038604736 / Valid loss: 7.412661284492129

Epoch: 32
Training loss: 0.5764048099517822 / Valid loss: 7.280998588743664
Training loss: 0.5113650560379028 / Valid loss: 7.367281623113723
Training loss: 0.6987149119377136 / Valid loss: 7.1537339755467
Training loss: 0.6763530969619751 / Valid loss: 7.302435638791039
Training loss: 0.40101563930511475 / Valid loss: 7.059286726088751

Epoch: 33
Training loss: 0.5113840103149414 / Valid loss: 7.1135439055306575
Training loss: 0.5349781513214111 / Valid loss: 7.087219996679397
Training loss: 0.6310325860977173 / Valid loss: 7.095718697139195
Training loss: 0.5371191501617432 / Valid loss: 7.004399295080276
Training loss: 0.507249116897583 / Valid loss: 7.3798143068949384

Epoch: 34
Training loss: 0.5436150431632996 / Valid loss: 7.232485544113886
Training loss: 0.4780031740665436 / Valid loss: 7.341941125052315
Training loss: 0.4258914589881897 / Valid loss: 7.073844164893741
Training loss: 0.4654310345649719 / Valid loss: 7.1802079972766695
Training loss: 1.0374681949615479 / Valid loss: 7.182777913411458

Epoch: 35
Training loss: 0.497852087020874 / Valid loss: 7.21670823778425
Training loss: 0.44560810923576355 / Valid loss: 7.168662389119466
Training loss: 0.4226090908050537 / Valid loss: 7.356067375909714
Training loss: 0.8839817047119141 / Valid loss: 7.167708394640968
Training loss: 0.5082108378410339 / Valid loss: 7.3432659239996045

Epoch: 36
Training loss: 0.5811177492141724 / Valid loss: 7.195230915432885
Training loss: 0.6359755992889404 / Valid loss: 7.25275551478068
Training loss: 0.899041473865509 / Valid loss: 7.404790923708961
Training loss: 0.7599813938140869 / Valid loss: 7.32404781523205
Training loss: 0.44971805810928345 / Valid loss: 7.379141267140707

Epoch: 37
Training loss: 0.427994966506958 / Valid loss: 7.19166909626552
Training loss: 0.5508993268013 / Valid loss: 7.247384625389462
Training loss: 0.6634281277656555 / Valid loss: 7.252911006836664
Training loss: 1.3116095066070557 / Valid loss: 7.134212775457473
Training loss: 1.097012996673584 / Valid loss: 7.143299443381173

Epoch: 38
Training loss: 0.555056095123291 / Valid loss: 7.323564738319034
Training loss: 0.5902677774429321 / Valid loss: 7.29053978238787
Training loss: 0.7514911890029907 / Valid loss: 7.468572257813953
Training loss: 0.6326761245727539 / Valid loss: 7.287598019554501
Training loss: 0.7704033255577087 / Valid loss: 7.433572519393194

Epoch: 39
Training loss: 0.45064377784729004 / Valid loss: 7.351005395253499
Training loss: 0.6627054214477539 / Valid loss: 7.244148862929571
Training loss: 0.6148528456687927 / Valid loss: 7.2345537685212635
Training loss: 0.5794365406036377 / Valid loss: 7.033383732750302
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.4352276961008705
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.74432945251465 / Valid loss: 16.936128825233098
Model is saved in epoch 0, overall batch: 0
Training loss: 10.255775451660156 / Valid loss: 9.0914949235462
Model is saved in epoch 0, overall batch: 100
Training loss: 6.861440658569336 / Valid loss: 6.3175187928336
Model is saved in epoch 0, overall batch: 200
Training loss: 4.752138137817383 / Valid loss: 5.613537093571254
Model is saved in epoch 0, overall batch: 300
Training loss: 6.63250732421875 / Valid loss: 5.5998272010258265
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.880605220794678 / Valid loss: 5.612896079108829
Training loss: 5.965751647949219 / Valid loss: 5.659636320386614
Training loss: 4.4960784912109375 / Valid loss: 5.870590770812262
Training loss: 4.909627914428711 / Valid loss: 5.77850162869408
Training loss: 4.864717960357666 / Valid loss: 5.896412783577329

Epoch: 2
Training loss: 4.152695178985596 / Valid loss: 5.926211979275658
Training loss: 2.961866855621338 / Valid loss: 5.9046892711094445
Training loss: 4.0073089599609375 / Valid loss: 6.033502029237293
Training loss: 4.0556254386901855 / Valid loss: 5.914866261255174
Training loss: 3.256162405014038 / Valid loss: 5.857581828889392

Epoch: 3
Training loss: 2.7823503017425537 / Valid loss: 5.890679270880563
Training loss: 2.961962938308716 / Valid loss: 6.115531162988572
Training loss: 3.6901137828826904 / Valid loss: 6.0706972394670755
Training loss: 3.204866409301758 / Valid loss: 6.092096278780983
Training loss: 2.7003579139709473 / Valid loss: 6.195607516879127

Epoch: 4
Training loss: 2.634502410888672 / Valid loss: 6.191201823098319
Training loss: 2.6843066215515137 / Valid loss: 6.275999979745774
Training loss: 1.8500354290008545 / Valid loss: 6.496479706537156
Training loss: 2.148576259613037 / Valid loss: 6.513864312853132
Training loss: 4.58981990814209 / Valid loss: 6.585509447824387

Epoch: 5
Training loss: 1.6049089431762695 / Valid loss: 6.347467785789853
Training loss: 2.3463001251220703 / Valid loss: 6.535221256528582
Training loss: 2.257512331008911 / Valid loss: 6.625399793897357
Training loss: 3.0734047889709473 / Valid loss: 6.676313225428263
Training loss: 2.7748429775238037 / Valid loss: 6.805879713240124

Epoch: 6
Training loss: 2.831948757171631 / Valid loss: 6.806127970559256
Training loss: 2.137558937072754 / Valid loss: 6.720686265400478
Training loss: 1.9957339763641357 / Valid loss: 6.9829433645520895
Training loss: 2.33101749420166 / Valid loss: 6.857926841009231
Training loss: 2.586254358291626 / Valid loss: 6.930579798562186

Epoch: 7
Training loss: 2.0317416191101074 / Valid loss: 7.006668290637788
Training loss: 2.849317789077759 / Valid loss: 6.806613336290632
Training loss: 1.224393606185913 / Valid loss: 6.753216003236316
Training loss: 1.6887503862380981 / Valid loss: 6.898973632994152
Training loss: 2.954653739929199 / Valid loss: 6.9836669967288065

Epoch: 8
Training loss: 2.1213488578796387 / Valid loss: 6.873932043711345
Training loss: 2.322110891342163 / Valid loss: 6.883445487703596
Training loss: 2.0874834060668945 / Valid loss: 6.902101121629987
Training loss: 2.6827540397644043 / Valid loss: 6.894866575513567
Training loss: 1.63405442237854 / Valid loss: 6.86870677130563

Epoch: 9
Training loss: 0.8247317671775818 / Valid loss: 6.799669252123151
Training loss: 2.061488151550293 / Valid loss: 6.872730963570731
Training loss: 2.2955939769744873 / Valid loss: 6.73158403805324
Training loss: 1.5416114330291748 / Valid loss: 6.979794020879837

Epoch: 10
Training loss: 1.3465862274169922 / Valid loss: 6.93594852629162
Training loss: 1.7958898544311523 / Valid loss: 7.019283408210391
Training loss: 1.209741234779358 / Valid loss: 6.816713678269159
Training loss: 1.7784037590026855 / Valid loss: 7.023161670139857
Training loss: 1.2172162532806396 / Valid loss: 7.0372696967352

Epoch: 11
Training loss: 1.161630630493164 / Valid loss: 7.141717751820882
Training loss: 1.0076804161071777 / Valid loss: 6.9885348637898765
Training loss: 0.861333966255188 / Valid loss: 7.122540146963937
Training loss: 1.332932949066162 / Valid loss: 7.210485880715506
Training loss: 1.446713924407959 / Valid loss: 7.065785834902809

Epoch: 12
Training loss: 0.8455801606178284 / Valid loss: 7.119861775352842
Training loss: 0.8623643517494202 / Valid loss: 7.114030220395043
Training loss: 1.1027475595474243 / Valid loss: 7.109605419068109
Training loss: 1.077439785003662 / Valid loss: 7.126893947238014
Training loss: 1.3123480081558228 / Valid loss: 7.025424798329671

Epoch: 13
Training loss: 1.0718765258789062 / Valid loss: 6.965594332558768
Training loss: 1.071481704711914 / Valid loss: 7.0550017856416245
Training loss: 0.5934558510780334 / Valid loss: 6.8885507447378975
Training loss: 1.1000099182128906 / Valid loss: 7.242881906600226
Training loss: 1.1635087728500366 / Valid loss: 6.917319620223273

Epoch: 14
Training loss: 1.288534164428711 / Valid loss: 7.106048504511516
Training loss: 1.2986431121826172 / Valid loss: 6.971534106844947
Training loss: 1.0691263675689697 / Valid loss: 7.1670648393176855
Training loss: 1.0729072093963623 / Valid loss: 6.999122606004987
Training loss: 0.9121996164321899 / Valid loss: 7.0443350746518085

Epoch: 15
Training loss: 0.7553427815437317 / Valid loss: 7.008520675840832
Training loss: 1.2279316186904907 / Valid loss: 7.0252125967116585
Training loss: 0.8595569729804993 / Valid loss: 7.061358592623756
Training loss: 0.7703452110290527 / Valid loss: 6.986712094715664
Training loss: 1.0805318355560303 / Valid loss: 7.007369768051874

Epoch: 16
Training loss: 0.8717361092567444 / Valid loss: 7.050841849190848
Training loss: 1.1154141426086426 / Valid loss: 7.153955893289475
Training loss: 0.9654360413551331 / Valid loss: 7.270433094387963
Training loss: 0.8755689263343811 / Valid loss: 7.0424962361653645
Training loss: 0.8872227668762207 / Valid loss: 7.014984805243356

Epoch: 17
Training loss: 1.7439513206481934 / Valid loss: 7.110703150431315
Training loss: 1.1328928470611572 / Valid loss: 7.181099691845122
Training loss: 0.8347412943840027 / Valid loss: 7.120419297899518
Training loss: 0.7474542856216431 / Valid loss: 7.1339007718222485
Training loss: 0.5404580235481262 / Valid loss: 7.123395429338728

Epoch: 18
Training loss: 0.8210170865058899 / Valid loss: 6.966870101292928
Training loss: 1.120497226715088 / Valid loss: 7.157847953978039
Training loss: 1.088336706161499 / Valid loss: 7.092960357666016
Training loss: 0.8565361499786377 / Valid loss: 7.27117664246332
Training loss: 1.0380656719207764 / Valid loss: 7.035592991965157

Epoch: 19
Training loss: 1.0546503067016602 / Valid loss: 7.250383045559838
Training loss: 0.650555431842804 / Valid loss: 7.1854643004281185
Training loss: 1.3851810693740845 / Valid loss: 7.220650459471203
Training loss: 0.7188913822174072 / Valid loss: 7.069461559114002

Epoch: 20
Training loss: 0.7478998899459839 / Valid loss: 7.255912892023722
Training loss: 1.5067365169525146 / Valid loss: 7.113223970504034
Training loss: 0.9230068922042847 / Valid loss: 7.053042339143299
Training loss: 0.740269660949707 / Valid loss: 7.232757700057257
Training loss: 0.7985982894897461 / Valid loss: 7.151265121641613

Epoch: 21
Training loss: 1.2242352962493896 / Valid loss: 7.0659683908735005
Training loss: 1.2816107273101807 / Valid loss: 7.12638657433646
Training loss: 0.5243611335754395 / Valid loss: 7.156549421946208
Training loss: 0.5082360506057739 / Valid loss: 7.142299547649565
Training loss: 0.6477633714675903 / Valid loss: 7.2984963871183846

Epoch: 22
Training loss: 0.7991849184036255 / Valid loss: 7.134014349892026
Training loss: 0.9631532430648804 / Valid loss: 7.134556906563895
Training loss: 1.7599689960479736 / Valid loss: 7.095301278432211
Training loss: 0.4990578889846802 / Valid loss: 7.278894210997082
Training loss: 0.6802279949188232 / Valid loss: 7.250179224922544

Epoch: 23
Training loss: 0.7985839247703552 / Valid loss: 7.170951657068162
Training loss: 0.9562695026397705 / Valid loss: 6.950885836283366
Training loss: 1.036973237991333 / Valid loss: 7.131320726303827
Training loss: 0.7919453382492065 / Valid loss: 7.026248252959478
Training loss: 0.8728676438331604 / Valid loss: 7.18045962878636

Epoch: 24
Training loss: 0.8827678561210632 / Valid loss: 7.059485265186855
Training loss: 0.5473629236221313 / Valid loss: 7.060723543167114
Training loss: 0.7765661478042603 / Valid loss: 7.1137017795017785
Training loss: 0.788203239440918 / Valid loss: 7.319456836155482
Training loss: 1.301570177078247 / Valid loss: 7.016845258076986

Epoch: 25
Training loss: 0.6197844743728638 / Valid loss: 7.254284922281901
Training loss: 0.6548886895179749 / Valid loss: 6.954782826559884
Training loss: 0.4593532085418701 / Valid loss: 7.026447909218924
Training loss: 0.5918161869049072 / Valid loss: 7.10846529006958
Training loss: 0.8047553896903992 / Valid loss: 7.131687909080869

Epoch: 26
Training loss: 0.5575880408287048 / Valid loss: 7.077478949228922
Training loss: 1.4508476257324219 / Valid loss: 7.0555524871462865
Training loss: 0.44111335277557373 / Valid loss: 7.211074007125128
Training loss: 0.9326893091201782 / Valid loss: 7.070921427862984
Training loss: 0.5252488851547241 / Valid loss: 7.202188251132057

Epoch: 27
Training loss: 0.5935558080673218 / Valid loss: 7.026717449369885
Training loss: 0.6089229583740234 / Valid loss: 7.122794496445429
Training loss: 0.7706767320632935 / Valid loss: 7.0248459043956935
Training loss: 0.6738144159317017 / Valid loss: 7.09674745287214
Training loss: 0.5143125057220459 / Valid loss: 7.167327871776762

Epoch: 28
Training loss: 0.8420901298522949 / Valid loss: 7.290926787966773
Training loss: 0.7003096342086792 / Valid loss: 7.114208543868292
Training loss: 0.6310622692108154 / Valid loss: 7.194933505285354
Training loss: 0.5524366497993469 / Valid loss: 7.138139624822707
Training loss: 0.40991199016571045 / Valid loss: 7.21362784249442

Epoch: 29
Training loss: 0.5647534132003784 / Valid loss: 7.191947078704834
Training loss: 0.509646475315094 / Valid loss: 7.081453659420922
Training loss: 0.8880746364593506 / Valid loss: 7.502794937860398
Training loss: 0.6695472002029419 / Valid loss: 7.068049971262614

Epoch: 30
Training loss: 0.5905676484107971 / Valid loss: 7.214485200246175
Training loss: 0.990507960319519 / Valid loss: 7.191885226113456
Training loss: 0.7828719019889832 / Valid loss: 7.265506585439046
Training loss: 0.7225185632705688 / Valid loss: 7.124391047159831
Training loss: 0.9759272933006287 / Valid loss: 7.1497204326447985

Epoch: 31
Training loss: 0.49631747603416443 / Valid loss: 7.130033938090007
Training loss: 0.4972710907459259 / Valid loss: 7.120783696855818
Training loss: 0.9663468599319458 / Valid loss: 7.262605975923084
Training loss: 0.6192184090614319 / Valid loss: 7.225063492002941
Training loss: 0.6004104614257812 / Valid loss: 7.215120960417248

Epoch: 32
Training loss: 0.6356693506240845 / Valid loss: 7.191668896448045
Training loss: 0.6443685293197632 / Valid loss: 7.352786995115734
Training loss: 0.7655870914459229 / Valid loss: 7.174251143137614
Training loss: 0.7621787190437317 / Valid loss: 7.165206259772891
Training loss: 0.41677922010421753 / Valid loss: 7.007121914909

Epoch: 33
Training loss: 0.5822856426239014 / Valid loss: 7.119975367046538
Training loss: 0.4699956476688385 / Valid loss: 7.162360086895171
Training loss: 0.7498691082000732 / Valid loss: 7.0181401843116396
Training loss: 0.3358768820762634 / Valid loss: 7.111114009221395
Training loss: 0.5520572662353516 / Valid loss: 7.152618126642136

Epoch: 34
Training loss: 0.542447566986084 / Valid loss: 7.225206089019776
Training loss: 0.4881954491138458 / Valid loss: 7.235140155610584
Training loss: 0.33049649000167847 / Valid loss: 7.064404308228266
Training loss: 0.49773192405700684 / Valid loss: 7.2517661934807185
Training loss: 1.0936086177825928 / Valid loss: 7.062238688695999

Epoch: 35
Training loss: 0.5628207921981812 / Valid loss: 7.1463529495965865
Training loss: 0.44775840640068054 / Valid loss: 7.311866020020984
Training loss: 0.3441416025161743 / Valid loss: 7.226789687928699
Training loss: 0.7450801134109497 / Valid loss: 7.205405757540748
Training loss: 0.5214359164237976 / Valid loss: 7.121964872451056

Epoch: 36
Training loss: 0.6969141960144043 / Valid loss: 7.142946792784191
Training loss: 0.6555733680725098 / Valid loss: 7.21129039582752
Training loss: 1.043278455734253 / Valid loss: 7.194920667012533
Training loss: 0.6915234327316284 / Valid loss: 7.133344763801212
Training loss: 0.5533186197280884 / Valid loss: 7.016384851364863

Epoch: 37
Training loss: 0.4433782398700714 / Valid loss: 7.170319552648635
Training loss: 0.5179911255836487 / Valid loss: 7.183068493434361
Training loss: 0.4572486877441406 / Valid loss: 7.099868327095395
Training loss: 1.0714547634124756 / Valid loss: 7.180120908646357
Training loss: 1.274412989616394 / Valid loss: 7.144158631279355

Epoch: 38
Training loss: 0.4740109443664551 / Valid loss: 7.124516098839896
Training loss: 0.5846754908561707 / Valid loss: 7.118606951123192
Training loss: 0.8045423030853271 / Valid loss: 7.037946998505365
Training loss: 0.61716628074646 / Valid loss: 7.184351630437941
Training loss: 0.7716478109359741 / Valid loss: 7.232084896450951

Epoch: 39
Training loss: 0.4460783004760742 / Valid loss: 7.21719586054484
Training loss: 0.675780177116394 / Valid loss: 7.339371018182664
Training loss: 0.7724539637565613 / Valid loss: 7.113364551180885
Training loss: 0.8026286363601685 / Valid loss: 7.0221032097226095
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 400): 5.447887318474906
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.448760986328125 / Valid loss: 15.706126085917155
Model is saved in epoch 0, overall batch: 0
Training loss: 6.890535354614258 / Valid loss: 9.661181468055362
Model is saved in epoch 0, overall batch: 100
Training loss: 4.651227951049805 / Valid loss: 8.209714258284796
Model is saved in epoch 0, overall batch: 200
Training loss: 3.8958239555358887 / Valid loss: 6.372969529742286
Model is saved in epoch 0, overall batch: 300
Training loss: 4.1479997634887695 / Valid loss: 7.569711539858863

Epoch: 1
Training loss: 4.125919342041016 / Valid loss: 7.271030135381789
Training loss: 4.770803928375244 / Valid loss: 7.1527592545463925
Training loss: 5.3636932373046875 / Valid loss: 6.629746523357573
Training loss: 4.712769508361816 / Valid loss: 6.832279234840756
Training loss: 4.198955535888672 / Valid loss: 7.178543667566209

Epoch: 2
Training loss: 3.628997802734375 / Valid loss: 6.999606420880272
Training loss: 3.10695481300354 / Valid loss: 7.094603797367641
Training loss: 4.1287922859191895 / Valid loss: 6.692788982391358
Training loss: 3.200191020965576 / Valid loss: 7.142991654078165
Training loss: 3.721827983856201 / Valid loss: 6.395178347542172

Epoch: 3
Training loss: 2.3093550205230713 / Valid loss: 6.278673206056867
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.89129638671875 / Valid loss: 6.265482198624384
Model is saved in epoch 3, overall batch: 1600
Training loss: 3.0715770721435547 / Valid loss: 6.858368832724435
Training loss: 3.7671546936035156 / Valid loss: 6.211043369202387
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.967967987060547 / Valid loss: 6.840340182894752

Epoch: 4
Training loss: 2.407914638519287 / Valid loss: 6.171718490691412
Model is saved in epoch 4, overall batch: 2000
Training loss: 3.155996322631836 / Valid loss: 6.569641006560553
Training loss: 2.0298941135406494 / Valid loss: 6.330537578037807
Training loss: 2.3866493701934814 / Valid loss: 6.242818180720011
Training loss: 3.580599308013916 / Valid loss: 6.199824846358526

Epoch: 5
Training loss: 2.3635671138763428 / Valid loss: 6.483251544407436
Training loss: 2.195451259613037 / Valid loss: 6.358098418372018
Training loss: 2.778700590133667 / Valid loss: 6.330801416578747
Training loss: 2.6119375228881836 / Valid loss: 6.444596367790585
Training loss: 2.2749757766723633 / Valid loss: 7.180350680578322

Epoch: 6
Training loss: 1.5483429431915283 / Valid loss: 6.555968711489723
Training loss: 3.0561296939849854 / Valid loss: 6.437137585594541
Training loss: 2.0502512454986572 / Valid loss: 6.657545221419562
Training loss: 1.9175291061401367 / Valid loss: 7.241863396054223
Training loss: 2.30965518951416 / Valid loss: 6.825819076810564

Epoch: 7
Training loss: 1.79911208152771 / Valid loss: 6.658964972268968
Training loss: 1.7695965766906738 / Valid loss: 6.982274323418027
Training loss: 2.2412283420562744 / Valid loss: 7.003856427328927
Training loss: 2.2430777549743652 / Valid loss: 6.8125983874003095
Training loss: 2.126885175704956 / Valid loss: 6.823903201875233

Epoch: 8
Training loss: 1.1154805421829224 / Valid loss: 6.5596815200079055
Training loss: 1.3595408201217651 / Valid loss: 6.690894401641119
Training loss: 2.0220956802368164 / Valid loss: 6.363621325719924
Training loss: 1.7912790775299072 / Valid loss: 6.433234346480597
Training loss: 2.542006015777588 / Valid loss: 7.142423525310698

Epoch: 9
Training loss: 1.289496660232544 / Valid loss: 6.587787480581374
Training loss: 1.1033477783203125 / Valid loss: 6.638764156614031
Training loss: 1.4298113584518433 / Valid loss: 6.651921385810489
Training loss: 2.4609761238098145 / Valid loss: 6.523179789951869

Epoch: 10
Training loss: 1.517492651939392 / Valid loss: 6.5852389222099665
Training loss: 1.3282743692398071 / Valid loss: 6.648589783623105
Training loss: 1.7660034894943237 / Valid loss: 6.741047895522344
Training loss: 1.3852763175964355 / Valid loss: 6.886649699438186
Training loss: 1.6262164115905762 / Valid loss: 6.630347774142311

Epoch: 11
Training loss: 1.172263503074646 / Valid loss: 6.716285955338251
Training loss: 0.9512068629264832 / Valid loss: 6.749688055401757
Training loss: 1.2848856449127197 / Valid loss: 6.846798792339507
Training loss: 0.9678977727890015 / Valid loss: 6.718497526077997
Training loss: 2.4650700092315674 / Valid loss: 6.515829713003976

Epoch: 12
Training loss: 1.067451000213623 / Valid loss: 6.6296803111121765
Training loss: 1.0406358242034912 / Valid loss: 6.8023153532119025
Training loss: 1.0567569732666016 / Valid loss: 6.808343887329102
Training loss: 1.2314095497131348 / Valid loss: 7.21893664087568
Training loss: 1.356825590133667 / Valid loss: 6.802600551786877

Epoch: 13
Training loss: 1.678120732307434 / Valid loss: 6.821802834102086
Training loss: 0.9952585101127625 / Valid loss: 6.691298861730666
Training loss: 1.1668615341186523 / Valid loss: 6.886072054363432
Training loss: 1.1513326168060303 / Valid loss: 6.751308602378482
Training loss: 1.2849305868148804 / Valid loss: 7.30161927541097

Epoch: 14
Training loss: 1.1218979358673096 / Valid loss: 6.852625320071266
Training loss: 1.6425820589065552 / Valid loss: 6.737766497475761
Training loss: 1.6119563579559326 / Valid loss: 7.191044762021019
Training loss: 1.2066411972045898 / Valid loss: 6.814076578049432
Training loss: 0.7640821933746338 / Valid loss: 7.418647757030668

Epoch: 15
Training loss: 1.381466269493103 / Valid loss: 7.5548985390436085
Training loss: 0.5587685108184814 / Valid loss: 6.748144204275949
Training loss: 1.930391550064087 / Valid loss: 7.281446990512666
Training loss: 1.4227948188781738 / Valid loss: 7.008112603142148
Training loss: 2.5569281578063965 / Valid loss: 7.529790646689278

Epoch: 16
Training loss: 0.6344316005706787 / Valid loss: 7.069491518111456
Training loss: 1.4168939590454102 / Valid loss: 7.203939637683686
Training loss: 1.1820673942565918 / Valid loss: 6.883346196583339
Training loss: 1.207071304321289 / Valid loss: 6.808968800590152
Training loss: 0.9613628387451172 / Valid loss: 6.914326472509475

Epoch: 17
Training loss: 0.9399707913398743 / Valid loss: 6.986728409358434
Training loss: 0.858213484287262 / Valid loss: 7.03934410186041
Training loss: 0.8693902492523193 / Valid loss: 6.974117939812796
Training loss: 1.1535882949829102 / Valid loss: 6.794932428995768
Training loss: 0.9426265358924866 / Valid loss: 7.007577546437582

Epoch: 18
Training loss: 1.0339640378952026 / Valid loss: 6.8399708952222555
Training loss: 1.7187256813049316 / Valid loss: 6.722360887981597
Training loss: 1.172506332397461 / Valid loss: 7.225799787612188
Training loss: 0.9327353239059448 / Valid loss: 7.124911698840913
Training loss: 1.187467098236084 / Valid loss: 7.141803614298502

Epoch: 19
Training loss: 1.0387179851531982 / Valid loss: 7.14185977209182
Training loss: 0.7526177167892456 / Valid loss: 7.0389580726623535
Training loss: 0.6714494824409485 / Valid loss: 6.918389978862944
Training loss: 0.9344626665115356 / Valid loss: 7.085951469058082

Epoch: 20
Training loss: 0.8714518547058105 / Valid loss: 7.029950521105811
Training loss: 0.6755565404891968 / Valid loss: 7.0713742642175585
Training loss: 1.1598467826843262 / Valid loss: 7.040375509716216
Training loss: 0.6810997128486633 / Valid loss: 7.039264717556182
Training loss: 0.8603948354721069 / Valid loss: 7.05995880762736

Epoch: 21
Training loss: 0.8820482492446899 / Valid loss: 7.101642919722058
Training loss: 0.881889820098877 / Valid loss: 7.0046514056977776
Training loss: 0.7528960704803467 / Valid loss: 7.290175533294677
Training loss: 0.8466243743896484 / Valid loss: 7.024419012523833
Training loss: 0.5411194562911987 / Valid loss: 7.170285742623466

Epoch: 22
Training loss: 0.8366101980209351 / Valid loss: 6.990466908046177
Training loss: 0.7718896865844727 / Valid loss: 6.972426834560576
Training loss: 0.701927900314331 / Valid loss: 7.3142165910629995
Training loss: 1.3091237545013428 / Valid loss: 6.787454536982945
Training loss: 0.9173781871795654 / Valid loss: 7.204693044934954

Epoch: 23
Training loss: 0.7806086540222168 / Valid loss: 7.026168696085612
Training loss: 0.948228120803833 / Valid loss: 6.843180429367792
Training loss: 1.8226549625396729 / Valid loss: 7.311426753089542
Training loss: 0.6233053207397461 / Valid loss: 6.963797083355131
Training loss: 0.7421666979789734 / Valid loss: 6.8329316002982

Epoch: 24
Training loss: 0.6750420331954956 / Valid loss: 7.290414728437152
Training loss: 0.8257749080657959 / Valid loss: 7.677393141246977
Training loss: 1.1894274950027466 / Valid loss: 7.399034307116554
Training loss: 1.260816216468811 / Valid loss: 7.295943909599668
Training loss: 1.0029487609863281 / Valid loss: 6.854715844563075

Epoch: 25
Training loss: 1.2025281190872192 / Valid loss: 6.872964377630325
Training loss: 0.6216692924499512 / Valid loss: 7.019519964853923
Training loss: 0.9345323443412781 / Valid loss: 7.063353604362124
Training loss: 0.9726319909095764 / Valid loss: 7.30664750053769
Training loss: 0.9190603494644165 / Valid loss: 7.050133064814976

Epoch: 26
Training loss: 0.6019967794418335 / Valid loss: 7.003611959729876
Training loss: 0.8468056917190552 / Valid loss: 6.9271662802923295
Training loss: 0.8449474573135376 / Valid loss: 6.855719411940802
Training loss: 0.834293782711029 / Valid loss: 6.893189484732492
Training loss: 0.865094780921936 / Valid loss: 7.1991570313771565

Epoch: 27
Training loss: 0.5898944139480591 / Valid loss: 7.061840125492641
Training loss: 0.726946234703064 / Valid loss: 6.98290546962193
Training loss: 0.9705041646957397 / Valid loss: 6.952668948400588
Training loss: 0.5799963474273682 / Valid loss: 6.87744097936721
Training loss: 0.9839429259300232 / Valid loss: 7.36040020897275

Epoch: 28
Training loss: 0.5254379510879517 / Valid loss: 6.771699628375825
Training loss: 0.9275678396224976 / Valid loss: 7.339234243120466
Training loss: 1.6193091869354248 / Valid loss: 7.650208168938047
Training loss: 0.5219689607620239 / Valid loss: 7.059789453233991
Training loss: 0.4840755760669708 / Valid loss: 7.117293607620966

Epoch: 29
Training loss: 0.4588603377342224 / Valid loss: 6.882693649473644
Training loss: 0.6829211115837097 / Valid loss: 7.025168441590809
Training loss: 0.5904259085655212 / Valid loss: 6.781028000513713
Training loss: 0.9787876605987549 / Valid loss: 7.635027721949986

Epoch: 30
Training loss: 1.1157519817352295 / Valid loss: 6.99251937185015
Training loss: 0.6550267934799194 / Valid loss: 7.137741879054478
Training loss: 0.7466678023338318 / Valid loss: 6.900643155688331
Training loss: 0.9577459096908569 / Valid loss: 7.167792538234166
Training loss: 0.6975868940353394 / Valid loss: 7.084108956654867

Epoch: 31
Training loss: 0.9757589101791382 / Valid loss: 7.257648277282715
Training loss: 0.5308712124824524 / Valid loss: 7.093998236883254
Training loss: 0.585537314414978 / Valid loss: 7.325603485107422
Training loss: 0.740104079246521 / Valid loss: 7.072013845897856
Training loss: 0.7314845323562622 / Valid loss: 7.070335612978254

Epoch: 32
Training loss: 0.5785895586013794 / Valid loss: 7.0573458467211045
Training loss: 0.6713173389434814 / Valid loss: 7.133679755528768
Training loss: 0.6095253825187683 / Valid loss: 7.2223011607215515
Training loss: 1.10215425491333 / Valid loss: 7.044583370572044
Training loss: 0.38363224267959595 / Valid loss: 7.041248893737793

Epoch: 33
Training loss: 0.6551438570022583 / Valid loss: 7.2139412198747905
Training loss: 0.65776526927948 / Valid loss: 7.274845718202137
Training loss: 0.5941014885902405 / Valid loss: 6.984621747334798
Training loss: 0.5008828639984131 / Valid loss: 6.929518200102306
Training loss: 0.5768115520477295 / Valid loss: 7.021181617464338

Epoch: 34
Training loss: 0.8801385760307312 / Valid loss: 7.218565772828602
Training loss: 0.7438695430755615 / Valid loss: 6.991633928389776
Training loss: 0.7219887375831604 / Valid loss: 7.2435504322960265
Training loss: 0.5646573901176453 / Valid loss: 6.956211171831403
Training loss: 0.4884704649448395 / Valid loss: 7.006623177301316

Epoch: 35
Training loss: 0.7788835763931274 / Valid loss: 6.867368875231062
Training loss: 0.903814435005188 / Valid loss: 7.046200139181955
Training loss: 0.615138590335846 / Valid loss: 7.1332708313351585
Training loss: 0.6719610691070557 / Valid loss: 7.083827652250018
Training loss: 0.9001591205596924 / Valid loss: 6.9415259997049965

Epoch: 36
Training loss: 0.5901211500167847 / Valid loss: 7.061212839399065
Training loss: 0.8407654166221619 / Valid loss: 7.081104823521206
Training loss: 0.7535384297370911 / Valid loss: 6.788576098850795
Training loss: 0.5110756158828735 / Valid loss: 7.690123074395316
Training loss: 0.6616064310073853 / Valid loss: 6.907111874080839

Epoch: 37
Training loss: 0.9047057628631592 / Valid loss: 7.169668978736514
Training loss: 0.5354027152061462 / Valid loss: 7.558452510833741
Training loss: 1.247069239616394 / Valid loss: 6.90187117485773
Training loss: 0.7098245620727539 / Valid loss: 7.33754932085673
Training loss: 0.7953761219978333 / Valid loss: 7.194965541930426

Epoch: 38
Training loss: 0.5323144197463989 / Valid loss: 7.2106214614141555
Training loss: 0.47390440106391907 / Valid loss: 7.039488088516962
Training loss: 0.6751548051834106 / Valid loss: 7.12604375566755
Training loss: 0.5812765955924988 / Valid loss: 6.9901166439056395
Training loss: 0.6368409991264343 / Valid loss: 7.195389529636928

Epoch: 39
Training loss: 0.9675420522689819 / Valid loss: 6.865316463652111
Training loss: 0.7312985062599182 / Valid loss: 6.915024076189313
Training loss: 0.41187766194343567 / Valid loss: 7.125297718956357
Training loss: 0.4884580969810486 / Valid loss: 6.9966564450945175
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2000): 6.049951358068557
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.448760986328125 / Valid loss: 15.706126085917155
Model is saved in epoch 0, overall batch: 0
Training loss: 6.890539169311523 / Valid loss: 9.661187271844772
Model is saved in epoch 0, overall batch: 100
Training loss: 4.65123176574707 / Valid loss: 8.209703132084437
Model is saved in epoch 0, overall batch: 200
Training loss: 3.895819902420044 / Valid loss: 6.372970867156982
Model is saved in epoch 0, overall batch: 300
Training loss: 4.154671669006348 / Valid loss: 7.5844438348497665

Epoch: 1
Training loss: 4.096489906311035 / Valid loss: 7.213136441367013
Training loss: 5.171797752380371 / Valid loss: 7.832309445880708
Training loss: 5.941662788391113 / Valid loss: 7.058169719151088
Training loss: 4.908956527709961 / Valid loss: 7.021065287362962
Training loss: 4.045224666595459 / Valid loss: 6.657788172222319

Epoch: 2
Training loss: 3.241790294647217 / Valid loss: 6.902282299314226
Training loss: 2.6625189781188965 / Valid loss: 6.680627141680036
Training loss: 3.730377674102783 / Valid loss: 6.531135917845226
Training loss: 3.5713515281677246 / Valid loss: 6.811313554218837
Training loss: 3.9606029987335205 / Valid loss: 6.1477440743219285
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 2.2696070671081543 / Valid loss: 6.222709660303025
Training loss: 5.011468887329102 / Valid loss: 6.481693390437535
Training loss: 3.2870230674743652 / Valid loss: 7.501323600042434
Training loss: 4.077034950256348 / Valid loss: 6.324619801839193
Training loss: 4.3765411376953125 / Valid loss: 6.527521249226162

Epoch: 4
Training loss: 2.5355305671691895 / Valid loss: 6.365808125904628
Training loss: 3.0472373962402344 / Valid loss: 6.715546828224546
Training loss: 2.049197196960449 / Valid loss: 6.7060070264907115
Training loss: 2.225205421447754 / Valid loss: 6.454243271691459
Training loss: 3.6748886108398438 / Valid loss: 6.318069560187204

Epoch: 5
Training loss: 2.13352108001709 / Valid loss: 6.588066250937326
Training loss: 2.2191123962402344 / Valid loss: 6.28523652667091
Training loss: 2.701087474822998 / Valid loss: 6.612991986955915
Training loss: 2.739950180053711 / Valid loss: 6.508457533518473
Training loss: 2.1242289543151855 / Valid loss: 6.776236173084804

Epoch: 6
Training loss: 1.668042778968811 / Valid loss: 6.652455981572469
Training loss: 2.7691378593444824 / Valid loss: 7.217098735627674
Training loss: 1.9190781116485596 / Valid loss: 6.534707755134219
Training loss: 2.0024006366729736 / Valid loss: 6.647466303053356
Training loss: 1.9109007120132446 / Valid loss: 6.892487687156314

Epoch: 7
Training loss: 1.6661325693130493 / Valid loss: 6.551114922478085
Training loss: 1.9183517694473267 / Valid loss: 6.918796107882545
Training loss: 1.8752626180648804 / Valid loss: 6.498909185046242
Training loss: 1.788633108139038 / Valid loss: 6.489403134300595
Training loss: 2.3862316608428955 / Valid loss: 6.980125717889695

Epoch: 8
Training loss: 1.3389884233474731 / Valid loss: 6.588810807182675
Training loss: 1.7933485507965088 / Valid loss: 6.468528699874878
Training loss: 1.629144549369812 / Valid loss: 6.57511263120742
Training loss: 1.3705310821533203 / Valid loss: 6.48751327196757
Training loss: 2.5130162239074707 / Valid loss: 7.2140835148947575

Epoch: 9
Training loss: 1.4197311401367188 / Valid loss: 6.904582357406616
Training loss: 1.2627427577972412 / Valid loss: 7.145566633769444
Training loss: 1.5365521907806396 / Valid loss: 6.457186642147246
Training loss: 2.5125341415405273 / Valid loss: 6.820330964951288

Epoch: 10
Training loss: 1.6908822059631348 / Valid loss: 6.756133538200742
Training loss: 1.268042802810669 / Valid loss: 6.557733835492815
Training loss: 1.2810046672821045 / Valid loss: 6.632238992055258
Training loss: 1.4213778972625732 / Valid loss: 6.9236237253461566
Training loss: 2.004879951477051 / Valid loss: 6.63320756639753

Epoch: 11
Training loss: 1.2537143230438232 / Valid loss: 6.755351604734148
Training loss: 1.1137208938598633 / Valid loss: 7.438649477277483
Training loss: 1.2252713441848755 / Valid loss: 6.790385564168294
Training loss: 1.2363351583480835 / Valid loss: 6.749091359547206
Training loss: 2.298490285873413 / Valid loss: 6.616286763690767

Epoch: 12
Training loss: 1.0766003131866455 / Valid loss: 6.673088032858712
Training loss: 0.9478240013122559 / Valid loss: 6.962747296832856
Training loss: 0.863037109375 / Valid loss: 7.053818071456183
Training loss: 1.5252338647842407 / Valid loss: 7.507597246624175
Training loss: 1.041552186012268 / Valid loss: 6.675400447845459

Epoch: 13
Training loss: 1.8649508953094482 / Valid loss: 6.748454489026751
Training loss: 0.8735299110412598 / Valid loss: 7.159865663165138
Training loss: 1.502661943435669 / Valid loss: 7.395623350143433
Training loss: 1.187342882156372 / Valid loss: 6.709914822805495
Training loss: 1.4835588932037354 / Valid loss: 6.83336732955206

Epoch: 14
Training loss: 0.8619503974914551 / Valid loss: 6.80112502461388
Training loss: 1.3535597324371338 / Valid loss: 6.806182016645159
Training loss: 1.0532917976379395 / Valid loss: 7.84456661769322
Training loss: 1.3081680536270142 / Valid loss: 7.016971465519497
Training loss: 0.8641349077224731 / Valid loss: 7.001322401137579

Epoch: 15
Training loss: 1.2024405002593994 / Valid loss: 7.154645856221517
Training loss: 0.710111141204834 / Valid loss: 6.963477493467785
Training loss: 1.9583133459091187 / Valid loss: 7.10662625176566
Training loss: 1.4598491191864014 / Valid loss: 7.181952249436152
Training loss: 2.6001381874084473 / Valid loss: 7.07385493687221

Epoch: 16
Training loss: 0.6283815503120422 / Valid loss: 7.323343047641573
Training loss: 1.500966191291809 / Valid loss: 7.398809346698579
Training loss: 1.3788882493972778 / Valid loss: 7.199809594381423
Training loss: 1.3251290321350098 / Valid loss: 7.388865543547131
Training loss: 1.1246567964553833 / Valid loss: 6.68186251776559

Epoch: 17
Training loss: 0.9080334901809692 / Valid loss: 6.791910280500139
Training loss: 0.915806770324707 / Valid loss: 7.167067264375232
Training loss: 0.8995765447616577 / Valid loss: 7.000244935353597
Training loss: 1.4395798444747925 / Valid loss: 6.741684096200125
Training loss: 0.8916397094726562 / Valid loss: 6.830016953604562

Epoch: 18
Training loss: 0.9030358791351318 / Valid loss: 7.0563470658801855
Training loss: 1.5719826221466064 / Valid loss: 6.819583681651524
Training loss: 1.3867042064666748 / Valid loss: 7.231097212291899
Training loss: 0.9238853454589844 / Valid loss: 7.532514263334728
Training loss: 0.7426472902297974 / Valid loss: 7.120591354370117

Epoch: 19
Training loss: 1.1703729629516602 / Valid loss: 7.023571772802444
Training loss: 0.9546381831169128 / Valid loss: 6.877419358208066
Training loss: 0.6037251949310303 / Valid loss: 7.013897441682362
Training loss: 1.136528730392456 / Valid loss: 7.06746271224249

Epoch: 20
Training loss: 0.7727768421173096 / Valid loss: 6.992167622702462
Training loss: 0.6534557342529297 / Valid loss: 6.782795753933135
Training loss: 1.339320182800293 / Valid loss: 6.892014834994361
Training loss: 0.7337405681610107 / Valid loss: 7.061063060306368
Training loss: 0.782833456993103 / Valid loss: 7.099760455176944

Epoch: 21
Training loss: 0.7828288078308105 / Valid loss: 7.326296552022298
Training loss: 0.8637574911117554 / Valid loss: 7.098542372385661
Training loss: 0.643179178237915 / Valid loss: 7.147198502222697
Training loss: 0.7399243116378784 / Valid loss: 7.289398054849534
Training loss: 0.765097975730896 / Valid loss: 6.986377057575044

Epoch: 22
Training loss: 0.6711094975471497 / Valid loss: 7.277496083577474
Training loss: 0.532117486000061 / Valid loss: 7.226833920251756
Training loss: 0.6190050840377808 / Valid loss: 7.013261922200521
Training loss: 1.1918375492095947 / Valid loss: 6.771744046892438
Training loss: 1.2184048891067505 / Valid loss: 7.162645494370233

Epoch: 23
Training loss: 0.6460161805152893 / Valid loss: 7.073689606076195
Training loss: 0.9354667663574219 / Valid loss: 6.8713196345738
Training loss: 1.80973482131958 / Valid loss: 7.879847576504662
Training loss: 0.6588289737701416 / Valid loss: 6.925172278994606
Training loss: 0.6619203090667725 / Valid loss: 6.820029515311831

Epoch: 24
Training loss: 0.5764742493629456 / Valid loss: 7.178465148380824
Training loss: 0.9226056337356567 / Valid loss: 7.459646215892973
Training loss: 1.2098002433776855 / Valid loss: 7.291312417529878
Training loss: 0.9768650531768799 / Valid loss: 7.035801551455543
Training loss: 0.7913349270820618 / Valid loss: 7.397037619636173

Epoch: 25
Training loss: 1.0601773262023926 / Valid loss: 7.110468809945242
Training loss: 0.5659735798835754 / Valid loss: 7.062912327902658
Training loss: 0.8997548222541809 / Valid loss: 7.149249290284656
Training loss: 0.9811450242996216 / Valid loss: 7.49475417137146
Training loss: 1.0260818004608154 / Valid loss: 7.134169928232828

Epoch: 26
Training loss: 0.4855788052082062 / Valid loss: 7.032872052419753
Training loss: 0.8807849884033203 / Valid loss: 7.0839612007141115
Training loss: 0.7345726490020752 / Valid loss: 7.301819624219622
Training loss: 0.7591539025306702 / Valid loss: 6.984340940202985
Training loss: 0.781490683555603 / Valid loss: 7.137864712306431

Epoch: 27
Training loss: 0.6087375283241272 / Valid loss: 7.021975683030628
Training loss: 0.7618036270141602 / Valid loss: 7.189928563435872
Training loss: 1.055816650390625 / Valid loss: 7.068867410932269
Training loss: 0.8143085241317749 / Valid loss: 7.151336011432466
Training loss: 0.76801598072052 / Valid loss: 7.591537489209856

Epoch: 28
Training loss: 0.6361346244812012 / Valid loss: 6.845238338197981
Training loss: 0.6254512071609497 / Valid loss: 7.206827499752953
Training loss: 1.3073773384094238 / Valid loss: 7.947261347089495
Training loss: 0.5474965572357178 / Valid loss: 7.19509531656901
Training loss: 0.634925127029419 / Valid loss: 6.977447954813639

Epoch: 29
Training loss: 0.5957015752792358 / Valid loss: 6.935308488210042
Training loss: 0.6059203147888184 / Valid loss: 7.374567099979946
Training loss: 0.7605857849121094 / Valid loss: 7.020545269194104
Training loss: 1.1935086250305176 / Valid loss: 7.556238083612351

Epoch: 30
Training loss: 0.8872272968292236 / Valid loss: 7.082076588131133
Training loss: 0.6009435057640076 / Valid loss: 7.4101330825260705
Training loss: 0.7579128742218018 / Valid loss: 6.871645977383568
Training loss: 0.8982366919517517 / Valid loss: 7.598736413319906
Training loss: 0.8285447359085083 / Valid loss: 7.105099326088315

Epoch: 31
Training loss: 1.0578749179840088 / Valid loss: 7.265027077992757
Training loss: 0.5693609118461609 / Valid loss: 7.0653281529744465
Training loss: 0.6557076573371887 / Valid loss: 7.4101306915283205
Training loss: 0.6880950331687927 / Valid loss: 7.1140411149887814
Training loss: 0.7000505924224854 / Valid loss: 7.049296851385208

Epoch: 32
Training loss: 0.7084257006645203 / Valid loss: 6.962121250515892
Training loss: 0.7226331233978271 / Valid loss: 7.14140530086699
Training loss: 0.6352847814559937 / Valid loss: 7.1633170740945
Training loss: 1.046371340751648 / Valid loss: 7.250693825313023
Training loss: 0.374617338180542 / Valid loss: 6.93825642267863

Epoch: 33
Training loss: 0.5653923153877258 / Valid loss: 7.253479903084891
Training loss: 0.7005953788757324 / Valid loss: 7.298323683511644
Training loss: 0.7165948152542114 / Valid loss: 7.059986754826137
Training loss: 0.29899516701698303 / Valid loss: 6.909680539085752
Training loss: 0.6378843784332275 / Valid loss: 7.239147081829253

Epoch: 34
Training loss: 1.160522699356079 / Valid loss: 7.077367700849261
Training loss: 0.5081903338432312 / Valid loss: 7.1262944857279455
Training loss: 0.785808801651001 / Valid loss: 7.47711421421596
Training loss: 0.6049056053161621 / Valid loss: 7.179306438991001
Training loss: 0.5024545788764954 / Valid loss: 6.9931597300938195

Epoch: 35
Training loss: 0.5766445398330688 / Valid loss: 7.05172317595709
Training loss: 0.913033664226532 / Valid loss: 7.200625474112375
Training loss: 0.8426501750946045 / Valid loss: 7.140844472249349
Training loss: 0.45045584440231323 / Valid loss: 6.989802583058675
Training loss: 0.9296638369560242 / Valid loss: 6.994309743245442

Epoch: 36
Training loss: 0.4752614498138428 / Valid loss: 7.251161666143508
Training loss: 0.8881124258041382 / Valid loss: 7.363427593594506
Training loss: 0.5972984433174133 / Valid loss: 6.938822673615955
Training loss: 0.47191962599754333 / Valid loss: 7.420589278993153
Training loss: 0.8091015219688416 / Valid loss: 7.390584659576416

Epoch: 37
Training loss: 0.9296634197235107 / Valid loss: 7.184568818410238
Training loss: 0.5186128616333008 / Valid loss: 7.249404455366589
Training loss: 1.1794259548187256 / Valid loss: 6.949448040553501
Training loss: 1.0513386726379395 / Valid loss: 7.091382321857271
Training loss: 0.6760857701301575 / Valid loss: 7.2607841900416785

Epoch: 38
Training loss: 0.7287738919258118 / Valid loss: 7.270136737823487
Training loss: 0.4959249794483185 / Valid loss: 7.123664996737525
Training loss: 0.6925765872001648 / Valid loss: 7.417569242204938
Training loss: 0.6253228187561035 / Valid loss: 7.146737130482991
Training loss: 0.6084889769554138 / Valid loss: 7.153584176018125

Epoch: 39
Training loss: 0.9304218888282776 / Valid loss: 6.895582905269804
Training loss: 0.8407484292984009 / Valid loss: 6.89701323963347
Training loss: 0.5339077711105347 / Valid loss: 7.233424731663296
Training loss: 0.4733496904373169 / Valid loss: 7.091227949233282
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1400): 5.982483604976109
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.259333610534668 / Valid loss: 16.36912917182559
Model is saved in epoch 0, overall batch: 0
Training loss: 7.1007914543151855 / Valid loss: 6.49007484345209
Model is saved in epoch 0, overall batch: 100
Training loss: 4.949613571166992 / Valid loss: 6.030377197265625
Model is saved in epoch 0, overall batch: 200
Training loss: 3.988673210144043 / Valid loss: 6.389976864769345
Training loss: 5.679952144622803 / Valid loss: 6.504121201378958

Epoch: 1
Training loss: 5.140875816345215 / Valid loss: 8.315560963040307
Training loss: 4.015322685241699 / Valid loss: 7.516069848196847
Training loss: 3.8259191513061523 / Valid loss: 6.592014144715809
Training loss: 4.838013648986816 / Valid loss: 6.230399140857515
Training loss: 5.3244194984436035 / Valid loss: 5.9161274523962115
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 2.3709139823913574 / Valid loss: 5.9344990457807265
Training loss: 2.8689632415771484 / Valid loss: 6.34981517791748
Training loss: 3.278181552886963 / Valid loss: 6.1327953633807955
Training loss: 2.2428369522094727 / Valid loss: 6.067248092378889
Training loss: 2.88092041015625 / Valid loss: 6.160893708183652

Epoch: 3
Training loss: 2.1867499351501465 / Valid loss: 6.106396484375
Training loss: 2.3626253604888916 / Valid loss: 6.275686398006621
Training loss: 2.282167911529541 / Valid loss: 6.309563443774269
Training loss: 2.300792694091797 / Valid loss: 6.206449258895147
Training loss: 3.298093795776367 / Valid loss: 7.207615411849249

Epoch: 4
Training loss: 1.7865020036697388 / Valid loss: 6.451532772609165
Training loss: 1.2830935716629028 / Valid loss: 6.412676200412569
Training loss: 1.4345006942749023 / Valid loss: 6.682982222239176
Training loss: 2.747363567352295 / Valid loss: 6.390978397641863
Training loss: 1.4248089790344238 / Valid loss: 6.420751621609642

Epoch: 5
Training loss: 1.1438997983932495 / Valid loss: 6.482637164706276
Training loss: 1.8115553855895996 / Valid loss: 6.468757286525908
Training loss: 0.9336926937103271 / Valid loss: 6.499457318442208
Training loss: 1.845290184020996 / Valid loss: 6.566975911458333
Training loss: 1.8858033418655396 / Valid loss: 6.756297817684355

Epoch: 6
Training loss: 1.1087656021118164 / Valid loss: 6.532407456352598
Training loss: 1.5596559047698975 / Valid loss: 6.805931191217332
Training loss: 1.8673630952835083 / Valid loss: 7.304858707246327
Training loss: 0.8873451948165894 / Valid loss: 6.628103517350697
Training loss: 2.314746379852295 / Valid loss: 6.61799198332287

Epoch: 7
Training loss: 1.0830786228179932 / Valid loss: 6.796721989767892
Training loss: 0.6655288338661194 / Valid loss: 6.661226372491746
Training loss: 1.262969732284546 / Valid loss: 6.677414215178716
Training loss: 1.0302138328552246 / Valid loss: 6.687455404372442
Training loss: 1.143622636795044 / Valid loss: 6.697448171888079

Epoch: 8
Training loss: 0.8318603038787842 / Valid loss: 6.72551634652274
Training loss: 0.9667956829071045 / Valid loss: 6.713156211943853
Training loss: 0.712992787361145 / Valid loss: 6.656599215098789
Training loss: 0.6576616764068604 / Valid loss: 6.827831670216152
Training loss: 0.9670589566230774 / Valid loss: 6.768667452675956

Epoch: 9
Training loss: 0.6817881464958191 / Valid loss: 6.720138325010027
Training loss: 0.4581611156463623 / Valid loss: 6.841921433948335
Training loss: 0.5965158343315125 / Valid loss: 6.691810119719732
Training loss: 0.3855069875717163 / Valid loss: 6.741448093595959

Epoch: 10
Training loss: 0.41856423020362854 / Valid loss: 6.628741475514003
Training loss: 0.6306754350662231 / Valid loss: 6.917553838094076
Training loss: 0.45307567715644836 / Valid loss: 7.177558787663778
Training loss: 1.1583139896392822 / Valid loss: 6.798480092911493
Training loss: 0.6108807921409607 / Valid loss: 6.769464265732538

Epoch: 11
Training loss: 0.34336569905281067 / Valid loss: 6.8210981005714055
Training loss: 0.5599323511123657 / Valid loss: 6.756401332219442
Training loss: 0.3165076971054077 / Valid loss: 7.319014535631452
Training loss: 0.5059919953346252 / Valid loss: 6.880319908687047
Training loss: 0.5803744792938232 / Valid loss: 6.753576208296276

Epoch: 12
Training loss: 0.47553786635398865 / Valid loss: 6.844068945021856
Training loss: 0.4473282992839813 / Valid loss: 6.729422741844541
Training loss: 0.8124035596847534 / Valid loss: 6.694142116819109
Training loss: 0.4790823459625244 / Valid loss: 6.782257343473889
Training loss: 0.41318655014038086 / Valid loss: 6.769181033543178

Epoch: 13
Training loss: 0.5874452590942383 / Valid loss: 6.983023502713158
Training loss: 0.2950495779514313 / Valid loss: 6.837919811975389
Training loss: 0.46394553780555725 / Valid loss: 6.75121873219808
Training loss: 0.27630388736724854 / Valid loss: 6.709622787293934
Training loss: 0.8186517357826233 / Valid loss: 6.6731001626877555

Epoch: 14
Training loss: 0.39785170555114746 / Valid loss: 6.813760721115839
Training loss: 0.33535236120224 / Valid loss: 6.838597333998907
Training loss: 0.6425836086273193 / Valid loss: 7.366675311043149
Training loss: 0.4500066936016083 / Valid loss: 6.871221605936686
Training loss: 0.3509218096733093 / Valid loss: 6.745265161423456

Epoch: 15
Training loss: 0.5953339338302612 / Valid loss: 7.058011752083188
Training loss: 0.35529398918151855 / Valid loss: 6.935349419003441
Training loss: 0.4434853196144104 / Valid loss: 6.758992549351284
Training loss: 0.7056445479393005 / Valid loss: 6.924341278984433
Training loss: 0.32431674003601074 / Valid loss: 6.82429270971389

Epoch: 16
Training loss: 0.45186346769332886 / Valid loss: 6.8996767180306575
Training loss: 0.45902326703071594 / Valid loss: 7.051702426728748
Training loss: 0.2912953794002533 / Valid loss: 6.845059222266787
Training loss: 0.45845723152160645 / Valid loss: 6.8963228021349225
Training loss: 0.4358406662940979 / Valid loss: 6.776915109725225

Epoch: 17
Training loss: 0.33454686403274536 / Valid loss: 6.788824814841861
Training loss: 0.24358627200126648 / Valid loss: 6.788073380788167
Training loss: 0.33747783303260803 / Valid loss: 6.891449324289957
Training loss: 0.3821417987346649 / Valid loss: 6.8235771951221285
Training loss: 0.34291234612464905 / Valid loss: 6.939803786504836

Epoch: 18
Training loss: 0.43215152621269226 / Valid loss: 7.201699960799444
Training loss: 0.5858746767044067 / Valid loss: 6.8168551558539985
Training loss: 0.7737001776695251 / Valid loss: 6.946676860536848
Training loss: 0.3650397062301636 / Valid loss: 6.756357928684779
Training loss: 0.3361383378505707 / Valid loss: 6.875693112327939

Epoch: 19
Training loss: 0.5667504668235779 / Valid loss: 6.961126268477667
Training loss: 0.34659141302108765 / Valid loss: 6.821725822630382
Training loss: 0.49813514947891235 / Valid loss: 6.753939310709636
Training loss: 0.34432971477508545 / Valid loss: 6.763210587274461

Epoch: 20
Training loss: 0.375241219997406 / Valid loss: 6.8419644719078425
Training loss: 0.418634295463562 / Valid loss: 6.832098429543631
Training loss: 0.29556524753570557 / Valid loss: 6.777380716233027
Training loss: 0.2863622009754181 / Valid loss: 6.786166697456723
Training loss: 0.3475145399570465 / Valid loss: 6.6688554582141695

Epoch: 21
Training loss: 0.2093939483165741 / Valid loss: 7.034864845730009
Training loss: 0.3211385905742645 / Valid loss: 6.7846558570861815
Training loss: 0.6022888422012329 / Valid loss: 6.910147296814691
Training loss: 0.4772012531757355 / Valid loss: 6.8150688171386715
Training loss: 0.4303872585296631 / Valid loss: 7.301674061729795

Epoch: 22
Training loss: 0.4203468859195709 / Valid loss: 6.844072909582229
Training loss: 0.18863412737846375 / Valid loss: 6.904582568577358
Training loss: 0.4315587282180786 / Valid loss: 6.973779760088239
Training loss: 0.35090601444244385 / Valid loss: 6.84840034303211
Training loss: 0.2653559446334839 / Valid loss: 6.8205856187003

Epoch: 23
Training loss: 0.28022098541259766 / Valid loss: 6.787727360498337
Training loss: 0.28022998571395874 / Valid loss: 6.928704643249512
Training loss: 0.341846764087677 / Valid loss: 6.854914626621064
Training loss: 0.4417381286621094 / Valid loss: 6.8053570724668955
Training loss: 0.2510852813720703 / Valid loss: 6.951556821096511

Epoch: 24
Training loss: 0.26079872250556946 / Valid loss: 6.841576558067685
Training loss: 0.3161674737930298 / Valid loss: 6.775215702965146
Training loss: 0.6531252861022949 / Valid loss: 6.812923322405134
Training loss: 0.2758519649505615 / Valid loss: 6.8991388366335915
Training loss: 0.30525511503219604 / Valid loss: 6.927718021756127

Epoch: 25
Training loss: 0.4482247233390808 / Valid loss: 6.908046141124907
Training loss: 0.3044523596763611 / Valid loss: 6.848413744426909
Training loss: 0.3050282597541809 / Valid loss: 6.850602281661261
Training loss: 0.2762264609336853 / Valid loss: 6.835538346426827
Training loss: 0.21387669444084167 / Valid loss: 6.833589640117827

Epoch: 26
Training loss: 0.3477429747581482 / Valid loss: 6.809264146713984
Training loss: 0.4049074351787567 / Valid loss: 6.837898885636102
Training loss: 0.2074543982744217 / Valid loss: 6.843226507731846
Training loss: 0.3150363266468048 / Valid loss: 7.112994075956799
Training loss: 0.46006160974502563 / Valid loss: 6.83149528503418

Epoch: 27
Training loss: 0.5650789141654968 / Valid loss: 6.801273205166772
Training loss: 0.406432181596756 / Valid loss: 6.764076280593872
Training loss: 0.2040335237979889 / Valid loss: 6.854847251801264
Training loss: 0.16482782363891602 / Valid loss: 6.793713873908633
Training loss: 0.22226907312870026 / Valid loss: 6.938351177033924

Epoch: 28
Training loss: 0.21065570414066315 / Valid loss: 6.8299453054155626
Training loss: 0.37444987893104553 / Valid loss: 6.948992611113049
Training loss: 0.4946112334728241 / Valid loss: 6.785971518925258
Training loss: 0.35486316680908203 / Valid loss: 6.976960177648635
Training loss: 0.23771081864833832 / Valid loss: 6.873619429270426

Epoch: 29
Training loss: 0.7247276306152344 / Valid loss: 7.04153352919079
Training loss: 0.16497057676315308 / Valid loss: 6.8368161587488085
Training loss: 0.3781743049621582 / Valid loss: 6.760604699452718
Training loss: 0.45798298716545105 / Valid loss: 6.791371640704927

Epoch: 30
Training loss: 0.5052330493927002 / Valid loss: 6.879114832196917
Training loss: 0.40211427211761475 / Valid loss: 6.877062529609317
Training loss: 0.3665836453437805 / Valid loss: 6.879903134845552
Training loss: 0.6031038165092468 / Valid loss: 6.7771563098544165
Training loss: 0.18803243339061737 / Valid loss: 6.79404882249378

Epoch: 31
Training loss: 0.22201178967952728 / Valid loss: 7.108677023933048
Training loss: 0.2466612011194229 / Valid loss: 6.892106165204729
Training loss: 0.16943952441215515 / Valid loss: 6.808579442614601
Training loss: 0.3967006802558899 / Valid loss: 6.819849091484433
Training loss: 0.4876612424850464 / Valid loss: 6.774162744340442

Epoch: 32
Training loss: 0.2179487943649292 / Valid loss: 6.938426158541724
Training loss: 0.17966315150260925 / Valid loss: 6.790546648842948
Training loss: 0.2038685381412506 / Valid loss: 6.811749505996704
Training loss: 0.16812172532081604 / Valid loss: 6.867161310286749
Training loss: 0.42612430453300476 / Valid loss: 6.858673817770821

Epoch: 33
Training loss: 0.6618191003799438 / Valid loss: 6.930226662045434
Training loss: 0.2996799349784851 / Valid loss: 6.895044817243304
Training loss: 0.2426527738571167 / Valid loss: 6.821443948291597
Training loss: 0.2589399814605713 / Valid loss: 6.824554906572614
Training loss: 0.2322915494441986 / Valid loss: 6.815357680547805

Epoch: 34
Training loss: 0.17800982296466827 / Valid loss: 6.807978380294073
Training loss: 0.4623047113418579 / Valid loss: 6.836433260781424
Training loss: 0.3161935806274414 / Valid loss: 6.970910612742106
Training loss: 0.41842323541641235 / Valid loss: 6.795483966100783
Training loss: 0.19543230533599854 / Valid loss: 6.782236807686942

Epoch: 35
Training loss: 0.29729485511779785 / Valid loss: 6.89652822585333
Training loss: 0.26890528202056885 / Valid loss: 6.8368105070931575
Training loss: 0.32519251108169556 / Valid loss: 6.820458312261672
Training loss: 0.143440380692482 / Valid loss: 6.80333194732666
Training loss: 0.3687378168106079 / Valid loss: 6.883892624718802

Epoch: 36
Training loss: 0.1763184368610382 / Valid loss: 6.845488829839797
Training loss: 0.2355707585811615 / Valid loss: 6.8204896381923135
Training loss: 0.2298465371131897 / Valid loss: 6.788106196267265
Training loss: 0.26823166012763977 / Valid loss: 6.856078701927548
Training loss: 0.21783481538295746 / Valid loss: 6.877879206339518

Epoch: 37
Training loss: 0.18623687326908112 / Valid loss: 6.7645515532720655
Training loss: 1.063665747642517 / Valid loss: 6.9730254082452685
Training loss: 0.24434798955917358 / Valid loss: 6.802776359376454
Training loss: 0.20896515250205994 / Valid loss: 6.80771267754691
Training loss: 0.29910367727279663 / Valid loss: 6.879916499909901

Epoch: 38
Training loss: 0.251578688621521 / Valid loss: 6.86994575318836
Training loss: 0.22435404360294342 / Valid loss: 6.825067016056606
Training loss: 0.3906904458999634 / Valid loss: 6.9919355029151555
Training loss: 0.21266499161720276 / Valid loss: 6.794718519846598
Training loss: 0.456555038690567 / Valid loss: 6.848751990000407

Epoch: 39
Training loss: 0.23793062567710876 / Valid loss: 6.951316810789562
Training loss: 0.2757362723350525 / Valid loss: 6.795279060091291
Training loss: 0.36299824714660645 / Valid loss: 6.7113572756449384
Training loss: 0.4107831120491028 / Valid loss: 6.831346064522153
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 900): 5.755787649608794
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.259333610534668 / Valid loss: 16.36912917182559
Model is saved in epoch 0, overall batch: 0
Training loss: 7.100890159606934 / Valid loss: 6.490067986079625
Model is saved in epoch 0, overall batch: 100
Training loss: 4.949953556060791 / Valid loss: 6.025431705656506
Model is saved in epoch 0, overall batch: 200
Training loss: 4.000515937805176 / Valid loss: 6.349189454033262
Training loss: 5.650411605834961 / Valid loss: 6.625817750749134

Epoch: 1
Training loss: 5.127540588378906 / Valid loss: 8.368601281302317
Training loss: 3.9976282119750977 / Valid loss: 7.4905172620500835
Training loss: 3.8165111541748047 / Valid loss: 6.370307481856573
Training loss: 4.934202194213867 / Valid loss: 6.211567410968599
Training loss: 5.252541542053223 / Valid loss: 5.929729654675438
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 2.3286213874816895 / Valid loss: 5.869074469520932
Model is saved in epoch 2, overall batch: 1000
Training loss: 2.875460147857666 / Valid loss: 6.425258348101662
Training loss: 3.2995810508728027 / Valid loss: 6.342558844884237
Training loss: 2.330626964569092 / Valid loss: 6.073848740259806
Training loss: 2.9190902709960938 / Valid loss: 6.118291337149484

Epoch: 3
Training loss: 2.382147789001465 / Valid loss: 6.153818523316156
Training loss: 2.461857318878174 / Valid loss: 6.2433425857907245
Training loss: 2.292283058166504 / Valid loss: 6.381796911784581
Training loss: 2.2517688274383545 / Valid loss: 6.210887927100772
Training loss: 3.296772003173828 / Valid loss: 7.500841290610177

Epoch: 4
Training loss: 1.705385446548462 / Valid loss: 6.385028848193941
Training loss: 1.3287664651870728 / Valid loss: 6.402332349050613
Training loss: 1.360162615776062 / Valid loss: 6.858433655330113
Training loss: 2.7632956504821777 / Valid loss: 6.361515998840332
Training loss: 1.4632712602615356 / Valid loss: 6.558577796391078

Epoch: 5
Training loss: 1.1730141639709473 / Valid loss: 6.506991320564634
Training loss: 1.9472500085830688 / Valid loss: 6.512348844891503
Training loss: 1.0306994915008545 / Valid loss: 6.529317733219692
Training loss: 1.9295036792755127 / Valid loss: 6.504767492839268
Training loss: 1.8004202842712402 / Valid loss: 6.7837524777367

Epoch: 6
Training loss: 1.117559552192688 / Valid loss: 6.517806039537702
Training loss: 1.6245678663253784 / Valid loss: 7.257887454259963
Training loss: 1.9735572338104248 / Valid loss: 6.938621511913481
Training loss: 0.8979705572128296 / Valid loss: 6.590460257303147
Training loss: 2.490154504776001 / Valid loss: 6.583582144691831

Epoch: 7
Training loss: 1.1531506776809692 / Valid loss: 6.733546615782238
Training loss: 0.6889451146125793 / Valid loss: 6.718236101241339
Training loss: 1.110811710357666 / Valid loss: 6.658556920006162
Training loss: 1.0674293041229248 / Valid loss: 6.851546578180223
Training loss: 1.2191047668457031 / Valid loss: 6.6971462567647295

Epoch: 8
Training loss: 0.8940421342849731 / Valid loss: 6.761787519000825
Training loss: 1.0134327411651611 / Valid loss: 6.702470702216739
Training loss: 0.7042489051818848 / Valid loss: 6.770892719995408
Training loss: 0.6691510677337646 / Valid loss: 6.7382885978335425
Training loss: 0.8689976930618286 / Valid loss: 6.665978947139922

Epoch: 9
Training loss: 0.7258273959159851 / Valid loss: 6.762069720313662
Training loss: 0.48720043897628784 / Valid loss: 6.799856145041329
Training loss: 0.6108014583587646 / Valid loss: 6.6735406262534
Training loss: 0.3584126830101013 / Valid loss: 6.743606603713262

Epoch: 10
Training loss: 0.4932207465171814 / Valid loss: 6.701485699699038
Training loss: 0.603746771812439 / Valid loss: 6.793137277875628
Training loss: 0.4503413140773773 / Valid loss: 7.0538663546244305
Training loss: 1.229205846786499 / Valid loss: 6.711852314358666
Training loss: 0.6072841882705688 / Valid loss: 6.756834009715489

Epoch: 11
Training loss: 0.32804208993911743 / Valid loss: 6.816674881889707
Training loss: 0.5417413115501404 / Valid loss: 6.707427174704415
Training loss: 0.32679861783981323 / Valid loss: 7.354137929280599
Training loss: 0.4856473207473755 / Valid loss: 6.783399904341925
Training loss: 0.5516257286071777 / Valid loss: 6.795056411198208

Epoch: 12
Training loss: 0.47004640102386475 / Valid loss: 6.923784146990095
Training loss: 0.4270002543926239 / Valid loss: 6.737527004877726
Training loss: 0.7940303087234497 / Valid loss: 6.706748776208787
Training loss: 0.5030805468559265 / Valid loss: 6.697549976621356
Training loss: 0.40791088342666626 / Valid loss: 6.807162664050147

Epoch: 13
Training loss: 0.5897507667541504 / Valid loss: 7.126319422040667
Training loss: 0.30017340183258057 / Valid loss: 6.97502878279913
Training loss: 0.4506405293941498 / Valid loss: 6.7001143637157625
Training loss: 0.32526955008506775 / Valid loss: 6.753899365379697
Training loss: 0.9106902480125427 / Valid loss: 6.718659705207461

Epoch: 14
Training loss: 0.42213910818099976 / Valid loss: 6.812119579315185
Training loss: 0.3316505551338196 / Valid loss: 6.877749265943255
Training loss: 0.7304860353469849 / Valid loss: 7.62634551184518
Training loss: 0.4620162844657898 / Valid loss: 6.7706269264221195
Training loss: 0.3532695770263672 / Valid loss: 6.707685093652635

Epoch: 15
Training loss: 0.6218134164810181 / Valid loss: 6.936553462346395
Training loss: 0.3822188079357147 / Valid loss: 6.891838673182896
Training loss: 0.4919087290763855 / Valid loss: 6.93885308220273
Training loss: 0.5951540470123291 / Valid loss: 6.907497115362258
Training loss: 0.31765803694725037 / Valid loss: 6.818922887529646

Epoch: 16
Training loss: 0.40558895468711853 / Valid loss: 7.068406195867629
Training loss: 0.42956459522247314 / Valid loss: 6.9367124966212685
Training loss: 0.30017679929733276 / Valid loss: 6.840429907753354
Training loss: 0.4915294051170349 / Valid loss: 6.846704219636463
Training loss: 0.4669945240020752 / Valid loss: 6.7758709362574985

Epoch: 17
Training loss: 0.23315022885799408 / Valid loss: 6.77766458874657
Training loss: 0.24196025729179382 / Valid loss: 6.775477640969413
Training loss: 0.3770504593849182 / Valid loss: 6.8446740014212475
Training loss: 0.37079501152038574 / Valid loss: 6.837545229139782
Training loss: 0.39507150650024414 / Valid loss: 6.9305175645010815

Epoch: 18
Training loss: 0.38866090774536133 / Valid loss: 6.887349841708229
Training loss: 0.5117419362068176 / Valid loss: 6.8246443021865115
Training loss: 0.7029781341552734 / Valid loss: 6.913717024666923
Training loss: 0.3440573811531067 / Valid loss: 6.840953415916079
Training loss: 0.34638532996177673 / Valid loss: 6.9033058121090844

Epoch: 19
Training loss: 0.5898057222366333 / Valid loss: 6.797203484035673
Training loss: 0.3248175382614136 / Valid loss: 6.795089022318522
Training loss: 0.540669858455658 / Valid loss: 6.755299050467355
Training loss: 0.3320121169090271 / Valid loss: 6.888162208738781

Epoch: 20
Training loss: 0.3546462059020996 / Valid loss: 6.828954901014056
Training loss: 0.4119454026222229 / Valid loss: 6.846137446448917
Training loss: 0.3374772071838379 / Valid loss: 6.816635654086158
Training loss: 0.31633782386779785 / Valid loss: 6.770789618719192
Training loss: 0.3389309048652649 / Valid loss: 6.787948113396054

Epoch: 21
Training loss: 0.20646080374717712 / Valid loss: 6.981568234307425
Training loss: 0.31109005212783813 / Valid loss: 6.791179030282157
Training loss: 0.5950382947921753 / Valid loss: 6.829658381144205
Training loss: 0.40675243735313416 / Valid loss: 6.805681996118455
Training loss: 0.46419692039489746 / Valid loss: 6.998659227007911

Epoch: 22
Training loss: 0.3825773596763611 / Valid loss: 6.93339433670044
Training loss: 0.18730616569519043 / Valid loss: 6.938158807300386
Training loss: 0.46631282567977905 / Valid loss: 7.0263648691631495
Training loss: 0.3392028212547302 / Valid loss: 6.811113234928676
Training loss: 0.2846772074699402 / Valid loss: 6.852535461244129

Epoch: 23
Training loss: 0.23663893342018127 / Valid loss: 6.772377150399344
Training loss: 0.3188619017601013 / Valid loss: 6.874267001379104
Training loss: 0.32176411151885986 / Valid loss: 6.809643738610404
Training loss: 0.5095240473747253 / Valid loss: 6.790625894637335
Training loss: 0.23787282407283783 / Valid loss: 7.0281677859170095

Epoch: 24
Training loss: 0.26420754194259644 / Valid loss: 6.870588652292887
Training loss: 0.2993415594100952 / Valid loss: 6.784144238063267
Training loss: 0.6855770349502563 / Valid loss: 6.746643847510928
Training loss: 0.2564631700515747 / Valid loss: 6.895654523940314
Training loss: 0.26807060837745667 / Valid loss: 7.007383837018694

Epoch: 25
Training loss: 0.438051700592041 / Valid loss: 6.811116940634591
Training loss: 0.2899519205093384 / Valid loss: 6.830443214234852
Training loss: 0.306232750415802 / Valid loss: 6.859147780282157
Training loss: 0.2818872928619385 / Valid loss: 6.840418229784285
Training loss: 0.22498038411140442 / Valid loss: 6.873952447800409

Epoch: 26
Training loss: 0.34146636724472046 / Valid loss: 6.855786505199614
Training loss: 0.4423121511936188 / Valid loss: 6.838276404426211
Training loss: 0.22831761837005615 / Valid loss: 6.808371210098267
Training loss: 0.35876619815826416 / Valid loss: 7.048194394792829
Training loss: 0.49404600262641907 / Valid loss: 6.7777845791408

Epoch: 27
Training loss: 0.5580245852470398 / Valid loss: 6.802168998264131
Training loss: 0.4253808259963989 / Valid loss: 6.80211633727664
Training loss: 0.22078672051429749 / Valid loss: 6.898652953193301
Training loss: 0.19693070650100708 / Valid loss: 6.784427586055937
Training loss: 0.16772975027561188 / Valid loss: 6.884498378208705

Epoch: 28
Training loss: 0.24644820392131805 / Valid loss: 6.8051634107317245
Training loss: 0.3449990153312683 / Valid loss: 6.916420414334252
Training loss: 0.5886842012405396 / Valid loss: 6.788756561279297
Training loss: 0.3087928891181946 / Valid loss: 6.9499261243002755
Training loss: 0.2324579954147339 / Valid loss: 6.860848451796032

Epoch: 29
Training loss: 0.7468366622924805 / Valid loss: 7.1121799196515765
Training loss: 0.18998612463474274 / Valid loss: 6.844005952562605
Training loss: 0.3831719160079956 / Valid loss: 6.729433291299003
Training loss: 0.4649585485458374 / Valid loss: 6.793005970546178

Epoch: 30
Training loss: 0.4581983983516693 / Valid loss: 6.980145885830834
Training loss: 0.3764418363571167 / Valid loss: 6.807765211377825
Training loss: 0.4194318652153015 / Valid loss: 6.842940048944382
Training loss: 0.6676746606826782 / Valid loss: 6.784186363220215
Training loss: 0.1820567548274994 / Valid loss: 6.810362166450137

Epoch: 31
Training loss: 0.2187737673521042 / Valid loss: 6.855126476287841
Training loss: 0.28686392307281494 / Valid loss: 6.784166015897479
Training loss: 0.1845337152481079 / Valid loss: 6.805718035925002
Training loss: 0.3995053172111511 / Valid loss: 6.795892860775902
Training loss: 0.4996120035648346 / Valid loss: 6.769281428200858

Epoch: 32
Training loss: 0.18769201636314392 / Valid loss: 6.871660831996373
Training loss: 0.18118628859519958 / Valid loss: 6.7718292917524066
Training loss: 0.21980269253253937 / Valid loss: 6.831200088773455
Training loss: 0.18910814821720123 / Valid loss: 6.800550699234009
Training loss: 0.4614267945289612 / Valid loss: 6.8407371112278526

Epoch: 33
Training loss: 0.5713893175125122 / Valid loss: 6.7873856090364
Training loss: 0.3012896776199341 / Valid loss: 6.876767113095238
Training loss: 0.2586289942264557 / Valid loss: 6.804496717453003
Training loss: 0.2567017078399658 / Valid loss: 6.835614844730922
Training loss: 0.2675965428352356 / Valid loss: 6.793246564410982

Epoch: 34
Training loss: 0.17182320356369019 / Valid loss: 6.755208932785761
Training loss: 0.4733745753765106 / Valid loss: 6.878373918079195
Training loss: 0.3237670063972473 / Valid loss: 6.9662495113554455
Training loss: 0.398794949054718 / Valid loss: 6.856767917814709
Training loss: 0.1684763878583908 / Valid loss: 6.926049459548223

Epoch: 35
Training loss: 0.27854660153388977 / Valid loss: 6.766201959337507
Training loss: 0.301494300365448 / Valid loss: 6.813134336471558
Training loss: 0.35792970657348633 / Valid loss: 6.806884633927118
Training loss: 0.1541520208120346 / Valid loss: 6.834687033153716
Training loss: 0.3340216279029846 / Valid loss: 6.871820822216216

Epoch: 36
Training loss: 0.17166000604629517 / Valid loss: 6.889332725888207
Training loss: 0.23495787382125854 / Valid loss: 6.766316005161831
Training loss: 0.1878475546836853 / Valid loss: 6.749093114762079
Training loss: 0.26715946197509766 / Valid loss: 6.84561406090146
Training loss: 0.2002078890800476 / Valid loss: 6.954293026242937

Epoch: 37
Training loss: 0.1687849462032318 / Valid loss: 6.7995705286661785
Training loss: 0.9960142374038696 / Valid loss: 6.993113395145961
Training loss: 0.23111678659915924 / Valid loss: 6.8290689695449105
Training loss: 0.19547587633132935 / Valid loss: 6.818921466100783
Training loss: 0.3154151439666748 / Valid loss: 6.796657866523379

Epoch: 38
Training loss: 0.24693363904953003 / Valid loss: 6.836949339367094
Training loss: 0.22658959031105042 / Valid loss: 6.799772498721168
Training loss: 0.4144277572631836 / Valid loss: 6.924328697295416
Training loss: 0.27058422565460205 / Valid loss: 6.788475327264695
Training loss: 0.48726212978363037 / Valid loss: 6.906600793202718

Epoch: 39
Training loss: 0.24457988142967224 / Valid loss: 6.850956340063186
Training loss: 0.2652459740638733 / Valid loss: 6.771121810731434
Training loss: 0.352489709854126 / Valid loss: 6.814328829447429
Training loss: 0.3274669349193573 / Valid loss: 6.862770909354801
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1000): 5.800738341467721
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.74432945251465 / Valid loss: 16.987547265915644
Model is saved in epoch 0, overall batch: 0
Training loss: 18.298664093017578 / Valid loss: 15.51221916562035
Model is saved in epoch 0, overall batch: 100
Training loss: 13.449944496154785 / Valid loss: 14.26864634468442
Model is saved in epoch 0, overall batch: 200
Training loss: 12.476020812988281 / Valid loss: 13.430799080076671
Model is saved in epoch 0, overall batch: 300
Training loss: 14.044178009033203 / Valid loss: 12.718281019301642
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 11.86660099029541 / Valid loss: 12.284618890853155
Model is saved in epoch 1, overall batch: 500
Training loss: 12.915748596191406 / Valid loss: 11.552785469236827
Model is saved in epoch 1, overall batch: 600
Training loss: 8.748652458190918 / Valid loss: 10.520240556626092
Model is saved in epoch 1, overall batch: 700
Training loss: 7.819911479949951 / Valid loss: 10.258843757992699
Model is saved in epoch 1, overall batch: 800
Training loss: 11.052324295043945 / Valid loss: 9.754019973391578
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 8.194683074951172 / Valid loss: 9.205420857384091
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.674378395080566 / Valid loss: 8.433078425271171
Model is saved in epoch 2, overall batch: 1100
Training loss: 7.822020530700684 / Valid loss: 8.289484133039203
Model is saved in epoch 2, overall batch: 1200
Training loss: 4.070475101470947 / Valid loss: 8.212529100690569
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.952649116516113 / Valid loss: 7.702017727352324
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 6.753789901733398 / Valid loss: 7.664767067773002
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.741519451141357 / Valid loss: 7.645556699661982
Model is saved in epoch 3, overall batch: 1600
Training loss: 4.3874311447143555 / Valid loss: 7.374861626397996
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.306049346923828 / Valid loss: 7.2787395931425545
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.536162376403809 / Valid loss: 6.991863055456252
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 5.237165927886963 / Valid loss: 6.9627034300849555
Model is saved in epoch 4, overall batch: 2000
Training loss: 4.581789016723633 / Valid loss: 6.856748340243385
Model is saved in epoch 4, overall batch: 2100
Training loss: 4.731267929077148 / Valid loss: 6.775092202141171
Model is saved in epoch 4, overall batch: 2200
Training loss: 4.099074840545654 / Valid loss: 6.627346256801061
Model is saved in epoch 4, overall batch: 2300
Training loss: 4.401995658874512 / Valid loss: 6.968194259916033

Epoch: 5
Training loss: 2.7668919563293457 / Valid loss: 6.527543830871582
Model is saved in epoch 5, overall batch: 2500
Training loss: 4.132393836975098 / Valid loss: 6.524363095419748
Model is saved in epoch 5, overall batch: 2600
Training loss: 5.247529983520508 / Valid loss: 6.544124759946551
Training loss: 6.057152271270752 / Valid loss: 6.5513325237092515
Training loss: 5.186150550842285 / Valid loss: 6.579523032052176

Epoch: 6
Training loss: 4.175628185272217 / Valid loss: 6.629126943860736
Training loss: 4.2174882888793945 / Valid loss: 6.636503483000256
Training loss: 2.9820339679718018 / Valid loss: 6.452828784216018
Model is saved in epoch 6, overall batch: 3200
Training loss: 4.360167503356934 / Valid loss: 6.398381083352225
Model is saved in epoch 6, overall batch: 3300
Training loss: 4.639245986938477 / Valid loss: 6.3988802546546575

Epoch: 7
Training loss: 4.020491123199463 / Valid loss: 6.505770987556094
Training loss: 4.749550819396973 / Valid loss: 6.595276648657663
Training loss: 2.2244579792022705 / Valid loss: 6.425203037261963
Training loss: 3.10086727142334 / Valid loss: 6.480804629552932
Training loss: 5.6995439529418945 / Valid loss: 6.586355720247541

Epoch: 8
Training loss: 2.5461015701293945 / Valid loss: 6.519270476840791
Training loss: 3.6162238121032715 / Valid loss: 6.786087126958938
Training loss: 2.798518657684326 / Valid loss: 6.685348833174933
Training loss: 3.183590888977051 / Valid loss: 6.688883829116821
Training loss: 2.7190957069396973 / Valid loss: 6.646490735099429

Epoch: 9
Training loss: 1.6355782747268677 / Valid loss: 6.540813600449335
Training loss: 3.501093864440918 / Valid loss: 6.725762131100609
Training loss: 3.131822109222412 / Valid loss: 6.657616356440953
Training loss: 2.2966880798339844 / Valid loss: 6.722631935846238

Epoch: 10
Training loss: 2.6863536834716797 / Valid loss: 6.631197125571115
Training loss: 2.405783176422119 / Valid loss: 6.688032477242606
Training loss: 2.322906970977783 / Valid loss: 6.648245464052473
Training loss: 2.563915729522705 / Valid loss: 7.0260659671965096
Training loss: 2.022780418395996 / Valid loss: 6.755471878960019

Epoch: 11
Training loss: 2.2509992122650146 / Valid loss: 6.761194388071696
Training loss: 1.7504096031188965 / Valid loss: 6.699660900660923
Training loss: 1.745298147201538 / Valid loss: 6.713225051334926
Training loss: 2.2433900833129883 / Valid loss: 6.913246318272182
Training loss: 1.8568484783172607 / Valid loss: 6.7505604335239955

Epoch: 12
Training loss: 1.5390064716339111 / Valid loss: 6.828392628261021
Training loss: 1.7102856636047363 / Valid loss: 6.845691219965617
Training loss: 1.889138102531433 / Valid loss: 6.894899422781808
Training loss: 2.103127956390381 / Valid loss: 6.883361062549409
Training loss: 2.2052717208862305 / Valid loss: 6.776166911352249

Epoch: 13
Training loss: 1.2841875553131104 / Valid loss: 6.853531092689151
Training loss: 1.9570984840393066 / Valid loss: 7.062803014119466
Training loss: 1.2353811264038086 / Valid loss: 6.812054833911714
Training loss: 2.1495187282562256 / Valid loss: 6.908250645228795
Training loss: 1.8456584215164185 / Valid loss: 6.841768964131673

Epoch: 14
Training loss: 1.3763841390609741 / Valid loss: 6.76603080204555
Training loss: 2.400456666946411 / Valid loss: 6.793808669135684
Training loss: 1.569531798362732 / Valid loss: 6.948728529612223
Training loss: 1.7863317728042603 / Valid loss: 7.000636066709246
Training loss: 2.2643089294433594 / Valid loss: 6.869375353767758

Epoch: 15
Training loss: 1.3921771049499512 / Valid loss: 6.8548073496137345
Training loss: 1.3359923362731934 / Valid loss: 6.896681040809268
Training loss: 1.8938186168670654 / Valid loss: 6.995543343680246
Training loss: 1.7710274457931519 / Valid loss: 6.991950907026019
Training loss: 1.6482276916503906 / Valid loss: 7.005847485860189

Epoch: 16
Training loss: 1.6952686309814453 / Valid loss: 6.976717376708985
Training loss: 1.6321992874145508 / Valid loss: 7.052648526146299
Training loss: 1.7805230617523193 / Valid loss: 7.0593672479902
Training loss: 1.9553900957107544 / Valid loss: 6.979869270324707
Training loss: 1.1755955219268799 / Valid loss: 6.939109279995873

Epoch: 17
Training loss: 2.141432762145996 / Valid loss: 7.036036723000663
Training loss: 1.94727623462677 / Valid loss: 7.015439378647577
Training loss: 1.594871997833252 / Valid loss: 7.011472079867408
Training loss: 1.7175993919372559 / Valid loss: 6.94792084920974
Training loss: 1.514169692993164 / Valid loss: 7.024565224420456

Epoch: 18
Training loss: 1.5633243322372437 / Valid loss: 6.83834855897086
Training loss: 1.4657909870147705 / Valid loss: 7.044740277244931
Training loss: 1.1869701147079468 / Valid loss: 6.97908079964774
Training loss: 1.5811848640441895 / Valid loss: 6.82658161889939
Training loss: 0.9494991302490234 / Valid loss: 6.939977968306769

Epoch: 19
Training loss: 1.7381048202514648 / Valid loss: 6.950710596357073
Training loss: 1.1897143125534058 / Valid loss: 6.866477126166934
Training loss: 1.9015913009643555 / Valid loss: 7.088513919285366
Training loss: 1.1951327323913574 / Valid loss: 7.003358273279099

Epoch: 20
Training loss: 1.0624672174453735 / Valid loss: 7.053032361893427
Training loss: 2.3004753589630127 / Valid loss: 7.01310499055045
Training loss: 1.1179289817810059 / Valid loss: 6.901549897875104
Training loss: 1.75713050365448 / Valid loss: 6.984262975056966
Training loss: 0.9489142298698425 / Valid loss: 7.024342888877506

Epoch: 21
Training loss: 1.410182237625122 / Valid loss: 6.976200698670887
Training loss: 1.9465160369873047 / Valid loss: 6.866378566196986
Training loss: 1.0308105945587158 / Valid loss: 7.062673393885294
Training loss: 1.2026962041854858 / Valid loss: 7.016040693010603
Training loss: 0.9112812280654907 / Valid loss: 6.963497071039109

Epoch: 22
Training loss: 1.0251893997192383 / Valid loss: 6.96128853389195
Training loss: 1.7847239971160889 / Valid loss: 6.945355292728969
Training loss: 2.337447166442871 / Valid loss: 7.213213761647542
Training loss: 0.937768816947937 / Valid loss: 7.073714274451846
Training loss: 1.3733083009719849 / Valid loss: 7.019591267903646

Epoch: 23
Training loss: 1.0886104106903076 / Valid loss: 6.8967341014317105
Training loss: 1.340095043182373 / Valid loss: 7.10335776692345
Training loss: 1.0870518684387207 / Valid loss: 6.963137009030297
Training loss: 1.0905964374542236 / Valid loss: 6.955607623145694
Training loss: 1.0761306285858154 / Valid loss: 6.9576981839679535

Epoch: 24
Training loss: 1.143472671508789 / Valid loss: 6.926108893894014
Training loss: 1.5692129135131836 / Valid loss: 7.030073651813326
Training loss: 1.1745285987854004 / Valid loss: 7.1757300740196595
Training loss: 0.8161857724189758 / Valid loss: 7.163669917696998
Training loss: 1.7526910305023193 / Valid loss: 6.862327153342111

Epoch: 25
Training loss: 1.0336096286773682 / Valid loss: 7.198517186301095
Training loss: 1.142683982849121 / Valid loss: 6.987996703102475
Training loss: 0.9577763676643372 / Valid loss: 6.9510472320374985
Training loss: 1.0405282974243164 / Valid loss: 7.036292484828404
Training loss: 1.5014622211456299 / Valid loss: 6.881473970413208

Epoch: 26
Training loss: 0.8768882155418396 / Valid loss: 6.93705575125558
Training loss: 1.713128685951233 / Valid loss: 6.972993743987311
Training loss: 1.2144646644592285 / Valid loss: 7.058012244814918
Training loss: 1.0351033210754395 / Valid loss: 7.126998592558361
Training loss: 1.084537386894226 / Valid loss: 7.250063828059605

Epoch: 27
Training loss: 1.2634880542755127 / Valid loss: 7.103267156510126
Training loss: 1.027431607246399 / Valid loss: 7.098197482881092
Training loss: 1.0381743907928467 / Valid loss: 7.066842612766084
Training loss: 1.2020435333251953 / Valid loss: 7.001570081710815
Training loss: 1.2463979721069336 / Valid loss: 7.002155449276879

Epoch: 28
Training loss: 1.499140739440918 / Valid loss: 7.20209903717041
Training loss: 0.760413408279419 / Valid loss: 7.233857377370199
Training loss: 1.1009376049041748 / Valid loss: 7.008430533182054
Training loss: 1.118645191192627 / Valid loss: 6.94707301457723
Training loss: 0.992344081401825 / Valid loss: 7.124074132101876

Epoch: 29
Training loss: 1.4312381744384766 / Valid loss: 7.087240705035982
Training loss: 0.6404751539230347 / Valid loss: 7.088413483755929
Training loss: 1.1858127117156982 / Valid loss: 7.159601874578566
Training loss: 1.2281694412231445 / Valid loss: 6.985766229175386

Epoch: 30
Training loss: 0.6884691119194031 / Valid loss: 6.97591476667495
Training loss: 1.4452836513519287 / Valid loss: 6.991939546948387
Training loss: 1.2666780948638916 / Valid loss: 7.104182098025367
Training loss: 0.9424172639846802 / Valid loss: 7.079567241668701
Training loss: 1.3500559329986572 / Valid loss: 6.991981202080137

Epoch: 31
Training loss: 1.0948858261108398 / Valid loss: 7.1208815756298245
Training loss: 0.6197570562362671 / Valid loss: 7.120388489677793
Training loss: 1.338343858718872 / Valid loss: 7.121221764882406
Training loss: 1.053157091140747 / Valid loss: 7.194069276537214
Training loss: 1.383541226387024 / Valid loss: 7.14451607295445

Epoch: 32
Training loss: 0.7433838844299316 / Valid loss: 7.04143202645438
Training loss: 1.2340670824050903 / Valid loss: 7.1402315934499105
Training loss: 0.8978726863861084 / Valid loss: 7.081654083161127
Training loss: 0.9229481220245361 / Valid loss: 7.102698462350028
Training loss: 0.5979222059249878 / Valid loss: 6.996517126900809

Epoch: 33
Training loss: 1.1181361675262451 / Valid loss: 7.142229893094018
Training loss: 0.9877987504005432 / Valid loss: 6.969002235503424
Training loss: 0.813106119632721 / Valid loss: 7.004307383582706
Training loss: 0.7124032974243164 / Valid loss: 7.158051649729411
Training loss: 1.2319847345352173 / Valid loss: 7.086480756033034

Epoch: 34
Training loss: 0.8218141794204712 / Valid loss: 7.116597745532081
Training loss: 1.2546213865280151 / Valid loss: 7.205023701985677
Training loss: 1.0102903842926025 / Valid loss: 7.088421249389649
Training loss: 0.8913732171058655 / Valid loss: 7.0324998265221
Training loss: 1.6462202072143555 / Valid loss: 6.982213258743286

Epoch: 35
Training loss: 1.122978925704956 / Valid loss: 7.103334940047491
Training loss: 0.9003676772117615 / Valid loss: 7.1356399672372
Training loss: 0.627047598361969 / Valid loss: 7.153601385298229
Training loss: 1.0810441970825195 / Valid loss: 7.248381528400239
Training loss: 0.7592894434928894 / Valid loss: 7.0166801180158345

Epoch: 36
Training loss: 1.2875888347625732 / Valid loss: 7.084813040778751
Training loss: 1.1051304340362549 / Valid loss: 7.04850838070824
Training loss: 1.5793240070343018 / Valid loss: 7.163517865680513
Training loss: 1.0302146673202515 / Valid loss: 7.121808083852132
Training loss: 0.9120941758155823 / Valid loss: 7.070267650059291

Epoch: 37
Training loss: 0.9180407524108887 / Valid loss: 7.05665359270005
Training loss: 0.8568018674850464 / Valid loss: 7.0561777841477165
Training loss: 0.9326286315917969 / Valid loss: 7.088130119868687
Training loss: 1.4341686964035034 / Valid loss: 7.211556807018462
Training loss: 1.5879312753677368 / Valid loss: 7.0953357242402575

Epoch: 38
Training loss: 0.8269667625427246 / Valid loss: 7.198899786812919
Training loss: 0.7581896185874939 / Valid loss: 7.020527138028826
Training loss: 1.0831544399261475 / Valid loss: 7.07640532311939
Training loss: 0.7292654514312744 / Valid loss: 7.091926225026449
Training loss: 1.0789649486541748 / Valid loss: 7.223085635049003

Epoch: 39
Training loss: 0.713604211807251 / Valid loss: 7.182129914419992
Training loss: 0.7543864250183105 / Valid loss: 7.107077712104434
Training loss: 1.1119399070739746 / Valid loss: 7.031677659352621
Training loss: 0.8075588345527649 / Valid loss: 7.033574794587635
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 3300): 6.206942624137515
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.3, 0.2, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.74432945251465 / Valid loss: 16.987547265915644
Model is saved in epoch 0, overall batch: 0
Training loss: 18.29866600036621 / Valid loss: 15.512219438098725
Model is saved in epoch 0, overall batch: 100
Training loss: 13.449947357177734 / Valid loss: 14.268647448221843
Model is saved in epoch 0, overall batch: 200
Training loss: 12.476020812988281 / Valid loss: 13.430801514216832
Model is saved in epoch 0, overall batch: 300
Training loss: 14.044178009033203 / Valid loss: 12.718282922108967
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 11.866604804992676 / Valid loss: 12.284620948064894
Model is saved in epoch 1, overall batch: 500
Training loss: 12.915754318237305 / Valid loss: 11.552788820720854
Model is saved in epoch 1, overall batch: 600
Training loss: 8.765006065368652 / Valid loss: 10.537726220630464
Model is saved in epoch 1, overall batch: 700
Training loss: 7.801878929138184 / Valid loss: 10.249634797232492
Model is saved in epoch 1, overall batch: 800
Training loss: 11.033767700195312 / Valid loss: 9.817030107407342
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 8.233062744140625 / Valid loss: 9.180784261794317
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.655551910400391 / Valid loss: 8.345165216355097
Model is saved in epoch 2, overall batch: 1100
Training loss: 7.743889808654785 / Valid loss: 8.409508637019567
Training loss: 4.005822658538818 / Valid loss: 8.204202007112048
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.996890068054199 / Valid loss: 7.552177594956897
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 6.57021951675415 / Valid loss: 7.561736933390299
Training loss: 4.735583305358887 / Valid loss: 7.527423504420689
Model is saved in epoch 3, overall batch: 1600
Training loss: 4.224279403686523 / Valid loss: 7.3276191983904155
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.2760748863220215 / Valid loss: 7.149223382132394
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.363391399383545 / Valid loss: 6.751222719464983
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 5.566740989685059 / Valid loss: 6.864969430650984
Training loss: 4.6374053955078125 / Valid loss: 6.647862693241664
Model is saved in epoch 4, overall batch: 2100
Training loss: 4.583972454071045 / Valid loss: 6.731421327590942
Training loss: 4.301102638244629 / Valid loss: 6.438174547467913
Model is saved in epoch 4, overall batch: 2300
Training loss: 4.324040412902832 / Valid loss: 6.849995154426211

Epoch: 5
Training loss: 2.5699825286865234 / Valid loss: 6.445312032245455
Training loss: 4.285478591918945 / Valid loss: 6.556481776918684
Training loss: 5.357005596160889 / Valid loss: 6.578946713038853
Training loss: 5.731556415557861 / Valid loss: 6.5347775323050366
Training loss: 5.199761390686035 / Valid loss: 6.5875425656636555

Epoch: 6
Training loss: 3.892726421356201 / Valid loss: 6.517724275588989
Training loss: 4.353362560272217 / Valid loss: 6.482585425603958
Training loss: 3.0209782123565674 / Valid loss: 6.487204422269548
Training loss: 4.171182632446289 / Valid loss: 6.41191656930106
Model is saved in epoch 6, overall batch: 3300
Training loss: 5.128671646118164 / Valid loss: 6.384872595469157
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 4.23117733001709 / Valid loss: 6.376465209325155
Model is saved in epoch 7, overall batch: 3500
Training loss: 4.563689708709717 / Valid loss: 6.403792369933355
Training loss: 2.0857744216918945 / Valid loss: 6.436562737964448
Training loss: 2.839883327484131 / Valid loss: 6.554075595310756
Training loss: 5.288249969482422 / Valid loss: 6.724720805031913

Epoch: 8
Training loss: 2.664355516433716 / Valid loss: 6.540908972422282
Training loss: 3.480207920074463 / Valid loss: 6.765857174282982
Training loss: 2.8045120239257812 / Valid loss: 6.62487188066755
Training loss: 3.077627182006836 / Valid loss: 6.6271465823763895
Training loss: 2.825533866882324 / Valid loss: 6.664759520121983

Epoch: 9
Training loss: 1.6122584342956543 / Valid loss: 6.568301060086205
Training loss: 3.688549041748047 / Valid loss: 6.606680620284307
Training loss: 3.040299892425537 / Valid loss: 6.711202194577171
Training loss: 2.327669620513916 / Valid loss: 6.768533418292091

Epoch: 10
Training loss: 2.4677486419677734 / Valid loss: 6.628075277237665
Training loss: 2.338265895843506 / Valid loss: 6.698881087984358
Training loss: 2.3565008640289307 / Valid loss: 6.691033744812012
Training loss: 2.5094339847564697 / Valid loss: 6.954493384134202
Training loss: 2.2323949337005615 / Valid loss: 6.671353317442394

Epoch: 11
Training loss: 2.3672561645507812 / Valid loss: 6.8655470166887556
Training loss: 1.7049527168273926 / Valid loss: 6.674391882760184
Training loss: 2.0356428623199463 / Valid loss: 6.71279661996024
Training loss: 2.3429994583129883 / Valid loss: 6.928200776236398
Training loss: 1.9525235891342163 / Valid loss: 6.730129396347772

Epoch: 12
Training loss: 1.6111091375350952 / Valid loss: 6.78065125374567
Training loss: 1.7789608240127563 / Valid loss: 6.794261902854556
Training loss: 1.749011754989624 / Valid loss: 6.825743175688244
Training loss: 1.9830926656723022 / Valid loss: 6.92596530460176
Training loss: 1.947859764099121 / Valid loss: 6.7463675953093025

Epoch: 13
Training loss: 1.2797821760177612 / Valid loss: 6.782195842833746
Training loss: 2.334866523742676 / Valid loss: 7.101382023947579
Training loss: 1.0357587337493896 / Valid loss: 6.827157374790737
Training loss: 1.657242774963379 / Valid loss: 6.951489907219297
Training loss: 1.8417634963989258 / Valid loss: 6.823500272205898

Epoch: 14
Training loss: 1.5358366966247559 / Valid loss: 6.7729510330018545
Training loss: 2.409789562225342 / Valid loss: 6.757201621645973
Training loss: 1.5074536800384521 / Valid loss: 6.923785223279681
Training loss: 1.5812954902648926 / Valid loss: 6.9491442158108665
Training loss: 2.041858673095703 / Valid loss: 6.970319493611654

Epoch: 15
Training loss: 1.4760539531707764 / Valid loss: 6.855893863950457
Training loss: 1.4820541143417358 / Valid loss: 6.87428225562686
Training loss: 2.1895298957824707 / Valid loss: 7.08592316309611
Training loss: 1.7127630710601807 / Valid loss: 7.027588971455892
Training loss: 1.6139404773712158 / Valid loss: 6.898351751055036

Epoch: 16
Training loss: 1.651789903640747 / Valid loss: 6.914469319298154
Training loss: 1.8149361610412598 / Valid loss: 7.05483618690854
Training loss: 1.7485377788543701 / Valid loss: 7.0119556563241145
Training loss: 2.095311403274536 / Valid loss: 6.880602050962902
Training loss: 1.3106704950332642 / Valid loss: 6.987246406646002

Epoch: 17
Training loss: 2.3183016777038574 / Valid loss: 6.994670527321952
Training loss: 1.4209785461425781 / Valid loss: 6.946609288170224
Training loss: 1.734619140625 / Valid loss: 6.975113714308966
Training loss: 1.696303129196167 / Valid loss: 6.961433437892369
Training loss: 1.4098182916641235 / Valid loss: 7.019052505493164

Epoch: 18
Training loss: 1.3903559446334839 / Valid loss: 6.827754227320353
Training loss: 1.8638540506362915 / Valid loss: 7.035656102498373
Training loss: 1.4141262769699097 / Valid loss: 7.124724964868455
Training loss: 1.4687856435775757 / Valid loss: 6.915802873883929
Training loss: 1.1527824401855469 / Valid loss: 7.024560546875

Epoch: 19
Training loss: 1.693737506866455 / Valid loss: 6.990000559034802
Training loss: 1.3365908861160278 / Valid loss: 6.853810884839013
Training loss: 1.9731674194335938 / Valid loss: 6.957177602677119
Training loss: 1.4031615257263184 / Valid loss: 7.013770280565534

Epoch: 20
Training loss: 1.0308911800384521 / Valid loss: 7.011576043991815
Training loss: 1.9425175189971924 / Valid loss: 6.942671185448056
Training loss: 1.3260505199432373 / Valid loss: 6.908903262728736
Training loss: 1.8881309032440186 / Valid loss: 7.042331995282854
Training loss: 0.8088706135749817 / Valid loss: 6.9600580851236975

Epoch: 21
Training loss: 1.6300897598266602 / Valid loss: 7.035864661988758
Training loss: 1.4667390584945679 / Valid loss: 6.927333949861072
Training loss: 1.115415334701538 / Valid loss: 7.0613415263947985
Training loss: 1.1423945426940918 / Valid loss: 7.012856919424874
Training loss: 0.7312166094779968 / Valid loss: 7.085975565229144

Epoch: 22
Training loss: 0.9624764323234558 / Valid loss: 6.990556387674241
Training loss: 1.63619065284729 / Valid loss: 7.085807845706031
Training loss: 2.2855238914489746 / Valid loss: 7.169602412269229
Training loss: 1.1734721660614014 / Valid loss: 7.185435463133312
Training loss: 1.4967331886291504 / Valid loss: 7.028476401737758

Epoch: 23
Training loss: 1.2736823558807373 / Valid loss: 6.971670736585344
Training loss: 1.1374130249023438 / Valid loss: 7.150622801553635
Training loss: 1.165855884552002 / Valid loss: 6.917039794013614
Training loss: 0.9944985508918762 / Valid loss: 6.918524801163446
Training loss: 1.176584243774414 / Valid loss: 6.9407574063255675

Epoch: 24
Training loss: 1.0397028923034668 / Valid loss: 6.929210036141532
Training loss: 1.4407840967178345 / Valid loss: 7.072138061977568
Training loss: 1.2121469974517822 / Valid loss: 7.106858621324812
Training loss: 0.8510844707489014 / Valid loss: 7.1749140149071104
Training loss: 1.7668955326080322 / Valid loss: 6.947345790408907

Epoch: 25
Training loss: 1.1159474849700928 / Valid loss: 7.1648823874337335
Training loss: 1.1107938289642334 / Valid loss: 7.002410286948795
Training loss: 0.8903036117553711 / Valid loss: 6.902695262999762
Training loss: 1.069886326789856 / Valid loss: 7.1262916019984655
Training loss: 1.3561110496520996 / Valid loss: 6.98920735404605

Epoch: 26
Training loss: 0.8166478872299194 / Valid loss: 6.943867619832357
Training loss: 1.6421222686767578 / Valid loss: 6.929628569739205
Training loss: 1.1975295543670654 / Valid loss: 7.014452879769461
Training loss: 1.1530414819717407 / Valid loss: 7.079980550493513
Training loss: 1.0601718425750732 / Valid loss: 7.2187781969706215

Epoch: 27
Training loss: 1.3281996250152588 / Valid loss: 7.111560258411226
Training loss: 1.1934354305267334 / Valid loss: 7.092612448192778
Training loss: 1.07741379737854 / Valid loss: 7.083816769009545
Training loss: 1.2743501663208008 / Valid loss: 6.99255421048119
Training loss: 1.1044402122497559 / Valid loss: 7.014565767560686

Epoch: 28
Training loss: 1.2685470581054688 / Valid loss: 7.126434153602236
Training loss: 0.7530949711799622 / Valid loss: 7.181958886555263
Training loss: 1.0102360248565674 / Valid loss: 7.0344738869439984
Training loss: 1.0618600845336914 / Valid loss: 6.978387928009033
Training loss: 0.9992672204971313 / Valid loss: 7.088654772440592

Epoch: 29
Training loss: 1.3940224647521973 / Valid loss: 7.078081203642346
Training loss: 0.6519343852996826 / Valid loss: 7.141327953338623
Training loss: 1.286548137664795 / Valid loss: 7.138127724329631
Training loss: 1.218316912651062 / Valid loss: 6.951978983197893

Epoch: 30
Training loss: 0.6806987524032593 / Valid loss: 7.025219983146304
Training loss: 1.628556251525879 / Valid loss: 6.970241324106852
Training loss: 1.303926944732666 / Valid loss: 7.152376642681303
Training loss: 0.9281251430511475 / Valid loss: 7.089537152789888
Training loss: 1.3616821765899658 / Valid loss: 7.006335278919765

Epoch: 31
Training loss: 1.0904593467712402 / Valid loss: 7.046102010636103
Training loss: 0.6754430532455444 / Valid loss: 7.109187053498768
Training loss: 1.3089227676391602 / Valid loss: 7.198456773303804
Training loss: 1.1381850242614746 / Valid loss: 7.224535583314442
Training loss: 1.2396023273468018 / Valid loss: 7.061702328636533

Epoch: 32
Training loss: 0.8592015504837036 / Valid loss: 7.09913310550508
Training loss: 1.1081352233886719 / Valid loss: 7.138297571454729
Training loss: 0.865631103515625 / Valid loss: 7.089932537078857
Training loss: 1.0114071369171143 / Valid loss: 7.195146478925433
Training loss: 0.6717267036437988 / Valid loss: 7.013972282409668

Epoch: 33
Training loss: 1.1144850254058838 / Valid loss: 7.151112624577114
Training loss: 1.0905654430389404 / Valid loss: 6.936889194306874
Training loss: 0.7380225658416748 / Valid loss: 6.953636893771944
Training loss: 0.6867181658744812 / Valid loss: 7.229095763251895
Training loss: 1.3221333026885986 / Valid loss: 7.190545363653274

Epoch: 34
Training loss: 0.8264455795288086 / Valid loss: 7.173146007174537
Training loss: 1.195422887802124 / Valid loss: 7.126857348850796
Training loss: 1.2081315517425537 / Valid loss: 7.158876153400966
Training loss: 0.8473210334777832 / Valid loss: 7.0951707067943754
Training loss: 1.4747297763824463 / Valid loss: 6.970401541392008

Epoch: 35
Training loss: 1.0892155170440674 / Valid loss: 7.116138344719296
Training loss: 0.9020477533340454 / Valid loss: 7.0709624699183875
Training loss: 0.6600058078765869 / Valid loss: 7.103191121419271
Training loss: 1.1321399211883545 / Valid loss: 7.219726058415004
Training loss: 0.8233524560928345 / Valid loss: 7.006445035480318

Epoch: 36
Training loss: 1.072711706161499 / Valid loss: 7.1085631370544435
Training loss: 1.187475323677063 / Valid loss: 7.081096290406727
Training loss: 1.3452298641204834 / Valid loss: 7.111503403527396
Training loss: 1.0562565326690674 / Valid loss: 7.177965325400943
Training loss: 0.9040094017982483 / Valid loss: 7.0520099185761955

Epoch: 37
Training loss: 0.9463394284248352 / Valid loss: 7.096868978227888
Training loss: 0.7540912628173828 / Valid loss: 7.12802152633667
Training loss: 1.0984597206115723 / Valid loss: 7.11338757106236
Training loss: 1.3870985507965088 / Valid loss: 7.2090791384379065
Training loss: 1.5169790983200073 / Valid loss: 7.103132361457462

Epoch: 38
Training loss: 0.9965246319770813 / Valid loss: 7.1535381407964795
Training loss: 0.6744629740715027 / Valid loss: 7.023634093148368
Training loss: 1.1114041805267334 / Valid loss: 7.0705457778204055
Training loss: 0.8170274496078491 / Valid loss: 7.07418046224685
Training loss: 1.1264445781707764 / Valid loss: 7.1615852401370095

Epoch: 39
Training loss: 0.7723273634910583 / Valid loss: 7.193966316041492
Training loss: 0.7218114733695984 / Valid loss: 7.171514983404251
Training loss: 1.0308055877685547 / Valid loss: 7.047450083778018
Training loss: 0.8454252481460571 / Valid loss: 7.037853458949498
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.2, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0.1, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 3500): 6.187643014817011
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.448760986328125 / Valid loss: 15.752877898443312
Model is saved in epoch 0, overall batch: 0
Training loss: 13.78660774230957 / Valid loss: 14.91492246900286
Model is saved in epoch 0, overall batch: 100
Training loss: 11.18950366973877 / Valid loss: 13.462301735650925
Model is saved in epoch 0, overall batch: 200
Training loss: 5.721175670623779 / Valid loss: 12.552456483386813
Model is saved in epoch 0, overall batch: 300
Training loss: 6.29721212387085 / Valid loss: 11.750171388898577
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.961673736572266 / Valid loss: 10.984721765064059
Model is saved in epoch 1, overall batch: 500
Training loss: 6.986238479614258 / Valid loss: 10.43820545105707
Model is saved in epoch 1, overall batch: 600
Training loss: 8.282472610473633 / Valid loss: 9.840163516998292
Model is saved in epoch 1, overall batch: 700
Training loss: 5.793664932250977 / Valid loss: 9.27642906279791
Model is saved in epoch 1, overall batch: 800
Training loss: 6.021389007568359 / Valid loss: 8.785516207558768
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.668573379516602 / Valid loss: 8.40702260789417
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.9253294467926025 / Valid loss: 8.139541439783006
Model is saved in epoch 2, overall batch: 1100
Training loss: 4.64329195022583 / Valid loss: 8.112293288821267
Model is saved in epoch 2, overall batch: 1200
Training loss: 3.7463483810424805 / Valid loss: 7.464516496658325
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.146299362182617 / Valid loss: 7.813715721312024

Epoch: 3
Training loss: 2.8611927032470703 / Valid loss: 7.1096853846595405
Model is saved in epoch 3, overall batch: 1500
Training loss: 6.887424945831299 / Valid loss: 7.200321937742688
Training loss: 5.782954216003418 / Valid loss: 7.368595468430292
Training loss: 5.934670925140381 / Valid loss: 7.06893317812965
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.019615173339844 / Valid loss: 7.042876697721935
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 4.071085453033447 / Valid loss: 6.513676230112711
Model is saved in epoch 4, overall batch: 2000
Training loss: 4.855741024017334 / Valid loss: 6.687790509632656
Training loss: 2.6333227157592773 / Valid loss: 6.192753496624174
Model is saved in epoch 4, overall batch: 2200
Training loss: 3.836050510406494 / Valid loss: 6.659675473258609
Training loss: 4.341379642486572 / Valid loss: 6.595652230580648

Epoch: 5
Training loss: 3.444044589996338 / Valid loss: 6.682834118888492
Training loss: 2.8884966373443604 / Valid loss: 6.335013253348214
Training loss: 3.9131288528442383 / Valid loss: 6.382612153462001
Training loss: 3.9444704055786133 / Valid loss: 6.339396612984793
Training loss: 3.323535680770874 / Valid loss: 6.558173722312564

Epoch: 6
Training loss: 2.59334397315979 / Valid loss: 6.141316506976173
Model is saved in epoch 6, overall batch: 3000
Training loss: 3.7353105545043945 / Valid loss: 6.123222732543946
Model is saved in epoch 6, overall batch: 3100
Training loss: 3.768263339996338 / Valid loss: 6.110037326812744
Model is saved in epoch 6, overall batch: 3200
Training loss: 3.1237103939056396 / Valid loss: 6.534683502288091
Training loss: 3.13665771484375 / Valid loss: 6.396256163006737

Epoch: 7
Training loss: 2.70048189163208 / Valid loss: 6.2663078694116505
Training loss: 2.8870975971221924 / Valid loss: 6.4562850634257
Training loss: 2.968329906463623 / Valid loss: 6.35234097526187
Training loss: 3.273469924926758 / Valid loss: 6.443308251244681
Training loss: 2.8572139739990234 / Valid loss: 6.384345547358195

Epoch: 8
Training loss: 1.875514030456543 / Valid loss: 6.16249495006743
Training loss: 2.7076902389526367 / Valid loss: 6.258789993467785
Training loss: 2.2921013832092285 / Valid loss: 6.519702931812831
Training loss: 1.6873588562011719 / Valid loss: 6.628208943775722
Training loss: 3.2119898796081543 / Valid loss: 6.612948792321341

Epoch: 9
Training loss: 1.9934662580490112 / Valid loss: 6.5254519712357295
Training loss: 2.0540618896484375 / Valid loss: 6.4203499453408375
Training loss: 2.1116981506347656 / Valid loss: 6.429842951184227
Training loss: 3.690168619155884 / Valid loss: 6.291840575990223

Epoch: 10
Training loss: 2.063119888305664 / Valid loss: 6.488755743844169
Training loss: 2.2924938201904297 / Valid loss: 6.651010606402442
Training loss: 1.6859534978866577 / Valid loss: 6.423586093811762
Training loss: 2.129889488220215 / Valid loss: 6.413453749247959
Training loss: 1.7529234886169434 / Valid loss: 6.383045378185454

Epoch: 11
Training loss: 1.480947732925415 / Valid loss: 6.735823208945138
Training loss: 1.9185264110565186 / Valid loss: 6.9865526176634285
Training loss: 1.7675449848175049 / Valid loss: 6.670145071120489
Training loss: 1.6594464778900146 / Valid loss: 6.60529960450672
Training loss: 3.523685932159424 / Valid loss: 6.357350076947894

Epoch: 12
Training loss: 1.286501407623291 / Valid loss: 6.587676454725719
Training loss: 1.5736055374145508 / Valid loss: 6.898316142672584
Training loss: 1.2240415811538696 / Valid loss: 6.769272690727597
Training loss: 1.7732553482055664 / Valid loss: 6.5616335051400325
Training loss: 2.1032233238220215 / Valid loss: 6.5739800725664415

Epoch: 13
Training loss: 2.1645073890686035 / Valid loss: 6.67478019396464
Training loss: 1.238714337348938 / Valid loss: 6.557490923291161
Training loss: 1.5771560668945312 / Valid loss: 6.857547959827241
Training loss: 1.604972243309021 / Valid loss: 6.50599125453404
Training loss: 1.9260634183883667 / Valid loss: 6.6156424658639095

Epoch: 14
Training loss: 1.1673688888549805 / Valid loss: 6.579074991317023
Training loss: 2.2382261753082275 / Valid loss: 6.681922483444214
Training loss: 1.7397398948669434 / Valid loss: 7.294833492097401
Training loss: 1.36928129196167 / Valid loss: 6.915184543246315
Training loss: 1.0939310789108276 / Valid loss: 6.5570600214458645

Epoch: 15
Training loss: 1.671586513519287 / Valid loss: 6.6177682740347725
Training loss: 1.0492204427719116 / Valid loss: 6.598595819019136
Training loss: 1.9967752695083618 / Valid loss: 6.965383216312953
Training loss: 1.922672152519226 / Valid loss: 7.34156597001212
Training loss: 2.9713194370269775 / Valid loss: 6.752686318897066

Epoch: 16
Training loss: 1.0999430418014526 / Valid loss: 7.10301360856919
Training loss: 2.075657844543457 / Valid loss: 6.8104159514109295
Training loss: 2.012324810028076 / Valid loss: 6.7622642426263715
Training loss: 1.5727144479751587 / Valid loss: 6.6880988575163345
Training loss: 0.9532549381256104 / Valid loss: 6.577368572780064

Epoch: 17
Training loss: 0.9618441462516785 / Valid loss: 6.5047434715997605
Training loss: 0.8274127244949341 / Valid loss: 6.933468596140544
Training loss: 1.4132249355316162 / Valid loss: 6.88270069531032
Training loss: 2.115762233734131 / Valid loss: 6.567166269393194
Training loss: 1.63632333278656 / Valid loss: 6.5797154835292275

Epoch: 18
Training loss: 1.3455058336257935 / Valid loss: 6.666566222054618
Training loss: 1.798243761062622 / Valid loss: 6.827597777048747
Training loss: 2.010901927947998 / Valid loss: 6.8684789748418895
Training loss: 1.319176435470581 / Valid loss: 6.9370254698253815
Training loss: 1.1753507852554321 / Valid loss: 7.195854255131313

Epoch: 19
Training loss: 1.437069058418274 / Valid loss: 6.990275775818597
Training loss: 1.1171636581420898 / Valid loss: 6.7043352195194785
Training loss: 0.9839929938316345 / Valid loss: 6.768835960115705
Training loss: 1.0617079734802246 / Valid loss: 6.805376561482747

Epoch: 20
Training loss: 1.5333665609359741 / Valid loss: 7.031247615814209
Training loss: 1.16058349609375 / Valid loss: 6.843072185062227
Training loss: 1.3302522897720337 / Valid loss: 7.065548942202613
Training loss: 0.6401342153549194 / Valid loss: 6.767746319089617
Training loss: 1.0305211544036865 / Valid loss: 7.1071639810289655

Epoch: 21
Training loss: 1.1924636363983154 / Valid loss: 7.087352135067895
Training loss: 1.4151164293289185 / Valid loss: 7.128921363467262
Training loss: 1.0624966621398926 / Valid loss: 7.295726326533726
Training loss: 1.0674699544906616 / Valid loss: 7.191713680539812
Training loss: 1.2198621034622192 / Valid loss: 6.821748034159342

Epoch: 22
Training loss: 1.3514493703842163 / Valid loss: 6.656673172542027
Training loss: 1.224168062210083 / Valid loss: 6.80171602567037
Training loss: 1.1412826776504517 / Valid loss: 7.0224029813494
Training loss: 1.7209622859954834 / Valid loss: 6.646990063076927
Training loss: 1.2972368001937866 / Valid loss: 6.890382001513527

Epoch: 23
Training loss: 1.102046251296997 / Valid loss: 6.9422245139167424
Training loss: 1.362888216972351 / Valid loss: 7.056273614792596
Training loss: 2.0573244094848633 / Valid loss: 6.978905432564872
Training loss: 0.6096299886703491 / Valid loss: 6.791171082996187
Training loss: 1.1471294164657593 / Valid loss: 6.919756698608398

Epoch: 24
Training loss: 0.9016528725624084 / Valid loss: 6.815499001457578
Training loss: 1.3758907318115234 / Valid loss: 7.283622103645688
Training loss: 1.1567422151565552 / Valid loss: 7.426434748513358
Training loss: 1.618664026260376 / Valid loss: 6.864806529453823
Training loss: 1.617934226989746 / Valid loss: 7.38555958157494

Epoch: 25
Training loss: 1.2334206104278564 / Valid loss: 7.062881905691964
Training loss: 0.9036969542503357 / Valid loss: 7.086210414341518
Training loss: 1.239924430847168 / Valid loss: 7.0236995379130045
Training loss: 1.6522526741027832 / Valid loss: 7.297969688688005
Training loss: 1.3836703300476074 / Valid loss: 6.770596847080049

Epoch: 26
Training loss: 0.7499890327453613 / Valid loss: 6.937705094473703
Training loss: 0.9587342739105225 / Valid loss: 6.949504162016369
Training loss: 0.7072398662567139 / Valid loss: 6.946040362403506
Training loss: 1.0458085536956787 / Valid loss: 6.8089640481131415
Training loss: 1.2431226968765259 / Valid loss: 6.794122239521571

Epoch: 27
Training loss: 1.096840262413025 / Valid loss: 7.145241146995907
Training loss: 1.1905710697174072 / Valid loss: 6.988914966583252
Training loss: 1.073604941368103 / Valid loss: 6.891704636528378
Training loss: 1.239924430847168 / Valid loss: 6.820909388860067
Training loss: 1.0882633924484253 / Valid loss: 7.391555050441197

Epoch: 28
Training loss: 0.6764612197875977 / Valid loss: 7.091084012531099
Training loss: 1.052922010421753 / Valid loss: 6.966327894301641
Training loss: 1.4441204071044922 / Valid loss: 7.379977707635788
Training loss: 0.8056044578552246 / Valid loss: 7.176034146263486
Training loss: 0.9768344163894653 / Valid loss: 6.835147380828857

Epoch: 29
Training loss: 0.705482006072998 / Valid loss: 6.694023908887591
Training loss: 0.9683439135551453 / Valid loss: 6.8709318047478085
Training loss: 1.2003974914550781 / Valid loss: 7.087759948912121
Training loss: 0.9355260729789734 / Valid loss: 6.8545006729307625

Epoch: 30
Training loss: 1.271903395652771 / Valid loss: 6.730345530737014
Training loss: 0.8344773054122925 / Valid loss: 7.141315551031203
Training loss: 0.8390507102012634 / Valid loss: 6.822793719882057
Training loss: 0.9946259260177612 / Valid loss: 7.10822115625654
Training loss: 1.1294946670532227 / Valid loss: 7.027766931624639

Epoch: 31
Training loss: 0.913908839225769 / Valid loss: 6.961583196549189
Training loss: 0.8589540123939514 / Valid loss: 6.888577901749384
Training loss: 0.9450557231903076 / Valid loss: 7.069618356795538
Training loss: 0.8727802634239197 / Valid loss: 7.113901238214402
Training loss: 0.7385732531547546 / Valid loss: 7.161290264129638

Epoch: 32
Training loss: 0.7788892984390259 / Valid loss: 7.0343994049798875
Training loss: 0.7181578874588013 / Valid loss: 6.9179825033460345
Training loss: 1.0708434581756592 / Valid loss: 7.169553452446348
Training loss: 1.4713631868362427 / Valid loss: 6.880618054526193
Training loss: 0.8104274272918701 / Valid loss: 7.125654057094029

Epoch: 33
Training loss: 0.8793814778327942 / Valid loss: 7.476480241048904
Training loss: 1.036355972290039 / Valid loss: 7.238972886403402
Training loss: 0.4406973719596863 / Valid loss: 6.9200605846586685
Training loss: 0.7180810570716858 / Valid loss: 7.012112131572905
Training loss: 0.7737494707107544 / Valid loss: 7.33425973256429

Epoch: 34
Training loss: 0.8602628111839294 / Valid loss: 6.969572925567627
Training loss: 0.7120628356933594 / Valid loss: 6.758721837543306
Training loss: 0.8759222030639648 / Valid loss: 7.072565805344355
Training loss: 0.8543205261230469 / Valid loss: 7.362420377277193
Training loss: 0.7074867486953735 / Valid loss: 7.1343191010611395

Epoch: 35
Training loss: 0.8811670541763306 / Valid loss: 7.150241978963217
Training loss: 1.1415784358978271 / Valid loss: 6.918673052106585
Training loss: 0.7649574875831604 / Valid loss: 7.129093374524798
Training loss: 0.771198034286499 / Valid loss: 7.0746944132305325
Training loss: 1.0558457374572754 / Valid loss: 6.9610654149736675

Epoch: 36
Training loss: 0.7210501432418823 / Valid loss: 7.0674063591730025
Training loss: 1.1782855987548828 / Valid loss: 6.998414366585868
Training loss: 0.9576355814933777 / Valid loss: 7.012726552145821
Training loss: 0.6826105117797852 / Valid loss: 7.008073845363799
Training loss: 0.8673537373542786 / Valid loss: 6.9732169355664935

Epoch: 37
Training loss: 1.0484600067138672 / Valid loss: 7.12141630536034
Training loss: 0.828134298324585 / Valid loss: 7.239954698653448
Training loss: 1.429248571395874 / Valid loss: 6.756030123574393
Training loss: 0.7082390189170837 / Valid loss: 7.08964885757083
Training loss: 0.7749350070953369 / Valid loss: 7.116321908859979

Epoch: 38
Training loss: 1.1053354740142822 / Valid loss: 7.120122614360991
Training loss: 0.8746119737625122 / Valid loss: 7.163971692039853
Training loss: 0.702112078666687 / Valid loss: 7.362757614680699
Training loss: 0.8837127685546875 / Valid loss: 6.9325108800615585
Training loss: 0.9596477150917053 / Valid loss: 7.17517123903547

Epoch: 39
Training loss: 0.911908745765686 / Valid loss: 7.189847582862491
Training loss: 1.1560988426208496 / Valid loss: 6.917549591972715
Training loss: 0.9263691306114197 / Valid loss: 7.211293206896101
Training loss: 0.6496679782867432 / Valid loss: 6.934018623261225
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 3200): 5.960884773163569
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.448760986328125 / Valid loss: 15.752877916608538
Model is saved in epoch 0, overall batch: 0
Training loss: 13.786606788635254 / Valid loss: 14.91492268698556
Model is saved in epoch 0, overall batch: 100
Training loss: 11.189504623413086 / Valid loss: 13.46230264391218
Model is saved in epoch 0, overall batch: 200
Training loss: 5.721177101135254 / Valid loss: 12.552457714080811
Model is saved in epoch 0, overall batch: 300
Training loss: 6.297214508056641 / Valid loss: 11.750172814868746
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.961677074432373 / Valid loss: 10.984723872230166
Model is saved in epoch 1, overall batch: 500
Training loss: 6.986241817474365 / Valid loss: 10.438297149113247
Model is saved in epoch 1, overall batch: 600
Training loss: 8.327556610107422 / Valid loss: 9.781752377464658
Model is saved in epoch 1, overall batch: 700
Training loss: 5.738579750061035 / Valid loss: 9.359128461565291
Model is saved in epoch 1, overall batch: 800
Training loss: 5.971167087554932 / Valid loss: 8.7011229015532
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.637128829956055 / Valid loss: 8.414134493328277
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.9697089195251465 / Valid loss: 8.099243668147496
Model is saved in epoch 2, overall batch: 1100
Training loss: 4.7836198806762695 / Valid loss: 8.100090830666678
Training loss: 3.921623706817627 / Valid loss: 7.31443027541751
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.060299396514893 / Valid loss: 7.661701286406744

Epoch: 3
Training loss: 2.990216016769409 / Valid loss: 7.282346212296259
Model is saved in epoch 3, overall batch: 1500
Training loss: 6.700129508972168 / Valid loss: 7.316374372300648
Training loss: 5.761591911315918 / Valid loss: 7.494782661256336
Training loss: 5.868833541870117 / Valid loss: 7.143408116840181
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.80825662612915 / Valid loss: 6.678664554868425
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 4.001059532165527 / Valid loss: 6.54089545295352
Model is saved in epoch 4, overall batch: 2000
Training loss: 4.6884565353393555 / Valid loss: 6.683596804028466
Training loss: 2.7588350772857666 / Valid loss: 6.346606436229887
Model is saved in epoch 4, overall batch: 2200
Training loss: 3.8358724117279053 / Valid loss: 6.604844726834979
Training loss: 4.273836135864258 / Valid loss: 6.520496772584461

Epoch: 5
Training loss: 3.4672322273254395 / Valid loss: 6.673485769544329
Training loss: 3.0120770931243896 / Valid loss: 6.277747483480544
Model is saved in epoch 5, overall batch: 2600
Training loss: 3.8384833335876465 / Valid loss: 6.539127776736305
Training loss: 3.856076955795288 / Valid loss: 6.345437703813825
Training loss: 3.351533889770508 / Valid loss: 6.424726433981032

Epoch: 6
Training loss: 2.550454616546631 / Valid loss: 6.194002528417678
Model is saved in epoch 6, overall batch: 3000
Training loss: 3.79011607170105 / Valid loss: 6.194089076632545
Training loss: 3.584144115447998 / Valid loss: 6.079122198195685
Model is saved in epoch 6, overall batch: 3200
Training loss: 3.091554880142212 / Valid loss: 6.567532273701259
Training loss: 2.9958200454711914 / Valid loss: 6.414988735743932

Epoch: 7
Training loss: 2.6432979106903076 / Valid loss: 6.3943627811613535
Training loss: 3.1081111431121826 / Valid loss: 6.399506991250174
Training loss: 2.602715492248535 / Valid loss: 6.269287956328619
Training loss: 3.454440116882324 / Valid loss: 6.314714524859474
Training loss: 2.9935455322265625 / Valid loss: 6.45695386387053

Epoch: 8
Training loss: 1.7765746116638184 / Valid loss: 6.208074746813093
Training loss: 2.5377211570739746 / Valid loss: 6.234124462945121
Training loss: 2.397825241088867 / Valid loss: 6.716538463320051
Training loss: 1.731029748916626 / Valid loss: 6.659231801260085
Training loss: 3.140960693359375 / Valid loss: 6.580350544339135

Epoch: 9
Training loss: 2.229954481124878 / Valid loss: 6.412584016436623
Training loss: 2.1008667945861816 / Valid loss: 6.515544859568278
Training loss: 2.0345377922058105 / Valid loss: 6.522573625473749
Training loss: 3.795480489730835 / Valid loss: 6.307269743510655

Epoch: 10
Training loss: 1.9966557025909424 / Valid loss: 6.522817729768299
Training loss: 2.4578347206115723 / Valid loss: 6.527217996688116
Training loss: 1.8061378002166748 / Valid loss: 6.464303457169306
Training loss: 2.385126829147339 / Valid loss: 6.460714183534894
Training loss: 1.7870450019836426 / Valid loss: 6.523467749641055

Epoch: 11
Training loss: 1.5785393714904785 / Valid loss: 6.733022233418056
Training loss: 1.948187232017517 / Valid loss: 6.99525515238444
Training loss: 1.820483922958374 / Valid loss: 6.579734906696138
Training loss: 1.6643517017364502 / Valid loss: 6.404639330364409
Training loss: 3.4384167194366455 / Valid loss: 6.3890647797357465

Epoch: 12
Training loss: 1.1873875856399536 / Valid loss: 6.638288604645502
Training loss: 1.5506442785263062 / Valid loss: 6.77611672991798
Training loss: 1.3243849277496338 / Valid loss: 6.663199240820749
Training loss: 1.8409477472305298 / Valid loss: 6.511689072563534
Training loss: 2.3127310276031494 / Valid loss: 6.692185386021932

Epoch: 13
Training loss: 2.225618362426758 / Valid loss: 6.736973882856823
Training loss: 1.1693583726882935 / Valid loss: 6.585229698816935
Training loss: 1.4681086540222168 / Valid loss: 6.751994437263125
Training loss: 1.618757963180542 / Valid loss: 6.494561719894409
Training loss: 1.7497029304504395 / Valid loss: 6.670911041895549

Epoch: 14
Training loss: 1.1017454862594604 / Valid loss: 6.567866311754499
Training loss: 2.3040995597839355 / Valid loss: 6.770342472621373
Training loss: 1.7050491571426392 / Valid loss: 7.058463323683966
Training loss: 1.4858382940292358 / Valid loss: 7.048627698989142
Training loss: 1.2075130939483643 / Valid loss: 6.641820975712368

Epoch: 15
Training loss: 1.6854259967803955 / Valid loss: 6.7866009258088615
Training loss: 1.1519012451171875 / Valid loss: 6.613939725785029
Training loss: 1.8908318281173706 / Valid loss: 7.060725203014555
Training loss: 1.8591651916503906 / Valid loss: 7.2095569883074075
Training loss: 2.8481879234313965 / Valid loss: 6.870066365741548

Epoch: 16
Training loss: 1.0198436975479126 / Valid loss: 7.032434924443563
Training loss: 1.9740577936172485 / Valid loss: 6.813873111634027
Training loss: 2.057554006576538 / Valid loss: 6.688942777542841
Training loss: 1.6146080493927002 / Valid loss: 6.691768955049061
Training loss: 1.0571411848068237 / Valid loss: 6.589307921273368

Epoch: 17
Training loss: 0.9441908597946167 / Valid loss: 6.506057843707857
Training loss: 1.0508190393447876 / Valid loss: 6.8928406692686535
Training loss: 1.4214274883270264 / Valid loss: 6.848649556296213
Training loss: 2.1181366443634033 / Valid loss: 6.518138989948091
Training loss: 1.5483505725860596 / Valid loss: 6.584715189252581

Epoch: 18
Training loss: 1.218575119972229 / Valid loss: 6.678087566012428
Training loss: 1.6891194581985474 / Valid loss: 6.795061234065464
Training loss: 2.1715164184570312 / Valid loss: 6.712054218564715
Training loss: 1.3322856426239014 / Valid loss: 6.897782493772961
Training loss: 1.2493051290512085 / Valid loss: 7.219428943452381

Epoch: 19
Training loss: 1.2451634407043457 / Valid loss: 7.098783851805187
Training loss: 1.147460699081421 / Valid loss: 6.715231232416063
Training loss: 1.1253254413604736 / Valid loss: 6.79018969081697
Training loss: 1.3422094583511353 / Valid loss: 6.894095670609247

Epoch: 20
Training loss: 1.5379260778427124 / Valid loss: 6.905268682752337
Training loss: 1.0919767618179321 / Valid loss: 6.825888626916068
Training loss: 1.3729729652404785 / Valid loss: 6.950018737429664
Training loss: 0.638678789138794 / Valid loss: 6.697241360800607
Training loss: 1.0358436107635498 / Valid loss: 6.889921769641695

Epoch: 21
Training loss: 1.1498916149139404 / Valid loss: 7.043798174176898
Training loss: 1.5175673961639404 / Valid loss: 7.08040375936599
Training loss: 1.0441035032272339 / Valid loss: 7.0991717792692635
Training loss: 1.106163740158081 / Valid loss: 7.248442838305519
Training loss: 1.1852161884307861 / Valid loss: 6.774934854961577

Epoch: 22
Training loss: 1.2852410078048706 / Valid loss: 6.7111472992669965
Training loss: 1.1579599380493164 / Valid loss: 6.850841853732154
Training loss: 1.237013339996338 / Valid loss: 7.16523270152864
Training loss: 1.593384027481079 / Valid loss: 6.65764179002671
Training loss: 1.3985333442687988 / Valid loss: 6.90817695799328

Epoch: 23
Training loss: 0.9318007230758667 / Valid loss: 6.890909097308204
Training loss: 1.4004971981048584 / Valid loss: 7.197075314748854
Training loss: 1.9272218942642212 / Valid loss: 7.142167702175322
Training loss: 0.7655868530273438 / Valid loss: 6.835025969005766
Training loss: 1.303661823272705 / Valid loss: 7.028460302807036

Epoch: 24
Training loss: 0.9072836637496948 / Valid loss: 6.874256610870361
Training loss: 1.463738203048706 / Valid loss: 7.249494800113497
Training loss: 1.2614871263504028 / Valid loss: 7.6790270987011136
Training loss: 1.8656795024871826 / Valid loss: 6.93865327835083
Training loss: 1.6463844776153564 / Valid loss: 7.353651811963036

Epoch: 25
Training loss: 1.2910799980163574 / Valid loss: 7.123238908676874
Training loss: 0.9603445529937744 / Valid loss: 6.970714428311303
Training loss: 1.2259690761566162 / Valid loss: 7.184761960165841
Training loss: 1.490932583808899 / Valid loss: 7.326941126868839
Training loss: 1.3987452983856201 / Valid loss: 6.7197547503880095

Epoch: 26
Training loss: 0.7861974835395813 / Valid loss: 6.917752965291341
Training loss: 0.9276399612426758 / Valid loss: 6.958292302631197
Training loss: 0.7174285054206848 / Valid loss: 6.893567945843651
Training loss: 1.1799683570861816 / Valid loss: 6.857216019857497
Training loss: 1.2906467914581299 / Valid loss: 6.855250755945842

Epoch: 27
Training loss: 1.1917082071304321 / Valid loss: 7.113608628227597
Training loss: 1.2695585489273071 / Valid loss: 7.065813441503615
Training loss: 1.074073076248169 / Valid loss: 6.9700325557163785
Training loss: 1.068669080734253 / Valid loss: 6.864497012183779
Training loss: 1.1122016906738281 / Valid loss: 7.382117448534284

Epoch: 28
Training loss: 0.6927800178527832 / Valid loss: 7.069400496709914
Training loss: 1.0920995473861694 / Valid loss: 6.958289893468221
Training loss: 1.3357678651809692 / Valid loss: 7.35264105115618
Training loss: 0.7036999464035034 / Valid loss: 7.18602842603411
Training loss: 0.8448824286460876 / Valid loss: 6.954890848341442

Epoch: 29
Training loss: 0.8129819631576538 / Valid loss: 6.676578399113247
Training loss: 0.9716343879699707 / Valid loss: 7.108568595704578
Training loss: 1.2503061294555664 / Valid loss: 6.947793043227422
Training loss: 0.9644453525543213 / Valid loss: 6.926000794910249

Epoch: 30
Training loss: 1.29988431930542 / Valid loss: 6.745769998005458
Training loss: 0.830925464630127 / Valid loss: 7.0951499802725655
Training loss: 0.982170581817627 / Valid loss: 6.9818279198237825
Training loss: 0.9939125776290894 / Valid loss: 7.203937398819696
Training loss: 1.1071534156799316 / Valid loss: 7.168711435227167

Epoch: 31
Training loss: 0.9299470782279968 / Valid loss: 6.973700552894956
Training loss: 0.8468973636627197 / Valid loss: 6.883939089093889
Training loss: 0.9655123949050903 / Valid loss: 7.158388319469633
Training loss: 1.0069719552993774 / Valid loss: 7.1903081893920895
Training loss: 0.7859938144683838 / Valid loss: 7.104474601291475

Epoch: 32
Training loss: 0.8507964015007019 / Valid loss: 7.064040270305815
Training loss: 0.8071850538253784 / Valid loss: 7.020189205805461
Training loss: 0.9651157259941101 / Valid loss: 7.282162048703149
Training loss: 1.2890229225158691 / Valid loss: 6.858470812298003
Training loss: 0.7280094623565674 / Valid loss: 6.995293163117909

Epoch: 33
Training loss: 0.8951378464698792 / Valid loss: 7.54032735143389
Training loss: 1.0199568271636963 / Valid loss: 7.267397551309495
Training loss: 0.4846551716327667 / Valid loss: 6.94464248248509
Training loss: 0.655680239200592 / Valid loss: 7.0118333521343414
Training loss: 0.6668874025344849 / Valid loss: 7.201136339278448

Epoch: 34
Training loss: 0.8159488439559937 / Valid loss: 7.0389862877982
Training loss: 0.6981614828109741 / Valid loss: 6.830832449595134
Training loss: 0.8422548174858093 / Valid loss: 7.1994220234098885
Training loss: 0.8266403675079346 / Valid loss: 7.260476280394054
Training loss: 0.6657344698905945 / Valid loss: 7.123586386726016

Epoch: 35
Training loss: 0.9335864782333374 / Valid loss: 6.9973343667529875
Training loss: 1.0877031087875366 / Valid loss: 6.961214546930222
Training loss: 0.8270500302314758 / Valid loss: 7.073332959129697
Training loss: 0.7281770706176758 / Valid loss: 7.009426659629458
Training loss: 1.0511329174041748 / Valid loss: 6.957224686940511

Epoch: 36
Training loss: 0.7307430505752563 / Valid loss: 7.0236590884980705
Training loss: 1.2371857166290283 / Valid loss: 7.022353326706659
Training loss: 0.9364384412765503 / Valid loss: 7.0606168883187435
Training loss: 0.6283242702484131 / Valid loss: 7.040362539745512
Training loss: 0.8809385895729065 / Valid loss: 7.107246987024943

Epoch: 37
Training loss: 1.0243953466415405 / Valid loss: 7.063058594294957
Training loss: 0.825311005115509 / Valid loss: 7.317543352217902
Training loss: 1.4768478870391846 / Valid loss: 6.773789771397909
Training loss: 0.6189262270927429 / Valid loss: 6.9911947976975215
Training loss: 0.7565938234329224 / Valid loss: 7.075246293204171

Epoch: 38
Training loss: 1.0097010135650635 / Valid loss: 7.0408857073102675
Training loss: 0.8933136463165283 / Valid loss: 7.014933976672944
Training loss: 0.6321536302566528 / Valid loss: 7.395801793961298
Training loss: 0.8896594643592834 / Valid loss: 6.83089359828404
Training loss: 1.0949478149414062 / Valid loss: 7.130780052003407

Epoch: 39
Training loss: 1.030619502067566 / Valid loss: 7.324914820988973
Training loss: 1.0362414121627808 / Valid loss: 6.944248476482573
Training loss: 1.0245888233184814 / Valid loss: 7.209893090384347
Training loss: 0.6435633301734924 / Valid loss: 6.914280246552967
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.1, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 3200): 5.929407192411877
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.259333610534668 / Valid loss: 16.435145014808292
Model is saved in epoch 0, overall batch: 0
Training loss: 9.656736373901367 / Valid loss: 11.915806461515881
Model is saved in epoch 0, overall batch: 100
Training loss: 7.360171318054199 / Valid loss: 8.676773657117572
Model is saved in epoch 0, overall batch: 200
Training loss: 4.653666019439697 / Valid loss: 7.596614256359282
Model is saved in epoch 0, overall batch: 300
Training loss: 6.201526641845703 / Valid loss: 6.986035737537202
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.183350563049316 / Valid loss: 6.655073729015532
Model is saved in epoch 1, overall batch: 500
Training loss: 5.545454978942871 / Valid loss: 6.240480704534622
Model is saved in epoch 1, overall batch: 600
Training loss: 4.544404029846191 / Valid loss: 6.26844885916937
Training loss: 6.507339954376221 / Valid loss: 6.177277592250279
Model is saved in epoch 1, overall batch: 800
Training loss: 6.113231658935547 / Valid loss: 6.008907284055438
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 4.290149211883545 / Valid loss: 5.972205815996443
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.929659366607666 / Valid loss: 5.848566084816342
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.574641227722168 / Valid loss: 5.854276466369629
Training loss: 3.610111713409424 / Valid loss: 5.777844565255301
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.312253952026367 / Valid loss: 5.814828034809658

Epoch: 3
Training loss: 4.0945024490356445 / Valid loss: 5.7372501259758355
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.808259010314941 / Valid loss: 5.731258875983102
Model is saved in epoch 3, overall batch: 1600
Training loss: 4.473724842071533 / Valid loss: 5.700148668743315
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.226859092712402 / Valid loss: 5.656503568376814
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.762414932250977 / Valid loss: 5.61295229820978
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.2497735023498535 / Valid loss: 5.634256328855242
Training loss: 3.1714327335357666 / Valid loss: 5.646207934334164
Training loss: 3.5227012634277344 / Valid loss: 5.598605380739484
Model is saved in epoch 4, overall batch: 2200
Training loss: 4.316646575927734 / Valid loss: 5.631692352749052
Training loss: 3.242931604385376 / Valid loss: 5.629306523005168

Epoch: 5
Training loss: 3.478403091430664 / Valid loss: 5.584209966659546
Model is saved in epoch 5, overall batch: 2500
Training loss: 3.8265724182128906 / Valid loss: 5.622919852393014
Training loss: 2.5614500045776367 / Valid loss: 5.609800568081083
Training loss: 5.639426231384277 / Valid loss: 5.626148843765259
Training loss: 4.399801254272461 / Valid loss: 5.601672962733677

Epoch: 6
Training loss: 3.1725308895111084 / Valid loss: 5.624337882087344
Training loss: 4.201813697814941 / Valid loss: 5.638484262284779
Training loss: 3.9868218898773193 / Valid loss: 5.636259176617577
Training loss: 3.381117343902588 / Valid loss: 5.635718297958374
Training loss: 5.117909908294678 / Valid loss: 5.640564596085321

Epoch: 7
Training loss: 3.392228126525879 / Valid loss: 5.657495469138736
Training loss: 2.8876395225524902 / Valid loss: 5.672541761398316
Training loss: 3.0711007118225098 / Valid loss: 5.6715065933409194
Training loss: 2.138035535812378 / Valid loss: 5.685601992834182
Training loss: 2.6881890296936035 / Valid loss: 5.677337176459176

Epoch: 8
Training loss: 3.6227238178253174 / Valid loss: 5.748072637830462
Training loss: 3.5123443603515625 / Valid loss: 5.720457447142828
Training loss: 1.8070886135101318 / Valid loss: 5.711460989997501
Training loss: 3.0351791381835938 / Valid loss: 5.7303375766390845
Training loss: 2.715576171875 / Valid loss: 5.733317958740961

Epoch: 9
Training loss: 2.016134738922119 / Valid loss: 5.763715244474865
Training loss: 2.9120874404907227 / Valid loss: 5.79701158886864
Training loss: 2.6318607330322266 / Valid loss: 5.761129901522682
Training loss: 3.322821617126465 / Valid loss: 5.766206882113502

Epoch: 10
Training loss: 2.768263339996338 / Valid loss: 5.827017091569447
Training loss: 2.387207508087158 / Valid loss: 5.796718263626099
Training loss: 2.5322747230529785 / Valid loss: 5.826635780788603
Training loss: 3.324629783630371 / Valid loss: 5.884460301626296
Training loss: 3.0467867851257324 / Valid loss: 5.819215297698975

Epoch: 11
Training loss: 1.389395833015442 / Valid loss: 5.827604602632069
Training loss: 2.3037447929382324 / Valid loss: 5.862783808935256
Training loss: 2.087580680847168 / Valid loss: 5.844909749712262
Training loss: 2.232163429260254 / Valid loss: 5.885317977269491
Training loss: 3.0844056606292725 / Valid loss: 5.903852941876366

Epoch: 12
Training loss: 2.545865058898926 / Valid loss: 5.956541701725551
Training loss: 1.746414303779602 / Valid loss: 5.917016444887434
Training loss: 2.7158780097961426 / Valid loss: 5.940202792485555
Training loss: 2.604160785675049 / Valid loss: 5.938649393263317
Training loss: 1.6757984161376953 / Valid loss: 5.925249099731445

Epoch: 13
Training loss: 2.738624095916748 / Valid loss: 5.969541706357684
Training loss: 1.8818329572677612 / Valid loss: 5.95171806925819
Training loss: 2.1500566005706787 / Valid loss: 5.990481753576369
Training loss: 1.3220293521881104 / Valid loss: 5.967006170182001
Training loss: 2.197519540786743 / Valid loss: 5.989609788713001

Epoch: 14
Training loss: 1.9561529159545898 / Valid loss: 6.001707935333252
Training loss: 1.8068511486053467 / Valid loss: 6.002240255900792
Training loss: 1.6961767673492432 / Valid loss: 6.002722306478591
Training loss: 1.7200853824615479 / Valid loss: 6.015922728038969
Training loss: 1.5523817539215088 / Valid loss: 6.048717260360718

Epoch: 15
Training loss: 1.7469444274902344 / Valid loss: 6.021402104695638
Training loss: 1.642721176147461 / Valid loss: 6.061073995771862
Training loss: 1.7255394458770752 / Valid loss: 6.043357417696998
Training loss: 2.445814847946167 / Valid loss: 6.032468918391636
Training loss: 1.1910885572433472 / Valid loss: 6.077316577093942

Epoch: 16
Training loss: 1.021675944328308 / Valid loss: 6.073178484326317
Training loss: 1.1219109296798706 / Valid loss: 6.091363066718692
Training loss: 1.0187398195266724 / Valid loss: 6.069835238229661
Training loss: 1.2830840349197388 / Valid loss: 6.11912796156747
Training loss: 2.4268412590026855 / Valid loss: 6.100916085924421

Epoch: 17
Training loss: 1.8424110412597656 / Valid loss: 6.119933166958037
Training loss: 0.858673095703125 / Valid loss: 6.186771737961542
Training loss: 1.7459907531738281 / Valid loss: 6.10324791045416
Training loss: 1.6097674369812012 / Valid loss: 6.098671118418376
Training loss: 1.1459778547286987 / Valid loss: 6.210958573931739

Epoch: 18
Training loss: 1.6109700202941895 / Valid loss: 6.190766770499093
Training loss: 1.4670581817626953 / Valid loss: 6.2030980655125205
Training loss: 1.4454874992370605 / Valid loss: 6.169508607046945
Training loss: 1.5398361682891846 / Valid loss: 6.183945446922666
Training loss: 0.8964177966117859 / Valid loss: 6.162225155603318

Epoch: 19
Training loss: 1.8565316200256348 / Valid loss: 6.213686579749698
Training loss: 0.9229567050933838 / Valid loss: 6.252277392432803
Training loss: 1.326017141342163 / Valid loss: 6.2725553285507925
Training loss: 1.4902656078338623 / Valid loss: 6.333448210216704

Epoch: 20
Training loss: 1.3366141319274902 / Valid loss: 6.207834924970355
Training loss: 1.759479284286499 / Valid loss: 6.272898428780692
Training loss: 0.9756330251693726 / Valid loss: 6.226418009258452
Training loss: 1.259208083152771 / Valid loss: 6.262751615615118
Training loss: 1.5531425476074219 / Valid loss: 6.267481277102516

Epoch: 21
Training loss: 0.7692980766296387 / Valid loss: 6.287862634658813
Training loss: 1.2185003757476807 / Valid loss: 6.270938598541987
Training loss: 1.4358627796173096 / Valid loss: 6.3235563527970085
Training loss: 1.2190207242965698 / Valid loss: 6.283862740652902
Training loss: 1.4086079597473145 / Valid loss: 6.286919598352341

Epoch: 22
Training loss: 0.9707290530204773 / Valid loss: 6.286649054572695
Training loss: 0.891342282295227 / Valid loss: 6.2995107469104585
Training loss: 1.017005443572998 / Valid loss: 6.277640156518846
Training loss: 0.8528577089309692 / Valid loss: 6.345868514832996
Training loss: 1.035997748374939 / Valid loss: 6.297364684513637

Epoch: 23
Training loss: 1.130850076675415 / Valid loss: 6.270120579855782
Training loss: 1.4320564270019531 / Valid loss: 6.322908551352365
Training loss: 1.0157794952392578 / Valid loss: 6.393754164377849
Training loss: 1.3953096866607666 / Valid loss: 6.305495477858043
Training loss: 1.1636085510253906 / Valid loss: 6.327693721226283

Epoch: 24
Training loss: 0.6582541465759277 / Valid loss: 6.370045730045864
Training loss: 1.0160961151123047 / Valid loss: 6.360620544070289
Training loss: 1.6070035696029663 / Valid loss: 6.320773674192883
Training loss: 0.8850827217102051 / Valid loss: 6.38423570224217
Training loss: 0.7623178362846375 / Valid loss: 6.345044948941186

Epoch: 25
Training loss: 1.3036996126174927 / Valid loss: 6.3590433960869195
Training loss: 1.163585901260376 / Valid loss: 6.373905150095622
Training loss: 1.347421407699585 / Valid loss: 6.373064052490961
Training loss: 0.91635662317276 / Valid loss: 6.359265141260057
Training loss: 0.8425555229187012 / Valid loss: 6.39049988020034

Epoch: 26
Training loss: 0.6943379640579224 / Valid loss: 6.378095136369978
Training loss: 1.2939444780349731 / Valid loss: 6.406750765300933
Training loss: 0.5741463303565979 / Valid loss: 6.518595911207653
Training loss: 0.8556338548660278 / Valid loss: 6.385843004499163
Training loss: 1.0081290006637573 / Valid loss: 6.404215208689371

Epoch: 27
Training loss: 1.4640259742736816 / Valid loss: 6.3727906363351
Training loss: 1.363945722579956 / Valid loss: 6.415803827558245
Training loss: 0.9687461256980896 / Valid loss: 6.480388486953009
Training loss: 0.43115314841270447 / Valid loss: 6.415302417391822
Training loss: 0.5296947956085205 / Valid loss: 6.439090097518195

Epoch: 28
Training loss: 0.6300480365753174 / Valid loss: 6.402006621587844
Training loss: 1.072167992591858 / Valid loss: 6.470792316255116
Training loss: 1.570549488067627 / Valid loss: 6.430376445679437
Training loss: 0.827499508857727 / Valid loss: 6.467253430684408
Training loss: 0.8325491547584534 / Valid loss: 6.476352805183048

Epoch: 29
Training loss: 1.089832067489624 / Valid loss: 6.474161643073672
Training loss: 0.7706984877586365 / Valid loss: 6.46599391301473
Training loss: 1.1473255157470703 / Valid loss: 6.464371513185047
Training loss: 1.0451476573944092 / Valid loss: 6.412331674212501

Epoch: 30
Training loss: 0.7786692976951599 / Valid loss: 6.44972783043271
Training loss: 0.7155652046203613 / Valid loss: 6.485316880544027
Training loss: 1.2454054355621338 / Valid loss: 6.4182964733668735
Training loss: 1.1534003019332886 / Valid loss: 6.509569822038923
Training loss: 0.7171438932418823 / Valid loss: 6.4412029765901115

Epoch: 31
Training loss: 0.7481035590171814 / Valid loss: 6.4928104718526205
Training loss: 0.8635866641998291 / Valid loss: 6.472892636344547
Training loss: 0.39187055826187134 / Valid loss: 6.49988260950361
Training loss: 0.6325938701629639 / Valid loss: 6.534470181238084
Training loss: 1.4693429470062256 / Valid loss: 6.446711158752441

Epoch: 32
Training loss: 0.5463472604751587 / Valid loss: 6.469114498865037
Training loss: 0.8369996547698975 / Valid loss: 6.5240378743126275
Training loss: 0.6368176341056824 / Valid loss: 6.4674192156110495
Training loss: 0.4699992537498474 / Valid loss: 6.52179833820888
Training loss: 0.9670567512512207 / Valid loss: 6.548185845783778

Epoch: 33
Training loss: 0.9706600904464722 / Valid loss: 6.490886256808326
Training loss: 0.34264475107192993 / Valid loss: 6.479342017854963
Training loss: 0.5566620826721191 / Valid loss: 6.509813899085636
Training loss: 0.5178676247596741 / Valid loss: 6.553862271990095
Training loss: 0.34633147716522217 / Valid loss: 6.564504455384754

Epoch: 34
Training loss: 0.38718852400779724 / Valid loss: 6.523391784940447
Training loss: 0.6692057847976685 / Valid loss: 6.491994687489101
Training loss: 0.4162120223045349 / Valid loss: 6.564286795116606
Training loss: 0.5795543193817139 / Valid loss: 6.534366189865839
Training loss: 0.6742663383483887 / Valid loss: 6.506174855005174

Epoch: 35
Training loss: 0.495042622089386 / Valid loss: 6.551986707959856
Training loss: 0.44483494758605957 / Valid loss: 6.52120478720892
Training loss: 0.7068341970443726 / Valid loss: 6.561132258460635
Training loss: 0.5355184674263 / Valid loss: 6.626845169067383
Training loss: 0.6875354051589966 / Valid loss: 6.54656769434611

Epoch: 36
Training loss: 0.281777560710907 / Valid loss: 6.588646023614066
Training loss: 0.33971768617630005 / Valid loss: 6.564140156337193
Training loss: 0.576815128326416 / Valid loss: 6.560635534922282
Training loss: 0.7688194513320923 / Valid loss: 6.549560326621646
Training loss: 0.5027329921722412 / Valid loss: 6.618017514546712

Epoch: 37
Training loss: 0.2860528230667114 / Valid loss: 6.602712908245269
Training loss: 0.943156361579895 / Valid loss: 6.6465270065125965
Training loss: 0.37280094623565674 / Valid loss: 6.578622931525821
Training loss: 0.6105754971504211 / Valid loss: 6.56970382872082
Training loss: 0.4593372344970703 / Valid loss: 6.564340303057716

Epoch: 38
Training loss: 0.38764625787734985 / Valid loss: 6.571746299380348
Training loss: 0.4368615746498108 / Valid loss: 6.560213347843715
Training loss: 0.5223396420478821 / Valid loss: 6.627786688577562
Training loss: 0.40541505813598633 / Valid loss: 6.557292761121477
Training loss: 0.6811208724975586 / Valid loss: 6.578739999589466

Epoch: 39
Training loss: 0.48693540692329407 / Valid loss: 6.584630067007883
Training loss: 0.4633547365665436 / Valid loss: 6.598547031765892
Training loss: 0.5510274171829224 / Valid loss: 6.618063556580316
Training loss: 0.31495213508605957 / Valid loss: 6.582462106432233
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2500): 5.441747617721558
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.1
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.259333610534668 / Valid loss: 16.435145014808292
Model is saved in epoch 0, overall batch: 0
Training loss: 9.656743049621582 / Valid loss: 11.915828105381557
Model is saved in epoch 0, overall batch: 100
Training loss: 7.36015510559082 / Valid loss: 8.676798575265067
Model is saved in epoch 0, overall batch: 200
Training loss: 4.653659820556641 / Valid loss: 7.596634456089565
Model is saved in epoch 0, overall batch: 300
Training loss: 6.201533317565918 / Valid loss: 6.986035129002162
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.183323383331299 / Valid loss: 6.655951835995629
Model is saved in epoch 1, overall batch: 500
Training loss: 5.545433044433594 / Valid loss: 6.2408271585192
Model is saved in epoch 1, overall batch: 600
Training loss: 4.544597625732422 / Valid loss: 6.268429449626377
Training loss: 6.5068206787109375 / Valid loss: 6.177431735538301
Model is saved in epoch 1, overall batch: 800
Training loss: 6.113285541534424 / Valid loss: 6.00710270972479
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 4.290239334106445 / Valid loss: 5.979392367317563
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.929746389389038 / Valid loss: 5.848816492443993
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.574531555175781 / Valid loss: 5.854441477003552
Training loss: 3.611762523651123 / Valid loss: 5.778041403634208
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.315954208374023 / Valid loss: 5.808241210665021

Epoch: 3
Training loss: 4.0967864990234375 / Valid loss: 5.74089305514381
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.809889316558838 / Valid loss: 5.731424234026954
Model is saved in epoch 3, overall batch: 1600
Training loss: 4.474191665649414 / Valid loss: 5.705007167089553
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.226729393005371 / Valid loss: 5.652192301977248
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.752376556396484 / Valid loss: 5.613283979325067
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.248035430908203 / Valid loss: 5.635170634587606
Training loss: 3.1670641899108887 / Valid loss: 5.644302300044468
Training loss: 3.5212929248809814 / Valid loss: 5.597260116395496
Model is saved in epoch 4, overall batch: 2200
Training loss: 4.314368724822998 / Valid loss: 5.624925854092552
Training loss: 3.238691568374634 / Valid loss: 5.638681248256138

Epoch: 5
Training loss: 3.482004165649414 / Valid loss: 5.583972242900304
Model is saved in epoch 5, overall batch: 2500
Training loss: 3.8473968505859375 / Valid loss: 5.618407194955008
Training loss: 2.566376209259033 / Valid loss: 5.613142479033697
Training loss: 5.654492378234863 / Valid loss: 5.6246663842882425
Training loss: 4.3667192459106445 / Valid loss: 5.600653925396148

Epoch: 6
Training loss: 3.169863224029541 / Valid loss: 5.621766807919457
Training loss: 4.22108793258667 / Valid loss: 5.636230738957723
Training loss: 3.99206805229187 / Valid loss: 5.6372660001118975
Training loss: 3.3731956481933594 / Valid loss: 5.635098123550415
Training loss: 5.128280162811279 / Valid loss: 5.641755487805321

Epoch: 7
Training loss: 3.3966495990753174 / Valid loss: 5.658609365281604
Training loss: 2.875617265701294 / Valid loss: 5.673463090260824
Training loss: 3.0773816108703613 / Valid loss: 5.673545440038045
Training loss: 2.128023147583008 / Valid loss: 5.687909789312453
Training loss: 2.695061445236206 / Valid loss: 5.682799491428193

Epoch: 8
Training loss: 3.6054439544677734 / Valid loss: 5.729840959821429
Training loss: 3.510230779647827 / Valid loss: 5.721938946133569
Training loss: 1.8194236755371094 / Valid loss: 5.7108898503439764
Training loss: 3.034679889678955 / Valid loss: 5.731125736236573
Training loss: 2.7069244384765625 / Valid loss: 5.729051637649536

Epoch: 9
Training loss: 2.0146610736846924 / Valid loss: 5.763382418950399
Training loss: 2.908608913421631 / Valid loss: 5.796623132342384
Training loss: 2.6390390396118164 / Valid loss: 5.76075035958063
Training loss: 3.3577303886413574 / Valid loss: 5.768542239779518

Epoch: 10
Training loss: 2.772714138031006 / Valid loss: 5.813191847574143
Training loss: 2.360445976257324 / Valid loss: 5.802108174278622
Training loss: 2.5357437133789062 / Valid loss: 5.828940019153413
Training loss: 3.3284707069396973 / Valid loss: 5.874680805206299
Training loss: 3.0402026176452637 / Valid loss: 5.821887904121763

Epoch: 11
Training loss: 1.394750714302063 / Valid loss: 5.8300415129888625
Training loss: 2.298013925552368 / Valid loss: 5.8578329858325775
Training loss: 2.125310182571411 / Valid loss: 5.843875655673799
Training loss: 2.2441396713256836 / Valid loss: 5.884641093299503
Training loss: 3.1039390563964844 / Valid loss: 5.895423071725028

Epoch: 12
Training loss: 2.5618371963500977 / Valid loss: 5.965671848115467
Training loss: 1.750932216644287 / Valid loss: 5.92578562554859
Training loss: 2.74318790435791 / Valid loss: 5.942974435715448
Training loss: 2.6054611206054688 / Valid loss: 5.946618023372832
Training loss: 1.6773531436920166 / Valid loss: 5.925232047126407

Epoch: 13
Training loss: 2.7457058429718018 / Valid loss: 5.959202784583682
Training loss: 1.9131460189819336 / Valid loss: 5.963440263838995
Training loss: 2.158616065979004 / Valid loss: 6.001882575807118
Training loss: 1.3296568393707275 / Valid loss: 5.978852816990444
Training loss: 2.2117202281951904 / Valid loss: 5.993765290578207

Epoch: 14
Training loss: 1.9534802436828613 / Valid loss: 6.000421208427066
Training loss: 1.806417465209961 / Valid loss: 6.005483541034517
Training loss: 1.6979118585586548 / Valid loss: 5.996688020797003
Training loss: 1.7133067846298218 / Valid loss: 6.013765605290731
Training loss: 1.5590513944625854 / Valid loss: 6.066223271687826

Epoch: 15
Training loss: 1.7574996948242188 / Valid loss: 6.028542214348203
Training loss: 1.6456973552703857 / Valid loss: 6.062782691773914
Training loss: 1.731769323348999 / Valid loss: 6.051178369067964
Training loss: 2.45741605758667 / Valid loss: 6.028413277580625
Training loss: 1.1889077425003052 / Valid loss: 6.072294376009986

Epoch: 16
Training loss: 1.0255837440490723 / Valid loss: 6.091011217662266
Training loss: 1.1166824102401733 / Valid loss: 6.11062810080392
Training loss: 1.0268704891204834 / Valid loss: 6.0688813709077385
Training loss: 1.2927744388580322 / Valid loss: 6.121830095563616
Training loss: 2.4122250080108643 / Valid loss: 6.096461791083926

Epoch: 17
Training loss: 1.844943881034851 / Valid loss: 6.112821710677374
Training loss: 0.8658031225204468 / Valid loss: 6.18530905133202
Training loss: 1.7324000597000122 / Valid loss: 6.107709550857544
Training loss: 1.6256855726242065 / Valid loss: 6.1089833486647835
Training loss: 1.1482632160186768 / Valid loss: 6.235634267897833

Epoch: 18
Training loss: 1.6176929473876953 / Valid loss: 6.222371503284999
Training loss: 1.465078592300415 / Valid loss: 6.192992494219825
Training loss: 1.4225614070892334 / Valid loss: 6.1783950124468126
Training loss: 1.5508499145507812 / Valid loss: 6.183992983046032
Training loss: 0.9093891382217407 / Valid loss: 6.168911931628273

Epoch: 19
Training loss: 1.839840292930603 / Valid loss: 6.200035610653106
Training loss: 0.920336902141571 / Valid loss: 6.26189439410255
Training loss: 1.3134368658065796 / Valid loss: 6.238217113131569
Training loss: 1.498415231704712 / Valid loss: 6.353581614721389

Epoch: 20
Training loss: 1.3504400253295898 / Valid loss: 6.21079470316569
Training loss: 1.7554503679275513 / Valid loss: 6.279201028460548
Training loss: 0.9879138469696045 / Valid loss: 6.2227390698024205
Training loss: 1.2434051036834717 / Valid loss: 6.27664916628883
Training loss: 1.5496866703033447 / Valid loss: 6.264705108460926

Epoch: 21
Training loss: 0.7813289165496826 / Valid loss: 6.291418550128029
Training loss: 1.2056092023849487 / Valid loss: 6.265340092068627
Training loss: 1.425512671470642 / Valid loss: 6.310863885425386
Training loss: 1.209689974784851 / Valid loss: 6.268365024384998
Training loss: 1.4303991794586182 / Valid loss: 6.298833283923921

Epoch: 22
Training loss: 0.9750193953514099 / Valid loss: 6.2879411197844
Training loss: 0.8897969126701355 / Valid loss: 6.3010790824890135
Training loss: 1.003349781036377 / Valid loss: 6.281572494052705
Training loss: 0.8584295511245728 / Valid loss: 6.356659553164527
Training loss: 1.0628774166107178 / Valid loss: 6.2982520058041525

Epoch: 23
Training loss: 1.1803879737854004 / Valid loss: 6.277848629724412
Training loss: 1.4444972276687622 / Valid loss: 6.318027809688023
Training loss: 1.0167540311813354 / Valid loss: 6.382052271706717
Training loss: 1.406493902206421 / Valid loss: 6.308758054460798
Training loss: 1.1793992519378662 / Valid loss: 6.340469925744193

Epoch: 24
Training loss: 0.6523464322090149 / Valid loss: 6.3962541035243445
Training loss: 1.0215498208999634 / Valid loss: 6.370502780732655
Training loss: 1.6088351011276245 / Valid loss: 6.308223027274722
Training loss: 0.8943999409675598 / Valid loss: 6.393934290749686
Training loss: 0.7445492744445801 / Valid loss: 6.34001240276155

Epoch: 25
Training loss: 1.2979421615600586 / Valid loss: 6.369876748039609
Training loss: 1.1415040493011475 / Valid loss: 6.362417588915144
Training loss: 1.3283175230026245 / Valid loss: 6.359952047892979
Training loss: 0.9101882576942444 / Valid loss: 6.352322092510405
Training loss: 0.840482234954834 / Valid loss: 6.383712103253319

Epoch: 26
Training loss: 0.704574465751648 / Valid loss: 6.377656205495199
Training loss: 1.2594401836395264 / Valid loss: 6.393088847114926
Training loss: 0.5787103772163391 / Valid loss: 6.506090813591367
Training loss: 0.8661280870437622 / Valid loss: 6.392279436474754
Training loss: 1.0050393342971802 / Valid loss: 6.398721813020252

Epoch: 27
Training loss: 1.4615652561187744 / Valid loss: 6.394275919596354
Training loss: 1.3749891519546509 / Valid loss: 6.428466771897815
Training loss: 0.9668968319892883 / Valid loss: 6.4733627410162065
Training loss: 0.4325372576713562 / Valid loss: 6.408987245105561
Training loss: 0.5311695337295532 / Valid loss: 6.4437762623741515

Epoch: 28
Training loss: 0.6402966976165771 / Valid loss: 6.40121359598069
Training loss: 1.0856897830963135 / Valid loss: 6.479388236999512
Training loss: 1.5779834985733032 / Valid loss: 6.429793083100092
Training loss: 0.8265392184257507 / Valid loss: 6.469316809517997
Training loss: 0.8314162492752075 / Valid loss: 6.461826955704462

Epoch: 29
Training loss: 1.0921735763549805 / Valid loss: 6.489464827946255
Training loss: 0.7827434539794922 / Valid loss: 6.449318751834688
Training loss: 1.1549763679504395 / Valid loss: 6.473545828319732
Training loss: 1.0624711513519287 / Valid loss: 6.40806664512271

Epoch: 30
Training loss: 0.8002250790596008 / Valid loss: 6.464316270464942
Training loss: 0.7092769742012024 / Valid loss: 6.507359518323626
Training loss: 1.2295198440551758 / Valid loss: 6.415665136064802
Training loss: 1.1244783401489258 / Valid loss: 6.546830111458188
Training loss: 0.7214834690093994 / Valid loss: 6.4404632795424686

Epoch: 31
Training loss: 0.7556278705596924 / Valid loss: 6.500296801612491
Training loss: 0.8588359355926514 / Valid loss: 6.46826080594744
Training loss: 0.3858790993690491 / Valid loss: 6.506213821683612
Training loss: 0.6337944269180298 / Valid loss: 6.530867787769862
Training loss: 1.4774682521820068 / Valid loss: 6.450102547236852

Epoch: 32
Training loss: 0.5369887948036194 / Valid loss: 6.475226797376361
Training loss: 0.8271461725234985 / Valid loss: 6.52224007334028
Training loss: 0.6343143582344055 / Valid loss: 6.468603152320498
Training loss: 0.46854352951049805 / Valid loss: 6.515615722111293
Training loss: 0.9598367214202881 / Valid loss: 6.544172455015636

Epoch: 33
Training loss: 0.9742313623428345 / Valid loss: 6.4773042815072195
Training loss: 0.33332565426826477 / Valid loss: 6.485061541057768
Training loss: 0.5536550283432007 / Valid loss: 6.509533614204043
Training loss: 0.5106611251831055 / Valid loss: 6.558767981756301
Training loss: 0.32761818170547485 / Valid loss: 6.563262678328014

Epoch: 34
Training loss: 0.39075469970703125 / Valid loss: 6.525292791639056
Training loss: 0.6759459376335144 / Valid loss: 6.501220174062819
Training loss: 0.42167338728904724 / Valid loss: 6.549793933686756
Training loss: 0.5998241305351257 / Valid loss: 6.53500079654512
Training loss: 0.6780955195426941 / Valid loss: 6.505870140166509

Epoch: 35
Training loss: 0.5025414228439331 / Valid loss: 6.549769996461414
Training loss: 0.4385240972042084 / Valid loss: 6.526422929763794
Training loss: 0.7335925698280334 / Valid loss: 6.56326892035348
Training loss: 0.5256472229957581 / Valid loss: 6.630492423829578
Training loss: 0.687150239944458 / Valid loss: 6.552840212413243

Epoch: 36
Training loss: 0.2944227457046509 / Valid loss: 6.598394734518869
Training loss: 0.3520016670227051 / Valid loss: 6.565612661270868
Training loss: 0.5891422033309937 / Valid loss: 6.563053458077567
Training loss: 0.754538893699646 / Valid loss: 6.555079818907238
Training loss: 0.49852287769317627 / Valid loss: 6.617569124130975

Epoch: 37
Training loss: 0.2908424437046051 / Valid loss: 6.611855538686116
Training loss: 0.9636273980140686 / Valid loss: 6.646649789810181
Training loss: 0.3662620186805725 / Valid loss: 6.577524621146066
Training loss: 0.6199964284896851 / Valid loss: 6.569069358280727
Training loss: 0.46499621868133545 / Valid loss: 6.570479924338205

Epoch: 38
Training loss: 0.377290815114975 / Valid loss: 6.5653764997209825
Training loss: 0.4618781805038452 / Valid loss: 6.562416344597226
Training loss: 0.5059029459953308 / Valid loss: 6.633222312018985
Training loss: 0.4108535051345825 / Valid loss: 6.56386802764166
Training loss: 0.6787098050117493 / Valid loss: 6.575755332765125

Epoch: 39
Training loss: 0.47644591331481934 / Valid loss: 6.581936214083717
Training loss: 0.4585091173648834 / Valid loss: 6.606940482911609
Training loss: 0.5588209629058838 / Valid loss: 6.601020681290399
Training loss: 0.31905198097229004 / Valid loss: 6.581930455707369
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.1, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2500): 5.442645286378406
