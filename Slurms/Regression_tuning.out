********************************************************************************************
** WARNING: 'The 'pre2019' module environment is deprecated. Please consider switching
             to the '2019' or '2020' module environment. You can read more about our
             software policy on this page:
             https://userinfo.surfsara.nl/documentation/software-policy-lisacartesius

             If you have any question, please contact us via http://servicedesk.surfsara.nl.'
********************************************************************************************
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 13.399635451180595
Model is saved in epoch 0, overall batch: 0
Training loss: 6.127030372619629 / Valid loss: 8.819717675163632
Model is saved in epoch 0, overall batch: 100
Training loss: 4.364099502563477 / Valid loss: 6.371737811678932
Model is saved in epoch 0, overall batch: 200
Training loss: 6.64052152633667 / Valid loss: 5.719195552099318
Model is saved in epoch 0, overall batch: 300
Training loss: 6.042747497558594 / Valid loss: 5.520333778290522
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.577243804931641 / Valid loss: 5.533714669091361
Training loss: 5.004234313964844 / Valid loss: 5.621288937614078
Training loss: 5.9308390617370605 / Valid loss: 5.55015804654076
Training loss: 4.668268203735352 / Valid loss: 5.53403328259786
Training loss: 4.401596546173096 / Valid loss: 5.578369969413394

Epoch: 2
Training loss: 6.164501190185547 / Valid loss: 5.500020435878208
Model is saved in epoch 2, overall batch: 1000
Training loss: 6.425787448883057 / Valid loss: 5.61622618720645
Training loss: 5.164256572723389 / Valid loss: 5.628493234089443
Training loss: 6.742783546447754 / Valid loss: 5.578919303984869
Training loss: 4.455643177032471 / Valid loss: 5.567807924179804

Epoch: 3
Training loss: 3.558081865310669 / Valid loss: 5.672660146440778
Training loss: 4.075849533081055 / Valid loss: 5.8182059742155525
Training loss: 5.273836135864258 / Valid loss: 5.57822737239656
Training loss: 3.2550714015960693 / Valid loss: 5.85000859215146
Training loss: 5.04761266708374 / Valid loss: 5.77729852994283

Epoch: 4
Training loss: 4.5832037925720215 / Valid loss: 5.890043653760638
Training loss: 4.638767242431641 / Valid loss: 5.875842223848616
Training loss: 4.182924270629883 / Valid loss: 5.857748565219698
Training loss: 5.140944480895996 / Valid loss: 5.778268911724999
Training loss: 4.2131147384643555 / Valid loss: 5.800540404092698

Epoch: 5
Training loss: 6.79914665222168 / Valid loss: 6.263358293260847
Training loss: 4.713563919067383 / Valid loss: 6.228647985912505
Training loss: 3.157804489135742 / Valid loss: 6.161113966078985
Training loss: 3.8821210861206055 / Valid loss: 5.930653780982608
Training loss: 3.887453556060791 / Valid loss: 5.941768146696544

Epoch: 6
Training loss: 6.122968673706055 / Valid loss: 6.3214335487002415
Training loss: 3.285597324371338 / Valid loss: 6.374911083493914
Training loss: 3.578014373779297 / Valid loss: 6.5801569416409444
Training loss: 3.965419292449951 / Valid loss: 6.214591870989119
Training loss: 3.384624719619751 / Valid loss: 6.468242118472145

Epoch: 7
Training loss: 3.21475887298584 / Valid loss: 6.497412256967454
Training loss: 2.1728620529174805 / Valid loss: 6.684479788371495
Training loss: 3.4841299057006836 / Valid loss: 6.710339098884946
Training loss: 4.321338653564453 / Valid loss: 6.417580468314035
Training loss: 2.972651481628418 / Valid loss: 6.868036967232114

Epoch: 8
Training loss: 1.7738990783691406 / Valid loss: 7.195124421800886
Training loss: 2.1441798210144043 / Valid loss: 6.642926300139655
Training loss: 3.251882553100586 / Valid loss: 6.759008693695068
Training loss: 4.096506118774414 / Valid loss: 6.78841270946321
Training loss: 3.183248519897461 / Valid loss: 6.636155123937698

Epoch: 9
Training loss: 2.029506206512451 / Valid loss: 6.818800658271426
Training loss: 2.288989543914795 / Valid loss: 6.828665347326369
Training loss: 3.3808815479278564 / Valid loss: 6.763192390260242
Training loss: 2.3981070518493652 / Valid loss: 7.739439296722412

Epoch: 10
Training loss: 1.4216480255126953 / Valid loss: 7.388351267860049
Training loss: 1.8197413682937622 / Valid loss: 7.151865641276042
Training loss: 1.5796442031860352 / Valid loss: 6.846092355818976
Training loss: 2.577202081680298 / Valid loss: 6.945623552231561
Training loss: 1.7377214431762695 / Valid loss: 7.782596015930176

Epoch: 11
Training loss: 1.0891811847686768 / Valid loss: 7.376577949523925
Training loss: 1.5657374858856201 / Valid loss: 7.172021066574823
Training loss: 1.6767587661743164 / Valid loss: 7.089032963344029
Training loss: 1.4473447799682617 / Valid loss: 7.44668386777242
Training loss: 2.353316307067871 / Valid loss: 7.667879840305873

Epoch: 12
Training loss: 0.8911303877830505 / Valid loss: 7.021634292602539
Training loss: 1.141905665397644 / Valid loss: 7.188700862157912
Training loss: 1.278510332107544 / Valid loss: 6.898002352033343
Training loss: 1.1357593536376953 / Valid loss: 7.265919263022287
Training loss: 1.4851073026657104 / Valid loss: 7.375135644276937

Epoch: 13
Training loss: 1.0675407648086548 / Valid loss: 7.076451192583357
Training loss: 1.0887376070022583 / Valid loss: 7.194370133536203
Training loss: 1.1945703029632568 / Valid loss: 7.195398943764823
Training loss: 1.4118218421936035 / Valid loss: 6.989992854708717
Training loss: 1.1957453489303589 / Valid loss: 7.465697842552548

Epoch: 14
Training loss: 0.9668686985969543 / Valid loss: 7.549950263613746
Training loss: 1.7238690853118896 / Valid loss: 7.141253562200637
Training loss: 1.3160755634307861 / Valid loss: 7.293925380706787
Training loss: 0.8440595865249634 / Valid loss: 7.21663301104591
Training loss: 1.5144141912460327 / Valid loss: 7.172609601702009

Epoch: 15
Training loss: 0.5426336526870728 / Valid loss: 7.424824369521368
Training loss: 1.441624641418457 / Valid loss: 7.28578322728475
Training loss: 0.6639300584793091 / Valid loss: 7.345029090699695
Training loss: 0.9602261185646057 / Valid loss: 7.381763621738979
Training loss: 1.199022650718689 / Valid loss: 7.348576191493443

Epoch: 16
Training loss: 0.8247591853141785 / Valid loss: 7.196861439659482
Training loss: 0.6559747457504272 / Valid loss: 7.253304481506348
Training loss: 0.5706334114074707 / Valid loss: 7.4278565043494815
Training loss: 1.1128830909729004 / Valid loss: 7.461133911496117
Training loss: 0.9497281312942505 / Valid loss: 7.330151507967995

Epoch: 17
Training loss: 1.0422170162200928 / Valid loss: 7.381999202001662
Training loss: 1.3179240226745605 / Valid loss: 7.1618699482509065
Training loss: 1.3021745681762695 / Valid loss: 7.322104590279715
Training loss: 0.7087141275405884 / Valid loss: 7.289461694444928
Training loss: 0.6131398677825928 / Valid loss: 7.1717011315482

Epoch: 18
Training loss: 0.6964162588119507 / Valid loss: 7.2683129946390785
Training loss: 0.7655309438705444 / Valid loss: 7.08795215969994
Training loss: 0.6940097808837891 / Valid loss: 7.168793280919393
Training loss: 0.9227856397628784 / Valid loss: 7.104040431976318
Training loss: 0.9503320455551147 / Valid loss: 7.439989203498477

Epoch: 19
Training loss: 1.2825490236282349 / Valid loss: 7.452070376986549
Training loss: 1.5297114849090576 / Valid loss: 7.438924239930652
Training loss: 1.0516955852508545 / Valid loss: 7.21810854048956
Training loss: 0.7481666803359985 / Valid loss: 7.474696667989095

Epoch: 20
Training loss: 0.5240218043327332 / Valid loss: 7.2247215861365905
Training loss: 1.0359026193618774 / Valid loss: 7.01406051544916
Training loss: 0.6213411688804626 / Valid loss: 7.096994400024414
Training loss: 1.0393610000610352 / Valid loss: 7.072528766450428
Training loss: 1.505245327949524 / Valid loss: 7.322477567763555

Epoch: 21
Training loss: 0.6410533785820007 / Valid loss: 7.139811697460356
Training loss: 0.5327749848365784 / Valid loss: 7.614635058811733
Training loss: 0.6896448135375977 / Valid loss: 7.432232411702474
Training loss: 0.7246840596199036 / Valid loss: 7.034297870454334
Training loss: 0.6732176542282104 / Valid loss: 7.112529209681919

Epoch: 22
Training loss: 0.7488296031951904 / Valid loss: 7.0415241922651015
Training loss: 0.43810153007507324 / Valid loss: 7.121858891986665
Training loss: 0.5923314094543457 / Valid loss: 7.196047010875883
Training loss: 1.1224586963653564 / Valid loss: 7.148241783323742
Training loss: 1.0960543155670166 / Valid loss: 7.282926409585135

Epoch: 23
Training loss: 0.6613410711288452 / Valid loss: 7.379645656404041
Training loss: 0.7701973915100098 / Valid loss: 7.032446177800496
Training loss: 0.7753510475158691 / Valid loss: 7.334156890142531
Training loss: 0.5383292436599731 / Valid loss: 7.004515000752041
Training loss: 0.7604090571403503 / Valid loss: 7.103110722133091

Epoch: 24
Training loss: 1.2114282846450806 / Valid loss: 6.867120738256546
Training loss: 0.5419753789901733 / Valid loss: 6.920538143884568
Training loss: 0.5897265672683716 / Valid loss: 7.347378356116159
Training loss: 0.8014463186264038 / Valid loss: 7.242246850331624
Training loss: 1.3461661338806152 / Valid loss: 7.075789029257638

Epoch: 25
Training loss: 0.6527553796768188 / Valid loss: 6.899279732931228
Training loss: 0.4484539330005646 / Valid loss: 6.924172608057658
Training loss: 0.7834568023681641 / Valid loss: 7.003453665687925
Training loss: 0.43774452805519104 / Valid loss: 7.3492748805454795
Training loss: 0.5079165101051331 / Valid loss: 7.132343700953892

Epoch: 26
Training loss: 1.0213607549667358 / Valid loss: 7.035318111238025
Training loss: 0.8303967118263245 / Valid loss: 6.960005424136207
Training loss: 0.6095854640007019 / Valid loss: 6.990484467006865
Training loss: 0.3739844560623169 / Valid loss: 6.997977036521548
Training loss: 0.6546720266342163 / Valid loss: 7.204307660602388

Epoch: 27
Training loss: 0.3490922451019287 / Valid loss: 6.8811381203787665
Training loss: 0.6245917081832886 / Valid loss: 6.948054860887074
Training loss: 0.6033289432525635 / Valid loss: 7.143658179328555
Training loss: 0.5362590551376343 / Valid loss: 7.027526187896728
Training loss: 0.6844990849494934 / Valid loss: 7.0928855169387095

Epoch: 28
Training loss: 0.5631611347198486 / Valid loss: 7.220712157658168
Training loss: 0.4681992828845978 / Valid loss: 6.9808939297993975
Training loss: 0.5096921920776367 / Valid loss: 7.013929980141776
Training loss: 0.3915059268474579 / Valid loss: 7.077820673443022
Training loss: 0.7864702939987183 / Valid loss: 7.266643333435058

Epoch: 29
Training loss: 0.7675877809524536 / Valid loss: 7.167349783579509
Training loss: 0.5293459892272949 / Valid loss: 6.968165688287645
Training loss: 0.42066216468811035 / Valid loss: 7.012243166423979
Training loss: 0.4675798714160919 / Valid loss: 7.008248878660656

Epoch: 30
Training loss: 0.689152717590332 / Valid loss: 7.069259302956717
Training loss: 0.4842417240142822 / Valid loss: 7.137249219985235
Training loss: 0.3676629364490509 / Valid loss: 7.02804977326166
Training loss: 0.5203980207443237 / Valid loss: 7.0413317816598076
Training loss: 0.3044419586658478 / Valid loss: 7.050244948977515

Epoch: 31
Training loss: 0.42016178369522095 / Valid loss: 6.902105474472046
Training loss: 0.4183787703514099 / Valid loss: 6.970627067202614
Training loss: 0.49750757217407227 / Valid loss: 7.0643341700236
Training loss: 0.37181830406188965 / Valid loss: 7.037158845719837
Training loss: 0.6152487993240356 / Valid loss: 7.122200321015858

Epoch: 32
Training loss: 0.3848465085029602 / Valid loss: 6.935148438953218
Training loss: 0.4010522961616516 / Valid loss: 6.868158136095319
Training loss: 0.2944841980934143 / Valid loss: 7.047273454212007
Training loss: 0.35725146532058716 / Valid loss: 6.8795719282967704
Training loss: 0.4350997805595398 / Valid loss: 7.101126832053775

Epoch: 33
Training loss: 0.3530556559562683 / Valid loss: 6.957195618039086
Training loss: 0.40896081924438477 / Valid loss: 7.0333207402910505
Training loss: 0.4382176399230957 / Valid loss: 6.992292533602034
Training loss: 0.43732786178588867 / Valid loss: 6.922511766070412
Training loss: 0.4189184010028839 / Valid loss: 6.986839857555571

Epoch: 34
Training loss: 0.8735959529876709 / Valid loss: 7.031001631418864
Training loss: 0.36404505372047424 / Valid loss: 6.8136503037952245
Training loss: 0.48089858889579773 / Valid loss: 6.983895229157947
Training loss: 0.33155110478401184 / Valid loss: 6.808161989847819
Training loss: 0.34232407808303833 / Valid loss: 6.949971784864153

Epoch: 35
Training loss: 0.9045617580413818 / Valid loss: 6.858074319930304
Training loss: 0.5713363289833069 / Valid loss: 7.007368051438105
Training loss: 0.4803009033203125 / Valid loss: 6.9159299123854865
Training loss: 0.41560590267181396 / Valid loss: 6.989744990212577
Training loss: 0.25549936294555664 / Valid loss: 7.076335602714902

Epoch: 36
Training loss: 0.43812042474746704 / Valid loss: 6.964874267578125
Training loss: 0.7390400171279907 / Valid loss: 6.944810256503877
Training loss: 0.6809818148612976 / Valid loss: 7.060747923169817
Training loss: 0.49954742193222046 / Valid loss: 7.173601718175979
Training loss: 0.5935614705085754 / Valid loss: 6.901555665334066

Epoch: 37
Training loss: 0.38267982006073 / Valid loss: 6.777287381035941
Training loss: 0.27827808260917664 / Valid loss: 7.029321152823312
Training loss: 0.5360932350158691 / Valid loss: 7.144682629903158
Training loss: 1.5006515979766846 / Valid loss: 6.902246284484863
Training loss: 0.6049326658248901 / Valid loss: 7.252998483748663

Epoch: 38
Training loss: 0.55263751745224 / Valid loss: 6.903541265215193
Training loss: 0.3912277817726135 / Valid loss: 7.067876543317523
Training loss: 0.4043325185775757 / Valid loss: 7.0610686211358935
Training loss: 0.4284650683403015 / Valid loss: 7.244679419199626
Training loss: 0.7228416204452515 / Valid loss: 7.018387458437965

Epoch: 39
Training loss: 0.4735035002231598 / Valid loss: 7.049049045926049
Training loss: 0.40887895226478577 / Valid loss: 6.9995390347072055
Training loss: 0.3522275984287262 / Valid loss: 6.915446358635312
Training loss: 0.269299179315567 / Valid loss: 7.012306485857282
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1000): 5.320026810963949
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 13.401490120660691
Model is saved in epoch 0, overall batch: 0
Training loss: 6.062267303466797 / Valid loss: 8.8328503881182
Model is saved in epoch 0, overall batch: 100
Training loss: 4.281064987182617 / Valid loss: 6.432077296574911
Model is saved in epoch 0, overall batch: 200
Training loss: 6.606424331665039 / Valid loss: 5.75346112478347
Model is saved in epoch 0, overall batch: 300
Training loss: 6.108048915863037 / Valid loss: 5.517919642584665
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.685642242431641 / Valid loss: 5.521995144798642
Training loss: 4.933737754821777 / Valid loss: 5.606479635692778
Training loss: 6.155393600463867 / Valid loss: 5.5552044800349645
Training loss: 4.783810615539551 / Valid loss: 5.502032186871483
Model is saved in epoch 1, overall batch: 800
Training loss: 4.54670524597168 / Valid loss: 5.543153531210763

Epoch: 2
Training loss: 6.297420024871826 / Valid loss: 5.483916518801735
Model is saved in epoch 2, overall batch: 1000
Training loss: 6.4334588050842285 / Valid loss: 5.636432173138573
Training loss: 5.231154918670654 / Valid loss: 5.606047634851365
Training loss: 6.784687042236328 / Valid loss: 5.682467376618158
Training loss: 4.448791980743408 / Valid loss: 5.5370560555231005

Epoch: 3
Training loss: 3.6470894813537598 / Valid loss: 5.711421342123122
Training loss: 3.8811140060424805 / Valid loss: 5.838737826120286
Training loss: 5.355040073394775 / Valid loss: 5.61133766628447
Training loss: 3.25341796875 / Valid loss: 5.825427139373053
Training loss: 4.937747478485107 / Valid loss: 5.830638892310006

Epoch: 4
Training loss: 4.742067337036133 / Valid loss: 5.844737259546916
Training loss: 4.94169807434082 / Valid loss: 5.827898958751134
Training loss: 4.392953872680664 / Valid loss: 5.826064341408866
Training loss: 4.8652424812316895 / Valid loss: 5.744085225604829
Training loss: 4.080685615539551 / Valid loss: 5.75170267422994

Epoch: 5
Training loss: 6.674400806427002 / Valid loss: 6.123669576644898
Training loss: 4.8140974044799805 / Valid loss: 6.448792162395659
Training loss: 2.9247095584869385 / Valid loss: 5.968935133161999
Training loss: 3.6218533515930176 / Valid loss: 5.862080521810622
Training loss: 4.307451248168945 / Valid loss: 5.894108268192836

Epoch: 6
Training loss: 6.28933048248291 / Valid loss: 6.216589621135166
Training loss: 3.113718271255493 / Valid loss: 6.224018242245629
Training loss: 3.9564642906188965 / Valid loss: 6.399354821159726
Training loss: 3.9883978366851807 / Valid loss: 6.2247953074319025
Training loss: 3.888244867324829 / Valid loss: 6.504821695600238

Epoch: 7
Training loss: 3.3530216217041016 / Valid loss: 6.387912021364484
Training loss: 2.4651124477386475 / Valid loss: 6.503613394782657
Training loss: 3.807764768600464 / Valid loss: 6.52522295770191
Training loss: 4.606188774108887 / Valid loss: 6.372589020502
Training loss: 3.338189125061035 / Valid loss: 6.417215949013119

Epoch: 8
Training loss: 1.9308183193206787 / Valid loss: 6.647943410419282
Training loss: 1.9624744653701782 / Valid loss: 6.706254638944354
Training loss: 2.9688265323638916 / Valid loss: 6.563459507624308
Training loss: 4.084598541259766 / Valid loss: 6.846389845439366
Training loss: 3.6218085289001465 / Valid loss: 6.59662062327067

Epoch: 9
Training loss: 2.0851430892944336 / Valid loss: 6.785261292684646
Training loss: 2.1793787479400635 / Valid loss: 6.800834346952892
Training loss: 4.086414337158203 / Valid loss: 6.9469775926499135
Training loss: 2.5807743072509766 / Valid loss: 7.1643622035071965

Epoch: 10
Training loss: 1.6654915809631348 / Valid loss: 7.01785401843843
Training loss: 2.180286169052124 / Valid loss: 7.158507792154948
Training loss: 1.576514720916748 / Valid loss: 7.1116573560805545
Training loss: 2.9283552169799805 / Valid loss: 6.892370251246861
Training loss: 1.4153313636779785 / Valid loss: 7.019113000233968

Epoch: 11
Training loss: 1.0529701709747314 / Valid loss: 7.120029726482573
Training loss: 1.6260085105895996 / Valid loss: 7.141248930068243
Training loss: 1.5892243385314941 / Valid loss: 6.925596677689326
Training loss: 1.1943556070327759 / Valid loss: 7.34858706338065
Training loss: 2.4210777282714844 / Valid loss: 7.134882386525472

Epoch: 12
Training loss: 0.9479461908340454 / Valid loss: 7.067645422617594
Training loss: 1.274397373199463 / Valid loss: 6.958572732834589
Training loss: 1.2050890922546387 / Valid loss: 6.899266529083252
Training loss: 1.4948642253875732 / Valid loss: 6.994026256742932
Training loss: 1.604267954826355 / Valid loss: 7.210850520361038

Epoch: 13
Training loss: 0.7238699197769165 / Valid loss: 7.27593944186256
Training loss: 1.2693347930908203 / Valid loss: 7.198849855150495
Training loss: 1.1760371923446655 / Valid loss: 7.112915797460647
Training loss: 1.4267809391021729 / Valid loss: 7.013622120448521
Training loss: 1.1092993021011353 / Valid loss: 7.534227316720145

Epoch: 14
Training loss: 1.210228443145752 / Valid loss: 8.251349921453567
Training loss: 1.599923014640808 / Valid loss: 7.137060982840402
Training loss: 0.9854171276092529 / Valid loss: 7.132313825970605
Training loss: 0.699104905128479 / Valid loss: 7.51736364364624
Training loss: 1.9299331903457642 / Valid loss: 7.268952446892148

Epoch: 15
Training loss: 1.0112097263336182 / Valid loss: 7.319033372969854
Training loss: 1.5271663665771484 / Valid loss: 7.074865295773461
Training loss: 1.0637937784194946 / Valid loss: 7.27062021891276
Training loss: 0.9628312587738037 / Valid loss: 7.206915383111863
Training loss: 1.2099343538284302 / Valid loss: 7.157902485983712

Epoch: 16
Training loss: 0.9799960851669312 / Valid loss: 7.1080408232552665
Training loss: 0.673642635345459 / Valid loss: 6.942045793079195
Training loss: 0.9848495125770569 / Valid loss: 7.666436118171329
Training loss: 0.9099947214126587 / Valid loss: 7.340208562215169
Training loss: 1.076393723487854 / Valid loss: 7.4574902080354235

Epoch: 17
Training loss: 0.8900635242462158 / Valid loss: 7.143133336021787
Training loss: 1.3626673221588135 / Valid loss: 7.214506421770368
Training loss: 1.0551687479019165 / Valid loss: 7.319132491520473
Training loss: 0.6130558252334595 / Valid loss: 7.154738321758452
Training loss: 0.9761441946029663 / Valid loss: 7.082231889452253

Epoch: 18
Training loss: 0.90669184923172 / Valid loss: 7.157441584269206
Training loss: 1.0404186248779297 / Valid loss: 7.181848680405389
Training loss: 0.9802967309951782 / Valid loss: 7.183074764978318
Training loss: 0.9258209466934204 / Valid loss: 7.0041779200236
Training loss: 1.450009822845459 / Valid loss: 7.424750714074998

Epoch: 19
Training loss: 1.5483589172363281 / Valid loss: 7.117555622827439
Training loss: 1.1308021545410156 / Valid loss: 7.278476792290097
Training loss: 1.173615574836731 / Valid loss: 7.516715935298374
Training loss: 0.6416669487953186 / Valid loss: 7.118769663856143

Epoch: 20
Training loss: 0.8170551657676697 / Valid loss: 6.9850009827386765
Training loss: 1.0713863372802734 / Valid loss: 6.983958748408726
Training loss: 0.6866669654846191 / Valid loss: 7.036787373679025
Training loss: 1.3444621562957764 / Valid loss: 6.999169195265997
Training loss: 1.4696205854415894 / Valid loss: 7.122441098803566

Epoch: 21
Training loss: 0.5160819888114929 / Valid loss: 7.325299912407285
Training loss: 0.8647797703742981 / Valid loss: 7.216545077732631
Training loss: 0.7354907989501953 / Valid loss: 7.306163946787517
Training loss: 0.8119426965713501 / Valid loss: 7.083291535150437
Training loss: 0.7319866418838501 / Valid loss: 7.159115995679583

Epoch: 22
Training loss: 1.1576588153839111 / Valid loss: 6.987737869081043
Training loss: 0.601937472820282 / Valid loss: 6.874365207127163
Training loss: 0.8587098121643066 / Valid loss: 6.98452989487421
Training loss: 1.3270635604858398 / Valid loss: 7.18074544724964
Training loss: 1.2520034313201904 / Valid loss: 7.212317721048991

Epoch: 23
Training loss: 0.6435583829879761 / Valid loss: 7.211311453864688
Training loss: 0.9421743154525757 / Valid loss: 7.166177611123948
Training loss: 0.8710920810699463 / Valid loss: 7.355293523697626
Training loss: 0.6330481767654419 / Valid loss: 7.00095298857916
Training loss: 0.7201974391937256 / Valid loss: 7.121901793706985

Epoch: 24
Training loss: 1.0113234519958496 / Valid loss: 7.054033801669166
Training loss: 0.7256361246109009 / Valid loss: 6.895448680151077
Training loss: 0.79686039686203 / Valid loss: 7.141841847555978
Training loss: 0.8342465162277222 / Valid loss: 7.372313322339739
Training loss: 1.2081787586212158 / Valid loss: 7.0570939404623845

Epoch: 25
Training loss: 0.7671568393707275 / Valid loss: 6.979392544428507
Training loss: 0.751292884349823 / Valid loss: 6.893071015675862
Training loss: 1.2745682001113892 / Valid loss: 7.208185043789092
Training loss: 0.5100289583206177 / Valid loss: 7.293101274399531
Training loss: 0.9759160280227661 / Valid loss: 7.1150478090558735

Epoch: 26
Training loss: 1.3707380294799805 / Valid loss: 6.8958362760997955
Training loss: 0.6599274277687073 / Valid loss: 7.047783908389864
Training loss: 0.5203804969787598 / Valid loss: 6.965253191902524
Training loss: 0.5709750652313232 / Valid loss: 7.0213062331790015
Training loss: 0.67218416929245 / Valid loss: 6.996784573509579

Epoch: 27
Training loss: 0.4005941152572632 / Valid loss: 6.973646454584031
Training loss: 0.9481691718101501 / Valid loss: 6.870440823691232
Training loss: 0.5380675792694092 / Valid loss: 7.025534947713216
Training loss: 0.6793522834777832 / Valid loss: 7.175512009575254
Training loss: 0.9628987312316895 / Valid loss: 7.200041636966524

Epoch: 28
Training loss: 0.47377336025238037 / Valid loss: 6.954630611056373
Training loss: 0.6185547113418579 / Valid loss: 7.116030016399566
Training loss: 0.8287827968597412 / Valid loss: 7.030727817898705
Training loss: 0.6574162244796753 / Valid loss: 7.193347808292934
Training loss: 0.866424560546875 / Valid loss: 7.484568375632876

Epoch: 29
Training loss: 0.9230078458786011 / Valid loss: 7.1403533299764
Training loss: 0.5942624807357788 / Valid loss: 7.111485404060001
Training loss: 0.41779911518096924 / Valid loss: 7.026304347174508
Training loss: 0.5164081454277039 / Valid loss: 6.9554442632766

Epoch: 30
Training loss: 0.696561872959137 / Valid loss: 7.060396480560303
Training loss: 0.5686483383178711 / Valid loss: 6.950876326788039
Training loss: 0.4014432728290558 / Valid loss: 7.19293954031808
Training loss: 0.3866839110851288 / Valid loss: 7.48125696182251
Training loss: 0.685204029083252 / Valid loss: 7.0326364244733535

Epoch: 31
Training loss: 0.587900698184967 / Valid loss: 6.954195849100748
Training loss: 0.6300325393676758 / Valid loss: 6.955874774569557
Training loss: 0.686231791973114 / Valid loss: 6.9420564787728445
Training loss: 0.6671338081359863 / Valid loss: 7.062105165209089
Training loss: 0.5956989526748657 / Valid loss: 7.254437246776763

Epoch: 32
Training loss: 0.4774439036846161 / Valid loss: 6.9062239419846305
Training loss: 0.3573596477508545 / Valid loss: 6.930624628067017
Training loss: 0.4098314046859741 / Valid loss: 6.936984743390765
Training loss: 0.27898430824279785 / Valid loss: 6.918850088119507
Training loss: 0.6373131275177002 / Valid loss: 7.1782089097159245

Epoch: 33
Training loss: 0.44602370262145996 / Valid loss: 6.846308898925781
Training loss: 0.36838018894195557 / Valid loss: 7.004977476029169
Training loss: 0.5488486886024475 / Valid loss: 7.034589685712542
Training loss: 0.5549927949905396 / Valid loss: 7.387063848404657
Training loss: 0.471550315618515 / Valid loss: 7.015229786010016

Epoch: 34
Training loss: 0.861740231513977 / Valid loss: 6.9508398964291525
Training loss: 0.4433026909828186 / Valid loss: 6.893260438101632
Training loss: 0.5604914426803589 / Valid loss: 7.028548699333554
Training loss: 0.4090682864189148 / Valid loss: 7.014955913452875
Training loss: 0.5156020522117615 / Valid loss: 7.122148954300654

Epoch: 35
Training loss: 0.665322482585907 / Valid loss: 7.033210200355167
Training loss: 0.7825077772140503 / Valid loss: 6.901378538495019
Training loss: 0.7128542065620422 / Valid loss: 6.899662267594111
Training loss: 0.5111547708511353 / Valid loss: 7.120385269891648
Training loss: 0.29402005672454834 / Valid loss: 7.117226082938058

Epoch: 36
Training loss: 0.5312432646751404 / Valid loss: 6.960041763668968
Training loss: 0.6132869720458984 / Valid loss: 7.16515710467384
Training loss: 0.8205239176750183 / Valid loss: 7.1914196377708794
Training loss: 0.5756953358650208 / Valid loss: 7.167772824423654
Training loss: 0.6202996373176575 / Valid loss: 7.004833389463879

Epoch: 37
Training loss: 0.5141388773918152 / Valid loss: 6.804296566191174
Training loss: 0.30821746587753296 / Valid loss: 7.018720685868036
Training loss: 0.514823317527771 / Valid loss: 7.162132631029402
Training loss: 1.1152572631835938 / Valid loss: 6.843368337267921
Training loss: 0.5961647033691406 / Valid loss: 7.0546390851338705

Epoch: 38
Training loss: 0.8267818689346313 / Valid loss: 6.884455335707892
Training loss: 0.5258208513259888 / Valid loss: 7.022329421270461
Training loss: 0.3673804998397827 / Valid loss: 7.087516030811128
Training loss: 0.7852084040641785 / Valid loss: 7.2167782170431956
Training loss: 0.7004196643829346 / Valid loss: 7.105246471223377

Epoch: 39
Training loss: 0.5970690250396729 / Valid loss: 6.99689157576788
Training loss: 0.3395666778087616 / Valid loss: 6.90505788439796
Training loss: 0.4952620565891266 / Valid loss: 7.005407478695824
Training loss: 0.41947153210639954 / Valid loss: 7.028489442098708
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1000): 5.326819524310884
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.485506057739258 / Valid loss: 14.727781214032854
Model is saved in epoch 0, overall batch: 0
Training loss: 4.084541320800781 / Valid loss: 7.065146973019554
Model is saved in epoch 0, overall batch: 100
Training loss: 4.639308452606201 / Valid loss: 5.684344668615432
Model is saved in epoch 0, overall batch: 200
Training loss: 5.593152046203613 / Valid loss: 5.590488869803292
Model is saved in epoch 0, overall batch: 300
Training loss: 4.331737518310547 / Valid loss: 5.54173499970209
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.507933616638184 / Valid loss: 5.570422365551903
Training loss: 5.03676700592041 / Valid loss: 5.613300468808129
Training loss: 4.354218482971191 / Valid loss: 5.71876259077163
Training loss: 4.347367763519287 / Valid loss: 5.602560888017927
Training loss: 5.787589073181152 / Valid loss: 5.46172391573588
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.601531028747559 / Valid loss: 5.560890624636696
Training loss: 4.566441059112549 / Valid loss: 5.630791695912679
Training loss: 4.266874313354492 / Valid loss: 5.535887786320278
Training loss: 5.487154960632324 / Valid loss: 5.856110638663882
Training loss: 5.148183822631836 / Valid loss: 5.489890666235061

Epoch: 3
Training loss: 4.56959342956543 / Valid loss: 5.597435830888294
Training loss: 3.36702036857605 / Valid loss: 5.7787544409434
Training loss: 5.45075798034668 / Valid loss: 5.687821515401205
Training loss: 4.970407485961914 / Valid loss: 5.640556085677374
Training loss: 4.619922161102295 / Valid loss: 5.737907119024367

Epoch: 4
Training loss: 3.0268683433532715 / Valid loss: 5.809183036713373
Training loss: 4.513984680175781 / Valid loss: 5.79840285664513
Training loss: 4.119352340698242 / Valid loss: 5.854349147705805
Training loss: 4.55628776550293 / Valid loss: 5.841215242658342
Training loss: 5.38166618347168 / Valid loss: 5.795476002920242

Epoch: 5
Training loss: 4.772600173950195 / Valid loss: 6.085455853598458
Training loss: 4.2128167152404785 / Valid loss: 6.203831114087786
Training loss: 4.126201629638672 / Valid loss: 6.238782628377279
Training loss: 2.65157151222229 / Valid loss: 6.105154182797387
Training loss: 5.329333782196045 / Valid loss: 5.952133414858864

Epoch: 6
Training loss: 3.585573196411133 / Valid loss: 6.43375609261649
Training loss: 2.427706003189087 / Valid loss: 6.403943806602841
Training loss: 3.3778953552246094 / Valid loss: 6.320946638924735
Training loss: 3.4974899291992188 / Valid loss: 6.130150763193766
Training loss: 3.3988142013549805 / Valid loss: 6.184874364307949

Epoch: 7
Training loss: 3.1917731761932373 / Valid loss: 6.729487351008824
Training loss: 1.9710493087768555 / Valid loss: 7.1452867190043134
Training loss: 2.713913679122925 / Valid loss: 6.751288136981782
Training loss: 3.2826249599456787 / Valid loss: 6.547898006439209
Training loss: 2.231494665145874 / Valid loss: 6.945514420100621

Epoch: 8
Training loss: 2.974836587905884 / Valid loss: 6.786867232549758
Training loss: 2.6557254791259766 / Valid loss: 6.768648767471314
Training loss: 2.9730982780456543 / Valid loss: 7.706897853669666
Training loss: 2.9073212146759033 / Valid loss: 6.801637876601446
Training loss: 2.5893940925598145 / Valid loss: 7.0203120867411295

Epoch: 9
Training loss: 1.6607356071472168 / Valid loss: 7.250901344844273
Training loss: 3.003000259399414 / Valid loss: 6.903992203303746
Training loss: 3.6520400047302246 / Valid loss: 7.356825764973959
Training loss: 1.7452361583709717 / Valid loss: 6.900194976443336

Epoch: 10
Training loss: 1.921802043914795 / Valid loss: 7.28059497106643
Training loss: 1.554692268371582 / Valid loss: 7.1731242679414295
Training loss: 2.7501182556152344 / Valid loss: 7.04165206409636
Training loss: 2.2852447032928467 / Valid loss: 7.621666404179164
Training loss: 2.9342007637023926 / Valid loss: 7.024440788087391

Epoch: 11
Training loss: 2.339480400085449 / Valid loss: 6.744874077751523
Training loss: 1.7421303987503052 / Valid loss: 7.165397496450515
Training loss: 1.736325979232788 / Valid loss: 7.33206912449428
Training loss: 2.0836172103881836 / Valid loss: 7.296747003282819
Training loss: 1.9697494506835938 / Valid loss: 7.745774804978144

Epoch: 12
Training loss: 1.3215553760528564 / Valid loss: 7.607151340302967
Training loss: 1.428411602973938 / Valid loss: 7.986862193970453
Training loss: 1.7091624736785889 / Valid loss: 7.2623502595084055
Training loss: 1.4815216064453125 / Valid loss: 7.166558306557792
Training loss: 1.9444682598114014 / Valid loss: 7.124978444689796

Epoch: 13
Training loss: 1.3704614639282227 / Valid loss: 7.13262421517145
Training loss: 0.980102002620697 / Valid loss: 7.149479570842924
Training loss: 1.0197198390960693 / Valid loss: 7.261141899653843
Training loss: 1.221708059310913 / Valid loss: 7.5309386162530805
Training loss: 1.738562822341919 / Valid loss: 6.959201276870001

Epoch: 14
Training loss: 1.4852073192596436 / Valid loss: 7.160695253099714
Training loss: 1.3371450901031494 / Valid loss: 8.407311017172677
Training loss: 0.9767293930053711 / Valid loss: 7.4133056549798875
Training loss: 1.4024471044540405 / Valid loss: 7.186048607599168
Training loss: 1.7611618041992188 / Valid loss: 7.154868793487549

Epoch: 15
Training loss: 1.516613245010376 / Valid loss: 7.062687660398938
Training loss: 1.3272181749343872 / Valid loss: 7.227820796058292
Training loss: 1.1008169651031494 / Valid loss: 7.143176392146519
Training loss: 1.0648858547210693 / Valid loss: 7.34838190532866
Training loss: 0.9236446619033813 / Valid loss: 7.277192120324997

Epoch: 16
Training loss: 2.0853466987609863 / Valid loss: 7.230085974647885
Training loss: 0.9141938090324402 / Valid loss: 7.607573050544375
Training loss: 1.2358945608139038 / Valid loss: 7.554919978550502
Training loss: 1.314352035522461 / Valid loss: 7.44837680544172
Training loss: 1.3040990829467773 / Valid loss: 7.259091749645415

Epoch: 17
Training loss: 1.0369572639465332 / Valid loss: 7.429614614305042
Training loss: 1.2456907033920288 / Valid loss: 7.314611752827962
Training loss: 1.2524393796920776 / Valid loss: 7.294987238021124
Training loss: 1.3762683868408203 / Valid loss: 7.349094263712565
Training loss: 1.0622916221618652 / Valid loss: 7.503420838855562

Epoch: 18
Training loss: 0.816494345664978 / Valid loss: 7.219024774006435
Training loss: 0.8829970359802246 / Valid loss: 7.510570671444848
Training loss: 0.8666496276855469 / Valid loss: 7.326167765117827
Training loss: 1.204059362411499 / Valid loss: 7.660407593136742
Training loss: 1.2139842510223389 / Valid loss: 7.360418033599854

Epoch: 19
Training loss: 0.9616953730583191 / Valid loss: 7.192802383786156
Training loss: 1.204817295074463 / Valid loss: 7.173180861700149
Training loss: 1.1860488653182983 / Valid loss: 7.4403221130371096
Training loss: 1.0839942693710327 / Valid loss: 7.431657863798596

Epoch: 20
Training loss: 0.8600542545318604 / Valid loss: 7.495256378537133
Training loss: 0.7869523763656616 / Valid loss: 7.246940944308326
Training loss: 0.6944712400436401 / Valid loss: 7.228313782101586
Training loss: 1.2299251556396484 / Valid loss: 7.925821772075834
Training loss: 0.8115967512130737 / Valid loss: 8.324003664652507

Epoch: 21
Training loss: 0.6890562772750854 / Valid loss: 7.20938842410133
Training loss: 0.6995111703872681 / Valid loss: 7.513745326087588
Training loss: 1.0388389825820923 / Valid loss: 7.348260570707775
Training loss: 1.124869704246521 / Valid loss: 8.035612823849632
Training loss: 1.1026874780654907 / Valid loss: 7.896966162182036

Epoch: 22
Training loss: 0.8242351412773132 / Valid loss: 7.619624514806838
Training loss: 0.8992370367050171 / Valid loss: 7.281538513728551
Training loss: 1.1121529340744019 / Valid loss: 7.161973599025181
Training loss: 0.6095430254936218 / Valid loss: 7.395494274866014
Training loss: 0.6467241644859314 / Valid loss: 7.360105882372175

Epoch: 23
Training loss: 1.1472375392913818 / Valid loss: 7.451206856682187
Training loss: 1.1267988681793213 / Valid loss: 7.468046292804536
Training loss: 1.0119696855545044 / Valid loss: 7.463719613211495
Training loss: 0.949309766292572 / Valid loss: 8.038381431216285
Training loss: 0.9528309106826782 / Valid loss: 7.318645713442848

Epoch: 24
Training loss: 0.5348576307296753 / Valid loss: 7.237531734648205
Training loss: 0.8734790086746216 / Valid loss: 7.20814101809547
Training loss: 0.46205252408981323 / Valid loss: 7.154033719925653
Training loss: 0.7866871356964111 / Valid loss: 7.344131051926386
Training loss: 0.5544046759605408 / Valid loss: 7.448732548668271

Epoch: 25
Training loss: 0.5156476497650146 / Valid loss: 7.475396823883057
Training loss: 0.8292311429977417 / Valid loss: 7.285971968514579
Training loss: 0.8615312576293945 / Valid loss: 7.78051589784168
Training loss: 0.7361618876457214 / Valid loss: 7.462147490183512
Training loss: 0.7221405506134033 / Valid loss: 7.307742736453101

Epoch: 26
Training loss: 0.7038989067077637 / Valid loss: 7.061876887366886
Training loss: 0.6476266384124756 / Valid loss: 7.344306836809431
Training loss: 0.9572669267654419 / Valid loss: 7.69780109042213
Training loss: 0.58519446849823 / Valid loss: 7.353885945819673
Training loss: 1.2713624238967896 / Valid loss: 7.315994453430176

Epoch: 27
Training loss: 0.7807352542877197 / Valid loss: 7.120951389131092
Training loss: 0.5888660550117493 / Valid loss: 7.162556266784668
Training loss: 1.1513261795043945 / Valid loss: 7.4645621118091405
Training loss: 0.6299579739570618 / Valid loss: 7.43074598312378
Training loss: 0.6557662487030029 / Valid loss: 7.227800323849633

Epoch: 28
Training loss: 0.41298818588256836 / Valid loss: 7.119118032001314
Training loss: 0.43777579069137573 / Valid loss: 7.354726391746884
Training loss: 0.4716600477695465 / Valid loss: 7.99610416775658
Training loss: 0.6314114332199097 / Valid loss: 7.078541460491362
Training loss: 1.1770925521850586 / Valid loss: 7.222543339502244

Epoch: 29
Training loss: 0.6416466236114502 / Valid loss: 7.34916904540289
Training loss: 0.6902791857719421 / Valid loss: 7.111364568982806
Training loss: 0.6994762420654297 / Valid loss: 7.244328544253395
Training loss: 0.7707122564315796 / Valid loss: 7.413461916787284

Epoch: 30
Training loss: 0.6069107055664062 / Valid loss: 7.453847176688058
Training loss: 0.7636231184005737 / Valid loss: 7.141518506549653
Training loss: 0.511724054813385 / Valid loss: 7.313784290495373
Training loss: 0.6689708232879639 / Valid loss: 7.161179801395961
Training loss: 0.907219409942627 / Valid loss: 7.149088033040365

Epoch: 31
Training loss: 0.6110581159591675 / Valid loss: 7.229555370694115
Training loss: 1.2116092443466187 / Valid loss: 7.111853227161226
Training loss: 0.5267805457115173 / Valid loss: 7.169793283371698
Training loss: 0.7051632404327393 / Valid loss: 7.160288174947103
Training loss: 0.6829876899719238 / Valid loss: 7.180893857138497

Epoch: 32
Training loss: 0.3825839161872864 / Valid loss: 7.33749790645781
Training loss: 0.7912479043006897 / Valid loss: 7.228892115184239
Training loss: 0.6457353830337524 / Valid loss: 7.2153769311450775
Training loss: 0.4847678542137146 / Valid loss: 7.163332421439034
Training loss: 0.7592740058898926 / Valid loss: 7.037824826013474

Epoch: 33
Training loss: 0.6569952964782715 / Valid loss: 7.363598564692906
Training loss: 0.5387163758277893 / Valid loss: 7.18000347046625
Training loss: 0.7276545763015747 / Valid loss: 7.176579316457112
Training loss: 0.5158270597457886 / Valid loss: 7.227270716712589
Training loss: 0.6318497657775879 / Valid loss: 7.346579983120873

Epoch: 34
Training loss: 1.086794376373291 / Valid loss: 7.08503270149231
Training loss: 0.5708376169204712 / Valid loss: 7.116807065691266
Training loss: 0.5285670161247253 / Valid loss: 7.323873410906111
Training loss: 0.3875619173049927 / Valid loss: 7.180259091513498
Training loss: 1.0102839469909668 / Valid loss: 7.2381551015944705

Epoch: 35
Training loss: 0.5422807335853577 / Valid loss: 7.133153815496535
Training loss: 0.6607768535614014 / Valid loss: 7.2577286720275875
Training loss: 0.698561429977417 / Valid loss: 7.008691828591483
Training loss: 0.5954948663711548 / Valid loss: 7.218351636614118
Training loss: 0.5661472082138062 / Valid loss: 7.439202136085147

Epoch: 36
Training loss: 0.5922322869300842 / Valid loss: 7.027229204631987
Training loss: 0.4568168520927429 / Valid loss: 7.049499579838344
Training loss: 0.45074331760406494 / Valid loss: 7.080278451102121
Training loss: 0.9973118305206299 / Valid loss: 7.242448920295352
Training loss: 0.4830131530761719 / Valid loss: 7.197138232276553

Epoch: 37
Training loss: 0.5621105432510376 / Valid loss: 7.082863237744286
Training loss: 0.27124953269958496 / Valid loss: 7.125073641822452
Training loss: 0.4519268870353699 / Valid loss: 7.367381536392939
Training loss: 0.549884557723999 / Valid loss: 7.308317688533238
Training loss: 0.5428293943405151 / Valid loss: 7.16598939214434

Epoch: 38
Training loss: 0.4919784665107727 / Valid loss: 7.122518398648217
Training loss: 0.643317461013794 / Valid loss: 7.012989212217785
Training loss: 0.34803566336631775 / Valid loss: 7.0535520054045175
Training loss: 0.726897120475769 / Valid loss: 7.0877756482078915
Training loss: 1.1165282726287842 / Valid loss: 7.308023162115187

Epoch: 39
Training loss: 0.4511013925075531 / Valid loss: 7.21346648534139
Training loss: 0.4847315549850464 / Valid loss: 7.009142866588774
Training loss: 0.463386207818985 / Valid loss: 7.124725173768543
Training loss: 0.4445492625236511 / Valid loss: 7.365290523710705
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 900): 5.322395140784128
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.485506057739258 / Valid loss: 14.729837326776414
Model is saved in epoch 0, overall batch: 0
Training loss: 4.105766296386719 / Valid loss: 7.071110221317836
Model is saved in epoch 0, overall batch: 100
Training loss: 4.631260871887207 / Valid loss: 5.6879703885033015
Model is saved in epoch 0, overall batch: 200
Training loss: 5.740025043487549 / Valid loss: 5.5972224167415074
Model is saved in epoch 0, overall batch: 300
Training loss: 4.3588104248046875 / Valid loss: 5.542350085576375
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.5883331298828125 / Valid loss: 5.562262610026768
Training loss: 4.972980499267578 / Valid loss: 5.603170962560744
Training loss: 4.208123207092285 / Valid loss: 5.7387003921327135
Training loss: 4.479820251464844 / Valid loss: 5.570577664602371
Training loss: 5.783834457397461 / Valid loss: 5.458993891307286
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.614598274230957 / Valid loss: 5.55898680914016
Training loss: 4.420798301696777 / Valid loss: 5.672244748615083
Training loss: 4.116087436676025 / Valid loss: 5.517814536321731
Training loss: 5.580738544464111 / Valid loss: 5.783534642628261
Training loss: 5.138820648193359 / Valid loss: 5.491320171810332

Epoch: 3
Training loss: 4.778823375701904 / Valid loss: 5.5835426398686
Training loss: 3.27010440826416 / Valid loss: 5.695374690918696
Training loss: 5.238604545593262 / Valid loss: 5.6795753910428
Training loss: 4.5727667808532715 / Valid loss: 5.604768360228766
Training loss: 4.517436981201172 / Valid loss: 5.724281563077654

Epoch: 4
Training loss: 3.1086883544921875 / Valid loss: 5.825941980452765
Training loss: 4.14503812789917 / Valid loss: 5.802517641158331
Training loss: 4.095088005065918 / Valid loss: 5.736514920280094
Training loss: 4.761218547821045 / Valid loss: 5.81064053944179
Training loss: 5.17519474029541 / Valid loss: 5.815280092330206

Epoch: 5
Training loss: 4.581071853637695 / Valid loss: 6.15032156989688
Training loss: 4.0784382820129395 / Valid loss: 6.241589196523031
Training loss: 4.251758575439453 / Valid loss: 6.080956127530053
Training loss: 3.057352066040039 / Valid loss: 6.092193991797311
Training loss: 5.434845924377441 / Valid loss: 6.0029446874346055

Epoch: 6
Training loss: 3.821290969848633 / Valid loss: 6.351453955968221
Training loss: 2.179690361022949 / Valid loss: 6.593404933384487
Training loss: 3.5952329635620117 / Valid loss: 6.337370520546322
Training loss: 3.7857422828674316 / Valid loss: 6.213101289385841
Training loss: 3.64749813079834 / Valid loss: 6.133091013772147

Epoch: 7
Training loss: 3.0387215614318848 / Valid loss: 6.698912030174618
Training loss: 2.3059730529785156 / Valid loss: 7.6518487521580285
Training loss: 2.826799154281616 / Valid loss: 6.720108064015706
Training loss: 3.3336434364318848 / Valid loss: 6.600334256035941
Training loss: 2.6726510524749756 / Valid loss: 6.937212939489456

Epoch: 8
Training loss: 2.7835850715637207 / Valid loss: 6.666891860961914
Training loss: 2.5564029216766357 / Valid loss: 6.839836415790376
Training loss: 3.1329784393310547 / Valid loss: 8.184582310631162
Training loss: 2.9106693267822266 / Valid loss: 6.732419340951102
Training loss: 3.3351407051086426 / Valid loss: 7.113249115716844

Epoch: 9
Training loss: 1.629716157913208 / Valid loss: 7.065952532632011
Training loss: 2.8546576499938965 / Valid loss: 6.9094050498235795
Training loss: 3.5153098106384277 / Valid loss: 7.1404820305960515
Training loss: 1.793732762336731 / Valid loss: 6.851545401981899

Epoch: 10
Training loss: 1.72232985496521 / Valid loss: 6.96930821282523
Training loss: 1.799116611480713 / Valid loss: 6.997851280939011
Training loss: 2.6917858123779297 / Valid loss: 7.227139286767869
Training loss: 2.5135746002197266 / Valid loss: 7.2161522910708475
Training loss: 2.817281723022461 / Valid loss: 6.876064573015485

Epoch: 11
Training loss: 2.263150691986084 / Valid loss: 6.840481519699097
Training loss: 2.0009031295776367 / Valid loss: 7.226056380498977
Training loss: 1.5711746215820312 / Valid loss: 7.134744544256301
Training loss: 2.159590721130371 / Valid loss: 7.2183802105131605
Training loss: 1.378871202468872 / Valid loss: 7.450982488904681

Epoch: 12
Training loss: 1.6724334955215454 / Valid loss: 7.650090630849203
Training loss: 1.5680041313171387 / Valid loss: 7.030336402711415
Training loss: 1.9176204204559326 / Valid loss: 7.16757565452939
Training loss: 1.5100160837173462 / Valid loss: 7.7730285462879
Training loss: 1.9661054611206055 / Valid loss: 7.310747232891265

Epoch: 13
Training loss: 1.526615858078003 / Valid loss: 7.1328206130436485
Training loss: 1.6151150465011597 / Valid loss: 7.349777862003871
Training loss: 1.031437873840332 / Valid loss: 7.34105092003232
Training loss: 1.258286714553833 / Valid loss: 7.243540409633091
Training loss: 2.078435182571411 / Valid loss: 7.230885996137347

Epoch: 14
Training loss: 1.500993013381958 / Valid loss: 7.204698208400181
Training loss: 1.3683491945266724 / Valid loss: 7.4743827547345845
Training loss: 1.2745745182037354 / Valid loss: 7.271938492002941
Training loss: 1.047242522239685 / Valid loss: 7.546367518107096
Training loss: 1.8668227195739746 / Valid loss: 7.5079885119483585

Epoch: 15
Training loss: 1.7418674230575562 / Valid loss: 7.066635279428391
Training loss: 2.021324634552002 / Valid loss: 7.380734884171259
Training loss: 0.9822936058044434 / Valid loss: 7.839773950122652
Training loss: 1.360363483428955 / Valid loss: 7.32946370215643
Training loss: 0.9549113512039185 / Valid loss: 7.20893516994658

Epoch: 16
Training loss: 1.883887767791748 / Valid loss: 7.2432065736679805
Training loss: 1.090630054473877 / Valid loss: 7.262759136018299
Training loss: 1.1150743961334229 / Valid loss: 7.452201275598435
Training loss: 0.9715259671211243 / Valid loss: 7.447567907969157
Training loss: 1.1471492052078247 / Valid loss: 7.235526275634766

Epoch: 17
Training loss: 0.8787218332290649 / Valid loss: 7.345502810251145
Training loss: 1.3816068172454834 / Valid loss: 7.272664231345767
Training loss: 0.9450048804283142 / Valid loss: 7.325899006071545
Training loss: 1.3441461324691772 / Valid loss: 7.478320612226214
Training loss: 1.1182655096054077 / Valid loss: 8.132044542403449

Epoch: 18
Training loss: 1.0416148900985718 / Valid loss: 7.214669261659894
Training loss: 1.074600100517273 / Valid loss: 7.377779061453683
Training loss: 0.9132835865020752 / Valid loss: 7.224473371959868
Training loss: 1.2587645053863525 / Valid loss: 7.249950306756156
Training loss: 0.9754698276519775 / Valid loss: 7.449397159758068

Epoch: 19
Training loss: 1.0801656246185303 / Valid loss: 7.266356867835635
Training loss: 1.4552901983261108 / Valid loss: 7.327791836148217
Training loss: 1.0891804695129395 / Valid loss: 7.51166261945452
Training loss: 1.241682529449463 / Valid loss: 7.320861069361369

Epoch: 20
Training loss: 0.5861594676971436 / Valid loss: 7.483654149373373
Training loss: 0.9586563110351562 / Valid loss: 7.3640961192903065
Training loss: 0.6983942985534668 / Valid loss: 7.236245763869513
Training loss: 0.9401357769966125 / Valid loss: 7.4159703481765025
Training loss: 0.7526952624320984 / Valid loss: 7.862052254449754

Epoch: 21
Training loss: 0.9307175874710083 / Valid loss: 7.359191544850668
Training loss: 1.082311987876892 / Valid loss: 7.2590217136201405
Training loss: 0.7598441243171692 / Valid loss: 7.389952437082926
Training loss: 0.6658493280410767 / Valid loss: 8.277504294259208
Training loss: 0.9188554286956787 / Valid loss: 7.615461354028611

Epoch: 22
Training loss: 0.8789364099502563 / Valid loss: 7.240179670424689
Training loss: 0.6353714466094971 / Valid loss: 7.642304275149391
Training loss: 1.0537762641906738 / Valid loss: 7.150243818192255
Training loss: 0.6393990516662598 / Valid loss: 7.286588832310268
Training loss: 0.5963195562362671 / Valid loss: 7.358895871752784

Epoch: 23
Training loss: 0.7586542367935181 / Valid loss: 7.466121882484073
Training loss: 1.3001775741577148 / Valid loss: 7.4524240402948285
Training loss: 0.9245014190673828 / Valid loss: 7.574256029583159
Training loss: 1.0104179382324219 / Valid loss: 7.618902547018869
Training loss: 0.8316202163696289 / Valid loss: 7.394676356088548

Epoch: 24
Training loss: 0.4955488443374634 / Valid loss: 7.275772912161691
Training loss: 1.0499247312545776 / Valid loss: 7.215095470065163
Training loss: 0.5177677273750305 / Valid loss: 7.232710747491746
Training loss: 1.0927881002426147 / Valid loss: 7.264540268125988
Training loss: 0.7274269461631775 / Valid loss: 7.320282323019845

Epoch: 25
Training loss: 0.6963142156600952 / Valid loss: 7.897630305517287
Training loss: 0.9044950604438782 / Valid loss: 7.444532780420213
Training loss: 0.7592335939407349 / Valid loss: 7.468432680765788
Training loss: 0.8361130356788635 / Valid loss: 7.55245672861735
Training loss: 0.6325960159301758 / Valid loss: 7.218088522411528

Epoch: 26
Training loss: 0.7983284592628479 / Valid loss: 7.211676334199451
Training loss: 0.7659057378768921 / Valid loss: 7.255105320612589
Training loss: 0.9752143621444702 / Valid loss: 7.2654602595738
Training loss: 0.847043514251709 / Valid loss: 7.258920651390439
Training loss: 1.1121277809143066 / Valid loss: 7.320088204883394

Epoch: 27
Training loss: 0.7734946012496948 / Valid loss: 7.179319835844494
Training loss: 0.8982844352722168 / Valid loss: 7.920190257117862
Training loss: 1.638969898223877 / Valid loss: 7.521750102724348
Training loss: 0.5996910929679871 / Valid loss: 7.330675150099255
Training loss: 0.4715149402618408 / Valid loss: 7.398814982459658

Epoch: 28
Training loss: 0.5782885551452637 / Valid loss: 7.148830600011916
Training loss: 0.5538471341133118 / Valid loss: 7.219719269162133
Training loss: 0.5986999273300171 / Valid loss: 7.957001218341646
Training loss: 0.4268106520175934 / Valid loss: 7.233591265905471
Training loss: 0.7332754135131836 / Valid loss: 7.550905967894055

Epoch: 29
Training loss: 0.8638508319854736 / Valid loss: 7.6178099677676245
Training loss: 0.6476843357086182 / Valid loss: 7.415074966067359
Training loss: 0.706275224685669 / Valid loss: 7.2950041975293844
Training loss: 0.8489954471588135 / Valid loss: 7.713572942642939

Epoch: 30
Training loss: 0.6603989601135254 / Valid loss: 7.958868930453346
Training loss: 0.9453470706939697 / Valid loss: 7.2644559678577245
Training loss: 0.569132924079895 / Valid loss: 7.319097214653379
Training loss: 0.4921518564224243 / Valid loss: 7.21160850979033
Training loss: 0.7865285873413086 / Valid loss: 7.20660647437686

Epoch: 31
Training loss: 0.6372542381286621 / Valid loss: 7.209143883841378
Training loss: 1.2277345657348633 / Valid loss: 7.13895735967727
Training loss: 0.730250358581543 / Valid loss: 7.263890393575033
Training loss: 0.8540078401565552 / Valid loss: 7.265223716554187
Training loss: 0.8645862340927124 / Valid loss: 7.44376988638015

Epoch: 32
Training loss: 0.41966724395751953 / Valid loss: 7.353359967186337
Training loss: 0.5586945414543152 / Valid loss: 7.740270619165329
Training loss: 0.7560010552406311 / Valid loss: 7.367136255900065
Training loss: 0.3956136405467987 / Valid loss: 7.500979718707857
Training loss: 0.7805676460266113 / Valid loss: 7.271709646497454

Epoch: 33
Training loss: 0.4312138557434082 / Valid loss: 7.234583059946696
Training loss: 0.6053420305252075 / Valid loss: 7.236703423091344
Training loss: 0.7399791479110718 / Valid loss: 7.19126946585519
Training loss: 0.46370700001716614 / Valid loss: 7.449813413619995
Training loss: 0.9637587666511536 / Valid loss: 7.381303460257394

Epoch: 34
Training loss: 1.0285212993621826 / Valid loss: 7.455468532017299
Training loss: 0.5650491118431091 / Valid loss: 7.366378838675363
Training loss: 0.5929945707321167 / Valid loss: 7.451103469303676
Training loss: 0.5368311405181885 / Valid loss: 7.171650498253959
Training loss: 1.1016165018081665 / Valid loss: 7.30418153490339

Epoch: 35
Training loss: 0.570915699005127 / Valid loss: 7.1676872525896345
Training loss: 0.8288216590881348 / Valid loss: 7.264175764719645
Training loss: 0.46653929352760315 / Valid loss: 7.251001017434256
Training loss: 0.37496986985206604 / Valid loss: 7.335305895124163
Training loss: 0.5372462868690491 / Valid loss: 7.9055884815397715

Epoch: 36
Training loss: 0.42705038189888 / Valid loss: 7.196919999803815
Training loss: 0.4156717360019684 / Valid loss: 7.129904379163469
Training loss: 0.4959741532802582 / Valid loss: 7.146361596243722
Training loss: 1.0312060117721558 / Valid loss: 7.339077690669469
Training loss: 0.5428034067153931 / Valid loss: 7.429061789739699

Epoch: 37
Training loss: 0.5144367814064026 / Valid loss: 7.163846592676072
Training loss: 0.4263705015182495 / Valid loss: 7.339742930730184
Training loss: 0.3680623769760132 / Valid loss: 7.22187702088129
Training loss: 0.6535192728042603 / Valid loss: 7.346387876783099
Training loss: 0.7315571308135986 / Valid loss: 7.256704916272844

Epoch: 38
Training loss: 0.7723866105079651 / Valid loss: 7.343012830189296
Training loss: 0.6117786169052124 / Valid loss: 7.1466880207970025
Training loss: 0.3011387586593628 / Valid loss: 7.362502039046515
Training loss: 0.8948017358779907 / Valid loss: 7.262730920882452
Training loss: 0.9481068849563599 / Valid loss: 7.423563675653367

Epoch: 39
Training loss: 0.5662729740142822 / Valid loss: 7.113974593934559
Training loss: 0.56878662109375 / Valid loss: 7.087985529218401
Training loss: 0.4870458245277405 / Valid loss: 7.5037301245189845
Training loss: 0.34059256315231323 / Valid loss: 7.296097571509225
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 900): 5.311697196960449
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.62260627746582 / Valid loss: 13.353901926676432
Model is saved in epoch 0, overall batch: 0
Training loss: 6.209975242614746 / Valid loss: 5.938352648417155
Model is saved in epoch 0, overall batch: 100
Training loss: 6.657140731811523 / Valid loss: 5.6690251191457115
Model is saved in epoch 0, overall batch: 200
Training loss: 6.0860395431518555 / Valid loss: 5.649145680382138
Model is saved in epoch 0, overall batch: 300
Training loss: 8.649901390075684 / Valid loss: 5.861468551272438

Epoch: 1
Training loss: 5.354130268096924 / Valid loss: 5.59403352964492
Model is saved in epoch 1, overall batch: 500
Training loss: 5.722609043121338 / Valid loss: 5.7675690696353
Training loss: 6.917623996734619 / Valid loss: 5.58025381224496
Model is saved in epoch 1, overall batch: 700
Training loss: 6.027001857757568 / Valid loss: 5.6938111713954385
Training loss: 4.933468818664551 / Valid loss: 5.548583309991019
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.002928733825684 / Valid loss: 5.560127944038028
Training loss: 4.732935428619385 / Valid loss: 5.796114880698068
Training loss: 5.594146728515625 / Valid loss: 5.606353044509888
Training loss: 5.822414398193359 / Valid loss: 5.567305113020398
Training loss: 5.537273406982422 / Valid loss: 5.647305052621024

Epoch: 3
Training loss: 3.823054790496826 / Valid loss: 5.718273623784383
Training loss: 5.049328327178955 / Valid loss: 5.771990489959717
Training loss: 4.427206993103027 / Valid loss: 5.645728370121547
Training loss: 5.798254013061523 / Valid loss: 5.899954305376325
Training loss: 4.720412731170654 / Valid loss: 5.590448318208967

Epoch: 4
Training loss: 3.5838117599487305 / Valid loss: 5.629602745601109
Training loss: 4.065610885620117 / Valid loss: 5.675910640898205
Training loss: 4.036846160888672 / Valid loss: 5.795170277640933
Training loss: 4.963386535644531 / Valid loss: 5.828781543459211
Training loss: 7.163593292236328 / Valid loss: 5.716830062866211

Epoch: 5
Training loss: 3.495128631591797 / Valid loss: 5.864151777539934
Training loss: 3.4888532161712646 / Valid loss: 5.985439121155512
Training loss: 5.383694171905518 / Valid loss: 5.796123275302706
Training loss: 5.574288368225098 / Valid loss: 5.7908525535038535
Training loss: 6.91278076171875 / Valid loss: 6.362708786555699

Epoch: 6
Training loss: 4.081301212310791 / Valid loss: 5.947946185157413
Training loss: 3.5470738410949707 / Valid loss: 6.112155589603242
Training loss: 3.9780189990997314 / Valid loss: 5.8611877850123815
Training loss: 3.7408461570739746 / Valid loss: 6.07342255456107
Training loss: 3.112643003463745 / Valid loss: 5.89337387084961

Epoch: 7
Training loss: 2.6231627464294434 / Valid loss: 6.299786426907494
Training loss: 3.2061967849731445 / Valid loss: 6.275726904187883
Training loss: 3.86448335647583 / Valid loss: 6.139255510057722
Training loss: 4.203969478607178 / Valid loss: 6.142242656435285
Training loss: 4.170599937438965 / Valid loss: 6.186718143735614

Epoch: 8
Training loss: 3.2431578636169434 / Valid loss: 6.097833229246594
Training loss: 3.1231181621551514 / Valid loss: 6.105267070588612
Training loss: 4.691348075866699 / Valid loss: 6.269008257275536
Training loss: 3.699998140335083 / Valid loss: 6.730023118427821
Training loss: 3.2538070678710938 / Valid loss: 6.1342928568522135

Epoch: 9
Training loss: 3.135190010070801 / Valid loss: 6.307549242746262
Training loss: 3.7118332386016846 / Valid loss: 6.331766183035715
Training loss: 3.276899814605713 / Valid loss: 6.390090951465425
Training loss: 2.7179970741271973 / Valid loss: 6.393450441814604

Epoch: 10
Training loss: 2.623319149017334 / Valid loss: 6.388538560413179
Training loss: 2.446528911590576 / Valid loss: 6.383131576719738
Training loss: 2.99037504196167 / Valid loss: 6.476015390668596
Training loss: 2.648900032043457 / Valid loss: 6.372207532610212
Training loss: 3.119591474533081 / Valid loss: 7.101990036737352

Epoch: 11
Training loss: 2.794438362121582 / Valid loss: 7.4222832180204845
Training loss: 3.086254119873047 / Valid loss: 6.433704662322998
Training loss: 2.5391359329223633 / Valid loss: 7.5088509150913785
Training loss: 2.289174795150757 / Valid loss: 6.638603444326492
Training loss: 2.4202117919921875 / Valid loss: 6.719487567175002

Epoch: 12
Training loss: 2.5406594276428223 / Valid loss: 6.571852020990281
Training loss: 2.29537296295166 / Valid loss: 6.831959760756719
Training loss: 2.6605772972106934 / Valid loss: 6.773806095123291
Training loss: 2.3508501052856445 / Valid loss: 6.685031613849458
Training loss: 1.8600953817367554 / Valid loss: 6.736945224943615

Epoch: 13
Training loss: 2.169264793395996 / Valid loss: 6.714530447551183
Training loss: 1.85466468334198 / Valid loss: 6.9217297622135705
Training loss: 1.8603324890136719 / Valid loss: 7.396646767570859
Training loss: 1.9070500135421753 / Valid loss: 7.847225057511102
Training loss: 2.4426047801971436 / Valid loss: 7.129623226892381

Epoch: 14
Training loss: 1.9278308153152466 / Valid loss: 6.922301387786865
Training loss: 2.11203670501709 / Valid loss: 6.949532211394537
Training loss: 3.0372304916381836 / Valid loss: 6.890346735999698
Training loss: 2.2642130851745605 / Valid loss: 7.275088755289714
Training loss: 2.5430150032043457 / Valid loss: 7.476728898002988

Epoch: 15
Training loss: 1.373929738998413 / Valid loss: 6.961266086215065
Training loss: 2.4511141777038574 / Valid loss: 7.026880073547363
Training loss: 2.445409059524536 / Valid loss: 8.350712780725388
Training loss: 2.4700307846069336 / Valid loss: 7.113895016624814
Training loss: 1.7969317436218262 / Valid loss: 7.122629224686396

Epoch: 16
Training loss: 2.596043348312378 / Valid loss: 6.9939536957513715
Training loss: 1.7437063455581665 / Valid loss: 8.004189123426166
Training loss: 1.6254961490631104 / Valid loss: 7.436525594620478
Training loss: 2.5185835361480713 / Valid loss: 7.342761162349156
Training loss: 1.4941606521606445 / Valid loss: 8.309204705556233

Epoch: 17
Training loss: 1.6409211158752441 / Valid loss: 7.214138071877616
Training loss: 1.2705012559890747 / Valid loss: 7.644867747170585
Training loss: 1.8458242416381836 / Valid loss: 7.874201706477574
Training loss: 3.477997303009033 / Valid loss: 7.342672967910767
Training loss: 2.2723419666290283 / Valid loss: 7.704221798124768

Epoch: 18
Training loss: 1.8978021144866943 / Valid loss: 7.157495171683175
Training loss: 1.3347320556640625 / Valid loss: 7.3198031834193635
Training loss: 1.764765977859497 / Valid loss: 7.297768983386812
Training loss: 1.5336369276046753 / Valid loss: 7.377617758796329
Training loss: 2.1466546058654785 / Valid loss: 7.292234021141415

Epoch: 19
Training loss: 1.0225752592086792 / Valid loss: 7.458514318012056
Training loss: 1.466169834136963 / Valid loss: 7.536269351414272
Training loss: 1.7080810070037842 / Valid loss: 7.785770743233817
Training loss: 1.3061950206756592 / Valid loss: 7.79887417838687

Epoch: 20
Training loss: 1.3851466178894043 / Valid loss: 8.025678848084949
Training loss: 1.3684251308441162 / Valid loss: 7.481879938216436
Training loss: 1.4538863897323608 / Valid loss: 7.5011401358104886
Training loss: 0.9436802864074707 / Valid loss: 7.817584959665934
Training loss: 1.331653118133545 / Valid loss: 7.5005481220427015

Epoch: 21
Training loss: 1.1871711015701294 / Valid loss: 7.52879284449986
Training loss: 1.7626593112945557 / Valid loss: 7.457590589069185
Training loss: 1.2830915451049805 / Valid loss: 7.312341181437175
Training loss: 1.9131709337234497 / Valid loss: 7.491423802148728
Training loss: 1.3186510801315308 / Valid loss: 7.755304363795689

Epoch: 22
Training loss: 1.1764731407165527 / Valid loss: 8.564026296706427
Training loss: 1.4083671569824219 / Valid loss: 7.582614090329125
Training loss: 1.5111618041992188 / Valid loss: 8.225541932242256
Training loss: 1.6248233318328857 / Valid loss: 7.537430240994408
Training loss: 1.3273184299468994 / Valid loss: 7.634362011864072

Epoch: 23
Training loss: 0.7913104295730591 / Valid loss: 8.049646250406902
Training loss: 1.5149190425872803 / Valid loss: 8.403768825531007
Training loss: 1.2592211961746216 / Valid loss: 7.522228835877918
Training loss: 1.1059577465057373 / Valid loss: 7.388544963655018
Training loss: 0.8670718669891357 / Valid loss: 7.570934763408842

Epoch: 24
Training loss: 0.9996305108070374 / Valid loss: 7.698445188431513
Training loss: 1.3092812299728394 / Valid loss: 7.537623573484875
Training loss: 1.0210673809051514 / Valid loss: 7.474541237240746
Training loss: 0.7686432600021362 / Valid loss: 7.5173091797601606
Training loss: 1.112255573272705 / Valid loss: 8.002359326680502

Epoch: 25
Training loss: 0.8525614738464355 / Valid loss: 7.529256339300247
Training loss: 0.9037367701530457 / Valid loss: 7.58769854363941
Training loss: 1.4796773195266724 / Valid loss: 7.671196210952032
Training loss: 1.0481916666030884 / Valid loss: 7.62433446702503
Training loss: 1.2217562198638916 / Valid loss: 7.451361833299909

Epoch: 26
Training loss: 0.8761177659034729 / Valid loss: 7.406520384833926
Training loss: 0.7000260353088379 / Valid loss: 7.482192856924875
Training loss: 0.7811130285263062 / Valid loss: 7.611166590736026
Training loss: 0.8605505228042603 / Valid loss: 8.093085833958217
Training loss: 0.7834279537200928 / Valid loss: 7.438080506097703

Epoch: 27
Training loss: 0.893368124961853 / Valid loss: 7.428994260515485
Training loss: 1.0406368970870972 / Valid loss: 7.525310888744536
Training loss: 1.1294852495193481 / Valid loss: 7.588510209038144
Training loss: 0.8173052072525024 / Valid loss: 8.362544295901344
Training loss: 1.0240179300308228 / Valid loss: 7.515252985273089

Epoch: 28
Training loss: 0.602420449256897 / Valid loss: 7.567193794250488
Training loss: 0.7199393510818481 / Valid loss: 7.672983060564314
Training loss: 0.7237443923950195 / Valid loss: 7.517080683935256
Training loss: 1.1661889553070068 / Valid loss: 7.546648777098882
Training loss: 1.3271983861923218 / Valid loss: 8.093311332520985

Epoch: 29
Training loss: 0.7113842964172363 / Valid loss: 7.613020615350632
Training loss: 0.7784661054611206 / Valid loss: 7.6118932088216145
Training loss: 0.7533011436462402 / Valid loss: 7.628197188604446
Training loss: 0.8483590483665466 / Valid loss: 7.553379849025181

Epoch: 30
Training loss: 0.7243969440460205 / Valid loss: 7.498639942350842
Training loss: 0.8754979372024536 / Valid loss: 7.930885056086949
Training loss: 0.6403606534004211 / Valid loss: 8.651221951984224
Training loss: 0.8927487730979919 / Valid loss: 8.568883932204473
Training loss: 0.7910208702087402 / Valid loss: 9.489842210497175

Epoch: 31
Training loss: 0.8403601050376892 / Valid loss: 7.560801783062163
Training loss: 0.7941187620162964 / Valid loss: 7.754811275573004
Training loss: 0.9217752814292908 / Valid loss: 7.484380753835042
Training loss: 0.7902774810791016 / Valid loss: 7.720435001736596
Training loss: 0.8364449143409729 / Valid loss: 7.565843068985712

Epoch: 32
Training loss: 0.6388891935348511 / Valid loss: 7.465995143708729
Training loss: 0.5080510377883911 / Valid loss: 7.5541713941664925
Training loss: 0.744195818901062 / Valid loss: 8.16216893877302
Training loss: 0.8352315425872803 / Valid loss: 7.465145960308257
Training loss: 0.7278221845626831 / Valid loss: 8.455036762782505

Epoch: 33
Training loss: 0.6818739175796509 / Valid loss: 7.6341467902773905
Training loss: 0.8490628004074097 / Valid loss: 7.84659484681629
Training loss: 0.7665888071060181 / Valid loss: 7.82878063746861
Training loss: 0.5771690607070923 / Valid loss: 8.203138455890475
Training loss: 0.7682756185531616 / Valid loss: 7.696637598673503

Epoch: 34
Training loss: 0.5671889185905457 / Valid loss: 8.131012471516927
Training loss: 0.9935504198074341 / Valid loss: 7.550223091670445
Training loss: 0.571969747543335 / Valid loss: 7.703635115850539
Training loss: 0.7800058126449585 / Valid loss: 7.617918677557082
Training loss: 0.8880445957183838 / Valid loss: 7.4995071547372

Epoch: 35
Training loss: 0.6003599762916565 / Valid loss: 7.526818466186524
Training loss: 0.7571032047271729 / Valid loss: 7.623536445980981
Training loss: 0.6221694946289062 / Valid loss: 7.573490047454834
Training loss: 0.7256249189376831 / Valid loss: 7.56898483094715
Training loss: 0.812381386756897 / Valid loss: 7.518256832304455

Epoch: 36
Training loss: 0.8720254898071289 / Valid loss: 7.565527257465181
Training loss: 1.0495654344558716 / Valid loss: 7.523851083573841
Training loss: 0.64485764503479 / Valid loss: 7.680352015722366
Training loss: 0.6913378238677979 / Valid loss: 7.600678443908691
Training loss: 0.823438286781311 / Valid loss: 7.519838174184163

Epoch: 37
Training loss: 1.0217512845993042 / Valid loss: 8.063836878821963
Training loss: 0.794847309589386 / Valid loss: 7.64351148151216
Training loss: 1.0612019300460815 / Valid loss: 7.573200012388684
Training loss: 0.4978829324245453 / Valid loss: 7.538867895943778
Training loss: 0.8021085262298584 / Valid loss: 7.539363447825114

Epoch: 38
Training loss: 0.6747452616691589 / Valid loss: 8.160835901896158
Training loss: 0.8938506245613098 / Valid loss: 7.502346474783761
Training loss: 0.4844370484352112 / Valid loss: 7.557532437642416
Training loss: 0.834474503993988 / Valid loss: 7.594464756193615
Training loss: 0.7538855671882629 / Valid loss: 9.150615138099306

Epoch: 39
Training loss: 0.764763355255127 / Valid loss: 7.532930978139242
Training loss: 0.6772017478942871 / Valid loss: 7.522938278743199
Training loss: 0.5443825721740723 / Valid loss: 7.6536134946913945
Training loss: 0.3740144371986389 / Valid loss: 7.563319456009638
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 900): 5.371401709601993
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.62260627746582 / Valid loss: 13.353534662155878
Model is saved in epoch 0, overall batch: 0
Training loss: 6.212402820587158 / Valid loss: 5.954123401641846
Model is saved in epoch 0, overall batch: 100
Training loss: 6.629227638244629 / Valid loss: 5.664971397036598
Model is saved in epoch 0, overall batch: 200
Training loss: 6.093127727508545 / Valid loss: 5.6654205254146035
Training loss: 8.662590026855469 / Valid loss: 5.871203672318232

Epoch: 1
Training loss: 5.389215469360352 / Valid loss: 5.594400380906604
Model is saved in epoch 1, overall batch: 500
Training loss: 5.6406755447387695 / Valid loss: 5.781008059637887
Training loss: 6.9540557861328125 / Valid loss: 5.5812501566750665
Model is saved in epoch 1, overall batch: 700
Training loss: 6.027154445648193 / Valid loss: 5.703927964255923
Training loss: 5.040050506591797 / Valid loss: 5.5505722159431095
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.021651744842529 / Valid loss: 5.5627085481371195
Training loss: 4.757504463195801 / Valid loss: 5.8072953632899695
Training loss: 5.487943172454834 / Valid loss: 5.614096053441366
Training loss: 5.801032543182373 / Valid loss: 5.565968558901832
Training loss: 5.551462173461914 / Valid loss: 5.636608260018485

Epoch: 3
Training loss: 3.817931890487671 / Valid loss: 5.688123709814889
Training loss: 5.0878448486328125 / Valid loss: 5.785130557559786
Training loss: 4.374472141265869 / Valid loss: 5.661423562821888
Training loss: 5.811017990112305 / Valid loss: 5.90145222345988
Training loss: 4.652024745941162 / Valid loss: 5.584080178397042

Epoch: 4
Training loss: 3.6068594455718994 / Valid loss: 5.6271533625466486
Training loss: 4.0644025802612305 / Valid loss: 5.679051104046049
Training loss: 3.9139904975891113 / Valid loss: 5.794194580259777
Training loss: 4.939845561981201 / Valid loss: 5.818888755071731
Training loss: 7.06442928314209 / Valid loss: 5.733596769968669

Epoch: 5
Training loss: 3.4603965282440186 / Valid loss: 5.8812966074262345
Training loss: 3.3073019981384277 / Valid loss: 6.0687316712879
Training loss: 5.328269004821777 / Valid loss: 5.800704751695905
Training loss: 5.6253228187561035 / Valid loss: 5.8178859869639075
Training loss: 6.8614325523376465 / Valid loss: 6.353448795136951

Epoch: 6
Training loss: 4.277751922607422 / Valid loss: 5.914111607415336
Training loss: 3.476367712020874 / Valid loss: 6.065909642264956
Training loss: 4.062388896942139 / Valid loss: 5.892903702599662
Training loss: 3.554154634475708 / Valid loss: 6.213916274479457
Training loss: 2.97465443611145 / Valid loss: 5.90996797198341

Epoch: 7
Training loss: 2.5898971557617188 / Valid loss: 6.30123644556318
Training loss: 3.151329517364502 / Valid loss: 6.30175127074832
Training loss: 3.872847318649292 / Valid loss: 6.160562810443697
Training loss: 4.023150444030762 / Valid loss: 6.10135703768049
Training loss: 4.283905029296875 / Valid loss: 6.134664228984287

Epoch: 8
Training loss: 3.1696629524230957 / Valid loss: 6.103466306413923
Training loss: 3.1257505416870117 / Valid loss: 6.146554608572097
Training loss: 4.42165994644165 / Valid loss: 6.227019809541249
Training loss: 3.812818765640259 / Valid loss: 6.774720559801374
Training loss: 3.332159996032715 / Valid loss: 6.156112988789876

Epoch: 9
Training loss: 3.5287280082702637 / Valid loss: 6.344951829456147
Training loss: 3.5440473556518555 / Valid loss: 6.3075894968850275
Training loss: 3.4401791095733643 / Valid loss: 6.447949052992321
Training loss: 2.671557903289795 / Valid loss: 6.436436026436942

Epoch: 10
Training loss: 2.7777528762817383 / Valid loss: 6.341238639468238
Training loss: 2.6107823848724365 / Valid loss: 6.410265182313465
Training loss: 3.028109073638916 / Valid loss: 6.6500429312388105
Training loss: 2.9433553218841553 / Valid loss: 6.429871740795317
Training loss: 2.9913578033447266 / Valid loss: 7.373044717879522

Epoch: 11
Training loss: 3.1237125396728516 / Valid loss: 7.248871510369437
Training loss: 3.3867764472961426 / Valid loss: 6.426828643253871
Training loss: 2.4983856678009033 / Valid loss: 7.396805327279227
Training loss: 2.2771902084350586 / Valid loss: 6.792058422451928
Training loss: 2.64910626411438 / Valid loss: 6.793847579047793

Epoch: 12
Training loss: 2.657439708709717 / Valid loss: 6.657334209623791
Training loss: 2.3990726470947266 / Valid loss: 6.759455985114688
Training loss: 2.922187566757202 / Valid loss: 6.769995516822451
Training loss: 2.5525074005126953 / Valid loss: 6.764311531611852
Training loss: 1.7687984704971313 / Valid loss: 6.822280329749698

Epoch: 13
Training loss: 2.2603602409362793 / Valid loss: 6.714874667213077
Training loss: 1.8472239971160889 / Valid loss: 6.960564468020484
Training loss: 1.86164128780365 / Valid loss: 7.34539326259068
Training loss: 2.0172581672668457 / Valid loss: 7.469436881655739
Training loss: 2.1711020469665527 / Valid loss: 6.8630175681341266

Epoch: 14
Training loss: 2.0732569694519043 / Valid loss: 6.947562260854812
Training loss: 2.5082011222839355 / Valid loss: 6.991281350453694
Training loss: 3.4565744400024414 / Valid loss: 6.956306144169399
Training loss: 2.351963520050049 / Valid loss: 7.16757832027617
Training loss: 2.4875454902648926 / Valid loss: 7.387030774071103

Epoch: 15
Training loss: 1.3467289209365845 / Valid loss: 6.9788374265035
Training loss: 2.5733110904693604 / Valid loss: 7.245959574835641
Training loss: 2.563166379928589 / Valid loss: 7.636891796475365
Training loss: 2.368506669998169 / Valid loss: 7.100120340074811
Training loss: 2.3663330078125 / Valid loss: 7.231135481879825

Epoch: 16
Training loss: 2.278179168701172 / Valid loss: 7.032593509129115
Training loss: 1.7566924095153809 / Valid loss: 7.4561704136076425
Training loss: 1.3966784477233887 / Valid loss: 7.206410071963355
Training loss: 2.298290729522705 / Valid loss: 7.111599477132161
Training loss: 1.6264081001281738 / Valid loss: 7.733901005699521

Epoch: 17
Training loss: 1.563102126121521 / Valid loss: 7.22774103255499
Training loss: 1.4666473865509033 / Valid loss: 7.824425574711391
Training loss: 1.9181963205337524 / Valid loss: 7.510083938780285
Training loss: 3.5190224647521973 / Valid loss: 7.2286358742486865
Training loss: 2.012819290161133 / Valid loss: 7.1683315095447355

Epoch: 18
Training loss: 1.5538073778152466 / Valid loss: 7.299072563080561
Training loss: 1.4391701221466064 / Valid loss: 7.541660154433478
Training loss: 1.5919075012207031 / Valid loss: 7.241482289632161
Training loss: 1.8715773820877075 / Valid loss: 7.564235078720819
Training loss: 2.0029683113098145 / Valid loss: 7.396308526538667

Epoch: 19
Training loss: 1.1984429359436035 / Valid loss: 7.5476845877511165
Training loss: 1.4835048913955688 / Valid loss: 7.767005597977411
Training loss: 1.864819049835205 / Valid loss: 7.904602091653007
Training loss: 1.781468391418457 / Valid loss: 7.985719299316406

Epoch: 20
Training loss: 1.2396514415740967 / Valid loss: 8.382582918802898
Training loss: 1.3477139472961426 / Valid loss: 7.818990548451741
Training loss: 1.4588639736175537 / Valid loss: 7.549773443312872
Training loss: 0.8921393156051636 / Valid loss: 7.458566840489706
Training loss: 1.4296034574508667 / Valid loss: 7.438719576881045

Epoch: 21
Training loss: 1.2806177139282227 / Valid loss: 8.00414548601423
Training loss: 1.8584492206573486 / Valid loss: 7.720186033703032
Training loss: 1.511721134185791 / Valid loss: 7.29641911642892
Training loss: 1.6905393600463867 / Valid loss: 7.360040369487944
Training loss: 1.429431676864624 / Valid loss: 7.652651087443034

Epoch: 22
Training loss: 1.2854255437850952 / Valid loss: 7.619440855298723
Training loss: 1.347177505493164 / Valid loss: 8.817335914430164
Training loss: 1.7956035137176514 / Valid loss: 7.465467180524554
Training loss: 1.4532358646392822 / Valid loss: 7.936015551430838
Training loss: 1.2408829927444458 / Valid loss: 7.6273052079336985

Epoch: 23
Training loss: 0.8644731640815735 / Valid loss: 7.512866867156256
Training loss: 1.562097191810608 / Valid loss: 7.520038005283901
Training loss: 1.3830740451812744 / Valid loss: 7.4405240649268745
Training loss: 0.9845464825630188 / Valid loss: 7.4361664544968376
Training loss: 0.9493348598480225 / Valid loss: 7.736135991414388

Epoch: 24
Training loss: 0.9440937042236328 / Valid loss: 7.958882372719901
Training loss: 1.306893229484558 / Valid loss: 7.518388748168945
Training loss: 1.1588270664215088 / Valid loss: 7.5782027971176875
Training loss: 0.8253107666969299 / Valid loss: 7.636863040924072
Training loss: 0.9863452911376953 / Valid loss: 7.537425104777018

Epoch: 25
Training loss: 0.8431981801986694 / Valid loss: 7.729690140769595
Training loss: 0.7165787816047668 / Valid loss: 7.485150782267253
Training loss: 1.8020517826080322 / Valid loss: 7.650656327747163
Training loss: 1.189417839050293 / Valid loss: 8.280016976311094
Training loss: 1.2645483016967773 / Valid loss: 7.434419631958008

Epoch: 26
Training loss: 0.893248975276947 / Valid loss: 7.454727976662772
Training loss: 0.8740067481994629 / Valid loss: 7.45871569769723
Training loss: 1.0278457403182983 / Valid loss: 7.467200224740164
Training loss: 1.0432977676391602 / Valid loss: 7.604472596304757
Training loss: 0.5213952660560608 / Valid loss: 7.555491792588007

Epoch: 27
Training loss: 0.9641602039337158 / Valid loss: 7.435712328411284
Training loss: 1.1389216184616089 / Valid loss: 7.567131669180734
Training loss: 0.9628932476043701 / Valid loss: 7.507602464585077
Training loss: 0.8281464576721191 / Valid loss: 7.667348230452765
Training loss: 1.2309608459472656 / Valid loss: 7.412616034916469

Epoch: 28
Training loss: 0.5858606100082397 / Valid loss: 7.584477015904018
Training loss: 0.9866313338279724 / Valid loss: 7.50653414499192
Training loss: 0.8904409408569336 / Valid loss: 7.763009729839506
Training loss: 1.419878363609314 / Valid loss: 7.576574028105963
Training loss: 1.0500352382659912 / Valid loss: 7.847542635599772

Epoch: 29
Training loss: 1.0549577474594116 / Valid loss: 7.523958437783378
Training loss: 0.8674927949905396 / Valid loss: 8.023967665717715
Training loss: 0.9558587074279785 / Valid loss: 7.6089172181629
Training loss: 0.7300224900245667 / Valid loss: 7.7958758989969885

Epoch: 30
Training loss: 0.6852261424064636 / Valid loss: 7.662484795706613
Training loss: 0.9160200357437134 / Valid loss: 8.053937139965239
Training loss: 0.8006826639175415 / Valid loss: 8.04747910726638
Training loss: 1.099075436592102 / Valid loss: 8.739182326907203
Training loss: 0.5956091284751892 / Valid loss: 9.198542131696428

Epoch: 31
Training loss: 0.7204769849777222 / Valid loss: 7.611166795094808
Training loss: 0.8597352504730225 / Valid loss: 7.75241337730771
Training loss: 0.9651892185211182 / Valid loss: 7.623060576121012
Training loss: 0.8701165914535522 / Valid loss: 7.634134928385417
Training loss: 1.3647643327713013 / Valid loss: 7.643618651798794

Epoch: 32
Training loss: 0.5949510931968689 / Valid loss: 7.569277817862375
Training loss: 0.6078383922576904 / Valid loss: 7.717802115849087
Training loss: 0.8195375800132751 / Valid loss: 8.051155371893019
Training loss: 0.8084676265716553 / Valid loss: 7.737930899574643
Training loss: 0.7629880905151367 / Valid loss: 7.975680237724667

Epoch: 33
Training loss: 0.8307660818099976 / Valid loss: 7.4685942059471495
Training loss: 0.4782354533672333 / Valid loss: 7.478450965881348
Training loss: 0.6980006098747253 / Valid loss: 7.518813501085554
Training loss: 0.6146339774131775 / Valid loss: 7.66531708581107
Training loss: 0.7943907976150513 / Valid loss: 7.941364551725842

Epoch: 34
Training loss: 0.5301758050918579 / Valid loss: 7.603404962448847
Training loss: 0.957543671131134 / Valid loss: 7.538070501599993
Training loss: 0.6202106475830078 / Valid loss: 7.763364978063674
Training loss: 0.6686698794364929 / Valid loss: 8.008364836374918
Training loss: 0.985916256904602 / Valid loss: 7.59562680380685

Epoch: 35
Training loss: 0.7720913887023926 / Valid loss: 7.802448490687779
Training loss: 0.6823910474777222 / Valid loss: 7.6202948206946965
Training loss: 0.6308774948120117 / Valid loss: 7.56065536226545
Training loss: 0.8028873205184937 / Valid loss: 7.499820586613247
Training loss: 0.7380005121231079 / Valid loss: 8.197699433281308

Epoch: 36
Training loss: 0.6593772172927856 / Valid loss: 7.580519789741153
Training loss: 0.9863353967666626 / Valid loss: 7.399208509354365
Training loss: 0.6677074432373047 / Valid loss: 7.584333188193185
Training loss: 0.6216541528701782 / Valid loss: 7.545562217349098
Training loss: 0.8584308624267578 / Valid loss: 7.552101621173677

Epoch: 37
Training loss: 1.242292881011963 / Valid loss: 8.228931999206543
Training loss: 0.6731041073799133 / Valid loss: 7.581103206816174
Training loss: 0.9236083030700684 / Valid loss: 7.7608245123000374
Training loss: 0.6231496334075928 / Valid loss: 7.567578093210856
Training loss: 0.9917916059494019 / Valid loss: 7.53423428989592

Epoch: 38
Training loss: 0.4883909821510315 / Valid loss: 7.9651722226824075
Training loss: 0.9701802730560303 / Valid loss: 7.589724654243106
Training loss: 0.46329188346862793 / Valid loss: 8.281292856307257
Training loss: 1.1724121570587158 / Valid loss: 7.665367962065197
Training loss: 0.7538559436798096 / Valid loss: 8.37785427910941

Epoch: 39
Training loss: 0.9192272424697876 / Valid loss: 7.668912660507929
Training loss: 0.6568285822868347 / Valid loss: 7.8208812214079355
Training loss: 0.5359292030334473 / Valid loss: 7.576753543672107
Training loss: 0.46374839544296265 / Valid loss: 7.580719543638684
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 900): 5.369702893211728
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)

Epoch: 0
Training loss: 19.044994354248047 / Valid loss: 8.092049033301217
Model is saved in epoch 0, overall batch: 0
Training loss: 6.148061275482178 / Valid loss: 5.701571578071231
Model is saved in epoch 0, overall batch: 100
Training loss: 6.170613765716553 / Valid loss: 5.706854659035092
Training loss: 5.879120826721191 / Valid loss: 5.595105975014823
Model is saved in epoch 0, overall batch: 300
Training loss: 3.643350601196289 / Valid loss: 5.651296942574637

Epoch: 1
Training loss: 6.716653823852539 / Valid loss: 5.643029271988642
Training loss: 4.63120174407959 / Valid loss: 5.664374035880679
Training loss: 7.421722888946533 / Valid loss: 5.6524987833840505
Training loss: 4.275517463684082 / Valid loss: 6.113039071219308
Training loss: 4.8520708084106445 / Valid loss: 5.763112172626314

Epoch: 2
Training loss: 5.925057411193848 / Valid loss: 5.749677099500384
Training loss: 4.9208879470825195 / Valid loss: 5.655369467962355
Training loss: 5.841879367828369 / Valid loss: 5.649617410841442
Training loss: 3.730836868286133 / Valid loss: 5.665140671957107
Training loss: 4.4928975105285645 / Valid loss: 5.711184247334798

Epoch: 3
Training loss: 5.076797962188721 / Valid loss: 5.838827771232242
Training loss: 3.914113998413086 / Valid loss: 5.850191699890863
Training loss: 7.669660568237305 / Valid loss: 5.634028825305757
Training loss: 3.9252095222473145 / Valid loss: 5.771666631244478
Training loss: 5.706540107727051 / Valid loss: 5.748425783429827

Epoch: 4
Training loss: 5.83389949798584 / Valid loss: 5.7714359646751765
Training loss: 4.679776191711426 / Valid loss: 5.84993695077442
Training loss: 4.836705207824707 / Valid loss: 5.799211063839141
Training loss: 4.255773067474365 / Valid loss: 5.715037952150618
Training loss: 5.0942487716674805 / Valid loss: 5.631516508829026

Epoch: 5
Training loss: 6.93311882019043 / Valid loss: 5.809188656579881
Training loss: 5.0931243896484375 / Valid loss: 5.7080225694747195
Training loss: 5.0434346199035645 / Valid loss: 6.587805811564127
Training loss: 5.392146110534668 / Valid loss: 5.744984515508016
Training loss: 6.84440803527832 / Valid loss: 6.254306716010684

Epoch: 6
Training loss: 4.845000743865967 / Valid loss: 5.7676647208985825
Training loss: 5.218272686004639 / Valid loss: 5.975734853744507
Training loss: 5.098257064819336 / Valid loss: 5.766922655559721
Training loss: 4.618958950042725 / Valid loss: 5.737967459360759
Training loss: 6.732781410217285 / Valid loss: 5.823581641060965

Epoch: 7
Training loss: 4.464107513427734 / Valid loss: 5.711232609975905
Training loss: 3.9775307178497314 / Valid loss: 5.8027155944279265
Training loss: 6.677234649658203 / Valid loss: 5.787251688185192
Training loss: 5.358392715454102 / Valid loss: 5.688670378639585
Training loss: 5.044065475463867 / Valid loss: 5.7349123137337825

Epoch: 8
Training loss: 5.617430686950684 / Valid loss: 6.440773282732282
Training loss: 4.527408599853516 / Valid loss: 5.818193703606015
Training loss: 4.710190773010254 / Valid loss: 5.680916209447951
Training loss: 4.812481880187988 / Valid loss: 5.796826117379325
Training loss: 2.8229005336761475 / Valid loss: 5.865010665711902

Epoch: 9
Training loss: 3.912248373031616 / Valid loss: 5.704276062193371
Training loss: 5.340813636779785 / Valid loss: 5.725011128471011
Training loss: 7.440926551818848 / Valid loss: 5.881696909949893
Training loss: 6.299976825714111 / Valid loss: 6.293206001463391

Epoch: 10
Training loss: 3.9053196907043457 / Valid loss: 5.75392249198187
Training loss: 4.644098281860352 / Valid loss: 5.802572266260783
Training loss: 5.324538230895996 / Valid loss: 6.320408167157854
Training loss: 5.224365711212158 / Valid loss: 5.755738828295753
Training loss: 5.087753772735596 / Valid loss: 6.023292282649449

Epoch: 11
Training loss: 5.310728073120117 / Valid loss: 5.790553238278344
Training loss: 5.442705154418945 / Valid loss: 5.861511141913278
Training loss: 4.663771629333496 / Valid loss: 6.007434070677984
Training loss: 8.038566589355469 / Valid loss: 5.749161609013876
Training loss: 4.973113059997559 / Valid loss: 5.868695799509684

Epoch: 12
Training loss: 5.370931625366211 / Valid loss: 5.7683356012616835
Training loss: 6.095061302185059 / Valid loss: 5.800792759940737
Training loss: 3.7065019607543945 / Valid loss: 6.37833370027088
Training loss: 4.6184611320495605 / Valid loss: 5.76563591048831
Training loss: 4.034323692321777 / Valid loss: 5.813520851589384

Epoch: 13
Training loss: 4.606281757354736 / Valid loss: 5.772801673979986
Training loss: 4.794265270233154 / Valid loss: 6.3341354029519215
Training loss: 5.5112457275390625 / Valid loss: 5.8656221162705195
Training loss: 6.008370876312256 / Valid loss: 5.846045564469837
Training loss: 5.050445556640625 / Valid loss: 5.738816152300154

Epoch: 14
Training loss: 5.622304916381836 / Valid loss: 5.879973481950306
Training loss: 5.061078071594238 / Valid loss: 6.158573893138341
Training loss: 5.400570392608643 / Valid loss: 6.0460967994871595
Training loss: 3.6927876472473145 / Valid loss: 5.9460766270047145
Training loss: 5.562653541564941 / Valid loss: 6.952265875680106

Epoch: 15
Training loss: 5.764951705932617 / Valid loss: 5.872303647086734
Training loss: 5.777360916137695 / Valid loss: 5.787015031632923
Training loss: 4.667038440704346 / Valid loss: 5.848612882977441
Training loss: 5.417726993560791 / Valid loss: 5.87546596754165
Training loss: 4.996523857116699 / Valid loss: 5.940417855126517

Epoch: 16
Training loss: 4.851752281188965 / Valid loss: 5.864849126906622
Training loss: 4.116241931915283 / Valid loss: 7.265185574122838
Training loss: 4.945643424987793 / Valid loss: 5.8271178654261995
Training loss: 5.652191162109375 / Valid loss: 5.951425911131359
Training loss: 4.79814338684082 / Valid loss: 5.973514690853301

Epoch: 17
Training loss: 6.198210716247559 / Valid loss: 5.98662496294294
Training loss: 4.798887252807617 / Valid loss: 5.768560536702474
Training loss: 4.883077621459961 / Valid loss: 5.907729943593343
Training loss: 4.899740219116211 / Valid loss: 5.788184592837379
Training loss: 6.758345127105713 / Valid loss: 5.877930048533848

Epoch: 18
Training loss: 5.285200119018555 / Valid loss: 5.824147767112368
Training loss: 4.055123329162598 / Valid loss: 5.952373702185494
Training loss: 5.305145263671875 / Valid loss: 5.770377863021124
Training loss: 4.127040386199951 / Valid loss: 5.882430246898106
Training loss: 5.8969526290893555 / Valid loss: 5.8303480670565655

Epoch: 19
Training loss: 3.942387342453003 / Valid loss: 5.9011060237884525
Training loss: 4.236052513122559 / Valid loss: 5.834990294774373
Training loss: 4.804897308349609 / Valid loss: 5.837910981405349
Training loss: 4.012368679046631 / Valid loss: 5.942165129525321

Epoch: 20
Training loss: 4.617002964019775 / Valid loss: 5.998691299983434
Training loss: 5.120438575744629 / Valid loss: 5.981664587202526
Training loss: 3.671104669570923 / Valid loss: 6.06591109321231
Training loss: 3.308212995529175 / Valid loss: 5.818291257676624
Training loss: 4.81234073638916 / Valid loss: 5.8642415523529055

Epoch: 21
Training loss: 5.0070953369140625 / Valid loss: 6.014344013304937
Training loss: 6.309192657470703 / Valid loss: 6.0186362493605845
Training loss: 5.51052188873291 / Valid loss: 5.823638141722906
Training loss: 8.042821884155273 / Valid loss: 6.294996584029425
Training loss: 6.2492356300354 / Valid loss: 6.05375330334618

Epoch: 22
Training loss: 5.210292339324951 / Valid loss: 5.811033775692894
Training loss: 4.102896690368652 / Valid loss: 5.841734284446353
Training loss: 6.051554203033447 / Valid loss: 6.460799562363397
Training loss: 6.0299859046936035 / Valid loss: 5.931363214765276
Training loss: 4.529183387756348 / Valid loss: 5.9970928555443175

Epoch: 23
Training loss: 5.61714506149292 / Valid loss: 6.019429819924491
Training loss: 6.1220197677612305 / Valid loss: 6.013706770397368
Training loss: 5.066226959228516 / Valid loss: 5.864154529571533
Training loss: 4.791457176208496 / Valid loss: 5.882568479719616
Training loss: 6.942785739898682 / Valid loss: 6.186320899781727

Epoch: 24
Training loss: 6.200743675231934 / Valid loss: 6.409553293954758
Training loss: 5.291129112243652 / Valid loss: 5.914675244830904
Training loss: 5.3759026527404785 / Valid loss: 5.842975750423613
Training loss: 6.812552452087402 / Valid loss: 5.8731634821210585
Training loss: 3.611746311187744 / Valid loss: 6.301804172425043

Epoch: 25
Training loss: 3.581059694290161 / Valid loss: 5.906401382173811
Training loss: 3.0479977130889893 / Valid loss: 5.93978153410412
Training loss: 3.291269540786743 / Valid loss: 5.941342040470668
Training loss: 6.784536361694336 / Valid loss: 5.8635130019415
Training loss: 3.76632022857666 / Valid loss: 6.033148520333427

Epoch: 26
Training loss: 5.237804889678955 / Valid loss: 6.097309634799049
Training loss: 5.990858554840088 / Valid loss: 5.9426726114182244
Training loss: 6.037245273590088 / Valid loss: 5.933407556442988
Training loss: 5.629202842712402 / Valid loss: 5.982494379225232
Training loss: 5.496949195861816 / Valid loss: 5.974891828355335

Epoch: 27
Training loss: 8.645611763000488 / Valid loss: 6.147190300623576
Training loss: 4.596233367919922 / Valid loss: 5.990616877873738
Training loss: 5.177696704864502 / Valid loss: 5.926581739244007
Training loss: 6.417768478393555 / Valid loss: 5.94234904561724
Training loss: 5.8664116859436035 / Valid loss: 6.0675470238640195

Epoch: 28
Training loss: 4.160120964050293 / Valid loss: 5.871920701435634
Training loss: 3.899186372756958 / Valid loss: 6.179511347271148
Training loss: 5.370606422424316 / Valid loss: 6.546277727399554
Training loss: 4.721887588500977 / Valid loss: 5.936180541628883
Training loss: 5.337785720825195 / Valid loss: 5.834978471483503

Epoch: 29
Training loss: 6.1667585372924805 / Valid loss: 6.1867830957685195
Training loss: 5.113041877746582 / Valid loss: 5.935310899643671
Training loss: 6.116176605224609 / Valid loss: 5.944210020701091
Training loss: 6.307999610900879 / Valid loss: 6.00272907983689

Epoch: 30
Training loss: 6.525237083435059 / Valid loss: 6.533971250624884
Training loss: 5.228156089782715 / Valid loss: 5.983588404882521
Training loss: 4.700456142425537 / Valid loss: 6.245771353585379
Training loss: 5.0748395919799805 / Valid loss: 5.9786736806233725
Training loss: 4.536952972412109 / Valid loss: 5.904327051980155

Epoch: 31
Training loss: 5.218457221984863 / Valid loss: 5.924519961220877
Training loss: 5.628563404083252 / Valid loss: 6.12874634152367
Training loss: 4.6537604331970215 / Valid loss: 5.936181747345698
Training loss: 5.031530380249023 / Valid loss: 5.951277001698812
Training loss: 6.155279636383057 / Valid loss: 5.9404415448506676

Epoch: 32
Training loss: 5.7674384117126465 / Valid loss: 5.984739482970465
Training loss: 5.677141189575195 / Valid loss: 5.929248657680693
Training loss: 5.271806716918945 / Valid loss: 6.13948142187936
Training loss: 4.207383155822754 / Valid loss: 6.2536865279788065
Training loss: 7.375938892364502 / Valid loss: 5.9504937671479725

Epoch: 33
Training loss: 4.052079200744629 / Valid loss: 5.894197475342524
Training loss: 5.699511528015137 / Valid loss: 5.922623974936349
Training loss: 6.435791492462158 / Valid loss: 6.042905587241763
Training loss: 5.091592788696289 / Valid loss: 6.09133951323373
Training loss: 6.601558685302734 / Valid loss: 6.004621710096087

Epoch: 34
Training loss: 4.540563583374023 / Valid loss: 6.007995398839315
Training loss: 6.6331915855407715 / Valid loss: 6.009093913577852
Training loss: 5.542844772338867 / Valid loss: 6.43756688890003
Training loss: 5.831785678863525 / Valid loss: 6.781958443777902
Training loss: 3.873655319213867 / Valid loss: 6.176143796103341

Epoch: 35
Training loss: 5.702544212341309 / Valid loss: 6.479782495044526
Training loss: 4.708370685577393 / Valid loss: 5.925800209953671
Training loss: 5.703458786010742 / Valid loss: 5.906815256391253
Training loss: 4.780970096588135 / Valid loss: 6.026541782560803
Training loss: 4.953135013580322 / Valid loss: 6.11766867410569

Epoch: 36
Training loss: 3.8349859714508057 / Valid loss: 5.976738471076602
Training loss: 5.54894495010376 / Valid loss: 6.632401346025013
Training loss: 4.035051345825195 / Valid loss: 6.028754436402094
Training loss: 5.862283706665039 / Valid loss: 5.956950648625692
Training loss: 4.006731986999512 / Valid loss: 6.0086198352632065

Epoch: 37
Training loss: 5.232735633850098 / Valid loss: 5.951650887443906
Training loss: 4.093378067016602 / Valid loss: 6.001308584213257
Training loss: 4.18829870223999 / Valid loss: 5.990319429125105
Training loss: 6.093557357788086 / Valid loss: 6.0585090410141715
Training loss: 5.811321258544922 / Valid loss: 5.9185517901466005

Epoch: 38
Training loss: 5.709674835205078 / Valid loss: 6.16407946632022
Training loss: 6.062976837158203 / Valid loss: 6.134016727265857
Training loss: 5.398240089416504 / Valid loss: 6.109620602925618
Training loss: 4.647353649139404 / Valid loss: 5.960933769316901
Training loss: 4.389507293701172 / Valid loss: 5.929902898697626

Epoch: 39
Training loss: 4.218158721923828 / Valid loss: 5.874821124758039
Training loss: 5.0196380615234375 / Valid loss: 6.162768229984102
Training loss: 5.976421356201172 / Valid loss: 5.974669558661325
Training loss: 5.157122611999512 / Valid loss: 6.173231960478283
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 300): 5.450230148860387
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)

Epoch: 0
Training loss: 19.044994354248047 / Valid loss: 8.092048996970767
Model is saved in epoch 0, overall batch: 0
Training loss: 6.148061752319336 / Valid loss: 5.701571455455961
Model is saved in epoch 0, overall batch: 100
Training loss: 6.1706132888793945 / Valid loss: 5.70685464314052
Training loss: 5.87912130355835 / Valid loss: 5.595106065840948
Model is saved in epoch 0, overall batch: 300
Training loss: 3.643350601196289 / Valid loss: 5.651296785899571

Epoch: 1
Training loss: 6.7166547775268555 / Valid loss: 5.643028845105852
Training loss: 4.631204128265381 / Valid loss: 5.664373247964042
Training loss: 7.421722888946533 / Valid loss: 5.652497872852144
Training loss: 4.275518417358398 / Valid loss: 6.113036643891108
Training loss: 4.852072715759277 / Valid loss: 5.763110903331212

Epoch: 2
Training loss: 5.925057411193848 / Valid loss: 5.7496756735302155
Training loss: 4.920892238616943 / Valid loss: 5.655367801302956
Training loss: 5.841883659362793 / Valid loss: 5.649614742824009
Training loss: 3.7308363914489746 / Valid loss: 5.665138013022286
Training loss: 4.492891311645508 / Valid loss: 5.711182489849272

Epoch: 3
Training loss: 5.076812744140625 / Valid loss: 5.838825648171561
Training loss: 3.9141173362731934 / Valid loss: 5.850188564118885
Training loss: 7.6696648597717285 / Valid loss: 5.634025049209595
Training loss: 3.9252240657806396 / Valid loss: 5.771661349705288
Training loss: 5.70654296875 / Valid loss: 5.748419598170689

Epoch: 4
Training loss: 5.833906650543213 / Valid loss: 5.771432029633295
Training loss: 4.679795742034912 / Valid loss: 5.849931328637259
Training loss: 4.836704730987549 / Valid loss: 5.799205341793242
Training loss: 4.255781173706055 / Valid loss: 5.7150318191165015
Training loss: 5.094261646270752 / Valid loss: 5.631509935288202

Epoch: 5
Training loss: 6.933135032653809 / Valid loss: 5.809182033084688
Training loss: 5.093123912811279 / Valid loss: 5.708014115833101
Training loss: 5.043453216552734 / Valid loss: 6.58779909043085
Training loss: 5.392147064208984 / Valid loss: 5.744976300284976
Training loss: 6.844432830810547 / Valid loss: 6.254295421781994

Epoch: 6
Training loss: 4.845019817352295 / Valid loss: 5.767653737749372
Training loss: 5.218316078186035 / Valid loss: 5.9757232597896035
Training loss: 5.098300933837891 / Valid loss: 5.766911306835357
Training loss: 4.618960857391357 / Valid loss: 5.7379547868456156
Training loss: 6.732769966125488 / Valid loss: 5.82357169787089

Epoch: 7
Training loss: 4.464120864868164 / Valid loss: 5.711220868428549
Training loss: 3.9775583744049072 / Valid loss: 5.802703351066226
Training loss: 6.677218437194824 / Valid loss: 5.787237866719564
Training loss: 5.358428955078125 / Valid loss: 5.68865465436663
Training loss: 5.044074058532715 / Valid loss: 5.734897034508841

Epoch: 8
Training loss: 5.617445945739746 / Valid loss: 6.440758827754429
Training loss: 4.527398109436035 / Valid loss: 5.818179943448022
Training loss: 4.71016788482666 / Valid loss: 5.68089899562654
Training loss: 4.812495708465576 / Valid loss: 5.796810556593395
Training loss: 2.8229095935821533 / Valid loss: 5.864993933268956

Epoch: 9
Training loss: 3.912245512008667 / Valid loss: 5.704258728027344
Training loss: 5.340814113616943 / Valid loss: 5.724991535005115
Training loss: 7.44093656539917 / Valid loss: 5.88168527966454
Training loss: 6.300017833709717 / Valid loss: 6.293190536044893

Epoch: 10
Training loss: 3.905348777770996 / Valid loss: 5.753903618313017
Training loss: 4.6441192626953125 / Valid loss: 5.802555395307995
Training loss: 5.324554920196533 / Valid loss: 6.320380447024391
Training loss: 5.224370002746582 / Valid loss: 5.755717688515073
Training loss: 5.087730407714844 / Valid loss: 6.023263795035226

Epoch: 11
Training loss: 5.310761451721191 / Valid loss: 5.790530586242676
Training loss: 5.44271183013916 / Valid loss: 5.861486950374785
Training loss: 4.663767337799072 / Valid loss: 6.007423067092896
Training loss: 8.038583755493164 / Valid loss: 5.749135096867879
Training loss: 4.973097801208496 / Valid loss: 5.8686688513982865

Epoch: 12
Training loss: 5.370965957641602 / Valid loss: 5.7683087235405335
Training loss: 6.095085144042969 / Valid loss: 5.800765216918219
Training loss: 3.706490993499756 / Valid loss: 6.378288580122448
Training loss: 4.618486404418945 / Valid loss: 5.765606040046329
Training loss: 4.034348487854004 / Valid loss: 5.813489839008876

Epoch: 13
Training loss: 4.6063032150268555 / Valid loss: 5.772770929336548
Training loss: 4.794281005859375 / Valid loss: 6.334100028446742
Training loss: 5.511263847351074 / Valid loss: 5.865591426122756
Training loss: 6.008357048034668 / Valid loss: 5.846014174960908
Training loss: 5.050471305847168 / Valid loss: 5.738782676060994

Epoch: 14
Training loss: 5.6223297119140625 / Valid loss: 5.879939728691465
Training loss: 5.061093330383301 / Valid loss: 6.15853017171224
Training loss: 5.4006428718566895 / Valid loss: 6.046077437627883
Training loss: 3.6927804946899414 / Valid loss: 5.946038339251563
Training loss: 5.562758445739746 / Valid loss: 6.95223974046253

Epoch: 15
Training loss: 5.764974594116211 / Valid loss: 5.872267736707415
Training loss: 5.777369499206543 / Valid loss: 5.786976414635068
Training loss: 4.667037010192871 / Valid loss: 5.848574020749047
Training loss: 5.417791843414307 / Valid loss: 5.87542778196789
Training loss: 4.996553421020508 / Valid loss: 5.940374260856991

Epoch: 16
Training loss: 4.851751327514648 / Valid loss: 5.864813493546985
Training loss: 4.1162567138671875 / Valid loss: 7.2651382809593565
Training loss: 4.94569206237793 / Valid loss: 5.827079575402396
Training loss: 5.652225971221924 / Valid loss: 5.951375164304461
Training loss: 4.798192977905273 / Valid loss: 5.973470912660871

Epoch: 17
Training loss: 6.198215484619141 / Valid loss: 5.986585294632684
Training loss: 4.798896312713623 / Valid loss: 5.768515157699585
Training loss: 4.883143424987793 / Valid loss: 5.907680191312518
Training loss: 4.8997650146484375 / Valid loss: 5.788139393216087
Training loss: 6.7584333419799805 / Valid loss: 5.877890048708235

Epoch: 18
Training loss: 5.285281181335449 / Valid loss: 5.824099349975586
Training loss: 4.055199146270752 / Valid loss: 5.952324503944034
Training loss: 5.30515193939209 / Valid loss: 5.7703290485200425
Training loss: 4.127036094665527 / Valid loss: 5.882380901064192
Training loss: 5.896989822387695 / Valid loss: 5.830301877430507

Epoch: 19
Training loss: 3.942375898361206 / Valid loss: 5.9010542892274405
Training loss: 4.236111164093018 / Valid loss: 5.834933503468831
Training loss: 4.804897308349609 / Valid loss: 5.837860402606783
Training loss: 4.012406349182129 / Valid loss: 5.942108992167881

Epoch: 20
Training loss: 4.617053985595703 / Valid loss: 5.998639199847267
Training loss: 5.12043571472168 / Valid loss: 5.98160317057655
Training loss: 3.671067953109741 / Valid loss: 6.06584282148452
Training loss: 3.308222770690918 / Valid loss: 5.818230792454311
Training loss: 4.812386989593506 / Valid loss: 5.864184688386463

Epoch: 21
Training loss: 5.007099628448486 / Valid loss: 6.014283682051159
Training loss: 6.309196472167969 / Valid loss: 6.018612530117943
Training loss: 5.510650634765625 / Valid loss: 5.823582453954788
Training loss: 8.042783737182617 / Valid loss: 6.294922356378464
Training loss: 6.249358177185059 / Valid loss: 6.053682036626906

Epoch: 22
Training loss: 5.21036434173584 / Valid loss: 5.810968637466431
Training loss: 4.1029534339904785 / Valid loss: 5.841666530427479
Training loss: 6.0515923500061035 / Valid loss: 6.460750218800136
Training loss: 6.029973983764648 / Valid loss: 5.931290086110433
Training loss: 4.529219150543213 / Valid loss: 5.99703536487761

Epoch: 23
Training loss: 5.617152690887451 / Valid loss: 6.0193574996221635
Training loss: 6.122011184692383 / Valid loss: 6.013637908299764
Training loss: 5.066247940063477 / Valid loss: 5.86408113979158
Training loss: 4.7915496826171875 / Valid loss: 5.882502587636312
Training loss: 6.942805767059326 / Valid loss: 6.186270913623628

Epoch: 24
Training loss: 6.200875282287598 / Valid loss: 6.409459016436622
Training loss: 5.291121482849121 / Valid loss: 5.9146006992885045
Training loss: 5.375941276550293 / Valid loss: 5.842899690355574
Training loss: 6.812641143798828 / Valid loss: 5.873089665458316
Training loss: 3.611690044403076 / Valid loss: 6.301737989698138

Epoch: 25
Training loss: 3.5810961723327637 / Valid loss: 5.906321325756255
Training loss: 3.047999620437622 / Valid loss: 5.939706577573504
Training loss: 3.291318893432617 / Valid loss: 5.9412754535675045
Training loss: 6.7845587730407715 / Valid loss: 5.863437559491112
Training loss: 3.766310453414917 / Valid loss: 6.033047140212286

Epoch: 26
Training loss: 5.237835884094238 / Valid loss: 6.097219862256732
Training loss: 5.9908881187438965 / Valid loss: 5.942588374728248
Training loss: 6.037361145019531 / Valid loss: 5.933322858810425
Training loss: 5.629245758056641 / Valid loss: 5.982408800579253
Training loss: 5.496984958648682 / Valid loss: 5.974822357722691

Epoch: 27
Training loss: 8.645606994628906 / Valid loss: 6.14710039865403
Training loss: 4.59624719619751 / Valid loss: 5.990550325030372
Training loss: 5.177711486816406 / Valid loss: 5.926485963094802
Training loss: 6.417823314666748 / Valid loss: 5.942254209518433
Training loss: 5.866349220275879 / Valid loss: 6.067442889440628

Epoch: 28
Training loss: 4.16013240814209 / Valid loss: 5.871837609154838
Training loss: 3.8991732597351074 / Valid loss: 6.179436002458845
Training loss: 5.37060546875 / Valid loss: 6.546216145015898
Training loss: 4.721899032592773 / Valid loss: 5.936086597896757
Training loss: 5.337717056274414 / Valid loss: 5.834883755729312

Epoch: 29
Training loss: 6.166752338409424 / Valid loss: 6.186717178708031
Training loss: 5.113008975982666 / Valid loss: 5.9352148873465405
Training loss: 6.116181373596191 / Valid loss: 5.94411198752267
Training loss: 6.308036804199219 / Valid loss: 6.002622377304804

Epoch: 30
Training loss: 6.525312423706055 / Valid loss: 6.533902499789283
Training loss: 5.2281494140625 / Valid loss: 5.9834862822578065
Training loss: 4.700493812561035 / Valid loss: 6.245675388971964
Training loss: 5.074843406677246 / Valid loss: 5.9785763422648115
Training loss: 4.537013053894043 / Valid loss: 5.904224164145333

Epoch: 31
Training loss: 5.2184062004089355 / Valid loss: 5.924420515696208
Training loss: 5.628669738769531 / Valid loss: 6.128643181210473
Training loss: 4.653688430786133 / Valid loss: 5.9360691297621955
Training loss: 5.031479835510254 / Valid loss: 5.9511704240526475
Training loss: 6.15537166595459 / Valid loss: 5.9403304100036625

Epoch: 32
Training loss: 5.767467498779297 / Valid loss: 5.984634467533657
Training loss: 5.677275657653809 / Valid loss: 5.929138528733026
Training loss: 5.271780014038086 / Valid loss: 6.139386799221947
Training loss: 4.2074480056762695 / Valid loss: 6.253551950908842
Training loss: 7.375973224639893 / Valid loss: 5.950381397065662

Epoch: 33
Training loss: 4.052099704742432 / Valid loss: 5.894093524842035
Training loss: 5.699629783630371 / Valid loss: 5.922513394128709
Training loss: 6.435730934143066 / Valid loss: 6.042777561006092
Training loss: 5.091728687286377 / Valid loss: 6.091225446973528
Training loss: 6.6016845703125 / Valid loss: 6.004512630190168

Epoch: 34
Training loss: 4.54062557220459 / Valid loss: 6.007874123255411
Training loss: 6.633175373077393 / Valid loss: 6.008966143925985
Training loss: 5.542892932891846 / Valid loss: 6.437476335253034
Training loss: 5.831847190856934 / Valid loss: 6.781847442899432
Training loss: 3.8736512660980225 / Valid loss: 6.17604726155599

Epoch: 35
Training loss: 5.702639102935791 / Valid loss: 6.47963532493228
Training loss: 4.708464622497559 / Valid loss: 5.925679016113281
Training loss: 5.703561782836914 / Valid loss: 5.906684616633824
Training loss: 4.781096935272217 / Valid loss: 6.02641502334958
Training loss: 4.9531450271606445 / Valid loss: 6.11757006872268

Epoch: 36
Training loss: 3.834955930709839 / Valid loss: 5.976610167821248
Training loss: 5.549046039581299 / Valid loss: 6.63231337411063
Training loss: 4.034969806671143 / Valid loss: 6.028646691640218
Training loss: 5.862345218658447 / Valid loss: 5.956819266364688
Training loss: 4.006711006164551 / Valid loss: 6.0084888503665015

Epoch: 37
Training loss: 5.232783317565918 / Valid loss: 5.951513138271514
Training loss: 4.093353748321533 / Valid loss: 6.001172249657767
Training loss: 4.188388824462891 / Valid loss: 5.99018053327288
Training loss: 6.093584060668945 / Valid loss: 6.058385438010806
Training loss: 5.811306953430176 / Valid loss: 5.918415496462868

Epoch: 38
Training loss: 5.709724426269531 / Valid loss: 6.16394723937625
Training loss: 6.062953472137451 / Valid loss: 6.13388090814863
Training loss: 5.3983869552612305 / Valid loss: 6.109476741154989
Training loss: 4.647428035736084 / Valid loss: 5.960790917986915
Training loss: 4.3894500732421875 / Valid loss: 5.929756516502017

Epoch: 39
Training loss: 4.218161582946777 / Valid loss: 5.874678636732556
Training loss: 5.019801139831543 / Valid loss: 6.16262541725522
Training loss: 5.976459980010986 / Valid loss: 5.9745314234779
Training loss: 5.157322406768799 / Valid loss: 6.173145439511254
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 300): 5.45022995585487
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.114407076154436
Model is saved in epoch 0, overall batch: 0
Training loss: 8.912009239196777 / Valid loss: 12.768657457260858
Model is saved in epoch 0, overall batch: 100
Training loss: 9.925810813903809 / Valid loss: 12.196747702643984
Model is saved in epoch 0, overall batch: 200
Training loss: 15.783401489257812 / Valid loss: 11.78140204747518
Model is saved in epoch 0, overall batch: 300
Training loss: 11.472275733947754 / Valid loss: 11.121726526532854
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 9.930368423461914 / Valid loss: 10.86008365267799
Model is saved in epoch 1, overall batch: 500
Training loss: 9.093926429748535 / Valid loss: 10.686255200703938
Model is saved in epoch 1, overall batch: 600
Training loss: 8.314817428588867 / Valid loss: 10.258468723297119
Model is saved in epoch 1, overall batch: 700
Training loss: 8.375079154968262 / Valid loss: 9.384540948413667
Model is saved in epoch 1, overall batch: 800
Training loss: 6.540190696716309 / Valid loss: 9.523321387881325

Epoch: 2
Training loss: 9.234378814697266 / Valid loss: 8.769821966262091
Model is saved in epoch 2, overall batch: 1000
Training loss: 9.167750358581543 / Valid loss: 8.570415982745942
Model is saved in epoch 2, overall batch: 1100
Training loss: 8.85214614868164 / Valid loss: 8.411143775213333
Model is saved in epoch 2, overall batch: 1200
Training loss: 9.604043006896973 / Valid loss: 7.723625689461118
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.110571384429932 / Valid loss: 7.415789735884894
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.846023082733154 / Valid loss: 7.009148602258591
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.8052730560302734 / Valid loss: 6.786819939386277
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.784642219543457 / Valid loss: 6.671425762630644
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.459716796875 / Valid loss: 6.495018087114606
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.784595489501953 / Valid loss: 6.455052934374128
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 5.4322004318237305 / Valid loss: 7.28327697572254
Training loss: 3.8479514122009277 / Valid loss: 6.66725393931071
Training loss: 4.175478458404541 / Valid loss: 6.828827540079753
Training loss: 4.428637981414795 / Valid loss: 6.515154436656407
Training loss: 3.4555511474609375 / Valid loss: 6.198425113587152
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 5.734418869018555 / Valid loss: 6.558657866432553
Training loss: 3.515838146209717 / Valid loss: 6.385780620574951
Training loss: 2.704442024230957 / Valid loss: 6.671769423711868
Training loss: 3.083517074584961 / Valid loss: 6.238934716724214
Training loss: 3.3447868824005127 / Valid loss: 6.271911857241676

Epoch: 6
Training loss: 5.0640363693237305 / Valid loss: 7.03444573538644
Training loss: 2.181051254272461 / Valid loss: 6.528518865222023
Training loss: 3.046281337738037 / Valid loss: 7.094282913208008
Training loss: 3.0997061729431152 / Valid loss: 6.523180414381481
Training loss: 2.9891488552093506 / Valid loss: 6.588962502706618

Epoch: 7
Training loss: 2.6797165870666504 / Valid loss: 6.758096204485212
Training loss: 1.6568546295166016 / Valid loss: 6.918168528874715
Training loss: 2.4585399627685547 / Valid loss: 6.7552470752171105
Training loss: 2.754788398742676 / Valid loss: 6.856639943804059
Training loss: 2.473010540008545 / Valid loss: 6.809998221624465

Epoch: 8
Training loss: 0.8977401256561279 / Valid loss: 6.755013095764887
Training loss: 1.413731575012207 / Valid loss: 6.647775027865455
Training loss: 1.7390714883804321 / Valid loss: 6.902055279413859
Training loss: 2.391519546508789 / Valid loss: 6.718315172195434
Training loss: 2.360682487487793 / Valid loss: 6.881375512622651

Epoch: 9
Training loss: 1.6765280961990356 / Valid loss: 6.831627114613851
Training loss: 1.7128515243530273 / Valid loss: 6.944666762579055
Training loss: 2.782789707183838 / Valid loss: 8.034859157743908
Training loss: 2.0937094688415527 / Valid loss: 6.823229158492316

Epoch: 10
Training loss: 0.936830461025238 / Valid loss: 6.9010133402688165
Training loss: 1.2471961975097656 / Valid loss: 6.960337920415969
Training loss: 0.9596741199493408 / Valid loss: 7.017917206173851
Training loss: 1.8208965063095093 / Valid loss: 6.903125983192807
Training loss: 1.0655286312103271 / Valid loss: 7.356153034028553

Epoch: 11
Training loss: 0.6359700560569763 / Valid loss: 6.962087163471041
Training loss: 1.1494207382202148 / Valid loss: 6.8070917992364794
Training loss: 1.145169734954834 / Valid loss: 6.811951723552886
Training loss: 0.8636291027069092 / Valid loss: 7.136697780518305
Training loss: 1.28263521194458 / Valid loss: 7.200314989544096

Epoch: 12
Training loss: 0.808957576751709 / Valid loss: 6.973334121704101
Training loss: 0.8647042512893677 / Valid loss: 6.900782355808077
Training loss: 0.964614748954773 / Valid loss: 6.726683991295951
Training loss: 1.211578369140625 / Valid loss: 6.901320968355451
Training loss: 1.184460163116455 / Valid loss: 7.090978935786656

Epoch: 13
Training loss: 0.785484790802002 / Valid loss: 7.72143703188215
Training loss: 0.8137133717536926 / Valid loss: 6.860293036415463
Training loss: 0.9294409155845642 / Valid loss: 7.357797645387196
Training loss: 0.8816201090812683 / Valid loss: 6.893089180900937
Training loss: 0.7556352019309998 / Valid loss: 7.004174990881057

Epoch: 14
Training loss: 0.9726812243461609 / Valid loss: 7.121114428838094
Training loss: 0.6533987522125244 / Valid loss: 6.785783045632499
Training loss: 1.1122817993164062 / Valid loss: 6.872415744690668
Training loss: 0.6795768737792969 / Valid loss: 7.161128947848366
Training loss: 1.2054206132888794 / Valid loss: 7.020729609898159

Epoch: 15
Training loss: 0.6204168796539307 / Valid loss: 6.883711839857556
Training loss: 1.2871613502502441 / Valid loss: 7.012847995758056
Training loss: 0.8659645318984985 / Valid loss: 6.839447852543422
Training loss: 0.9139481782913208 / Valid loss: 6.939008117857433
Training loss: 0.8593806028366089 / Valid loss: 6.802086017245338

Epoch: 16
Training loss: 0.53940749168396 / Valid loss: 6.9785969370887395
Training loss: 0.5532370805740356 / Valid loss: 6.923516591389974
Training loss: 0.6132656335830688 / Valid loss: 6.90730920065017
Training loss: 0.6715787053108215 / Valid loss: 6.915744472685314
Training loss: 0.5064013004302979 / Valid loss: 7.258493564242408

Epoch: 17
Training loss: 0.8739537596702576 / Valid loss: 6.999483746574039
Training loss: 1.2047475576400757 / Valid loss: 6.833454740615118
Training loss: 0.814422070980072 / Valid loss: 7.017788610004243
Training loss: 0.5512487888336182 / Valid loss: 6.942955316816057
Training loss: 0.7930984497070312 / Valid loss: 6.872358394804455

Epoch: 18
Training loss: 0.6464129686355591 / Valid loss: 6.939476201647804
Training loss: 0.8608871698379517 / Valid loss: 7.0141416322617305
Training loss: 0.7545645236968994 / Valid loss: 7.13192974726359
Training loss: 0.7856454253196716 / Valid loss: 6.881633154551188
Training loss: 1.0869135856628418 / Valid loss: 7.21874339239938

Epoch: 19
Training loss: 1.059412956237793 / Valid loss: 7.130672954377674
Training loss: 1.1742831468582153 / Valid loss: 6.99260645820981
Training loss: 0.7287744879722595 / Valid loss: 6.95410057703654
Training loss: 0.663609504699707 / Valid loss: 6.842352708180745

Epoch: 20
Training loss: 0.5677882432937622 / Valid loss: 6.791662020910354
Training loss: 1.2225005626678467 / Valid loss: 6.889434378487723
Training loss: 0.47219428420066833 / Valid loss: 6.808534442810785
Training loss: 0.8761451840400696 / Valid loss: 6.8158988135201595
Training loss: 1.0279066562652588 / Valid loss: 6.7720060007912775

Epoch: 21
Training loss: 0.4475374221801758 / Valid loss: 6.937840098426456
Training loss: 0.5406501293182373 / Valid loss: 7.365072645459857
Training loss: 0.6003029346466064 / Valid loss: 7.08115739368257
Training loss: 0.7370328903198242 / Valid loss: 6.987155205862862
Training loss: 0.6210228204727173 / Valid loss: 6.9006127198537195

Epoch: 22
Training loss: 0.7910249829292297 / Valid loss: 6.7880048751831055
Training loss: 0.38785022497177124 / Valid loss: 6.869932310921805
Training loss: 0.6510220766067505 / Valid loss: 6.9390557062058225
Training loss: 1.272037386894226 / Valid loss: 6.995921611785889
Training loss: 0.9684562087059021 / Valid loss: 7.202456410725912

Epoch: 23
Training loss: 0.5385011434555054 / Valid loss: 6.851070585704985
Training loss: 1.321436882019043 / Valid loss: 6.890108635312035
Training loss: 0.9871132969856262 / Valid loss: 6.8747127146947955
Training loss: 0.6911038756370544 / Valid loss: 7.021406166894096
Training loss: 0.7842404842376709 / Valid loss: 6.97103283745902

Epoch: 24
Training loss: 0.8393596410751343 / Valid loss: 6.838232290177118
Training loss: 0.6872751712799072 / Valid loss: 6.7430394081842335
Training loss: 0.6175047159194946 / Valid loss: 6.862519613901774
Training loss: 0.6735203266143799 / Valid loss: 6.916020125434512
Training loss: 0.992667555809021 / Valid loss: 7.099601443608602

Epoch: 25
Training loss: 0.6753808856010437 / Valid loss: 6.762231259118943
Training loss: 0.43187612295150757 / Valid loss: 6.86820308140346
Training loss: 1.1698179244995117 / Valid loss: 6.860477633703322
Training loss: 0.4223865270614624 / Valid loss: 6.918870467231387
Training loss: 0.669593095779419 / Valid loss: 7.029194404965355

Epoch: 26
Training loss: 0.9046597480773926 / Valid loss: 6.797090825580415
Training loss: 1.0334348678588867 / Valid loss: 6.761004184541248
Training loss: 0.34703996777534485 / Valid loss: 6.90616911479405
Training loss: 0.3122824430465698 / Valid loss: 6.86344967796689
Training loss: 0.48323753476142883 / Valid loss: 7.03172699383327

Epoch: 27
Training loss: 0.36437496542930603 / Valid loss: 6.933468312308902
Training loss: 0.6029342412948608 / Valid loss: 6.901174036661784
Training loss: 0.614423394203186 / Valid loss: 6.947794346582322
Training loss: 0.4617449939250946 / Valid loss: 6.9538627465566
Training loss: 0.6803674697875977 / Valid loss: 7.0249468712579635

Epoch: 28
Training loss: 0.387935996055603 / Valid loss: 6.901970599946521
Training loss: 0.6165776252746582 / Valid loss: 6.909543144135248
Training loss: 0.5400832891464233 / Valid loss: 6.861772092183431
Training loss: 0.3511140048503876 / Valid loss: 7.040414101736886
Training loss: 0.539892315864563 / Valid loss: 6.983102199009487

Epoch: 29
Training loss: 0.5058392882347107 / Valid loss: 6.861928603762672
Training loss: 0.7486938238143921 / Valid loss: 7.155505398341588
Training loss: 0.3672358989715576 / Valid loss: 6.811366380964007
Training loss: 0.3118017911911011 / Valid loss: 6.879478425071353

Epoch: 30
Training loss: 0.6928936243057251 / Valid loss: 7.0182854947589695
Training loss: 0.44543325901031494 / Valid loss: 6.757925156184605
Training loss: 0.3234328627586365 / Valid loss: 7.008895224616641
Training loss: 0.45212942361831665 / Valid loss: 6.878146564392816
Training loss: 0.3734869956970215 / Valid loss: 6.909860987890334

Epoch: 31
Training loss: 0.35912203788757324 / Valid loss: 6.911509184610276
Training loss: 0.43792688846588135 / Valid loss: 7.023451936812628
Training loss: 0.5828892588615417 / Valid loss: 6.85651330947876
Training loss: 0.4035741984844208 / Valid loss: 7.197467427026658
Training loss: 0.5325425267219543 / Valid loss: 6.896336519150507

Epoch: 32
Training loss: 0.46798238158226013 / Valid loss: 6.920280415671212
Training loss: 0.5129421949386597 / Valid loss: 6.855489594595773
Training loss: 0.3181908130645752 / Valid loss: 6.914389796484084
Training loss: 0.29038310050964355 / Valid loss: 6.8406507128760925
Training loss: 0.5647710561752319 / Valid loss: 6.835189873831613

Epoch: 33
Training loss: 0.4031444489955902 / Valid loss: 6.939921156565348
Training loss: 0.49439507722854614 / Valid loss: 6.8127959501175654
Training loss: 0.36235883831977844 / Valid loss: 6.861376587549845
Training loss: 0.572578489780426 / Valid loss: 6.858007941927228
Training loss: 0.45658621191978455 / Valid loss: 6.850246170588902

Epoch: 34
Training loss: 0.6909438371658325 / Valid loss: 6.767181632632301
Training loss: 0.3100060224533081 / Valid loss: 6.902636882237026
Training loss: 0.48002326488494873 / Valid loss: 6.956700111570813
Training loss: 0.33002769947052 / Valid loss: 6.885793299902053
Training loss: 0.4485366642475128 / Valid loss: 7.008176167805989

Epoch: 35
Training loss: 0.9865694046020508 / Valid loss: 6.830015218825567
Training loss: 0.5776498913764954 / Valid loss: 6.897862325395857
Training loss: 0.5171666741371155 / Valid loss: 6.8616849853878925
Training loss: 0.3185175657272339 / Valid loss: 6.861160954974946
Training loss: 0.4183725118637085 / Valid loss: 6.805342181523641

Epoch: 36
Training loss: 0.4354248046875 / Valid loss: 6.7071679569426035
Training loss: 0.6866554021835327 / Valid loss: 6.8485821383340015
Training loss: 0.4942253530025482 / Valid loss: 6.748570701054164
Training loss: 0.30083948373794556 / Valid loss: 6.890221507208688
Training loss: 0.4545312523841858 / Valid loss: 6.851118828001477

Epoch: 37
Training loss: 0.5846376419067383 / Valid loss: 7.004612102962676
Training loss: 0.25425416231155396 / Valid loss: 6.803554030827113
Training loss: 0.46477121114730835 / Valid loss: 6.90566330864316
Training loss: 0.9308645725250244 / Valid loss: 6.774234317597889
Training loss: 0.6394990086555481 / Valid loss: 6.859247870672316

Epoch: 38
Training loss: 0.5229790806770325 / Valid loss: 6.859157198951358
Training loss: 0.4742722511291504 / Valid loss: 6.826846862974621
Training loss: 0.46572309732437134 / Valid loss: 6.895450078873408
Training loss: 0.3612024188041687 / Valid loss: 6.895119044894264
Training loss: 0.7249857187271118 / Valid loss: 6.882613790602911

Epoch: 39
Training loss: 0.46001923084259033 / Valid loss: 6.975711663564046
Training loss: 0.3459772765636444 / Valid loss: 6.810472106933593
Training loss: 0.4002076983451843 / Valid loss: 6.844748805818104
Training loss: 0.3169283866882324 / Valid loss: 6.756377181552705
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2400): 5.957203556242443
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.114483351934524
Model is saved in epoch 0, overall batch: 0
Training loss: 8.908889770507812 / Valid loss: 12.75966839563279
Model is saved in epoch 0, overall batch: 100
Training loss: 9.889101028442383 / Valid loss: 12.27577341170538
Model is saved in epoch 0, overall batch: 200
Training loss: 15.867733001708984 / Valid loss: 11.833713077363514
Model is saved in epoch 0, overall batch: 300
Training loss: 11.482006072998047 / Valid loss: 11.129673948742095
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 9.908028602600098 / Valid loss: 10.864711243765695
Model is saved in epoch 1, overall batch: 500
Training loss: 9.10493278503418 / Valid loss: 10.721832620529902
Model is saved in epoch 1, overall batch: 600
Training loss: 8.291854858398438 / Valid loss: 10.240278339385986
Model is saved in epoch 1, overall batch: 700
Training loss: 8.418543815612793 / Valid loss: 9.460223683856782
Model is saved in epoch 1, overall batch: 800
Training loss: 6.584105968475342 / Valid loss: 9.463806034269787

Epoch: 2
Training loss: 9.232141494750977 / Valid loss: 8.887255128224691
Model is saved in epoch 2, overall batch: 1000
Training loss: 9.202386856079102 / Valid loss: 8.807914152599515
Model is saved in epoch 2, overall batch: 1100
Training loss: 8.94570255279541 / Valid loss: 8.516312540145147
Model is saved in epoch 2, overall batch: 1200
Training loss: 9.624932289123535 / Valid loss: 7.791880071730841
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.171656608581543 / Valid loss: 7.37377689906529
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.753270149230957 / Valid loss: 7.01623987924485
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.822371482849121 / Valid loss: 6.859117675962902
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.744904041290283 / Valid loss: 6.813324549084618
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.5373411178588867 / Valid loss: 6.454868902478899
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.554923057556152 / Valid loss: 6.453168446677072
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 5.552934646606445 / Valid loss: 7.168850773856754
Training loss: 3.563714027404785 / Valid loss: 6.578398770377749
Training loss: 4.282883644104004 / Valid loss: 6.899021125975109
Training loss: 4.31737756729126 / Valid loss: 6.675585878463019
Training loss: 3.6173574924468994 / Valid loss: 6.104799502236503
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 5.654750823974609 / Valid loss: 6.757923923219953
Training loss: 3.8943748474121094 / Valid loss: 6.633499467940558
Training loss: 2.954258441925049 / Valid loss: 6.847631672450475
Training loss: 2.9388036727905273 / Valid loss: 6.3367920466831755
Training loss: 3.625385046005249 / Valid loss: 6.1480407124473935

Epoch: 6
Training loss: 5.105884552001953 / Valid loss: 7.018180551983061
Training loss: 1.957838535308838 / Valid loss: 6.557476041430519
Training loss: 3.2071659564971924 / Valid loss: 6.661296680995396
Training loss: 3.007631301879883 / Valid loss: 6.5023801712762745
Training loss: 2.6239686012268066 / Valid loss: 6.858098929268973

Epoch: 7
Training loss: 2.9081506729125977 / Valid loss: 6.636972740718297
Training loss: 1.6946567296981812 / Valid loss: 6.724763048262823
Training loss: 2.338574171066284 / Valid loss: 6.842090088980538
Training loss: 2.7283437252044678 / Valid loss: 6.842021456218901
Training loss: 1.9735872745513916 / Valid loss: 6.705236067090715

Epoch: 8
Training loss: 0.9705690145492554 / Valid loss: 6.849280732018607
Training loss: 1.2664077281951904 / Valid loss: 6.693085048312232
Training loss: 1.6409034729003906 / Valid loss: 6.891775176638649
Training loss: 2.3183228969573975 / Valid loss: 6.745391625449771
Training loss: 2.3436686992645264 / Valid loss: 6.809624231429327

Epoch: 9
Training loss: 1.6243984699249268 / Valid loss: 6.6623578525724865
Training loss: 2.0022294521331787 / Valid loss: 7.145125139327276
Training loss: 2.5870351791381836 / Valid loss: 6.831188465300061
Training loss: 1.7939071655273438 / Valid loss: 6.828704693203881

Epoch: 10
Training loss: 1.1219125986099243 / Valid loss: 6.950508560453143
Training loss: 1.2007560729980469 / Valid loss: 6.838703882126581
Training loss: 0.9611144661903381 / Valid loss: 6.8122397195725215
Training loss: 1.7606794834136963 / Valid loss: 6.899686717987061
Training loss: 0.9605780243873596 / Valid loss: 6.979318278176444

Epoch: 11
Training loss: 0.6914025545120239 / Valid loss: 6.959857590993246
Training loss: 1.1822948455810547 / Valid loss: 7.2500936190287275
Training loss: 1.1576985120773315 / Valid loss: 7.062100751059396
Training loss: 0.9904208183288574 / Valid loss: 7.00518756139846
Training loss: 1.154200553894043 / Valid loss: 7.197025153750465

Epoch: 12
Training loss: 0.8323202729225159 / Valid loss: 7.0675628412337534
Training loss: 1.0501971244812012 / Valid loss: 6.85383296467009
Training loss: 0.9281183481216431 / Valid loss: 6.85643948827471
Training loss: 1.2133475542068481 / Valid loss: 7.072347109658377
Training loss: 1.4583032131195068 / Valid loss: 6.871049921853202

Epoch: 13
Training loss: 0.7770556211471558 / Valid loss: 6.991551353817894
Training loss: 1.1429853439331055 / Valid loss: 6.869887324741908
Training loss: 0.7888211011886597 / Valid loss: 7.145133145650228
Training loss: 0.9990933537483215 / Valid loss: 7.470122518993559
Training loss: 0.8536421060562134 / Valid loss: 7.103294199988955

Epoch: 14
Training loss: 0.8143760561943054 / Valid loss: 7.1934434572855634
Training loss: 0.9104465842247009 / Valid loss: 6.762243116469611
Training loss: 0.8505488038063049 / Valid loss: 7.290207313355945
Training loss: 0.5181810259819031 / Valid loss: 6.910824117206392
Training loss: 1.1366146802902222 / Valid loss: 6.940998824437459

Epoch: 15
Training loss: 0.5783193707466125 / Valid loss: 6.936313000179473
Training loss: 1.0840296745300293 / Valid loss: 7.054695824214391
Training loss: 0.8588167428970337 / Valid loss: 6.9095718224843345
Training loss: 0.8379802107810974 / Valid loss: 6.916630822136288
Training loss: 0.8704365491867065 / Valid loss: 6.8463338942754834

Epoch: 16
Training loss: 0.697727620601654 / Valid loss: 6.965628483181908
Training loss: 0.526429295539856 / Valid loss: 6.954419222332183
Training loss: 0.6071270704269409 / Valid loss: 6.9394302050272625
Training loss: 0.8058458566665649 / Valid loss: 7.175640455881754
Training loss: 0.6265743970870972 / Valid loss: 7.012460849398658

Epoch: 17
Training loss: 1.0414118766784668 / Valid loss: 6.919318837211246
Training loss: 1.121394395828247 / Valid loss: 6.858706574212937
Training loss: 1.040895700454712 / Valid loss: 6.942865321749733
Training loss: 0.40991121530532837 / Valid loss: 6.931004256293887
Training loss: 0.8519729375839233 / Valid loss: 6.893187132335845

Epoch: 18
Training loss: 0.827326774597168 / Valid loss: 7.014196082523891
Training loss: 0.8725858926773071 / Valid loss: 7.02097939536685
Training loss: 0.918328046798706 / Valid loss: 6.928341788337344
Training loss: 0.7048296928405762 / Valid loss: 7.028553930918376
Training loss: 0.7853786945343018 / Valid loss: 7.071123245784214

Epoch: 19
Training loss: 1.193244218826294 / Valid loss: 6.93136397770473
Training loss: 1.3165339231491089 / Valid loss: 7.047872366224016
Training loss: 0.8668549060821533 / Valid loss: 7.078074668702625
Training loss: 0.5361595749855042 / Valid loss: 7.01968894913083

Epoch: 20
Training loss: 0.46323078870773315 / Valid loss: 6.79157228015718
Training loss: 1.117343783378601 / Valid loss: 6.774898015885126
Training loss: 0.5656572580337524 / Valid loss: 6.7234354427882606
Training loss: 0.9366224408149719 / Valid loss: 6.946455787477039
Training loss: 0.8610252737998962 / Valid loss: 6.8422768615541

Epoch: 21
Training loss: 0.6014280319213867 / Valid loss: 6.936105396634057
Training loss: 0.624936044216156 / Valid loss: 7.470533355077108
Training loss: 0.4623998701572418 / Valid loss: 7.10489992414202
Training loss: 0.6026308536529541 / Valid loss: 6.924998019990467
Training loss: 0.7077578902244568 / Valid loss: 7.003343441372826

Epoch: 22
Training loss: 0.7599740624427795 / Valid loss: 6.844064739772252
Training loss: 0.40276238322257996 / Valid loss: 6.808585593813942
Training loss: 0.7385974526405334 / Valid loss: 6.79426847639538
Training loss: 1.0574994087219238 / Valid loss: 7.002932746069772
Training loss: 0.9795268774032593 / Valid loss: 6.984853939783005

Epoch: 23
Training loss: 0.4677218794822693 / Valid loss: 6.758708953857422
Training loss: 1.0623011589050293 / Valid loss: 6.8751032988230385
Training loss: 1.1377191543579102 / Valid loss: 6.848219292504447
Training loss: 0.5319596529006958 / Valid loss: 6.923887825012207
Training loss: 0.6496936082839966 / Valid loss: 6.925333163851783

Epoch: 24
Training loss: 0.8852243423461914 / Valid loss: 6.765958726973761
Training loss: 0.7511134147644043 / Valid loss: 6.690888282230922
Training loss: 0.518207848072052 / Valid loss: 6.850369301296416
Training loss: 0.7854571342468262 / Valid loss: 6.897551009768532
Training loss: 1.1878080368041992 / Valid loss: 6.904299590701148

Epoch: 25
Training loss: 0.6203895807266235 / Valid loss: 6.641571717035203
Training loss: 0.490277498960495 / Valid loss: 6.716859990074521
Training loss: 1.0478674173355103 / Valid loss: 6.867931742895217
Training loss: 0.528637707233429 / Valid loss: 6.874913074856713
Training loss: 0.5252364277839661 / Valid loss: 6.906404268173945

Epoch: 26
Training loss: 0.6781438589096069 / Valid loss: 6.779802463168189
Training loss: 0.8792805075645447 / Valid loss: 6.780150440761021
Training loss: 0.42618653178215027 / Valid loss: 6.915697588239397
Training loss: 0.3820561468601227 / Valid loss: 6.923620501018706
Training loss: 0.4937906265258789 / Valid loss: 6.915510270709083

Epoch: 27
Training loss: 0.4867880940437317 / Valid loss: 6.847230638776507
Training loss: 0.6200078725814819 / Valid loss: 7.025835155305408
Training loss: 0.5116676092147827 / Valid loss: 6.852973483857655
Training loss: 0.5595400333404541 / Valid loss: 6.935061604636056
Training loss: 0.5986211895942688 / Valid loss: 7.178894074757894

Epoch: 28
Training loss: 0.31394702196121216 / Valid loss: 6.937650108337403
Training loss: 0.489215612411499 / Valid loss: 6.898119245256696
Training loss: 0.6846720576286316 / Valid loss: 6.982250567844936
Training loss: 0.28956204652786255 / Valid loss: 6.865274297623407
Training loss: 0.6038817167282104 / Valid loss: 6.880452024369013

Epoch: 29
Training loss: 0.7014957666397095 / Valid loss: 6.912215064820789
Training loss: 0.5201210975646973 / Valid loss: 6.895158050173805
Training loss: 0.277381956577301 / Valid loss: 6.883144573938279
Training loss: 0.2987216114997864 / Valid loss: 6.838234440485636

Epoch: 30
Training loss: 0.5389986634254456 / Valid loss: 7.053270798637754
Training loss: 0.44262492656707764 / Valid loss: 6.840029652913412
Training loss: 0.40146565437316895 / Valid loss: 6.839055295217605
Training loss: 0.4645858108997345 / Valid loss: 6.9706417810349235
Training loss: 0.44856321811676025 / Valid loss: 6.849125821249825

Epoch: 31
Training loss: 0.43073534965515137 / Valid loss: 7.041850037801833
Training loss: 0.484358549118042 / Valid loss: 6.9498781340462825
Training loss: 0.5776070356369019 / Valid loss: 6.7830291112264
Training loss: 0.37104618549346924 / Valid loss: 6.95129748071943
Training loss: 0.4902670383453369 / Valid loss: 7.19166347412836

Epoch: 32
Training loss: 0.46668392419815063 / Valid loss: 6.958358215150379
Training loss: 0.4231095612049103 / Valid loss: 6.772485178992862
Training loss: 0.3847997188568115 / Valid loss: 6.976103314899263
Training loss: 0.2922745943069458 / Valid loss: 6.817123247328259
Training loss: 0.6895589232444763 / Valid loss: 6.869498716081892

Epoch: 33
Training loss: 0.5040774941444397 / Valid loss: 6.764679590861003
Training loss: 0.46968886256217957 / Valid loss: 6.87881152062189
Training loss: 0.3738636374473572 / Valid loss: 7.0249474888756165
Training loss: 0.6303108930587769 / Valid loss: 6.946469520387195
Training loss: 0.4161486327648163 / Valid loss: 6.899018966583979

Epoch: 34
Training loss: 0.7348603010177612 / Valid loss: 6.837959868567331
Training loss: 0.48417460918426514 / Valid loss: 6.786624000186012
Training loss: 0.4102076292037964 / Valid loss: 6.871116987864176
Training loss: 0.23478932678699493 / Valid loss: 6.743425178527832
Training loss: 0.3972296118736267 / Valid loss: 6.904538878940401

Epoch: 35
Training loss: 0.9841682314872742 / Valid loss: 6.825352527981713
Training loss: 0.47061023116111755 / Valid loss: 6.771367504483178
Training loss: 0.7052760124206543 / Valid loss: 6.75522970926194
Training loss: 0.37328988313674927 / Valid loss: 6.926791629337129
Training loss: 0.3863188624382019 / Valid loss: 6.807530117034912

Epoch: 36
Training loss: 0.5039823651313782 / Valid loss: 6.715544943582444
Training loss: 0.657996416091919 / Valid loss: 6.816235857918149
Training loss: 0.5773594975471497 / Valid loss: 6.802448804037912
Training loss: 0.5617005825042725 / Valid loss: 7.017736714226859
Training loss: 0.36009907722473145 / Valid loss: 6.910492742629279

Epoch: 37
Training loss: 0.48155829310417175 / Valid loss: 6.738125537690662
Training loss: 0.22529757022857666 / Valid loss: 6.891021614982968
Training loss: 0.4388329088687897 / Valid loss: 6.896232754843576
Training loss: 0.9314467906951904 / Valid loss: 6.801308604649135
Training loss: 0.5406835079193115 / Valid loss: 6.847172369275774

Epoch: 38
Training loss: 0.5354229211807251 / Valid loss: 7.099115635099865
Training loss: 0.49385276436805725 / Valid loss: 6.779774979182652
Training loss: 0.3734056055545807 / Valid loss: 6.969909413655599
Training loss: 0.3162052631378174 / Valid loss: 7.000056080591111
Training loss: 0.6067602634429932 / Valid loss: 6.890658564794631

Epoch: 39
Training loss: 0.5873500108718872 / Valid loss: 6.803593036106655
Training loss: 0.2713117003440857 / Valid loss: 6.8924725623357865
Training loss: 0.3804124891757965 / Valid loss: 6.943823641822451
Training loss: 0.2755277454853058 / Valid loss: 6.699873731249855
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2400): 5.890339758282616
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.485506057739258 / Valid loss: 16.552648362659273
Model is saved in epoch 0, overall batch: 0
Training loss: 8.22925853729248 / Valid loss: 13.865889921642484
Model is saved in epoch 0, overall batch: 100
Training loss: 10.241255760192871 / Valid loss: 11.969333176385788
Model is saved in epoch 0, overall batch: 200
Training loss: 10.358463287353516 / Valid loss: 11.150789619627453
Model is saved in epoch 0, overall batch: 300
Training loss: 8.018839836120605 / Valid loss: 10.345452067965553
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 11.759272575378418 / Valid loss: 10.256315503801618
Model is saved in epoch 1, overall batch: 500
Training loss: 7.6574249267578125 / Valid loss: 9.549156325204033
Model is saved in epoch 1, overall batch: 600
Training loss: 6.881555557250977 / Valid loss: 8.997743847256615
Model is saved in epoch 1, overall batch: 700
Training loss: 5.406282424926758 / Valid loss: 8.67201589856829
Model is saved in epoch 1, overall batch: 800
Training loss: 8.251249313354492 / Valid loss: 7.469128517877488
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.6019368171691895 / Valid loss: 7.3130986032031835
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.7315545082092285 / Valid loss: 7.0377616178421745
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.523411750793457 / Valid loss: 6.408396620977492
Model is saved in epoch 2, overall batch: 1200
Training loss: 7.067471027374268 / Valid loss: 6.80925285021464
Training loss: 5.424326419830322 / Valid loss: 6.043492312658401
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.345699787139893 / Valid loss: 6.195551611128307
Training loss: 3.6437573432922363 / Valid loss: 5.968315624055409
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.085458755493164 / Valid loss: 5.953125179381598
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.98327112197876 / Valid loss: 5.703778941290719
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.883328437805176 / Valid loss: 5.83619107291812

Epoch: 4
Training loss: 2.928367853164673 / Valid loss: 5.942903162184216
Training loss: 3.861752510070801 / Valid loss: 5.812351724079677
Training loss: 4.301368713378906 / Valid loss: 5.991074403127034
Training loss: 4.796445369720459 / Valid loss: 5.775556686946324
Training loss: 5.227199554443359 / Valid loss: 5.77493687130156

Epoch: 5
Training loss: 4.842301845550537 / Valid loss: 5.890214225224086
Training loss: 4.513247013092041 / Valid loss: 5.931133249827794
Training loss: 3.9927265644073486 / Valid loss: 6.126261865525018
Training loss: 3.022768497467041 / Valid loss: 5.953989676066807
Training loss: 5.63494873046875 / Valid loss: 5.93120668502081

Epoch: 6
Training loss: 3.519346237182617 / Valid loss: 6.162134722300938
Training loss: 2.181300640106201 / Valid loss: 6.641494087945848
Training loss: 3.228849172592163 / Valid loss: 6.266521072387695
Training loss: 3.8385419845581055 / Valid loss: 6.138916531063262
Training loss: 3.5695643424987793 / Valid loss: 6.055485391616822

Epoch: 7
Training loss: 3.3118791580200195 / Valid loss: 6.782407374609084
Training loss: 1.7126188278198242 / Valid loss: 6.7648180371239075
Training loss: 2.619924545288086 / Valid loss: 6.315358772731963
Training loss: 3.4696474075317383 / Valid loss: 6.319690504528228
Training loss: 2.5432019233703613 / Valid loss: 6.735626838320777

Epoch: 8
Training loss: 2.498527765274048 / Valid loss: 7.006040059952509
Training loss: 2.7935991287231445 / Valid loss: 6.519283594403948
Training loss: 2.7384846210479736 / Valid loss: 7.590093176705497
Training loss: 2.713926315307617 / Valid loss: 6.501419051488241
Training loss: 3.2345125675201416 / Valid loss: 6.783861187526158

Epoch: 9
Training loss: 1.8865268230438232 / Valid loss: 6.789982500530424
Training loss: 3.0342159271240234 / Valid loss: 6.640279615493048
Training loss: 2.889190196990967 / Valid loss: 6.722147680464245
Training loss: 2.381490468978882 / Valid loss: 6.610790752229237

Epoch: 10
Training loss: 1.9844849109649658 / Valid loss: 6.814388954071772
Training loss: 1.961834192276001 / Valid loss: 6.682680452437628
Training loss: 2.622610330581665 / Valid loss: 6.772887584141323
Training loss: 2.3413150310516357 / Valid loss: 6.7594170615786595
Training loss: 2.4844555854797363 / Valid loss: 7.032551565624419

Epoch: 11
Training loss: 2.1771810054779053 / Valid loss: 6.772368540082659
Training loss: 1.747480869293213 / Valid loss: 6.786648530051822
Training loss: 2.035036087036133 / Valid loss: 6.800640919094994
Training loss: 2.247866153717041 / Valid loss: 8.502589162190755
Training loss: 1.629194974899292 / Valid loss: 7.138380890800839

Epoch: 12
Training loss: 1.3229968547821045 / Valid loss: 6.90823104495094
Training loss: 1.3064475059509277 / Valid loss: 6.834156631288074
Training loss: 1.6528788805007935 / Valid loss: 6.8817662988390245
Training loss: 1.6349018812179565 / Valid loss: 6.7977943602062405
Training loss: 1.834486484527588 / Valid loss: 6.892977734974452

Epoch: 13
Training loss: 1.4515258073806763 / Valid loss: 6.925282762164161
Training loss: 0.9892738461494446 / Valid loss: 6.737036541530064
Training loss: 1.2237510681152344 / Valid loss: 6.832575679960705
Training loss: 1.617051124572754 / Valid loss: 6.972277604965937
Training loss: 1.6354531049728394 / Valid loss: 6.948142210642497

Epoch: 14
Training loss: 1.5422146320343018 / Valid loss: 6.929645819891067
Training loss: 1.1413692235946655 / Valid loss: 7.430246914000739
Training loss: 1.0408620834350586 / Valid loss: 7.226158954983666
Training loss: 1.4732259511947632 / Valid loss: 7.049402055286226
Training loss: 1.0832334756851196 / Valid loss: 7.056605307261149

Epoch: 15
Training loss: 1.4463310241699219 / Valid loss: 7.056347061338879
Training loss: 1.392120599746704 / Valid loss: 7.028835360209147
Training loss: 1.381845474243164 / Valid loss: 7.00615059080578
Training loss: 1.2854466438293457 / Valid loss: 7.165252717336019
Training loss: 0.9642549157142639 / Valid loss: 6.984876187642415

Epoch: 16
Training loss: 1.7979986667633057 / Valid loss: 7.022317790985108
Training loss: 0.9639847278594971 / Valid loss: 7.687846615200951
Training loss: 1.0207526683807373 / Valid loss: 6.983453950427827
Training loss: 1.276789665222168 / Valid loss: 7.175333799634661
Training loss: 1.0586168766021729 / Valid loss: 7.093871171133859

Epoch: 17
Training loss: 0.8097612857818604 / Valid loss: 7.145289323443458
Training loss: 1.0120124816894531 / Valid loss: 7.111357359659104
Training loss: 1.1571483612060547 / Valid loss: 7.0435736315590995
Training loss: 1.1842131614685059 / Valid loss: 7.513935820261637
Training loss: 1.1856135129928589 / Valid loss: 7.122999186742874

Epoch: 18
Training loss: 0.7470845580101013 / Valid loss: 7.047831644330706
Training loss: 1.7169713973999023 / Valid loss: 6.960775597890218
Training loss: 1.107414960861206 / Valid loss: 7.3154444376627605
Training loss: 1.3543950319290161 / Valid loss: 7.177393761135283
Training loss: 1.103412389755249 / Valid loss: 7.190662670135498

Epoch: 19
Training loss: 0.9500818252563477 / Valid loss: 7.1410319691612605
Training loss: 1.1481575965881348 / Valid loss: 7.713646866026378
Training loss: 1.1512280702590942 / Valid loss: 7.430515271141416
Training loss: 1.027085781097412 / Valid loss: 7.135754585266113

Epoch: 20
Training loss: 0.6493785381317139 / Valid loss: 7.32870687303089
Training loss: 0.7466704249382019 / Valid loss: 7.259115550631568
Training loss: 0.6110968589782715 / Valid loss: 7.039972468784877
Training loss: 1.2642333507537842 / Valid loss: 7.216489147004627
Training loss: 0.767075777053833 / Valid loss: 7.309171785627092

Epoch: 21
Training loss: 0.9578944444656372 / Valid loss: 7.160520712534587
Training loss: 0.7571823000907898 / Valid loss: 7.195709632691883
Training loss: 1.0136897563934326 / Valid loss: 7.722887284415108
Training loss: 0.9289685487747192 / Valid loss: 7.965741816021147
Training loss: 1.06260085105896 / Valid loss: 7.220825708480108

Epoch: 22
Training loss: 0.6888749599456787 / Valid loss: 7.270315247490292
Training loss: 0.9841432571411133 / Valid loss: 7.112723355066208
Training loss: 1.0290039777755737 / Valid loss: 7.1294762202671595
Training loss: 0.5544905662536621 / Valid loss: 7.600159050169445
Training loss: 0.8756568431854248 / Valid loss: 7.307432665143694

Epoch: 23
Training loss: 0.6414084434509277 / Valid loss: 7.440625068119594
Training loss: 1.2075589895248413 / Valid loss: 7.042097436814081
Training loss: 1.1824485063552856 / Valid loss: 7.17326511655535
Training loss: 0.9570131301879883 / Valid loss: 7.246788924080985
Training loss: 0.8180095553398132 / Valid loss: 7.272665212267921

Epoch: 24
Training loss: 0.4693340063095093 / Valid loss: 7.131731868925549
Training loss: 1.2630606889724731 / Valid loss: 7.427453399839855
Training loss: 0.5064395070075989 / Valid loss: 7.133121535891578
Training loss: 0.8948311805725098 / Valid loss: 7.236420740400042
Training loss: 0.8293712139129639 / Valid loss: 7.657892454238165

Epoch: 25
Training loss: 0.8017382621765137 / Valid loss: 7.972182192121234
Training loss: 1.0829272270202637 / Valid loss: 7.186772895994641
Training loss: 0.6860487461090088 / Valid loss: 7.254732486179897
Training loss: 0.9759463667869568 / Valid loss: 7.462377439226423
Training loss: 0.7795689105987549 / Valid loss: 7.3229260262988864

Epoch: 26
Training loss: 0.8743743896484375 / Valid loss: 7.318362367720831
Training loss: 0.5629526972770691 / Valid loss: 7.507926577613468
Training loss: 1.0308730602264404 / Valid loss: 7.104080717904227
Training loss: 0.7438271045684814 / Valid loss: 7.224047570001511
Training loss: 0.7149567604064941 / Valid loss: 7.22013585681007

Epoch: 27
Training loss: 0.633277177810669 / Valid loss: 7.240124997638521
Training loss: 0.36785125732421875 / Valid loss: 7.15103455498105
Training loss: 1.3982067108154297 / Valid loss: 7.446244058154878
Training loss: 0.8192980885505676 / Valid loss: 7.142419499442691
Training loss: 0.4749196171760559 / Valid loss: 7.587369369325184

Epoch: 28
Training loss: 0.417674720287323 / Valid loss: 7.17670928864252
Training loss: 0.4325339198112488 / Valid loss: 7.198163420813424
Training loss: 0.5170741677284241 / Valid loss: 7.337751075199672
Training loss: 0.5556463003158569 / Valid loss: 7.16787778763544
Training loss: 0.8508818745613098 / Valid loss: 7.218611197244553

Epoch: 29
Training loss: 0.7658052444458008 / Valid loss: 7.8699110076541
Training loss: 0.43019384145736694 / Valid loss: 7.208646924155099
Training loss: 0.6360567808151245 / Valid loss: 7.166488633837019
Training loss: 0.9754325747489929 / Valid loss: 7.958179646446592

Epoch: 30
Training loss: 0.6516331434249878 / Valid loss: 7.184434105101086
Training loss: 1.042105793952942 / Valid loss: 7.287758318583171
Training loss: 0.38020944595336914 / Valid loss: 7.184267679850261
Training loss: 0.40461450815200806 / Valid loss: 7.1868491445268905
Training loss: 0.9297048449516296 / Valid loss: 7.262267684936523

Epoch: 31
Training loss: 0.5271829962730408 / Valid loss: 7.225095362890334
Training loss: 1.0828704833984375 / Valid loss: 7.038180510203044
Training loss: 0.6640037298202515 / Valid loss: 7.464965470631918
Training loss: 0.6236491203308105 / Valid loss: 7.207226385389055
Training loss: 0.7305420637130737 / Valid loss: 7.1773869332813085

Epoch: 32
Training loss: 0.44706854224205017 / Valid loss: 7.336225650424049
Training loss: 0.5903486609458923 / Valid loss: 7.258548872811454
Training loss: 0.5245429873466492 / Valid loss: 7.433815270378476
Training loss: 0.4944256544113159 / Valid loss: 7.2644818442208425
Training loss: 0.6380219459533691 / Valid loss: 7.259572880608695

Epoch: 33
Training loss: 0.43161541223526 / Valid loss: 7.156936986105783
Training loss: 0.7766989469528198 / Valid loss: 7.146816571553548
Training loss: 0.6814211010932922 / Valid loss: 7.217694150833856
Training loss: 0.5528929233551025 / Valid loss: 7.294396620705014
Training loss: 0.5293247699737549 / Valid loss: 7.887059643155053

Epoch: 34
Training loss: 0.9457837343215942 / Valid loss: 7.343801707313174
Training loss: 0.4446396231651306 / Valid loss: 7.141716107868013
Training loss: 0.45383015275001526 / Valid loss: 7.200739792415074
Training loss: 0.5024568438529968 / Valid loss: 7.176289213271368
Training loss: 0.9410631656646729 / Valid loss: 7.338185214996338

Epoch: 35
Training loss: 0.6484671235084534 / Valid loss: 7.190289193107969
Training loss: 0.4244862198829651 / Valid loss: 7.455016070320493
Training loss: 0.45706620812416077 / Valid loss: 7.186460681188674
Training loss: 0.4246668517589569 / Valid loss: 7.246242066792079
Training loss: 0.3787277340888977 / Valid loss: 7.626142179398309

Epoch: 36
Training loss: 0.4324735403060913 / Valid loss: 7.327129990713937
Training loss: 0.5639158487319946 / Valid loss: 7.17691102709089
Training loss: 0.7515487670898438 / Valid loss: 7.11541629972912
Training loss: 0.7582157850265503 / Valid loss: 7.0456661633082796
Training loss: 0.49600932002067566 / Valid loss: 7.089993554069882

Epoch: 37
Training loss: 0.5421404838562012 / Valid loss: 7.134672391982305
Training loss: 0.4929816722869873 / Valid loss: 7.109780783880325
Training loss: 0.35245025157928467 / Valid loss: 7.17235689163208
Training loss: 0.5098813772201538 / Valid loss: 7.212559504736038
Training loss: 0.5391055941581726 / Valid loss: 7.169134303501674

Epoch: 38
Training loss: 0.5214968323707581 / Valid loss: 7.059951950254894
Training loss: 0.3965655565261841 / Valid loss: 7.032583027794248
Training loss: 0.5592663288116455 / Valid loss: 7.165171423412505
Training loss: 0.8139569759368896 / Valid loss: 7.136174667449224
Training loss: 1.2661253213882446 / Valid loss: 7.20857055300758

Epoch: 39
Training loss: 0.26306453347206116 / Valid loss: 7.247021166483561
Training loss: 0.43685781955718994 / Valid loss: 7.041598769596645
Training loss: 0.31398433446884155 / Valid loss: 7.853271511622838
Training loss: 0.3933102488517761 / Valid loss: 7.141638324374244
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1800): 5.544980144500732
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.485506057739258 / Valid loss: 16.552646755036854
Model is saved in epoch 0, overall batch: 0
Training loss: 8.219486236572266 / Valid loss: 13.842695953732445
Model is saved in epoch 0, overall batch: 100
Training loss: 10.224428176879883 / Valid loss: 11.834143148149764
Model is saved in epoch 0, overall batch: 200
Training loss: 10.35300064086914 / Valid loss: 11.165998172760009
Model is saved in epoch 0, overall batch: 300
Training loss: 8.014925003051758 / Valid loss: 10.350691945212228
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 11.746452331542969 / Valid loss: 10.244138077327184
Model is saved in epoch 1, overall batch: 500
Training loss: 7.675751686096191 / Valid loss: 9.628966912769137
Model is saved in epoch 1, overall batch: 600
Training loss: 6.80963134765625 / Valid loss: 8.880905301230294
Model is saved in epoch 1, overall batch: 700
Training loss: 5.405893325805664 / Valid loss: 8.623281533377511
Model is saved in epoch 1, overall batch: 800
Training loss: 8.279170036315918 / Valid loss: 7.480731809706915
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.567861557006836 / Valid loss: 7.350690501076834
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.809311389923096 / Valid loss: 7.0699756190890355
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.5808939933776855 / Valid loss: 6.449579813366845
Model is saved in epoch 2, overall batch: 1200
Training loss: 6.986896991729736 / Valid loss: 6.853081437519618
Training loss: 5.433891296386719 / Valid loss: 5.940204132170904
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.33891487121582 / Valid loss: 6.202776818048386
Training loss: 3.609391689300537 / Valid loss: 5.9796052796500065
Training loss: 6.054928779602051 / Valid loss: 5.985523646218436
Training loss: 4.980792999267578 / Valid loss: 5.675325870513916
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.73798131942749 / Valid loss: 5.812697533198765

Epoch: 4
Training loss: 2.9933278560638428 / Valid loss: 5.94984864734468
Training loss: 3.8618149757385254 / Valid loss: 5.882209130695888
Training loss: 4.2419352531433105 / Valid loss: 5.979497457685925
Training loss: 4.911354064941406 / Valid loss: 5.737984684535435
Training loss: 5.316019058227539 / Valid loss: 5.783615886597406

Epoch: 5
Training loss: 4.799158096313477 / Valid loss: 5.982444218226841
Training loss: 4.232202529907227 / Valid loss: 6.016028790246873
Training loss: 4.065987586975098 / Valid loss: 6.121067982628232
Training loss: 3.2105469703674316 / Valid loss: 5.974589808781942
Training loss: 5.847111701965332 / Valid loss: 5.936399843579247

Epoch: 6
Training loss: 3.543731927871704 / Valid loss: 6.051052797408331
Training loss: 2.1184396743774414 / Valid loss: 6.475351449421474
Training loss: 3.356384754180908 / Valid loss: 6.1790489514668785
Training loss: 3.7842912673950195 / Valid loss: 6.129597332364037
Training loss: 3.4305636882781982 / Valid loss: 6.139355019160679

Epoch: 7
Training loss: 3.2343735694885254 / Valid loss: 6.284197746004377
Training loss: 1.822499394416809 / Valid loss: 6.785548255557106
Training loss: 2.715573310852051 / Valid loss: 6.4017107282366075
Training loss: 3.217799663543701 / Valid loss: 6.3804451261247905
Training loss: 2.4604897499084473 / Valid loss: 6.672141972042265

Epoch: 8
Training loss: 2.417318344116211 / Valid loss: 6.713183675493513
Training loss: 2.8996644020080566 / Valid loss: 6.655612900143578
Training loss: 2.647711992263794 / Valid loss: 7.347362686338879
Training loss: 2.8805596828460693 / Valid loss: 6.425763125646682
Training loss: 2.82716703414917 / Valid loss: 6.673831054142544

Epoch: 9
Training loss: 1.6946392059326172 / Valid loss: 6.710038148789179
Training loss: 3.031621217727661 / Valid loss: 6.640128417242141
Training loss: 3.3655407428741455 / Valid loss: 6.705636405944825
Training loss: 2.2334492206573486 / Valid loss: 7.2187202998570035

Epoch: 10
Training loss: 2.1449413299560547 / Valid loss: 6.793566535768055
Training loss: 1.4623374938964844 / Valid loss: 6.7958222252982
Training loss: 2.7885217666625977 / Valid loss: 6.690313838777088
Training loss: 2.383697986602783 / Valid loss: 7.4842033794948035
Training loss: 2.5803070068359375 / Valid loss: 7.441527984255836

Epoch: 11
Training loss: 2.0033469200134277 / Valid loss: 6.687241944812593
Training loss: 1.7262780666351318 / Valid loss: 6.997771317618234
Training loss: 1.647144079208374 / Valid loss: 6.8530822890145435
Training loss: 2.1081199645996094 / Valid loss: 7.532942186083113
Training loss: 1.5630275011062622 / Valid loss: 7.100531657536824

Epoch: 12
Training loss: 1.2099571228027344 / Valid loss: 6.764662906101772
Training loss: 1.5278899669647217 / Valid loss: 7.300971794128418
Training loss: 1.7206473350524902 / Valid loss: 6.870118774686541
Training loss: 1.5949472188949585 / Valid loss: 6.826452718462263
Training loss: 2.143001079559326 / Valid loss: 7.070548266456241

Epoch: 13
Training loss: 1.4748307466506958 / Valid loss: 6.951608458019439
Training loss: 0.9412148594856262 / Valid loss: 7.035115101223901
Training loss: 1.231534481048584 / Valid loss: 6.880181959697178
Training loss: 1.460290551185608 / Valid loss: 7.178212256658645
Training loss: 1.8826792240142822 / Valid loss: 7.112333284105573

Epoch: 14
Training loss: 1.692925214767456 / Valid loss: 6.972344366709391
Training loss: 1.2145960330963135 / Valid loss: 6.979640536081224
Training loss: 0.9637789726257324 / Valid loss: 7.057818776085263
Training loss: 1.592982530593872 / Valid loss: 7.025349081130255
Training loss: 1.307695984840393 / Valid loss: 7.15187520980835

Epoch: 15
Training loss: 1.3800880908966064 / Valid loss: 6.97950108391898
Training loss: 1.3955644369125366 / Valid loss: 7.146871532712664
Training loss: 1.3624016046524048 / Valid loss: 7.186014338902065
Training loss: 1.126110553741455 / Valid loss: 7.118725958324614
Training loss: 1.0548149347305298 / Valid loss: 7.091012854803176

Epoch: 16
Training loss: 2.0189032554626465 / Valid loss: 7.191078721909296
Training loss: 1.1467604637145996 / Valid loss: 7.108806996118455
Training loss: 0.828427791595459 / Valid loss: 7.134105232783726
Training loss: 1.1798644065856934 / Valid loss: 7.283150386810303
Training loss: 0.957021951675415 / Valid loss: 7.09938630149478

Epoch: 17
Training loss: 0.8078451156616211 / Valid loss: 7.780093415578206
Training loss: 1.277782678604126 / Valid loss: 7.270284668604533
Training loss: 0.981843113899231 / Valid loss: 7.213662313279651
Training loss: 1.3935580253601074 / Valid loss: 7.546832961127872
Training loss: 1.2291371822357178 / Valid loss: 7.17466459274292

Epoch: 18
Training loss: 0.8306933641433716 / Valid loss: 7.168631612686884
Training loss: 1.2552173137664795 / Valid loss: 7.09449720836821
Training loss: 1.0676008462905884 / Valid loss: 7.2057990641820995
Training loss: 1.272059679031372 / Valid loss: 7.125735789253598
Training loss: 1.310145616531372 / Valid loss: 7.408400040581113

Epoch: 19
Training loss: 1.1464495658874512 / Valid loss: 7.356662382398333
Training loss: 1.2020511627197266 / Valid loss: 7.39073124840146
Training loss: 1.1632270812988281 / Valid loss: 7.377167724427723
Training loss: 0.8303523659706116 / Valid loss: 7.338980211530413

Epoch: 20
Training loss: 0.8458108901977539 / Valid loss: 7.348205402919224
Training loss: 0.7606648206710815 / Valid loss: 7.156574378694807
Training loss: 0.8162119388580322 / Valid loss: 7.055813380650112
Training loss: 1.4671120643615723 / Valid loss: 8.00933034533546
Training loss: 0.8122574090957642 / Valid loss: 7.49398243313744

Epoch: 21
Training loss: 0.7195826768875122 / Valid loss: 7.170383117312476
Training loss: 0.7924116253852844 / Valid loss: 7.530583458855038
Training loss: 0.9866306781768799 / Valid loss: 7.242297045389811
Training loss: 1.0439146757125854 / Valid loss: 7.540931133996873
Training loss: 1.0532805919647217 / Valid loss: 7.319232740856353

Epoch: 22
Training loss: 0.7667382955551147 / Valid loss: 7.127392646244594
Training loss: 1.068111777305603 / Valid loss: 8.070131656101772
Training loss: 0.9239652752876282 / Valid loss: 7.131194155556815
Training loss: 0.7385979890823364 / Valid loss: 7.217661512465704
Training loss: 0.8377608060836792 / Valid loss: 7.312101800101144

Epoch: 23
Training loss: 0.7553396224975586 / Valid loss: 7.130744936352684
Training loss: 1.2621526718139648 / Valid loss: 7.341155011313302
Training loss: 0.9643779993057251 / Valid loss: 8.58649297441755
Training loss: 1.0203937292099 / Valid loss: 7.224752866654169
Training loss: 1.005949854850769 / Valid loss: 7.273100580487933

Epoch: 24
Training loss: 0.5030282735824585 / Valid loss: 7.461307080586751
Training loss: 0.9891395568847656 / Valid loss: 7.297685832069034
Training loss: 0.5898656845092773 / Valid loss: 7.17826992670695
Training loss: 0.9196341037750244 / Valid loss: 7.407896968296596
Training loss: 0.7794146537780762 / Valid loss: 7.363264147440592

Epoch: 25
Training loss: 0.6359618902206421 / Valid loss: 8.085087994166782
Training loss: 1.0608007907867432 / Valid loss: 7.281673222496396
Training loss: 0.7733113765716553 / Valid loss: 7.181710147857666
Training loss: 0.8945329189300537 / Valid loss: 7.395302143551055
Training loss: 0.7278333902359009 / Valid loss: 7.306096739996047

Epoch: 26
Training loss: 0.5889666080474854 / Valid loss: 7.116857996441069
Training loss: 0.6227881908416748 / Valid loss: 7.446256960005988
Training loss: 1.0252079963684082 / Valid loss: 7.608292647770473
Training loss: 0.7887219786643982 / Valid loss: 7.31569607598441
Training loss: 0.9115214347839355 / Valid loss: 7.36100975672404

Epoch: 27
Training loss: 0.6393331289291382 / Valid loss: 7.241570368267241
Training loss: 0.5528552532196045 / Valid loss: 7.264327934810093
Training loss: 1.0436017513275146 / Valid loss: 7.241396032060895
Training loss: 0.5918570756912231 / Valid loss: 7.96741327558245
Training loss: 0.6380915641784668 / Valid loss: 7.29144895644415

Epoch: 28
Training loss: 0.4603910446166992 / Valid loss: 7.202119722820464
Training loss: 0.5159573554992676 / Valid loss: 7.358629008701869
Training loss: 0.7296923398971558 / Valid loss: 7.735919416518438
Training loss: 0.49782562255859375 / Valid loss: 7.502034255436489
Training loss: 0.8990378379821777 / Valid loss: 7.4148949577694845

Epoch: 29
Training loss: 0.6891494989395142 / Valid loss: 7.241675549461728
Training loss: 0.43764758110046387 / Valid loss: 7.153343323298863
Training loss: 0.681516706943512 / Valid loss: 7.572144317626953
Training loss: 0.6796661615371704 / Valid loss: 7.719097464425223

Epoch: 30
Training loss: 0.5519498586654663 / Valid loss: 7.227802848815918
Training loss: 1.0698519945144653 / Valid loss: 7.356400871276856
Training loss: 0.32308679819107056 / Valid loss: 7.235542937687465
Training loss: 0.5801414251327515 / Valid loss: 7.23387923467727
Training loss: 0.9306362867355347 / Valid loss: 7.412653200966972

Epoch: 31
Training loss: 0.5125123262405396 / Valid loss: 7.309109129224505
Training loss: 1.4403932094573975 / Valid loss: 7.969477149418422
Training loss: 0.47493863105773926 / Valid loss: 7.251865250723703
Training loss: 0.9190322160720825 / Valid loss: 7.364259070441836
Training loss: 0.6375898718833923 / Valid loss: 7.41927406220209

Epoch: 32
Training loss: 0.40737271308898926 / Valid loss: 7.218039730616979
Training loss: 0.6837755441665649 / Valid loss: 7.232789854776292
Training loss: 0.6495028734207153 / Valid loss: 7.204083424522763
Training loss: 0.4789831042289734 / Valid loss: 7.607111249651227
Training loss: 0.6739655137062073 / Valid loss: 7.141101410275414

Epoch: 33
Training loss: 0.3462129533290863 / Valid loss: 7.536489586603074
Training loss: 0.5618709325790405 / Valid loss: 7.153042511712937
Training loss: 0.6498227119445801 / Valid loss: 7.132273188091459
Training loss: 0.6227340698242188 / Valid loss: 7.707172916049049
Training loss: 0.47152048349380493 / Valid loss: 7.236963190351214

Epoch: 34
Training loss: 0.9064661264419556 / Valid loss: 7.259236621856689
Training loss: 0.49162763357162476 / Valid loss: 7.178330003647577
Training loss: 0.45974260568618774 / Valid loss: 7.133571397690546
Training loss: 0.5818126201629639 / Valid loss: 7.479680179414295
Training loss: 0.8422331809997559 / Valid loss: 7.263320968264625

Epoch: 35
Training loss: 0.6552726030349731 / Valid loss: 7.237027740478515
Training loss: 0.5784815549850464 / Valid loss: 7.260769998459589
Training loss: 0.5315569043159485 / Valid loss: 7.0663123675755095
Training loss: 0.520434558391571 / Valid loss: 8.015858827318464
Training loss: 0.43871694803237915 / Valid loss: 7.3828620501926965

Epoch: 36
Training loss: 0.4100837707519531 / Valid loss: 7.210006763821556
Training loss: 0.28804466128349304 / Valid loss: 7.155039246877035
Training loss: 0.6391111016273499 / Valid loss: 7.251858393351237
Training loss: 0.727776050567627 / Valid loss: 7.128300444285075
Training loss: 0.4589166045188904 / Valid loss: 7.474583848317464

Epoch: 37
Training loss: 0.4292549788951874 / Valid loss: 7.100679679143997
Training loss: 0.42285600304603577 / Valid loss: 7.3063404174078075
Training loss: 0.3667851686477661 / Valid loss: 7.257774852571034
Training loss: 0.4826911687850952 / Valid loss: 7.107991595495315
Training loss: 0.7814438343048096 / Valid loss: 7.13877550760905

Epoch: 38
Training loss: 0.3251643776893616 / Valid loss: 7.092720603942871
Training loss: 0.38979029655456543 / Valid loss: 7.195138567969913
Training loss: 0.4729841649532318 / Valid loss: 7.35252195085798
Training loss: 0.6195186376571655 / Valid loss: 7.182529326847622
Training loss: 1.2120800018310547 / Valid loss: 7.609613146100726

Epoch: 39
Training loss: 0.46319442987442017 / Valid loss: 7.163579482123965
Training loss: 0.38726508617401123 / Valid loss: 7.293277740478516
Training loss: 0.37766051292419434 / Valid loss: 7.350680591946556
Training loss: 0.4526247978210449 / Valid loss: 7.1957329114278155
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1800): 5.514935952141172
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.62260627746582 / Valid loss: 16.113272512526738
Model is saved in epoch 0, overall batch: 0
Training loss: 11.981375694274902 / Valid loss: 11.816666507720948
Model is saved in epoch 0, overall batch: 100
Training loss: 8.561663627624512 / Valid loss: 7.799813327335176
Model is saved in epoch 0, overall batch: 200
Training loss: 6.4559478759765625 / Valid loss: 5.887999884287516
Model is saved in epoch 0, overall batch: 300
Training loss: 9.441091537475586 / Valid loss: 5.689365643546695
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.431851863861084 / Valid loss: 5.8672060194469635
Training loss: 5.661279678344727 / Valid loss: 5.838317135402135
Training loss: 6.85526704788208 / Valid loss: 5.729833997998919
Training loss: 6.035687446594238 / Valid loss: 5.881188433510917
Training loss: 5.18850040435791 / Valid loss: 5.561653956912813
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 4.818472385406494 / Valid loss: 5.588327001390003
Training loss: 4.661823749542236 / Valid loss: 5.776552708943685
Training loss: 5.4140472412109375 / Valid loss: 5.572110557556153
Training loss: 5.84920597076416 / Valid loss: 5.576434644063314
Training loss: 5.569860935211182 / Valid loss: 5.630689695903233

Epoch: 3
Training loss: 3.614938735961914 / Valid loss: 5.933127235230946
Training loss: 4.91295051574707 / Valid loss: 5.622013923100063
Training loss: 4.247612953186035 / Valid loss: 5.652283409663609
Training loss: 5.518445014953613 / Valid loss: 5.776180637450445
Training loss: 4.575778961181641 / Valid loss: 5.53904952548799
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.341827154159546 / Valid loss: 5.602780684970674
Training loss: 4.05409574508667 / Valid loss: 5.5975323677062985
Training loss: 3.8735411167144775 / Valid loss: 5.718776400883993
Training loss: 4.754057884216309 / Valid loss: 5.779694623038882
Training loss: 7.144676208496094 / Valid loss: 5.759812005360922

Epoch: 5
Training loss: 3.581448793411255 / Valid loss: 5.6877969492049445
Training loss: 3.308439254760742 / Valid loss: 5.753117713474092
Training loss: 5.523226261138916 / Valid loss: 5.689309249605452
Training loss: 5.694096088409424 / Valid loss: 5.736040231159755
Training loss: 6.5392866134643555 / Valid loss: 6.408744539533343

Epoch: 6
Training loss: 4.041530609130859 / Valid loss: 5.744074785141718
Training loss: 3.524935722351074 / Valid loss: 5.954942033404396
Training loss: 4.325168609619141 / Valid loss: 5.709680548168364
Training loss: 3.6437253952026367 / Valid loss: 5.906825769515264
Training loss: 3.0654921531677246 / Valid loss: 5.752376072747367

Epoch: 7
Training loss: 2.5358660221099854 / Valid loss: 6.314314011165074
Training loss: 3.2677907943725586 / Valid loss: 6.180979742322649
Training loss: 3.811722993850708 / Valid loss: 5.92688649041312
Training loss: 4.092374324798584 / Valid loss: 5.8820614269801546
Training loss: 4.191745758056641 / Valid loss: 5.928286877132598

Epoch: 8
Training loss: 3.3860950469970703 / Valid loss: 5.875133246467227
Training loss: 3.183361768722534 / Valid loss: 5.933499127342587
Training loss: 4.466777801513672 / Valid loss: 6.045709796178908
Training loss: 3.800304412841797 / Valid loss: 6.733478566578456
Training loss: 3.4802680015563965 / Valid loss: 5.857598309289841

Epoch: 9
Training loss: 3.1472668647766113 / Valid loss: 5.96273167246864
Training loss: 3.541856288909912 / Valid loss: 5.972053991045271
Training loss: 3.5373716354370117 / Valid loss: 6.088474984396072
Training loss: 2.7156848907470703 / Valid loss: 6.145434127535139

Epoch: 10
Training loss: 2.926503896713257 / Valid loss: 5.881049306052072
Training loss: 2.663496971130371 / Valid loss: 6.067717704318818
Training loss: 3.4793288707733154 / Valid loss: 6.361605794089181
Training loss: 3.2300314903259277 / Valid loss: 6.172431389490764
Training loss: 2.955963611602783 / Valid loss: 6.078214036850702

Epoch: 11
Training loss: 3.321547508239746 / Valid loss: 6.237705718903315
Training loss: 3.4064183235168457 / Valid loss: 6.215869930812291
Training loss: 2.7536144256591797 / Valid loss: 6.568471801848639
Training loss: 2.3717148303985596 / Valid loss: 6.136764492307391
Training loss: 2.8886327743530273 / Valid loss: 6.432164539609636

Epoch: 12
Training loss: 3.2298026084899902 / Valid loss: 6.065426047643026
Training loss: 2.9477248191833496 / Valid loss: 6.21222190402803
Training loss: 3.460266351699829 / Valid loss: 6.2207722618466335
Training loss: 2.850217819213867 / Valid loss: 6.200380902063279
Training loss: 2.1150879859924316 / Valid loss: 6.172502286093575

Epoch: 13
Training loss: 2.536099433898926 / Valid loss: 6.159731556120373
Training loss: 2.3612265586853027 / Valid loss: 6.387197285606748
Training loss: 2.2422685623168945 / Valid loss: 6.278357101622082
Training loss: 1.9804513454437256 / Valid loss: 7.4339762188139415
Training loss: 2.3795342445373535 / Valid loss: 6.340870201020014

Epoch: 14
Training loss: 1.9998698234558105 / Valid loss: 6.2935458887191045
Training loss: 2.6144165992736816 / Valid loss: 6.325336612973895
Training loss: 3.364734172821045 / Valid loss: 6.920889554704939
Training loss: 2.7031164169311523 / Valid loss: 6.418536186218262
Training loss: 2.646881341934204 / Valid loss: 6.622565414792016

Epoch: 15
Training loss: 1.6465891599655151 / Valid loss: 6.458289779935565
Training loss: 3.325824737548828 / Valid loss: 6.353709166390555
Training loss: 2.807063102722168 / Valid loss: 7.018291441599528
Training loss: 3.156320571899414 / Valid loss: 6.571274137496948
Training loss: 2.353762149810791 / Valid loss: 6.3464444001515705

Epoch: 16
Training loss: 2.838454246520996 / Valid loss: 6.447871866680327
Training loss: 2.054107189178467 / Valid loss: 7.025777044750395
Training loss: 1.6957250833511353 / Valid loss: 6.5024159703935895
Training loss: 2.757199287414551 / Valid loss: 6.359470821562267
Training loss: 2.1010870933532715 / Valid loss: 7.670064590090797

Epoch: 17
Training loss: 2.343766212463379 / Valid loss: 6.437376692181542
Training loss: 1.624578595161438 / Valid loss: 6.931774516332717
Training loss: 1.7253838777542114 / Valid loss: 6.857915551321847
Training loss: 4.290212631225586 / Valid loss: 6.504991245269776
Training loss: 2.145789384841919 / Valid loss: 6.659347665877569

Epoch: 18
Training loss: 1.759606122970581 / Valid loss: 6.466680706114996
Training loss: 1.4555472135543823 / Valid loss: 6.719832976659139
Training loss: 2.205173969268799 / Valid loss: 6.5625967752365835
Training loss: 1.7020766735076904 / Valid loss: 7.11601338613601
Training loss: 2.1983354091644287 / Valid loss: 6.577837365014212

Epoch: 19
Training loss: 1.0433483123779297 / Valid loss: 6.538500835782005
Training loss: 1.825883388519287 / Valid loss: 6.8521539642697284
Training loss: 2.1499006748199463 / Valid loss: 6.759799898238409
Training loss: 1.9177496433258057 / Valid loss: 6.935376880282448

Epoch: 20
Training loss: 1.7521512508392334 / Valid loss: 7.333484427134196
Training loss: 1.4996287822723389 / Valid loss: 6.873306969233922
Training loss: 1.5622040033340454 / Valid loss: 6.685383204051426
Training loss: 1.4596693515777588 / Valid loss: 7.983153388613746
Training loss: 1.2330151796340942 / Valid loss: 6.6697809105827695

Epoch: 21
Training loss: 1.6315827369689941 / Valid loss: 6.719668456486294
Training loss: 1.8557543754577637 / Valid loss: 6.887356208619617
Training loss: 1.7816932201385498 / Valid loss: 6.618310701279413
Training loss: 2.309722661972046 / Valid loss: 6.824562776656378
Training loss: 1.5546085834503174 / Valid loss: 7.077516576222011

Epoch: 22
Training loss: 1.1125141382217407 / Valid loss: 6.764394026710874
Training loss: 1.3339630365371704 / Valid loss: 7.212054525102888
Training loss: 1.8221278190612793 / Valid loss: 7.246674546741304
Training loss: 1.9226226806640625 / Valid loss: 6.649495070321219
Training loss: 1.5396450757980347 / Valid loss: 6.8369166419619605

Epoch: 23
Training loss: 1.0215978622436523 / Valid loss: 7.985856596628825
Training loss: 1.799208641052246 / Valid loss: 6.962226386297317
Training loss: 1.8007278442382812 / Valid loss: 6.918149398622059
Training loss: 1.4581668376922607 / Valid loss: 6.6818988936288015
Training loss: 1.1291555166244507 / Valid loss: 6.933845819745745

Epoch: 24
Training loss: 1.077491044998169 / Valid loss: 6.86520120984032
Training loss: 1.7446300983428955 / Valid loss: 6.811778159368606
Training loss: 1.4313552379608154 / Valid loss: 7.056212239038377
Training loss: 1.0099647045135498 / Valid loss: 6.891221268971761
Training loss: 1.4899396896362305 / Valid loss: 7.267281981876918

Epoch: 25
Training loss: 1.0751252174377441 / Valid loss: 6.887645362672352
Training loss: 0.8662316799163818 / Valid loss: 6.80979867435637
Training loss: 2.0769174098968506 / Valid loss: 7.772428839547294
Training loss: 1.3807885646820068 / Valid loss: 6.947183599926176
Training loss: 1.514533519744873 / Valid loss: 7.560202553158715

Epoch: 26
Training loss: 0.8501127362251282 / Valid loss: 7.360404777526855
Training loss: 0.876375675201416 / Valid loss: 6.904012223652431
Training loss: 0.9560344219207764 / Valid loss: 7.014984130859375
Training loss: 1.292166829109192 / Valid loss: 7.291907424018497
Training loss: 0.9584882259368896 / Valid loss: 6.949136833917527

Epoch: 27
Training loss: 1.12650465965271 / Valid loss: 6.878521569569906
Training loss: 0.992019534111023 / Valid loss: 6.868228776114328
Training loss: 1.3238856792449951 / Valid loss: 7.155717268444243
Training loss: 0.9730052947998047 / Valid loss: 7.198199063255673
Training loss: 1.4724560976028442 / Valid loss: 6.816399426687331

Epoch: 28
Training loss: 0.910829484462738 / Valid loss: 7.004383441380092
Training loss: 1.0024888515472412 / Valid loss: 7.004966608683268
Training loss: 1.0626568794250488 / Valid loss: 7.500833974565778
Training loss: 1.5328798294067383 / Valid loss: 6.963663469042097
Training loss: 0.9688473343849182 / Valid loss: 7.540891429356166

Epoch: 29
Training loss: 0.7330912351608276 / Valid loss: 6.934893462771461
Training loss: 0.7065658569335938 / Valid loss: 7.0368758428664435
Training loss: 1.331263542175293 / Valid loss: 7.502609979538691
Training loss: 1.1337894201278687 / Valid loss: 7.0096500215076265

Epoch: 30
Training loss: 0.5737733840942383 / Valid loss: 7.119925542104812
Training loss: 1.0138765573501587 / Valid loss: 7.24058209827968
Training loss: 0.6251097321510315 / Valid loss: 7.425715460096087
Training loss: 0.8038676977157593 / Valid loss: 7.041635295322963
Training loss: 0.7657690644264221 / Valid loss: 7.060984193711054

Epoch: 31
Training loss: 1.041504144668579 / Valid loss: 7.22672366187686
Training loss: 0.7323285937309265 / Valid loss: 6.920971597943987
Training loss: 0.9599181413650513 / Valid loss: 8.33665433157058
Training loss: 1.3180525302886963 / Valid loss: 6.999262196677072
Training loss: 1.2455828189849854 / Valid loss: 9.074341492425829

Epoch: 32
Training loss: 0.6012609004974365 / Valid loss: 7.2263910157339915
Training loss: 0.5022282600402832 / Valid loss: 7.165269002460298
Training loss: 0.662506103515625 / Valid loss: 7.188229270208449
Training loss: 0.8724859356880188 / Valid loss: 7.062272457849412
Training loss: 1.1619799137115479 / Valid loss: 8.443371786390031

Epoch: 33
Training loss: 0.6405733823776245 / Valid loss: 7.059629771822975
Training loss: 0.5651752352714539 / Valid loss: 7.54728361311413
Training loss: 1.0588269233703613 / Valid loss: 6.986931714557466
Training loss: 0.7190341949462891 / Valid loss: 7.181868948255267
Training loss: 1.0092896223068237 / Valid loss: 7.268625500088646

Epoch: 34
Training loss: 0.7248278856277466 / Valid loss: 7.323709728604271
Training loss: 1.3226895332336426 / Valid loss: 7.3575912203107565
Training loss: 0.5890219807624817 / Valid loss: 7.0245438121614
Training loss: 1.020698070526123 / Valid loss: 7.50722633089338
Training loss: 0.7777308225631714 / Valid loss: 7.167212418147495

Epoch: 35
Training loss: 0.9040263295173645 / Valid loss: 7.090397866566976
Training loss: 0.9612382650375366 / Valid loss: 7.139593662534441
Training loss: 0.8291218280792236 / Valid loss: 7.078316947392055
Training loss: 0.7749821543693542 / Valid loss: 7.595890031542097
Training loss: 1.2079875469207764 / Valid loss: 7.855515193939209

Epoch: 36
Training loss: 0.9758793115615845 / Valid loss: 7.121148381914411
Training loss: 1.1229145526885986 / Valid loss: 7.02674674987793
Training loss: 0.7529170513153076 / Valid loss: 7.114006814502535
Training loss: 0.651924192905426 / Valid loss: 7.120406573159354
Training loss: 0.9762295484542847 / Valid loss: 7.43028557187035

Epoch: 37
Training loss: 1.395491123199463 / Valid loss: 7.1707258179074245
Training loss: 0.9791371822357178 / Valid loss: 7.246573900041126
Training loss: 1.083543300628662 / Valid loss: 7.212270863850912
Training loss: 0.5844866037368774 / Valid loss: 7.182545952569871
Training loss: 0.7231588363647461 / Valid loss: 7.2324102447146466

Epoch: 38
Training loss: 0.5437463521957397 / Valid loss: 7.076689397721063
Training loss: 0.7325360774993896 / Valid loss: 7.071402590615409
Training loss: 0.5430324077606201 / Valid loss: 7.259533418927874
Training loss: 0.943889856338501 / Valid loss: 7.187190008163452
Training loss: 0.723518967628479 / Valid loss: 7.238671184721447

Epoch: 39
Training loss: 0.6689614057540894 / Valid loss: 7.166225478762672
Training loss: 0.6974279284477234 / Valid loss: 7.119683951423282
Training loss: 0.7966050505638123 / Valid loss: 7.691759929202852
Training loss: 0.8888695240020752 / Valid loss: 7.258959679376511
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1900): 5.391448016393753
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.62260627746582 / Valid loss: 16.113277962094262
Model is saved in epoch 0, overall batch: 0
Training loss: 11.979259490966797 / Valid loss: 11.801675324212937
Model is saved in epoch 0, overall batch: 100
Training loss: 8.571704864501953 / Valid loss: 7.76384631565639
Model is saved in epoch 0, overall batch: 200
Training loss: 6.487584114074707 / Valid loss: 5.887881328946068
Model is saved in epoch 0, overall batch: 300
Training loss: 9.437612533569336 / Valid loss: 5.69392899104527
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.416978359222412 / Valid loss: 5.87570192927406
Training loss: 5.683416843414307 / Valid loss: 5.842955010277884
Training loss: 6.8402228355407715 / Valid loss: 5.722472749437604
Training loss: 6.007376670837402 / Valid loss: 5.91313378016154
Training loss: 5.194140911102295 / Valid loss: 5.563268227804275
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 4.803097724914551 / Valid loss: 5.588917927514939
Training loss: 4.639689922332764 / Valid loss: 5.750391717184158
Training loss: 5.433716773986816 / Valid loss: 5.578525243486677
Training loss: 5.839730262756348 / Valid loss: 5.575306036358788
Training loss: 5.520908355712891 / Valid loss: 5.621869963691348

Epoch: 3
Training loss: 3.633943557739258 / Valid loss: 5.912793104989188
Training loss: 4.882447719573975 / Valid loss: 5.621377863202777
Training loss: 4.23842716217041 / Valid loss: 5.65698291460673
Training loss: 5.461603164672852 / Valid loss: 5.781912220092047
Training loss: 4.633998870849609 / Valid loss: 5.541519087836856
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.2493438720703125 / Valid loss: 5.608173465728759
Training loss: 4.018077850341797 / Valid loss: 5.598868142990839
Training loss: 3.909425735473633 / Valid loss: 5.725475159145537
Training loss: 4.822460174560547 / Valid loss: 5.764869163149879
Training loss: 7.122485637664795 / Valid loss: 5.779539053780692

Epoch: 5
Training loss: 3.655163288116455 / Valid loss: 5.684120055607387
Training loss: 3.3412866592407227 / Valid loss: 5.744381268819173
Training loss: 5.450267791748047 / Valid loss: 5.7041474592118036
Training loss: 5.6533203125 / Valid loss: 5.723937852042062
Training loss: 6.602140426635742 / Valid loss: 6.487933703831264

Epoch: 6
Training loss: 4.038620471954346 / Valid loss: 5.733348190216791
Training loss: 3.5736048221588135 / Valid loss: 5.952109523046584
Training loss: 4.189662456512451 / Valid loss: 5.708843181246803
Training loss: 3.685299873352051 / Valid loss: 5.912609084447225
Training loss: 3.1456172466278076 / Valid loss: 5.719413341794695

Epoch: 7
Training loss: 2.5647072792053223 / Valid loss: 6.313677599316551
Training loss: 3.3238778114318848 / Valid loss: 6.24087823232015
Training loss: 3.899127244949341 / Valid loss: 5.913767346881685
Training loss: 4.127683639526367 / Valid loss: 5.9058012144906185
Training loss: 4.169991493225098 / Valid loss: 5.904907099405924

Epoch: 8
Training loss: 3.4514660835266113 / Valid loss: 5.864300920849755
Training loss: 3.2069101333618164 / Valid loss: 5.910395256678263
Training loss: 4.452228546142578 / Valid loss: 6.072627063024612
Training loss: 3.819819450378418 / Valid loss: 6.811976696196057
Training loss: 3.3536229133605957 / Valid loss: 5.869156079065232

Epoch: 9
Training loss: 3.2316431999206543 / Valid loss: 5.960912631806873
Training loss: 3.5391201972961426 / Valid loss: 5.979906740642729
Training loss: 3.4536819458007812 / Valid loss: 6.085764451253982
Training loss: 2.669459104537964 / Valid loss: 6.1532859620593845

Epoch: 10
Training loss: 2.8202178478240967 / Valid loss: 5.875541909535726
Training loss: 2.703352689743042 / Valid loss: 6.08471641994658
Training loss: 3.51365327835083 / Valid loss: 6.346781671614874
Training loss: 3.2612063884735107 / Valid loss: 6.177292076746623
Training loss: 2.908527374267578 / Valid loss: 6.079802204313732

Epoch: 11
Training loss: 3.4064536094665527 / Valid loss: 6.289931903566633
Training loss: 3.4152162075042725 / Valid loss: 6.163586761837914
Training loss: 2.761538028717041 / Valid loss: 6.559638638723464
Training loss: 2.553500175476074 / Valid loss: 6.1565135456266855
Training loss: 2.996272563934326 / Valid loss: 6.456380015327817

Epoch: 12
Training loss: 3.1073319911956787 / Valid loss: 6.136265920457386
Training loss: 2.908492088317871 / Valid loss: 6.223529102688744
Training loss: 3.448554277420044 / Valid loss: 6.226528855732509
Training loss: 2.788163661956787 / Valid loss: 6.184953185490199
Training loss: 2.1950554847717285 / Valid loss: 6.185787403015864

Epoch: 13
Training loss: 2.4872307777404785 / Valid loss: 6.189204322724115
Training loss: 2.3806159496307373 / Valid loss: 6.370605123610724
Training loss: 2.290684223175049 / Valid loss: 6.252781631833031
Training loss: 1.887326955795288 / Valid loss: 7.495990153721401
Training loss: 2.375631809234619 / Valid loss: 6.29322661899385

Epoch: 14
Training loss: 1.9674733877182007 / Valid loss: 6.368952242533366
Training loss: 2.603881597518921 / Valid loss: 6.354084514436268
Training loss: 3.2479541301727295 / Valid loss: 6.711589917682466
Training loss: 2.7085342407226562 / Valid loss: 6.5162290096282955
Training loss: 2.6918482780456543 / Valid loss: 6.641325494221278

Epoch: 15
Training loss: 1.6497082710266113 / Valid loss: 6.392563674563453
Training loss: 3.139939785003662 / Valid loss: 6.335369516554333
Training loss: 2.8699216842651367 / Valid loss: 6.892810076758975
Training loss: 3.008089542388916 / Valid loss: 6.5785511788867765
Training loss: 2.3188958168029785 / Valid loss: 6.437361344837007

Epoch: 16
Training loss: 2.8781304359436035 / Valid loss: 6.360921637217204
Training loss: 1.9679921865463257 / Valid loss: 6.903385602860224
Training loss: 1.551413893699646 / Valid loss: 6.440324603943598
Training loss: 2.895613431930542 / Valid loss: 6.35850628671192
Training loss: 2.1725282669067383 / Valid loss: 7.372233095623198

Epoch: 17
Training loss: 2.284111499786377 / Valid loss: 6.487096632094611
Training loss: 1.716086506843567 / Valid loss: 6.860002422332764
Training loss: 1.6651923656463623 / Valid loss: 6.753741745721726
Training loss: 4.2849555015563965 / Valid loss: 6.513974253336588
Training loss: 2.1070775985717773 / Valid loss: 6.715637161618187

Epoch: 18
Training loss: 1.7187936305999756 / Valid loss: 6.43716496967134
Training loss: 1.4273638725280762 / Valid loss: 6.560122471763974
Training loss: 1.9956097602844238 / Valid loss: 6.522158645448231
Training loss: 1.7900135517120361 / Valid loss: 7.118087119147891
Training loss: 2.144134759902954 / Valid loss: 6.543319774809338

Epoch: 19
Training loss: 1.0962051153182983 / Valid loss: 6.5644000575655985
Training loss: 1.7119550704956055 / Valid loss: 6.8379238333020895
Training loss: 2.2170488834381104 / Valid loss: 6.795428198859805
Training loss: 1.9863691329956055 / Valid loss: 6.842529031208583

Epoch: 20
Training loss: 1.7813324928283691 / Valid loss: 7.476414021991548
Training loss: 1.6875526905059814 / Valid loss: 6.796589002155122
Training loss: 1.6377761363983154 / Valid loss: 6.608085332598005
Training loss: 1.41656494140625 / Valid loss: 7.469978627704439
Training loss: 1.261569857597351 / Valid loss: 6.7403024991353355

Epoch: 21
Training loss: 1.808388352394104 / Valid loss: 6.763776275089809
Training loss: 1.735398292541504 / Valid loss: 6.852431696937198
Training loss: 1.6954700946807861 / Valid loss: 6.630180862971715
Training loss: 2.2808456420898438 / Valid loss: 6.780470275878907
Training loss: 1.7633965015411377 / Valid loss: 7.286041464124407

Epoch: 22
Training loss: 1.1769046783447266 / Valid loss: 6.764617420378185
Training loss: 1.303318977355957 / Valid loss: 7.18108294123695
Training loss: 1.6849608421325684 / Valid loss: 6.9872176806132
Training loss: 1.9779698848724365 / Valid loss: 6.641576621645973
Training loss: 1.6298829317092896 / Valid loss: 6.849347532363165

Epoch: 23
Training loss: 0.9259861707687378 / Valid loss: 7.988977221080235
Training loss: 1.8269574642181396 / Valid loss: 6.922491409665063
Training loss: 1.9539716243743896 / Valid loss: 7.1156837917509534
Training loss: 1.288459062576294 / Valid loss: 6.666042141687303
Training loss: 1.2255363464355469 / Valid loss: 6.908357897258941

Epoch: 24
Training loss: 1.0905766487121582 / Valid loss: 6.7374354907444545
Training loss: 1.652724266052246 / Valid loss: 6.807917785644531
Training loss: 1.5279631614685059 / Valid loss: 7.040756534394764
Training loss: 1.0791813135147095 / Valid loss: 6.857382717586699
Training loss: 1.341012954711914 / Valid loss: 6.995647130693708

Epoch: 25
Training loss: 1.0719633102416992 / Valid loss: 7.1437413760593955
Training loss: 0.9225133657455444 / Valid loss: 6.799162403742472
Training loss: 2.160750389099121 / Valid loss: 7.856520398457845
Training loss: 1.3605635166168213 / Valid loss: 6.989812051682245
Training loss: 1.3167202472686768 / Valid loss: 6.979274100349063

Epoch: 26
Training loss: 1.0315027236938477 / Valid loss: 7.383847000485375
Training loss: 0.8663671016693115 / Valid loss: 6.784333406175886
Training loss: 0.8862740397453308 / Valid loss: 7.042379665374756
Training loss: 1.2027008533477783 / Valid loss: 7.1288701420738585
Training loss: 0.8904181122779846 / Valid loss: 7.1930183456057595

Epoch: 27
Training loss: 1.185448169708252 / Valid loss: 6.776526714506604
Training loss: 1.1298643350601196 / Valid loss: 6.973247487204415
Training loss: 1.3611810207366943 / Valid loss: 7.044444218136015
Training loss: 0.9026778340339661 / Valid loss: 7.221193554287865
Training loss: 1.5482009649276733 / Valid loss: 6.768624015081496

Epoch: 28
Training loss: 0.8552480340003967 / Valid loss: 7.029259118579683
Training loss: 0.9685423970222473 / Valid loss: 6.871367631639753
Training loss: 1.0401304960250854 / Valid loss: 7.356214073726109
Training loss: 1.6142197847366333 / Valid loss: 7.184740511576335
Training loss: 1.1399493217468262 / Valid loss: 7.412990606398809

Epoch: 29
Training loss: 0.7383702993392944 / Valid loss: 6.903457469031924
Training loss: 0.7487705945968628 / Valid loss: 7.242253834860666
Training loss: 1.2821738719940186 / Valid loss: 7.612298493158249
Training loss: 1.328986406326294 / Valid loss: 6.958910824003674

Epoch: 30
Training loss: 0.6986196041107178 / Valid loss: 7.1697124572027295
Training loss: 0.9660648703575134 / Valid loss: 7.303940164475214
Training loss: 0.6780081987380981 / Valid loss: 7.305516474587577
Training loss: 0.8170185685157776 / Valid loss: 7.047660250890822
Training loss: 0.8102566003799438 / Valid loss: 7.003089600517637

Epoch: 31
Training loss: 0.9414084553718567 / Valid loss: 7.027400802430653
Training loss: 0.8269694447517395 / Valid loss: 7.021843773978097
Training loss: 0.8820092678070068 / Valid loss: 7.3798253513517835
Training loss: 1.1566122770309448 / Valid loss: 6.991948145911807
Training loss: 1.1664177179336548 / Valid loss: 9.277829079400925

Epoch: 32
Training loss: 0.6030250787734985 / Valid loss: 6.968000380198161
Training loss: 0.49470198154449463 / Valid loss: 7.352312782832555
Training loss: 0.6539671421051025 / Valid loss: 7.133299895695278
Training loss: 0.8153016567230225 / Valid loss: 7.016583710625058
Training loss: 1.1613743305206299 / Valid loss: 8.329531120118641

Epoch: 33
Training loss: 0.7441724538803101 / Valid loss: 6.983644767034622
Training loss: 0.5294507741928101 / Valid loss: 7.589499909537179
Training loss: 0.9130557775497437 / Valid loss: 6.985093784332276
Training loss: 0.6706579923629761 / Valid loss: 7.6826321919759115
Training loss: 1.003231406211853 / Valid loss: 7.278465870448521

Epoch: 34
Training loss: 0.680914044380188 / Valid loss: 7.573177482968285
Training loss: 1.4114646911621094 / Valid loss: 7.2027735369546075
Training loss: 0.6033448576927185 / Valid loss: 6.989014294033959
Training loss: 0.8674666881561279 / Valid loss: 7.143534555889311
Training loss: 0.8234559893608093 / Valid loss: 7.1979837871733165

Epoch: 35
Training loss: 0.9154959917068481 / Valid loss: 7.137860225495838
Training loss: 1.0586941242218018 / Valid loss: 7.142775145031157
Training loss: 0.8075853586196899 / Valid loss: 7.508151585715158
Training loss: 0.8281263113021851 / Valid loss: 7.9306891168866835
Training loss: 1.0302976369857788 / Valid loss: 7.261938394818987

Epoch: 36
Training loss: 0.8753843307495117 / Valid loss: 7.069456000555129
Training loss: 0.9500987529754639 / Valid loss: 6.979678015481858
Training loss: 0.8212612271308899 / Valid loss: 7.150335411798387
Training loss: 0.7634521722793579 / Valid loss: 7.129182497660319
Training loss: 0.941246747970581 / Valid loss: 7.1184480757940385

Epoch: 37
Training loss: 1.417555809020996 / Valid loss: 7.572351056053525
Training loss: 0.909891664981842 / Valid loss: 7.004059555417015
Training loss: 1.1634042263031006 / Valid loss: 7.084910474504743
Training loss: 0.4886934161186218 / Valid loss: 7.8284799802870975
Training loss: 0.8477529287338257 / Valid loss: 7.192045438857305

Epoch: 38
Training loss: 0.4917343556880951 / Valid loss: 7.205329935891288
Training loss: 0.6773078441619873 / Valid loss: 7.006552619025821
Training loss: 0.6515181660652161 / Valid loss: 7.2543528920128235
Training loss: 1.0283581018447876 / Valid loss: 7.184497746967134
Training loss: 0.6789833307266235 / Valid loss: 7.152125558399018

Epoch: 39
Training loss: 0.646602988243103 / Valid loss: 7.143210715339297
Training loss: 0.8015397787094116 / Valid loss: 7.097225977125622
Training loss: 0.740781843662262 / Valid loss: 7.492462873458862
Training loss: 0.75223308801651 / Valid loss: 7.226149745214553
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1900): 5.394606928598313
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)

Epoch: 0
Training loss: 19.044994354248047 / Valid loss: 13.129112924848284
Model is saved in epoch 0, overall batch: 0
Training loss: 6.268510341644287 / Valid loss: 5.8205156326293945
Model is saved in epoch 0, overall batch: 100
Training loss: 6.482045650482178 / Valid loss: 5.7145655087062295
Model is saved in epoch 0, overall batch: 200
Training loss: 6.4690752029418945 / Valid loss: 5.665916363398234
Model is saved in epoch 0, overall batch: 300
Training loss: 3.6255550384521484 / Valid loss: 5.637516623451596
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.443761825561523 / Valid loss: 5.609866176332746
Model is saved in epoch 1, overall batch: 500
Training loss: 4.7841267585754395 / Valid loss: 5.5921299866267615
Model is saved in epoch 1, overall batch: 600
Training loss: 7.218555450439453 / Valid loss: 5.586084322702317
Model is saved in epoch 1, overall batch: 700
Training loss: 4.095907688140869 / Valid loss: 5.571328869320097
Model is saved in epoch 1, overall batch: 800
Training loss: 4.74068546295166 / Valid loss: 5.552520856403169
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.120925426483154 / Valid loss: 5.546814684640793
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.0970916748046875 / Valid loss: 5.534237298511323
Model is saved in epoch 2, overall batch: 1100
Training loss: 6.080886363983154 / Valid loss: 5.5331135182153615
Model is saved in epoch 2, overall batch: 1200
Training loss: 3.88415265083313 / Valid loss: 5.579568043209258
Training loss: 4.344699859619141 / Valid loss: 5.516222374779837
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.916259765625 / Valid loss: 5.53867780140468
Training loss: 3.5796151161193848 / Valid loss: 5.518697895322528
Training loss: 7.339484214782715 / Valid loss: 5.511235825220743
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.794491291046143 / Valid loss: 5.514074486777896
Training loss: 5.8917107582092285 / Valid loss: 5.529173412777129

Epoch: 4
Training loss: 6.055139064788818 / Valid loss: 5.536274101620629
Training loss: 4.793487071990967 / Valid loss: 5.518998761404128
Training loss: 4.535486698150635 / Valid loss: 5.5861195314498175
Training loss: 4.011403560638428 / Valid loss: 5.534883851096744
Training loss: 5.778960227966309 / Valid loss: 5.55570441427685

Epoch: 5
Training loss: 7.063654899597168 / Valid loss: 5.516865762074788
Training loss: 4.327145576477051 / Valid loss: 5.539654397964478
Training loss: 4.952714443206787 / Valid loss: 5.595981927145095
Training loss: 5.049341201782227 / Valid loss: 5.501711470740182
Model is saved in epoch 5, overall batch: 2800
Training loss: 6.2344770431518555 / Valid loss: 5.510142964408511

Epoch: 6
Training loss: 5.0662384033203125 / Valid loss: 5.4874446596418105
Model is saved in epoch 6, overall batch: 3000
Training loss: 6.036526679992676 / Valid loss: 5.505000191643124
Training loss: 5.344539642333984 / Valid loss: 5.5053633735293435
Training loss: 4.25537633895874 / Valid loss: 5.524209683282034
Training loss: 6.428157806396484 / Valid loss: 5.505239736466181

Epoch: 7
Training loss: 4.6567277908325195 / Valid loss: 5.518920868919009
Training loss: 4.213170528411865 / Valid loss: 5.501404419399443
Training loss: 5.8086090087890625 / Valid loss: 5.505869574773879
Training loss: 5.825793743133545 / Valid loss: 5.5119145961034866
Training loss: 5.194054126739502 / Valid loss: 5.502476953324818

Epoch: 8
Training loss: 4.835441589355469 / Valid loss: 5.48497557867141
Model is saved in epoch 8, overall batch: 4000
Training loss: 4.053650856018066 / Valid loss: 5.498404793512254
Training loss: 4.095859050750732 / Valid loss: 5.493126862389701
Training loss: 5.022775650024414 / Valid loss: 5.499519100643339
Training loss: 2.6940689086914062 / Valid loss: 5.501133705320813

Epoch: 9
Training loss: 4.0135393142700195 / Valid loss: 5.4889376367841445
Training loss: 5.218151092529297 / Valid loss: 5.478999215080624
Model is saved in epoch 9, overall batch: 4600
Training loss: 5.491938591003418 / Valid loss: 5.5104269118536084
Training loss: 6.224542617797852 / Valid loss: 5.506803882689703

Epoch: 10
Training loss: 4.1341047286987305 / Valid loss: 5.487884723572504
Training loss: 4.633673667907715 / Valid loss: 5.527044861657279
Training loss: 5.282918930053711 / Valid loss: 5.526882793789818
Training loss: 5.384278297424316 / Valid loss: 5.491148147128877
Training loss: 4.473493576049805 / Valid loss: 5.516018792561122

Epoch: 11
Training loss: 5.273750305175781 / Valid loss: 5.489752204077584
Training loss: 5.445207595825195 / Valid loss: 5.495031992594401
Training loss: 5.2608208656311035 / Valid loss: 5.484736969357445
Training loss: 7.819828987121582 / Valid loss: 5.511496355420067
Training loss: 5.1529998779296875 / Valid loss: 5.519785049983433

Epoch: 12
Training loss: 5.55267858505249 / Valid loss: 5.487533539817447
Training loss: 5.931863784790039 / Valid loss: 5.513217551367624
Training loss: 3.0109381675720215 / Valid loss: 5.527037622815087
Training loss: 4.458852767944336 / Valid loss: 5.494683054515294
Training loss: 4.160806655883789 / Valid loss: 5.532824573062715

Epoch: 13
Training loss: 4.795235633850098 / Valid loss: 5.493209411984398
Training loss: 4.42280912399292 / Valid loss: 5.496695173354376
Training loss: 6.058815002441406 / Valid loss: 5.497058398383004
Training loss: 5.800127029418945 / Valid loss: 5.504653739929199
Training loss: 4.990700721740723 / Valid loss: 5.495164746329898

Epoch: 14
Training loss: 5.980562686920166 / Valid loss: 5.499074563525972
Training loss: 5.388096332550049 / Valid loss: 5.506236426035563
Training loss: 5.679879188537598 / Valid loss: 5.5305070945194785
Training loss: 3.6280717849731445 / Valid loss: 5.5039329392569405
Training loss: 6.403841495513916 / Valid loss: 5.555675760904948

Epoch: 15
Training loss: 5.6831159591674805 / Valid loss: 5.491502559752691
Training loss: 5.444645881652832 / Valid loss: 5.532701278868176
Training loss: 4.500951766967773 / Valid loss: 5.514413054784139
Training loss: 6.080845832824707 / Valid loss: 5.492932158424741
Training loss: 4.98651123046875 / Valid loss: 5.58355773062933

Epoch: 16
Training loss: 4.450146675109863 / Valid loss: 5.5850849583035425
Training loss: 4.064596176147461 / Valid loss: 5.515714961006528
Training loss: 5.195715427398682 / Valid loss: 5.500394485110328
Training loss: 5.634490966796875 / Valid loss: 5.51566132363819
Training loss: 4.791279315948486 / Valid loss: 5.508806492033459

Epoch: 17
Training loss: 6.07611608505249 / Valid loss: 5.508183349881853
Training loss: 4.807071208953857 / Valid loss: 5.497280806586856
Training loss: 4.749369144439697 / Valid loss: 5.494707318714687
Training loss: 4.445265769958496 / Valid loss: 5.527152386165801
Training loss: 6.917873382568359 / Valid loss: 5.500626055399577

Epoch: 18
Training loss: 5.5816264152526855 / Valid loss: 5.536000994273595
Training loss: 4.919910430908203 / Valid loss: 5.501510670071556
Training loss: 5.456050395965576 / Valid loss: 5.50867877914792
Training loss: 4.318901538848877 / Valid loss: 5.514297841844105
Training loss: 4.957220554351807 / Valid loss: 5.494834661483765

Epoch: 19
Training loss: 4.0728960037231445 / Valid loss: 5.50114909807841
Training loss: 4.450469017028809 / Valid loss: 5.5084503083002
Training loss: 4.642572402954102 / Valid loss: 5.5274520646958125
Training loss: 4.006056308746338 / Valid loss: 5.498200875236875

Epoch: 20
Training loss: 4.780317306518555 / Valid loss: 5.522902073178972
Training loss: 5.044918060302734 / Valid loss: 5.497278040931338
Training loss: 3.100444793701172 / Valid loss: 5.5013368152436755
Training loss: 3.4275035858154297 / Valid loss: 5.501223997842698
Training loss: 4.943571090698242 / Valid loss: 5.508132285163516

Epoch: 21
Training loss: 4.771770477294922 / Valid loss: 5.520339513960339
Training loss: 6.298006057739258 / Valid loss: 5.501045778819493
Training loss: 5.7888312339782715 / Valid loss: 5.497353424344744
Training loss: 7.512767791748047 / Valid loss: 5.526576314653669
Training loss: 6.246817111968994 / Valid loss: 5.510463725952875

Epoch: 22
Training loss: 5.530355453491211 / Valid loss: 5.504226539248512
Training loss: 4.263248443603516 / Valid loss: 5.511502613340105
Training loss: 6.270808219909668 / Valid loss: 5.508685245968047
Training loss: 5.725137710571289 / Valid loss: 5.523370204653059
Training loss: 4.258124351501465 / Valid loss: 5.509586518151419

Epoch: 23
Training loss: 4.622230052947998 / Valid loss: 5.502703628085908
Training loss: 6.170592308044434 / Valid loss: 5.519173799242292
Training loss: 5.101165771484375 / Valid loss: 5.529845737275624
Training loss: 5.174187660217285 / Valid loss: 5.507236637387957
Training loss: 7.119589805603027 / Valid loss: 5.510544229689098

Epoch: 24
Training loss: 6.778641700744629 / Valid loss: 5.504507360004244
Training loss: 5.259664535522461 / Valid loss: 5.521439434233166
Training loss: 5.107412338256836 / Valid loss: 5.5151001044682095
Training loss: 7.081623077392578 / Valid loss: 5.515357444399879
Training loss: 3.416452646255493 / Valid loss: 5.530098544983637

Epoch: 25
Training loss: 3.8697829246520996 / Valid loss: 5.512104568027315
Training loss: 3.0461795330047607 / Valid loss: 5.606189927600679
Training loss: 3.6160924434661865 / Valid loss: 5.5069590863727385
Training loss: 6.993940353393555 / Valid loss: 5.521378131139846
Training loss: 3.4260261058807373 / Valid loss: 5.587253861200242

Epoch: 26
Training loss: 5.084988594055176 / Valid loss: 5.536805845442272
Training loss: 5.9992241859436035 / Valid loss: 5.516297742298671
Training loss: 5.955102920532227 / Valid loss: 5.517522534869966
Training loss: 5.6151580810546875 / Valid loss: 5.511801111130487
Training loss: 5.491486549377441 / Valid loss: 5.518405600956508

Epoch: 27
Training loss: 7.488737106323242 / Valid loss: 5.564374574025472
Training loss: 3.9674577713012695 / Valid loss: 5.512214106605167
Training loss: 5.03518009185791 / Valid loss: 5.500758152916318
Training loss: 6.0767717361450195 / Valid loss: 5.533495262690953
Training loss: 5.380934715270996 / Valid loss: 5.520670409429641

Epoch: 28
Training loss: 4.2744364738464355 / Valid loss: 5.516549787067231
Training loss: 4.072740077972412 / Valid loss: 5.553780837286086
Training loss: 4.98328971862793 / Valid loss: 5.525267712275187
Training loss: 4.614954471588135 / Valid loss: 5.5224081221080965
Training loss: 4.936821937561035 / Valid loss: 5.507197604860578

Epoch: 29
Training loss: 6.33676815032959 / Valid loss: 5.527415600277129
Training loss: 4.81869649887085 / Valid loss: 5.567223864509946
Training loss: 6.137674331665039 / Valid loss: 5.540587041491554
Training loss: 6.442243576049805 / Valid loss: 5.528965171178182

Epoch: 30
Training loss: 5.802284240722656 / Valid loss: 5.517720444997152
Training loss: 5.423418998718262 / Valid loss: 5.5279397169748945
Training loss: 4.733315467834473 / Valid loss: 5.643057432628813
Training loss: 4.590944766998291 / Valid loss: 5.547746220089141
Training loss: 4.800641059875488 / Valid loss: 5.5132416952224

Epoch: 31
Training loss: 5.096055030822754 / Valid loss: 5.526119436536517
Training loss: 5.332942008972168 / Valid loss: 5.521084333601452
Training loss: 4.379380702972412 / Valid loss: 5.518132116681054
Training loss: 5.007322788238525 / Valid loss: 5.542387549082438
Training loss: 6.223616600036621 / Valid loss: 5.525224488122123

Epoch: 32
Training loss: 6.002929210662842 / Valid loss: 5.521798013505482
Training loss: 6.241118431091309 / Valid loss: 5.567694057737078
Training loss: 4.749967575073242 / Valid loss: 5.531640154974801
Training loss: 4.273419380187988 / Valid loss: 5.628891944885254
Training loss: 7.390601634979248 / Valid loss: 5.522909125827607

Epoch: 33
Training loss: 3.979701519012451 / Valid loss: 5.513824835277739
Training loss: 6.375411033630371 / Valid loss: 5.5398459797813775
Training loss: 5.3277387619018555 / Valid loss: 5.5338926542372935
Training loss: 5.723964214324951 / Valid loss: 5.5660005365099225
Training loss: 6.629269123077393 / Valid loss: 5.527509525844029

Epoch: 34
Training loss: 4.85612678527832 / Valid loss: 5.533028816041492
Training loss: 6.279994487762451 / Valid loss: 5.555670461200532
Training loss: 6.139142990112305 / Valid loss: 5.536818218231201
Training loss: 6.134142875671387 / Valid loss: 5.525154238655453
Training loss: 3.979020357131958 / Valid loss: 5.5308452946799145

Epoch: 35
Training loss: 5.268239974975586 / Valid loss: 5.525958892277309
Training loss: 4.516629219055176 / Valid loss: 5.535664910361881
Training loss: 5.780996799468994 / Valid loss: 5.52758130573091
Training loss: 4.945207595825195 / Valid loss: 5.526650453749157
Training loss: 4.417022228240967 / Valid loss: 5.539230065118699

Epoch: 36
Training loss: 3.4599626064300537 / Valid loss: 5.56433214914231
Training loss: 6.484819412231445 / Valid loss: 5.542939733323597
Training loss: 3.449998378753662 / Valid loss: 5.537906953266689
Training loss: 6.162271499633789 / Valid loss: 5.53772162482852
Training loss: 3.9243721961975098 / Valid loss: 5.5482591470082605

Epoch: 37
Training loss: 5.30237340927124 / Valid loss: 5.550707360676356
Training loss: 3.893655776977539 / Valid loss: 5.5844385623931885
Training loss: 4.670575141906738 / Valid loss: 5.56959540730431
Training loss: 5.773092269897461 / Valid loss: 5.543593238648914
Training loss: 5.506038188934326 / Valid loss: 5.550005206607637

Epoch: 38
Training loss: 5.291510581970215 / Valid loss: 5.540982464381627
Training loss: 5.561369895935059 / Valid loss: 5.562149658657256
Training loss: 5.51554536819458 / Valid loss: 5.535958017621722
Training loss: 4.773528575897217 / Valid loss: 5.550307017280942
Training loss: 4.122597694396973 / Valid loss: 5.538489005679176

Epoch: 39
Training loss: 4.301779270172119 / Valid loss: 5.537814835139684
Training loss: 5.4082560539245605 / Valid loss: 5.553432008198329
Training loss: 5.64979362487793 / Valid loss: 5.548604047866094
Training loss: 5.012576580047607 / Valid loss: 5.546905983062017
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 4600): 5.316951828911191
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)

Epoch: 0
Training loss: 19.044994354248047 / Valid loss: 13.129112924848284
Model is saved in epoch 0, overall batch: 0
Training loss: 6.268510341644287 / Valid loss: 5.820515648523966
Model is saved in epoch 0, overall batch: 100
Training loss: 6.4820451736450195 / Valid loss: 5.7145655132475355
Model is saved in epoch 0, overall batch: 200
Training loss: 6.4690752029418945 / Valid loss: 5.665916293007987
Model is saved in epoch 0, overall batch: 300
Training loss: 3.6255550384521484 / Valid loss: 5.637516641616822
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.443761825561523 / Valid loss: 5.609866176332746
Model is saved in epoch 1, overall batch: 500
Training loss: 4.784127235412598 / Valid loss: 5.59213004339309
Model is saved in epoch 1, overall batch: 600
Training loss: 7.218555450439453 / Valid loss: 5.586084372656686
Model is saved in epoch 1, overall batch: 700
Training loss: 4.095908164978027 / Valid loss: 5.5713289351690385
Model is saved in epoch 1, overall batch: 800
Training loss: 4.74068546295166 / Valid loss: 5.552520976747785
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.120925426483154 / Valid loss: 5.546814823150635
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.0970916748046875 / Valid loss: 5.534237380254837
Model is saved in epoch 2, overall batch: 1100
Training loss: 6.0808868408203125 / Valid loss: 5.533113656725202
Model is saved in epoch 2, overall batch: 1200
Training loss: 3.884152889251709 / Valid loss: 5.579568127223424
Training loss: 4.344699382781982 / Valid loss: 5.516222390674409
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.916260719299316 / Valid loss: 5.538677928561256
Training loss: 3.579615831375122 / Valid loss: 5.518697999772572
Training loss: 7.339484214782715 / Valid loss: 5.511235861551194
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.794491291046143 / Valid loss: 5.514074532190959
Training loss: 5.8917107582092285 / Valid loss: 5.529173519497826

Epoch: 4
Training loss: 6.055139064788818 / Valid loss: 5.536274122056507
Training loss: 4.793486595153809 / Valid loss: 5.518998747780209
Training loss: 4.535486698150635 / Valid loss: 5.586119585945493
Training loss: 4.0114030838012695 / Valid loss: 5.53488389878046
Training loss: 5.778959274291992 / Valid loss: 5.555704452877953

Epoch: 5
Training loss: 7.063657283782959 / Valid loss: 5.516865809758504
Training loss: 4.327147483825684 / Valid loss: 5.539654341198149
Training loss: 4.952715873718262 / Valid loss: 5.595982045219058
Training loss: 5.04934024810791 / Valid loss: 5.501711475281488
Model is saved in epoch 5, overall batch: 2800
Training loss: 6.234478950500488 / Valid loss: 5.510142928078061

Epoch: 6
Training loss: 5.066238880157471 / Valid loss: 5.487444555191766
Model is saved in epoch 6, overall batch: 3000
Training loss: 6.036528587341309 / Valid loss: 5.505000109899612
Training loss: 5.344541549682617 / Valid loss: 5.505363319033668
Training loss: 4.25537633895874 / Valid loss: 5.524209574290684
Training loss: 6.428156852722168 / Valid loss: 5.5052396592639745

Epoch: 7
Training loss: 4.656729698181152 / Valid loss: 5.518920730409168
Training loss: 4.21317195892334 / Valid loss: 5.501404339926584
Training loss: 5.8086090087890625 / Valid loss: 5.50586945215861
Training loss: 5.825795650482178 / Valid loss: 5.51191436676752
Training loss: 5.194056510925293 / Valid loss: 5.502476789837792

Epoch: 8
Training loss: 4.835441589355469 / Valid loss: 5.484975385665893
Model is saved in epoch 8, overall batch: 4000
Training loss: 4.053650379180908 / Valid loss: 5.4984046073186965
Training loss: 4.0958571434021 / Valid loss: 5.493126662572225
Training loss: 5.022775650024414 / Valid loss: 5.499518968945458
Training loss: 2.6940698623657227 / Valid loss: 5.501133550916399

Epoch: 9
Training loss: 4.013538360595703 / Valid loss: 5.488937391553606
Training loss: 5.218151569366455 / Valid loss: 5.478998908542452
Model is saved in epoch 9, overall batch: 4600
Training loss: 5.491941452026367 / Valid loss: 5.510426532654535
Training loss: 6.224543571472168 / Valid loss: 5.506803548903692

Epoch: 10
Training loss: 4.134108066558838 / Valid loss: 5.487884360268002
Training loss: 4.633676052093506 / Valid loss: 5.527044584637596
Training loss: 5.282920837402344 / Valid loss: 5.526882301058088
Training loss: 5.384279251098633 / Valid loss: 5.491147813342867
Training loss: 4.473493576049805 / Valid loss: 5.516018352054414

Epoch: 11
Training loss: 5.273754119873047 / Valid loss: 5.489751808983939
Training loss: 5.445207595825195 / Valid loss: 5.495031515757243
Training loss: 5.260824203491211 / Valid loss: 5.484736472084409
Training loss: 7.819828987121582 / Valid loss: 5.511495780944824
Training loss: 5.152999401092529 / Valid loss: 5.519784448260353

Epoch: 12
Training loss: 5.5526814460754395 / Valid loss: 5.487532972154163
Training loss: 5.9318647384643555 / Valid loss: 5.513216997328259
Training loss: 3.0109386444091797 / Valid loss: 5.527037145977928
Training loss: 4.458856582641602 / Valid loss: 5.494682352883475
Training loss: 4.1608076095581055 / Valid loss: 5.532823875972203

Epoch: 13
Training loss: 4.795238971710205 / Valid loss: 5.493208653586251
Training loss: 4.422812461853027 / Valid loss: 5.496694478534517
Training loss: 6.0588178634643555 / Valid loss: 5.497057653608776
Training loss: 5.800127983093262 / Valid loss: 5.504652872539702
Training loss: 4.990699768066406 / Valid loss: 5.495163942518689

Epoch: 14
Training loss: 5.980563163757324 / Valid loss: 5.499073757444109
Training loss: 5.38809871673584 / Valid loss: 5.506235615412394
Training loss: 5.679887771606445 / Valid loss: 5.530506061372303
Training loss: 3.6280758380889893 / Valid loss: 5.503932074138096
Training loss: 6.403855323791504 / Valid loss: 5.555674852643694

Epoch: 15
Training loss: 5.683121204376221 / Valid loss: 5.491501499357677
Training loss: 5.444643974304199 / Valid loss: 5.532700118564424
Training loss: 4.500955104827881 / Valid loss: 5.514411844526019
Training loss: 6.080852508544922 / Valid loss: 5.492931068511236
Training loss: 4.986511707305908 / Valid loss: 5.583556776955014

Epoch: 16
Training loss: 4.4501495361328125 / Valid loss: 5.585083759398687
Training loss: 4.0645976066589355 / Valid loss: 5.515713664463588
Training loss: 5.195717811584473 / Valid loss: 5.500393340701149
Training loss: 5.634492874145508 / Valid loss: 5.515660140627906
Training loss: 4.791281223297119 / Valid loss: 5.508805327188401

Epoch: 17
Training loss: 6.076117992401123 / Valid loss: 5.508182116917202
Training loss: 4.807073593139648 / Valid loss: 5.497279453277588
Training loss: 4.74937629699707 / Valid loss: 5.494705942698888
Training loss: 4.4452667236328125 / Valid loss: 5.527151080540248
Training loss: 6.917879104614258 / Valid loss: 5.5006246135348364

Epoch: 18
Training loss: 5.581629753112793 / Valid loss: 5.535999668212164
Training loss: 4.919914722442627 / Valid loss: 5.501509198688326
Training loss: 5.456051826477051 / Valid loss: 5.50867725781032
Training loss: 4.318902969360352 / Valid loss: 5.514296379543486
Training loss: 4.957221031188965 / Valid loss: 5.49483312198094

Epoch: 19
Training loss: 4.072896957397461 / Valid loss: 5.50114749726795
Training loss: 4.450474739074707 / Valid loss: 5.508448628016881
Training loss: 4.642573356628418 / Valid loss: 5.527450400307065
Training loss: 4.006061553955078 / Valid loss: 5.498199167705717

Epoch: 20
Training loss: 4.78032112121582 / Valid loss: 5.522900411060879
Training loss: 5.044919490814209 / Valid loss: 5.497276281175159
Training loss: 3.100447654724121 / Valid loss: 5.501334903353737
Training loss: 3.4275033473968506 / Valid loss: 5.501222092764718
Training loss: 4.943573951721191 / Valid loss: 5.508130393709455

Epoch: 21
Training loss: 4.771773338317871 / Valid loss: 5.520337368193127
Training loss: 6.298003196716309 / Valid loss: 5.501043721607753
Training loss: 5.78884220123291 / Valid loss: 5.497351410275414
Training loss: 7.5127668380737305 / Valid loss: 5.5265744572594055
Training loss: 6.246827125549316 / Valid loss: 5.510461605162847

Epoch: 22
Training loss: 5.530363082885742 / Valid loss: 5.504224493390038
Training loss: 4.263256549835205 / Valid loss: 5.5115005311511815
Training loss: 6.270822525024414 / Valid loss: 5.50868308203561
Training loss: 5.725142955780029 / Valid loss: 5.523368047532581
Training loss: 4.258123874664307 / Valid loss: 5.509584161213466

Epoch: 23
Training loss: 4.622236251831055 / Valid loss: 5.50270136878604
Training loss: 6.170591354370117 / Valid loss: 5.519171290170579
Training loss: 5.101165771484375 / Valid loss: 5.529843312218076
Training loss: 5.174196720123291 / Valid loss: 5.50723428499131
Training loss: 7.119588375091553 / Valid loss: 5.5105418159848165

Epoch: 24
Training loss: 6.778660774230957 / Valid loss: 5.504504957653228
Training loss: 5.259666919708252 / Valid loss: 5.521436847959246
Training loss: 5.107417106628418 / Valid loss: 5.515097486405145
Training loss: 7.081636428833008 / Valid loss: 5.515354787735712
Training loss: 3.4164538383483887 / Valid loss: 5.530095974604289

Epoch: 25
Training loss: 3.8697924613952637 / Valid loss: 5.5121019749414355
Training loss: 3.0461795330047607 / Valid loss: 5.606186685108003
Training loss: 3.6161062717437744 / Valid loss: 5.506956243515015
Training loss: 6.993943214416504 / Valid loss: 5.521375379108247
Training loss: 3.42602801322937 / Valid loss: 5.587250527881441

Epoch: 26
Training loss: 5.084988594055176 / Valid loss: 5.536803163800921
Training loss: 5.999223709106445 / Valid loss: 5.516294860839844
Training loss: 5.955114364624023 / Valid loss: 5.517519607998076
Training loss: 5.615159511566162 / Valid loss: 5.511798032124838
Training loss: 5.491483688354492 / Valid loss: 5.518402685437883

Epoch: 27
Training loss: 7.488736152648926 / Valid loss: 5.56437129066104
Training loss: 3.9674651622772217 / Valid loss: 5.512210966291882
Training loss: 5.035182476043701 / Valid loss: 5.500754996708461
Training loss: 6.076773643493652 / Valid loss: 5.5334920406341555
Training loss: 5.3809404373168945 / Valid loss: 5.5206671147119435

Epoch: 28
Training loss: 4.274433612823486 / Valid loss: 5.516546578634353
Training loss: 4.072742462158203 / Valid loss: 5.553777458554222
Training loss: 4.9832916259765625 / Valid loss: 5.52526418595087
Training loss: 4.614959716796875 / Valid loss: 5.522404579889207
Training loss: 4.936827659606934 / Valid loss: 5.5071940353938515

Epoch: 29
Training loss: 6.336784362792969 / Valid loss: 5.527412108012608
Training loss: 4.818702697753906 / Valid loss: 5.567220665159679
Training loss: 6.137681484222412 / Valid loss: 5.5405834129878455
Training loss: 6.442249298095703 / Valid loss: 5.528961703890846

Epoch: 30
Training loss: 5.802286624908447 / Valid loss: 5.517716786974952
Training loss: 5.423429489135742 / Valid loss: 5.527935970397222
Training loss: 4.733325958251953 / Valid loss: 5.643053388595581
Training loss: 4.590949058532715 / Valid loss: 5.547742289588565
Training loss: 4.8006486892700195 / Valid loss: 5.513237848735991

Epoch: 31
Training loss: 5.096055507659912 / Valid loss: 5.526115667252314
Training loss: 5.332947731018066 / Valid loss: 5.521080357687814
Training loss: 4.3793792724609375 / Valid loss: 5.518128024964105
Training loss: 5.007328510284424 / Valid loss: 5.542383352915446
Training loss: 6.223628520965576 / Valid loss: 5.525220353262765

Epoch: 32
Training loss: 6.002939224243164 / Valid loss: 5.521793833233061
Training loss: 6.241124153137207 / Valid loss: 5.567689321154639
Training loss: 4.749970436096191 / Valid loss: 5.531635815756662
Training loss: 4.2734293937683105 / Valid loss: 5.6288875897725426
Training loss: 7.390602111816406 / Valid loss: 5.522904748008365

Epoch: 33
Training loss: 3.9797043800354004 / Valid loss: 5.513820534660702
Training loss: 6.375428676605225 / Valid loss: 5.539841552007766
Training loss: 5.3277387619018555 / Valid loss: 5.533888194674537
Training loss: 5.723975658416748 / Valid loss: 5.565996401650565
Training loss: 6.629273414611816 / Valid loss: 5.527504968643188

Epoch: 34
Training loss: 4.856145858764648 / Valid loss: 5.533024183909098
Training loss: 6.27999210357666 / Valid loss: 5.555665860857283
Training loss: 6.139143943786621 / Valid loss: 5.536813706443423
Training loss: 6.134150981903076 / Valid loss: 5.525149479366484
Training loss: 3.979024887084961 / Valid loss: 5.530840356009347

Epoch: 35
Training loss: 5.268247127532959 / Valid loss: 5.5259540739513575
Training loss: 4.516647815704346 / Valid loss: 5.535660071600051
Training loss: 5.781004428863525 / Valid loss: 5.527576348895118
Training loss: 4.945219993591309 / Valid loss: 5.526645392463321
Training loss: 4.417015075683594 / Valid loss: 5.539224901653472

Epoch: 36
Training loss: 3.4599692821502686 / Valid loss: 5.564326833543324
Training loss: 6.48482608795166 / Valid loss: 5.542934549422491
Training loss: 3.4500036239624023 / Valid loss: 5.537901655832926
Training loss: 6.162280082702637 / Valid loss: 5.5377164068676175
Training loss: 3.924363374710083 / Valid loss: 5.548254112970262

Epoch: 37
Training loss: 5.302372932434082 / Valid loss: 5.5507019474392845
Training loss: 3.893665313720703 / Valid loss: 5.58443326723008
Training loss: 4.670593738555908 / Valid loss: 5.569590032668341
Training loss: 5.773094654083252 / Valid loss: 5.543587800434658
Training loss: 5.506036758422852 / Valid loss: 5.54999969573248

Epoch: 38
Training loss: 5.29150915145874 / Valid loss: 5.540976712817238
Training loss: 5.561371326446533 / Valid loss: 5.562143879845029
Training loss: 5.515566825866699 / Valid loss: 5.535952683857509
Training loss: 4.773533344268799 / Valid loss: 5.550301145371937
Training loss: 4.1226019859313965 / Valid loss: 5.538483111063639

Epoch: 39
Training loss: 4.301782608032227 / Valid loss: 5.537809044974191
Training loss: 5.40826416015625 / Valid loss: 5.5534259251185825
Training loss: 5.649791717529297 / Valid loss: 5.548598041988554
Training loss: 5.012590408325195 / Valid loss: 5.546900068010602
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 4600): 5.316951529184977
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.8549747467041 / Valid loss: 16.974769047328405
Model is saved in epoch 0, overall batch: 0
Training loss: 13.96962833404541 / Valid loss: 11.932385876065208
Model is saved in epoch 0, overall batch: 100
Training loss: 7.026939868927002 / Valid loss: 7.509492842356364
Model is saved in epoch 0, overall batch: 200
Training loss: 5.229887962341309 / Valid loss: 5.984595482689993
Model is saved in epoch 0, overall batch: 300
Training loss: 6.353970050811768 / Valid loss: 5.807947649274554
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 3.773589611053467 / Valid loss: 5.667540706907
Model is saved in epoch 1, overall batch: 500
Training loss: 4.42996072769165 / Valid loss: 6.222200836454119
Training loss: 3.811056613922119 / Valid loss: 6.161750929696219
Training loss: 3.053370714187622 / Valid loss: 6.100447379975092
Training loss: 3.9689249992370605 / Valid loss: 6.293189001083374

Epoch: 2
Training loss: 2.730053424835205 / Valid loss: 6.401859733036586
Training loss: 1.6549737453460693 / Valid loss: 6.494033204941522
Training loss: 2.4211606979370117 / Valid loss: 6.7184971219017395
Training loss: 2.061708450317383 / Valid loss: 6.582486581802368
Training loss: 2.170557737350464 / Valid loss: 6.37138108980088

Epoch: 3
Training loss: 2.2084319591522217 / Valid loss: 6.661845416114444
Training loss: 2.039992332458496 / Valid loss: 6.60088370186942
Training loss: 1.8702974319458008 / Valid loss: 6.7366766884213405
Training loss: 1.3679521083831787 / Valid loss: 6.890186745779855
Training loss: 1.6138741970062256 / Valid loss: 6.644277213868641

Epoch: 4
Training loss: 1.7200782299041748 / Valid loss: 6.730484226771764
Training loss: 1.3341643810272217 / Valid loss: 6.811494775045485
Training loss: 0.8126428127288818 / Valid loss: 6.694632707323347
Training loss: 1.234389305114746 / Valid loss: 6.872702723457699
Training loss: 2.4009461402893066 / Valid loss: 6.889894803365071

Epoch: 5
Training loss: 0.7232390642166138 / Valid loss: 6.8080869424910775
Training loss: 0.6595641374588013 / Valid loss: 6.854381293342227
Training loss: 1.7364425659179688 / Valid loss: 6.893139503115699
Training loss: 1.5202198028564453 / Valid loss: 6.945148268200103
Training loss: 1.0105502605438232 / Valid loss: 7.081467483157203

Epoch: 6
Training loss: 0.7913210391998291 / Valid loss: 6.759181658426921
Training loss: 0.7923386096954346 / Valid loss: 6.771300134204683
Training loss: 0.6582398414611816 / Valid loss: 6.865367464792161
Training loss: 1.100479006767273 / Valid loss: 6.97673867997669
Training loss: 0.902996301651001 / Valid loss: 7.041066601162865

Epoch: 7
Training loss: 0.7045547366142273 / Valid loss: 6.797640650612967
Training loss: 0.8662700653076172 / Valid loss: 6.908676581155686
Training loss: 0.4174552261829376 / Valid loss: 6.842948999859038
Training loss: 0.5487631559371948 / Valid loss: 7.133337842850458
Training loss: 0.7183629274368286 / Valid loss: 7.036370018550328

Epoch: 8
Training loss: 0.962117612361908 / Valid loss: 6.988359385445005
Training loss: 0.8230038285255432 / Valid loss: 6.834904007684617
Training loss: 0.5811324119567871 / Valid loss: 6.9924082211085725
Training loss: 0.9246580600738525 / Valid loss: 6.8792292095365974
Training loss: 0.5284697413444519 / Valid loss: 6.948527681259882

Epoch: 9
Training loss: 0.3660949170589447 / Valid loss: 6.788198693593343
Training loss: 1.0461244583129883 / Valid loss: 6.822217939013527
Training loss: 1.13827645778656 / Valid loss: 6.898030898684547
Training loss: 0.5707057118415833 / Valid loss: 6.874756136394683

Epoch: 10
Training loss: 0.6397074460983276 / Valid loss: 6.714658096858433
Training loss: 0.5138578414916992 / Valid loss: 6.839397280556815
Training loss: 0.5082765817642212 / Valid loss: 6.871613913490659
Training loss: 0.5378353595733643 / Valid loss: 6.8626499857221335
Training loss: 0.3901427984237671 / Valid loss: 6.7626752807980495

Epoch: 11
Training loss: 0.46203652024269104 / Valid loss: 6.981022562299456
Training loss: 0.39341360330581665 / Valid loss: 6.953226407368978
Training loss: 0.26623910665512085 / Valid loss: 6.811293869926816
Training loss: 0.7252601385116577 / Valid loss: 6.8185479482014975
Training loss: 0.4471498131752014 / Valid loss: 6.956808623813448

Epoch: 12
Training loss: 0.3791780471801758 / Valid loss: 6.8764405023484
Training loss: 0.33632606267929077 / Valid loss: 6.8381615729559035
Training loss: 0.40234485268592834 / Valid loss: 6.825390511467344
Training loss: 0.6113224625587463 / Valid loss: 7.071878701164609
Training loss: 0.5121588706970215 / Valid loss: 6.983513318924677

Epoch: 13
Training loss: 0.5181628465652466 / Valid loss: 6.874415116083054
Training loss: 0.2841576635837555 / Valid loss: 6.882711805616107
Training loss: 0.28228825330734253 / Valid loss: 6.796470991770426
Training loss: 0.44527286291122437 / Valid loss: 7.033678574789138
Training loss: 0.7262670397758484 / Valid loss: 6.831825642358689

Epoch: 14
Training loss: 0.42486584186553955 / Valid loss: 6.875460270472935
Training loss: 0.5629675388336182 / Valid loss: 6.7810702187674385
Training loss: 0.386993944644928 / Valid loss: 7.0293294702257425
Training loss: 0.4425731897354126 / Valid loss: 6.889694279716128
Training loss: 0.5111596584320068 / Valid loss: 6.886731402079264

Epoch: 15
Training loss: 0.21653884649276733 / Valid loss: 6.805733029047648
Training loss: 0.38869601488113403 / Valid loss: 6.847716013590495
Training loss: 0.3845527768135071 / Valid loss: 6.887755303155808
Training loss: 0.3671751618385315 / Valid loss: 6.9443237667992
Training loss: 0.31838154792785645 / Valid loss: 6.982695886066982

Epoch: 16
Training loss: 0.24128147959709167 / Valid loss: 6.820712666284471
Training loss: 0.7222154140472412 / Valid loss: 6.908604167756581
Training loss: 0.416519433259964 / Valid loss: 7.0592114448547365
Training loss: 0.49863624572753906 / Valid loss: 7.070145109721593
Training loss: 0.4504089653491974 / Valid loss: 6.935830976849511

Epoch: 17
Training loss: 0.5156701803207397 / Valid loss: 6.985035305931454
Training loss: 0.1796080768108368 / Valid loss: 6.886530072348458
Training loss: 0.28460341691970825 / Valid loss: 6.784751151856922
Training loss: 0.25598302483558655 / Valid loss: 6.809001570656186
Training loss: 0.4747069478034973 / Valid loss: 6.812047935667492

Epoch: 18
Training loss: 0.4359683096408844 / Valid loss: 6.885851312818981
Training loss: 0.48908233642578125 / Valid loss: 6.983775497618176
Training loss: 0.1868511289358139 / Valid loss: 7.085348374502999
Training loss: 0.1604190617799759 / Valid loss: 6.881528241293771
Training loss: 0.33009952306747437 / Valid loss: 6.761141098113287

Epoch: 19
Training loss: 0.38465821743011475 / Valid loss: 6.8104134287152975
Training loss: 0.1966273933649063 / Valid loss: 6.845876062484015
Training loss: 0.5568547248840332 / Valid loss: 6.871862143561954
Training loss: 0.3409462571144104 / Valid loss: 6.937699084054856

Epoch: 20
Training loss: 0.22443214058876038 / Valid loss: 6.974955940246582
Training loss: 0.8298752307891846 / Valid loss: 6.825992588769822
Training loss: 0.4374881386756897 / Valid loss: 7.067449079241071
Training loss: 0.39287829399108887 / Valid loss: 6.819914533978417
Training loss: 0.22177501022815704 / Valid loss: 7.00377539225987

Epoch: 21
Training loss: 0.5696954727172852 / Valid loss: 7.051890591212682
Training loss: 0.5311688780784607 / Valid loss: 6.941854050045921
Training loss: 0.20971344411373138 / Valid loss: 6.8748203572772795
Training loss: 0.18545615673065186 / Valid loss: 6.92478688785008
Training loss: 0.2007129192352295 / Valid loss: 6.931965015048072

Epoch: 22
Training loss: 0.2286049723625183 / Valid loss: 7.03446455683027
Training loss: 0.4473211169242859 / Valid loss: 6.907368006025042
Training loss: 0.596305251121521 / Valid loss: 7.059801764715285
Training loss: 0.1882912814617157 / Valid loss: 6.911761840184529
Training loss: 0.4502078890800476 / Valid loss: 6.910237609772455

Epoch: 23
Training loss: 0.43237268924713135 / Valid loss: 6.887611909139724
Training loss: 0.3731710612773895 / Valid loss: 6.982212786447434
Training loss: 0.25550517439842224 / Valid loss: 6.8952160222189764
Training loss: 0.21175917983055115 / Valid loss: 6.864488483610607
Training loss: 0.26271212100982666 / Valid loss: 6.92677499680292

Epoch: 24
Training loss: 0.17805179953575134 / Valid loss: 6.953252805982317
Training loss: 0.22969603538513184 / Valid loss: 6.807367772147769
Training loss: 0.21033765375614166 / Valid loss: 7.024310988471622
Training loss: 0.16775605082511902 / Valid loss: 6.878455093928745
Training loss: 0.49908459186553955 / Valid loss: 6.908572128840855

Epoch: 25
Training loss: 0.24523970484733582 / Valid loss: 7.042650013878232
Training loss: 0.26725947856903076 / Valid loss: 6.957899288904099
Training loss: 0.2511305809020996 / Valid loss: 6.898936827977498
Training loss: 0.18283960223197937 / Valid loss: 6.943476972125826
Training loss: 0.25116050243377686 / Valid loss: 6.998228699820382

Epoch: 26
Training loss: 0.1638905107975006 / Valid loss: 6.827079772949219
Training loss: 0.6310980319976807 / Valid loss: 6.784702646164667
Training loss: 0.15258458256721497 / Valid loss: 6.921503439403716
Training loss: 0.22920949757099152 / Valid loss: 7.091877578553699
Training loss: 0.26612359285354614 / Valid loss: 7.03202797571818

Epoch: 27
Training loss: 0.1043216735124588 / Valid loss: 6.9108947708493185
Training loss: 0.22139763832092285 / Valid loss: 7.1278859297434485
Training loss: 0.390315979719162 / Valid loss: 6.9367271968296595
Training loss: 0.15940353274345398 / Valid loss: 6.942324288686117
Training loss: 0.12748128175735474 / Valid loss: 7.055759879520961

Epoch: 28
Training loss: 0.17050188779830933 / Valid loss: 6.911635632742019
Training loss: 0.17568275332450867 / Valid loss: 6.889032216299148
Training loss: 0.5433291792869568 / Valid loss: 6.962091963631766
Training loss: 0.1752215027809143 / Valid loss: 6.886219839822679
Training loss: 0.1644473373889923 / Valid loss: 7.01680485861642

Epoch: 29
Training loss: 0.19860610365867615 / Valid loss: 6.913442734309605
Training loss: 0.19516737759113312 / Valid loss: 7.0246130534580775
Training loss: 0.18017424643039703 / Valid loss: 7.013794860385713
Training loss: 0.40308699011802673 / Valid loss: 6.943145102546328

Epoch: 30
Training loss: 0.1073141023516655 / Valid loss: 7.09362093153454
Training loss: 0.5590545535087585 / Valid loss: 7.000029600234258
Training loss: 0.3073664903640747 / Valid loss: 7.0249346778506325
Training loss: 0.2923542261123657 / Valid loss: 6.909485648927235
Training loss: 0.33443281054496765 / Valid loss: 6.977350416637602

Epoch: 31
Training loss: 0.12776674330234528 / Valid loss: 7.086714894430978
Training loss: 0.18158316612243652 / Valid loss: 6.972035267239526
Training loss: 0.29204821586608887 / Valid loss: 7.0939789999099006
Training loss: 0.08199670165777206 / Valid loss: 7.016810178756714
Training loss: 0.2604881227016449 / Valid loss: 7.084262684413365

Epoch: 32
Training loss: 0.24063202738761902 / Valid loss: 7.141737567810785
Training loss: 0.234274223446846 / Valid loss: 7.0331819579714825
Training loss: 0.2071111649274826 / Valid loss: 7.050380348023914
Training loss: 0.22041618824005127 / Valid loss: 6.876171557108561
Training loss: 0.13131260871887207 / Valid loss: 7.006029269808814

Epoch: 33
Training loss: 0.14103229343891144 / Valid loss: 6.9805631319681805
Training loss: 0.13580869138240814 / Valid loss: 6.896853397006081
Training loss: 0.11168328672647476 / Valid loss: 6.9228622118632
Training loss: 0.08689545094966888 / Valid loss: 6.845611231667655
Training loss: 0.17038866877555847 / Valid loss: 7.1044649941580635

Epoch: 34
Training loss: 0.1886788010597229 / Valid loss: 7.151003644579933
Training loss: 0.2085399329662323 / Valid loss: 7.022111670176188
Training loss: 0.15724869072437286 / Valid loss: 6.954712459019253
Training loss: 0.13877813518047333 / Valid loss: 6.912409251076834
Training loss: 0.26939666271209717 / Valid loss: 6.919051456451416

Epoch: 35
Training loss: 0.15313158929347992 / Valid loss: 6.937540817260742
Training loss: 0.11604170501232147 / Valid loss: 6.925864959898449
Training loss: 0.13419970870018005 / Valid loss: 6.977060828890119
Training loss: 0.19794024527072906 / Valid loss: 6.987786436080933
Training loss: 0.1780630499124527 / Valid loss: 7.0341133208501905

Epoch: 36
Training loss: 0.3160252869129181 / Valid loss: 6.971670055389405
Training loss: 0.5965589284896851 / Valid loss: 7.147240161895752
Training loss: 0.7073046565055847 / Valid loss: 6.9712114288693385
Training loss: 0.41634753346443176 / Valid loss: 7.050921535491943
Training loss: 0.18115264177322388 / Valid loss: 6.931022943769182

Epoch: 37
Training loss: 0.08028087764978409 / Valid loss: 6.911332409722465
Training loss: 0.1650528907775879 / Valid loss: 7.059201133818854
Training loss: 0.20936602354049683 / Valid loss: 6.935306746619088
Training loss: 0.5773135423660278 / Valid loss: 7.052749043419247
Training loss: 0.285707950592041 / Valid loss: 7.001244649432954

Epoch: 38
Training loss: 0.16202068328857422 / Valid loss: 7.03424585887364
Training loss: 0.1104569062590599 / Valid loss: 7.054315339951288
Training loss: 0.2690185308456421 / Valid loss: 6.925544629778181
Training loss: 0.24906687438488007 / Valid loss: 6.991949490138462
Training loss: 0.3112507462501526 / Valid loss: 6.955429808298747

Epoch: 39
Training loss: 0.0957954078912735 / Valid loss: 7.00460033416748
Training loss: 0.17185944318771362 / Valid loss: 6.996263876415434
Training loss: 0.10470522940158844 / Valid loss: 6.971412320364089
Training loss: 0.09794867038726807 / Valid loss: 6.941170692443848
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.606704682395572
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.8549747467041 / Valid loss: 16.972832034883044
Model is saved in epoch 0, overall batch: 0
Training loss: 13.647671699523926 / Valid loss: 11.816990747905912
Model is saved in epoch 0, overall batch: 100
Training loss: 7.404419422149658 / Valid loss: 7.568688896724156
Model is saved in epoch 0, overall batch: 200
Training loss: 5.127141952514648 / Valid loss: 6.032643479392642
Model is saved in epoch 0, overall batch: 300
Training loss: 5.981081008911133 / Valid loss: 5.764338911147345
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.340922832489014 / Valid loss: 5.591074716477167
Model is saved in epoch 1, overall batch: 500
Training loss: 4.126700401306152 / Valid loss: 6.150216899599347
Training loss: 4.000586986541748 / Valid loss: 6.07281297047933
Training loss: 3.4333062171936035 / Valid loss: 5.932096286047073
Training loss: 4.244292736053467 / Valid loss: 6.127997616359166

Epoch: 2
Training loss: 2.92983341217041 / Valid loss: 6.29200116339184
Training loss: 1.7965502738952637 / Valid loss: 6.404367365155902
Training loss: 1.883266806602478 / Valid loss: 6.742142559233166
Training loss: 2.2423911094665527 / Valid loss: 6.54614387466794
Training loss: 1.7019950151443481 / Valid loss: 6.342351622808547

Epoch: 3
Training loss: 2.0965676307678223 / Valid loss: 6.589468978700184
Training loss: 2.0136055946350098 / Valid loss: 6.606124564579555
Training loss: 1.549224615097046 / Valid loss: 6.825691827138265
Training loss: 1.339261531829834 / Valid loss: 6.8512919198899045
Training loss: 1.3663915395736694 / Valid loss: 6.542200256529308

Epoch: 4
Training loss: 1.314447045326233 / Valid loss: 6.858114664895194
Training loss: 0.9426013827323914 / Valid loss: 6.785247532526652
Training loss: 1.0152301788330078 / Valid loss: 6.698400511060442
Training loss: 1.1334953308105469 / Valid loss: 6.914201977139427
Training loss: 2.253722906112671 / Valid loss: 6.891275074368432

Epoch: 5
Training loss: 0.615575909614563 / Valid loss: 6.858668522607712
Training loss: 1.039097547531128 / Valid loss: 6.831661042712984
Training loss: 1.9612340927124023 / Valid loss: 6.895904441106887
Training loss: 1.6847676038742065 / Valid loss: 6.81324759892055
Training loss: 1.2359309196472168 / Valid loss: 7.128388532002767

Epoch: 6
Training loss: 0.6869102716445923 / Valid loss: 6.741835898444766
Training loss: 1.1195805072784424 / Valid loss: 6.71171388171968
Training loss: 0.7215290665626526 / Valid loss: 6.9973285856701075
Training loss: 1.355214238166809 / Valid loss: 6.995336555299305
Training loss: 1.086766242980957 / Valid loss: 6.930073558716547

Epoch: 7
Training loss: 0.8280007243156433 / Valid loss: 6.832260903858003
Training loss: 1.223882794380188 / Valid loss: 6.823358090718587
Training loss: 0.7231432795524597 / Valid loss: 6.93732408796038
Training loss: 0.7286882996559143 / Valid loss: 6.9887965928940545
Training loss: 0.8076030015945435 / Valid loss: 6.944858237675258

Epoch: 8
Training loss: 0.6887437105178833 / Valid loss: 7.008635475521996
Training loss: 0.7959907054901123 / Valid loss: 6.912453310830252
Training loss: 0.46107232570648193 / Valid loss: 6.924024061929612
Training loss: 1.0979349613189697 / Valid loss: 6.885131431761242
Training loss: 0.5594043731689453 / Valid loss: 6.935152819043114

Epoch: 9
Training loss: 0.601777970790863 / Valid loss: 6.881875210716611
Training loss: 1.0202486515045166 / Valid loss: 6.869155970073882
Training loss: 1.5370755195617676 / Valid loss: 6.834657541910807
Training loss: 0.40671107172966003 / Valid loss: 6.876993547167097

Epoch: 10
Training loss: 0.694146990776062 / Valid loss: 6.679365984598795
Training loss: 0.5514280796051025 / Valid loss: 6.791399329049247
Training loss: 0.4005957841873169 / Valid loss: 6.94297969681876
Training loss: 0.7639608979225159 / Valid loss: 6.788176999773298
Training loss: 0.5408332347869873 / Valid loss: 6.85706679934547

Epoch: 11
Training loss: 0.6312445998191833 / Valid loss: 6.791498888106573
Training loss: 0.37388646602630615 / Valid loss: 6.8086454346066425
Training loss: 0.3459557890892029 / Valid loss: 6.762081159864153
Training loss: 0.6525139212608337 / Valid loss: 6.836961816606068
Training loss: 0.6436294317245483 / Valid loss: 6.842946892692929

Epoch: 12
Training loss: 0.47153857350349426 / Valid loss: 6.662421380905878
Training loss: 0.40395450592041016 / Valid loss: 6.867382022312709
Training loss: 0.4076429605484009 / Valid loss: 6.865966047559466
Training loss: 0.5360982418060303 / Valid loss: 6.914915747869582
Training loss: 0.5165121555328369 / Valid loss: 6.978683557964507

Epoch: 13
Training loss: 0.6337191462516785 / Valid loss: 6.9580665247780935
Training loss: 0.31051105260849 / Valid loss: 6.868334479559035
Training loss: 0.393330454826355 / Valid loss: 6.849894496372768
Training loss: 0.5698186159133911 / Valid loss: 6.957068257104783
Training loss: 1.1477200984954834 / Valid loss: 6.7864200909932455

Epoch: 14
Training loss: 0.3765721321105957 / Valid loss: 6.7112477506910055
Training loss: 0.7406794428825378 / Valid loss: 6.882168147677467
Training loss: 0.34504395723342896 / Valid loss: 6.9791357131231395
Training loss: 0.6822687983512878 / Valid loss: 6.812162971496582
Training loss: 0.586000919342041 / Valid loss: 6.946560678027925

Epoch: 15
Training loss: 0.22643378376960754 / Valid loss: 6.819992213022141
Training loss: 0.44868484139442444 / Valid loss: 6.78439466158549
Training loss: 0.3898632824420929 / Valid loss: 6.867024235498338
Training loss: 0.403502881526947 / Valid loss: 6.947605982280913
Training loss: 0.4380931854248047 / Valid loss: 6.841575191134498

Epoch: 16
Training loss: 0.5485886335372925 / Valid loss: 6.875750001271566
Training loss: 0.7823197841644287 / Valid loss: 6.91151255653018
Training loss: 0.4930896759033203 / Valid loss: 7.050018896375383
Training loss: 0.6479690074920654 / Valid loss: 6.789296141124907
Training loss: 0.47015276551246643 / Valid loss: 7.012920942760649

Epoch: 17
Training loss: 0.7140692472457886 / Valid loss: 6.856340589977446
Training loss: 0.2939554452896118 / Valid loss: 6.834014091037568
Training loss: 0.5437642335891724 / Valid loss: 6.843541118076869
Training loss: 0.37306874990463257 / Valid loss: 6.8541556767054965
Training loss: 0.5347611308097839 / Valid loss: 6.767567611875988

Epoch: 18
Training loss: 0.3758806586265564 / Valid loss: 6.852362644104731
Training loss: 0.5642596483230591 / Valid loss: 6.834178892771403
Training loss: 0.4660700559616089 / Valid loss: 6.924992963245937
Training loss: 0.48565590381622314 / Valid loss: 6.881563150315058
Training loss: 0.3385198414325714 / Valid loss: 6.792926974523635

Epoch: 19
Training loss: 0.3515663743019104 / Valid loss: 6.88734458287557
Training loss: 0.22191017866134644 / Valid loss: 6.9354744774954655
Training loss: 0.8694108724594116 / Valid loss: 6.833233147575742
Training loss: 0.43671518564224243 / Valid loss: 6.75971060253325

Epoch: 20
Training loss: 0.4070107042789459 / Valid loss: 7.021335815248035
Training loss: 0.658024787902832 / Valid loss: 6.796641549609956
Training loss: 0.44732314348220825 / Valid loss: 6.926908002580915
Training loss: 0.676039457321167 / Valid loss: 6.7237367357526505
Training loss: 0.3093993067741394 / Valid loss: 7.003803139641171

Epoch: 21
Training loss: 0.5901188850402832 / Valid loss: 6.911573369162423
Training loss: 0.606593132019043 / Valid loss: 6.794892960502988
Training loss: 0.3493000566959381 / Valid loss: 6.842829681578136
Training loss: 0.23001539707183838 / Valid loss: 6.935336221967424
Training loss: 0.23240497708320618 / Valid loss: 6.832930832817441

Epoch: 22
Training loss: 0.3879798650741577 / Valid loss: 6.830565750031244
Training loss: 0.6689121127128601 / Valid loss: 6.843037062599546
Training loss: 0.5778406262397766 / Valid loss: 6.904582618531727
Training loss: 0.19937646389007568 / Valid loss: 6.831493334543137
Training loss: 0.34923046827316284 / Valid loss: 6.877113371803647

Epoch: 23
Training loss: 0.5902986526489258 / Valid loss: 6.816918754577637
Training loss: 0.472389280796051 / Valid loss: 6.869289057595389
Training loss: 0.3892342448234558 / Valid loss: 6.7538390931629
Training loss: 0.30095598101615906 / Valid loss: 6.824385061718169
Training loss: 0.556451141834259 / Valid loss: 6.916667461395264

Epoch: 24
Training loss: 0.24527424573898315 / Valid loss: 6.744153177170526
Training loss: 0.40138280391693115 / Valid loss: 6.8832287652151924
Training loss: 0.3384987413883209 / Valid loss: 6.896642339797247
Training loss: 0.2915390133857727 / Valid loss: 6.755749952225458
Training loss: 0.5963389277458191 / Valid loss: 6.735339546203614

Epoch: 25
Training loss: 0.6205034255981445 / Valid loss: 7.0201178028469995
Training loss: 0.39860135316848755 / Valid loss: 6.797230273201352
Training loss: 0.24846547842025757 / Valid loss: 6.758980342320034
Training loss: 0.24485836923122406 / Valid loss: 6.890868223281133
Training loss: 0.4225170612335205 / Valid loss: 6.897036534263974

Epoch: 26
Training loss: 0.2188425213098526 / Valid loss: 6.7074609438578285
Training loss: 0.7183660268783569 / Valid loss: 6.887646461668469
Training loss: 0.24379825592041016 / Valid loss: 6.879914208820888
Training loss: 0.35963648557662964 / Valid loss: 7.151512509300595
Training loss: 0.4255650043487549 / Valid loss: 6.949500951312837

Epoch: 27
Training loss: 0.18457293510437012 / Valid loss: 6.970796948387509
Training loss: 0.2404845952987671 / Valid loss: 7.0696284975324355
Training loss: 0.3589373230934143 / Valid loss: 7.050098637172154
Training loss: 0.16056454181671143 / Valid loss: 6.981004478817894
Training loss: 0.25874054431915283 / Valid loss: 6.979169039499192

Epoch: 28
Training loss: 0.43746715784072876 / Valid loss: 7.003673800967989
Training loss: 0.37913602590560913 / Valid loss: 6.990599455152239
Training loss: 0.7296979427337646 / Valid loss: 6.935297682171776
Training loss: 0.16755720973014832 / Valid loss: 6.94106912612915
Training loss: 0.27200543880462646 / Valid loss: 6.865859045301165

Epoch: 29
Training loss: 0.2840924859046936 / Valid loss: 6.8953325544084825
Training loss: 0.24425682425498962 / Valid loss: 6.837654749552409
Training loss: 0.2954826056957245 / Valid loss: 6.987378892444429
Training loss: 0.5320647954940796 / Valid loss: 6.859900306520008

Epoch: 30
Training loss: 0.267487108707428 / Valid loss: 7.015218148912703
Training loss: 0.6659557819366455 / Valid loss: 6.795310642605736
Training loss: 0.3515559434890747 / Valid loss: 6.947167528243292
Training loss: 0.32873088121414185 / Valid loss: 6.79254957380749
Training loss: 0.4087783098220825 / Valid loss: 6.752862771352132

Epoch: 31
Training loss: 0.319219708442688 / Valid loss: 6.999793695268177
Training loss: 0.26545655727386475 / Valid loss: 6.870786396662394
Training loss: 0.34763771295547485 / Valid loss: 7.023352264222645
Training loss: 0.1632736176252365 / Valid loss: 6.952883781705584
Training loss: 0.3137541711330414 / Valid loss: 7.018612135024298

Epoch: 32
Training loss: 0.2857760787010193 / Valid loss: 7.022929750170026
Training loss: 0.2345896065235138 / Valid loss: 7.028617427462623
Training loss: 0.25828951597213745 / Valid loss: 6.9422086920057025
Training loss: 0.38185906410217285 / Valid loss: 6.915837410518101
Training loss: 0.21463274955749512 / Valid loss: 6.954606642041887

Epoch: 33
Training loss: 0.20750974118709564 / Valid loss: 6.912514791034517
Training loss: 0.2954444885253906 / Valid loss: 6.804809222902571
Training loss: 0.20748800039291382 / Valid loss: 7.066426472436814
Training loss: 0.21728895604610443 / Valid loss: 6.925530687967936
Training loss: 0.24907620251178741 / Valid loss: 7.121434500103905

Epoch: 34
Training loss: 0.41395241022109985 / Valid loss: 7.033140965870449
Training loss: 0.16166727244853973 / Valid loss: 6.809786969139463
Training loss: 0.2476980984210968 / Valid loss: 6.889641028358823
Training loss: 0.1813468039035797 / Valid loss: 6.908883971259708
Training loss: 0.5819758772850037 / Valid loss: 6.915378856658935

Epoch: 35
Training loss: 0.41890281438827515 / Valid loss: 6.824618602934338
Training loss: 0.15237349271774292 / Valid loss: 6.84966215860276
Training loss: 0.3118339776992798 / Valid loss: 6.943165847233364
Training loss: 0.3239334225654602 / Valid loss: 7.014776997339158
Training loss: 0.27739936113357544 / Valid loss: 7.016688256036668

Epoch: 36
Training loss: 0.5124061107635498 / Valid loss: 6.905619902837844
Training loss: 0.42989930510520935 / Valid loss: 7.083842064085461
Training loss: 0.890791654586792 / Valid loss: 6.884828494843982
Training loss: 0.3475929796695709 / Valid loss: 6.933047580718994
Training loss: 0.2814868092536926 / Valid loss: 6.8278215181259885

Epoch: 37
Training loss: 0.25208431482315063 / Valid loss: 6.847883508318946
Training loss: 0.17226576805114746 / Valid loss: 7.031412424360003
Training loss: 0.34617429971694946 / Valid loss: 6.947485851106189
Training loss: 0.8586851954460144 / Valid loss: 7.018286539259411
Training loss: 0.4239364266395569 / Valid loss: 7.048894178299677

Epoch: 38
Training loss: 0.14976775646209717 / Valid loss: 6.991291563851493
Training loss: 0.13477908074855804 / Valid loss: 7.07116977373759
Training loss: 0.32396823167800903 / Valid loss: 6.8496794019426614
Training loss: 0.24224434792995453 / Valid loss: 7.0080574671427405
Training loss: 0.39082515239715576 / Valid loss: 6.991599087488083

Epoch: 39
Training loss: 0.40894579887390137 / Valid loss: 6.986893617539179
Training loss: 0.17576690018177032 / Valid loss: 7.074411996205648
Training loss: 0.28991609811782837 / Valid loss: 6.891296604701451
Training loss: 0.22703006863594055 / Valid loss: 6.943802842639742
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.529609155654907
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.313697814941406 / Valid loss: 15.73991907210577
Model is saved in epoch 0, overall batch: 0
Training loss: 8.40890121459961 / Valid loss: 9.4806762332008
Model is saved in epoch 0, overall batch: 100
Training loss: 4.984809875488281 / Valid loss: 6.232872719991775
Model is saved in epoch 0, overall batch: 200
Training loss: 3.5381689071655273 / Valid loss: 5.799423308599563
Model is saved in epoch 0, overall batch: 300
Training loss: 4.798099994659424 / Valid loss: 5.810558014824277

Epoch: 1
Training loss: 2.6791176795959473 / Valid loss: 5.723253969919114
Model is saved in epoch 1, overall batch: 500
Training loss: 3.695173740386963 / Valid loss: 5.991147717975434
Training loss: 4.615135192871094 / Valid loss: 5.954994383312407
Training loss: 3.6704447269439697 / Valid loss: 5.915320269266764
Training loss: 3.3417553901672363 / Valid loss: 5.991847238086518

Epoch: 2
Training loss: 1.4023618698120117 / Valid loss: 6.0816789150238035
Training loss: 1.4669214487075806 / Valid loss: 6.185417633964902
Training loss: 1.289663553237915 / Valid loss: 6.192259034656343
Training loss: 1.9312435388565063 / Valid loss: 6.186007694970994
Training loss: 2.37886643409729 / Valid loss: 6.164050113587153

Epoch: 3
Training loss: 1.6129900217056274 / Valid loss: 6.302328754606701
Training loss: 1.4630627632141113 / Valid loss: 6.351364730653309
Training loss: 1.689437747001648 / Valid loss: 6.301615315391904
Training loss: 2.1336328983306885 / Valid loss: 6.3544138885679695
Training loss: 2.385084629058838 / Valid loss: 6.385943249293736

Epoch: 4
Training loss: 1.0838344097137451 / Valid loss: 6.362908501852126
Training loss: 1.2128567695617676 / Valid loss: 6.408136783327375
Training loss: 1.3405842781066895 / Valid loss: 6.4409437792641775
Training loss: 1.3168511390686035 / Valid loss: 6.483259332747687
Training loss: 1.4557595252990723 / Valid loss: 6.478509367079962

Epoch: 5
Training loss: 0.7768599987030029 / Valid loss: 6.4534630116962255
Training loss: 1.006778359413147 / Valid loss: 6.480068297613235
Training loss: 1.1578760147094727 / Valid loss: 6.589505826859247
Training loss: 1.4145357608795166 / Valid loss: 6.497722089858282
Training loss: 1.1406645774841309 / Valid loss: 6.555000727517264

Epoch: 6
Training loss: 0.6252282857894897 / Valid loss: 6.490207663036528
Training loss: 0.9738853573799133 / Valid loss: 6.42975203423273
Training loss: 0.8263851404190063 / Valid loss: 6.570233472188314
Training loss: 0.681787371635437 / Valid loss: 6.649770623161679
Training loss: 0.801228404045105 / Valid loss: 6.678307542346773

Epoch: 7
Training loss: 0.6101146340370178 / Valid loss: 6.557184410095215
Training loss: 0.4471314549446106 / Valid loss: 6.6082686878386
Training loss: 0.727195143699646 / Valid loss: 6.614833009810675
Training loss: 0.7662248015403748 / Valid loss: 6.603225190298898
Training loss: 0.8377729654312134 / Valid loss: 6.533628829320272

Epoch: 8
Training loss: 0.6186429858207703 / Valid loss: 6.5926420030139745
Training loss: 0.6510307192802429 / Valid loss: 6.678785269601004
Training loss: 0.8761587142944336 / Valid loss: 6.588997379938761
Training loss: 0.4162680208683014 / Valid loss: 6.698398892084757
Training loss: 1.440967082977295 / Valid loss: 6.640564482552665

Epoch: 9
Training loss: 0.486930787563324 / Valid loss: 6.508606842585972
Training loss: 0.5336995124816895 / Valid loss: 6.540756666092646
Training loss: 0.9384230971336365 / Valid loss: 6.581932213192895
Training loss: 1.4331897497177124 / Valid loss: 6.628418082282657

Epoch: 10
Training loss: 0.638803243637085 / Valid loss: 6.611548451014928
Training loss: 0.4260116219520569 / Valid loss: 6.535487588246664
Training loss: 0.6661106944084167 / Valid loss: 6.617378648122152
Training loss: 0.4572290778160095 / Valid loss: 6.650396385647002
Training loss: 0.3525533080101013 / Valid loss: 6.647109717414493

Epoch: 11
Training loss: 0.7119911313056946 / Valid loss: 6.657587416966757
Training loss: 0.6103796362876892 / Valid loss: 6.5970988228207545
Training loss: 0.5087476968765259 / Valid loss: 6.709380678903489
Training loss: 0.6136796474456787 / Valid loss: 6.722576338904244
Training loss: 1.1439074277877808 / Valid loss: 6.59838429632641

Epoch: 12
Training loss: 0.625397264957428 / Valid loss: 6.5311532247634165
Training loss: 0.3849097788333893 / Valid loss: 6.5661600385393415
Training loss: 0.37115514278411865 / Valid loss: 6.648841233480544
Training loss: 0.7514262795448303 / Valid loss: 6.601895854586647
Training loss: 0.6375773549079895 / Valid loss: 6.709498228345598

Epoch: 13
Training loss: 0.7873992323875427 / Valid loss: 6.615289837973458
Training loss: 0.2394002377986908 / Valid loss: 6.582247043791272
Training loss: 0.4573310911655426 / Valid loss: 6.596089399428594
Training loss: 0.2799321711063385 / Valid loss: 6.653177551996141
Training loss: 0.3437681794166565 / Valid loss: 6.776682306471325

Epoch: 14
Training loss: 0.54777991771698 / Valid loss: 6.579004598799206
Training loss: 0.6812235116958618 / Valid loss: 6.617688315255301
Training loss: 0.5322567224502563 / Valid loss: 6.6904401824587865
Training loss: 0.6580634117126465 / Valid loss: 6.83607227688744
Training loss: 0.32422125339508057 / Valid loss: 6.72278428985959

Epoch: 15
Training loss: 0.5442158579826355 / Valid loss: 6.5887664885748
Training loss: 0.3646785020828247 / Valid loss: 6.665944871448335
Training loss: 1.5878313779830933 / Valid loss: 6.548986748286656
Training loss: 0.4322943091392517 / Valid loss: 6.678675476710001
Training loss: 1.9529753923416138 / Valid loss: 6.573012342907133

Epoch: 16
Training loss: 0.3928823471069336 / Valid loss: 6.5291276386805945
Training loss: 0.6051234602928162 / Valid loss: 6.5402542477562315
Training loss: 0.6202605962753296 / Valid loss: 6.6218590100606285
Training loss: 0.6963316202163696 / Valid loss: 6.723020006361462
Training loss: 0.47581684589385986 / Valid loss: 6.702125531151181

Epoch: 17
Training loss: 0.3743319511413574 / Valid loss: 6.632891028267997
Training loss: 0.22276532649993896 / Valid loss: 6.626405234563919
Training loss: 0.2875671982765198 / Valid loss: 6.687446971166701
Training loss: 0.619154691696167 / Valid loss: 6.726940754481724
Training loss: 0.3085215091705322 / Valid loss: 6.684088361830939

Epoch: 18
Training loss: 0.3016131520271301 / Valid loss: 6.59991363797869
Training loss: 0.4550592303276062 / Valid loss: 6.565271182287307
Training loss: 0.3518061339855194 / Valid loss: 6.519142611821493
Training loss: 0.3986769914627075 / Valid loss: 6.650240380423409
Training loss: 0.40623047947883606 / Valid loss: 6.561153779711042

Epoch: 19
Training loss: 0.6762123107910156 / Valid loss: 6.5188436008635025
Training loss: 0.396135151386261 / Valid loss: 6.581314366204398
Training loss: 0.272052526473999 / Valid loss: 6.666329379308792
Training loss: 0.2706495225429535 / Valid loss: 6.684407844997588

Epoch: 20
Training loss: 0.7736315727233887 / Valid loss: 6.7960758300054644
Training loss: 0.43375614285469055 / Valid loss: 6.559721642448789
Training loss: 0.5999448895454407 / Valid loss: 6.642843723297119
Training loss: 0.20184500515460968 / Valid loss: 6.601582820074899
Training loss: 0.3043774962425232 / Valid loss: 6.592939762842088

Epoch: 21
Training loss: 0.21849963068962097 / Valid loss: 6.597026990708851
Training loss: 0.3632775545120239 / Valid loss: 6.621979184377761
Training loss: 0.3112579882144928 / Valid loss: 6.507816130774362
Training loss: 0.2957606315612793 / Valid loss: 6.57359520594279
Training loss: 0.16084055602550507 / Valid loss: 6.580228249231975

Epoch: 22
Training loss: 0.34962785243988037 / Valid loss: 6.539291981288365
Training loss: 0.2718020975589752 / Valid loss: 6.6234647660028365
Training loss: 0.3797575831413269 / Valid loss: 6.56092803137643
Training loss: 0.42752617597579956 / Valid loss: 6.681999924069359
Training loss: 0.2981768846511841 / Valid loss: 6.66985247248695

Epoch: 23
Training loss: 0.22484847903251648 / Valid loss: 6.61436269850958
Training loss: 0.6339343786239624 / Valid loss: 6.574477581750779
Training loss: 1.169053554534912 / Valid loss: 6.518374170575823
Training loss: 0.3663841485977173 / Valid loss: 6.577614143916539
Training loss: 0.3050418496131897 / Valid loss: 6.630430739266532

Epoch: 24
Training loss: 0.3260456919670105 / Valid loss: 6.549938338143485
Training loss: 0.19813978672027588 / Valid loss: 6.582036408923921
Training loss: 0.5853467583656311 / Valid loss: 6.553005765733265
Training loss: 0.2618502974510193 / Valid loss: 6.613737601325625
Training loss: 0.5559120774269104 / Valid loss: 6.5638774871826175

Epoch: 25
Training loss: 0.6842944025993347 / Valid loss: 6.575691536494664
Training loss: 0.2718721926212311 / Valid loss: 6.554750374385288
Training loss: 0.21671858429908752 / Valid loss: 6.578230578558785
Training loss: 0.4131704866886139 / Valid loss: 6.601097885767619
Training loss: 0.3749891519546509 / Valid loss: 6.577929914565313

Epoch: 26
Training loss: 0.4192836582660675 / Valid loss: 6.515279154550462
Training loss: 0.2707139849662781 / Valid loss: 6.55296825681414
Training loss: 0.17949077486991882 / Valid loss: 6.572997392926897
Training loss: 0.2101268619298935 / Valid loss: 6.609686831065587
Training loss: 0.33889567852020264 / Valid loss: 6.5303938048226495

Epoch: 27
Training loss: 0.34801244735717773 / Valid loss: 6.550583526066371
Training loss: 0.3886342942714691 / Valid loss: 6.594682502746582
Training loss: 0.36056533455848694 / Valid loss: 6.5384062176659
Training loss: 0.21550002694129944 / Valid loss: 6.532033529735747
Training loss: 0.25549113750457764 / Valid loss: 6.51823072660537

Epoch: 28
Training loss: 0.17685770988464355 / Valid loss: 6.5035898344857355
Training loss: 0.2825200855731964 / Valid loss: 6.538754136221749
Training loss: 0.6514204740524292 / Valid loss: 6.5004965123676115
Training loss: 0.26376885175704956 / Valid loss: 6.549692571730841
Training loss: 0.2610223889350891 / Valid loss: 6.574190839131673

Epoch: 29
Training loss: 0.17651797831058502 / Valid loss: 6.551645033700126
Training loss: 0.24120815098285675 / Valid loss: 6.531851409730457
Training loss: 0.476404070854187 / Valid loss: 6.547164258502779
Training loss: 0.24811026453971863 / Valid loss: 6.559592810131255

Epoch: 30
Training loss: 0.3522603511810303 / Valid loss: 6.595011129833403
Training loss: 0.3103052079677582 / Valid loss: 6.4731142044067385
Training loss: 0.2525861859321594 / Valid loss: 6.492270401545933
Training loss: 0.2768222689628601 / Valid loss: 6.586585748763311
Training loss: 0.37941211462020874 / Valid loss: 6.59259276617141

Epoch: 31
Training loss: 0.5438792705535889 / Valid loss: 6.502590560913086
Training loss: 0.15075400471687317 / Valid loss: 6.519992764790853
Training loss: 0.258847177028656 / Valid loss: 6.567941615695045
Training loss: 0.23240149021148682 / Valid loss: 6.568245388212658
Training loss: 0.28295207023620605 / Valid loss: 6.498840465999785

Epoch: 32
Training loss: 0.30784904956817627 / Valid loss: 6.590494748524257
Training loss: 0.3257867693901062 / Valid loss: 6.567096496763684
Training loss: 0.39575690031051636 / Valid loss: 6.51065229688372
Training loss: 0.3870706558227539 / Valid loss: 6.484798781077067
Training loss: 0.21107986569404602 / Valid loss: 6.5615439732869465

Epoch: 33
Training loss: 0.21984897553920746 / Valid loss: 6.537792201269241
Training loss: 0.2512127161026001 / Valid loss: 6.526321215856643
Training loss: 0.25203558802604675 / Valid loss: 6.485603664034889
Training loss: 0.14274337887763977 / Valid loss: 6.509571077710106
Training loss: 0.22682499885559082 / Valid loss: 6.50188897450765

Epoch: 34
Training loss: 0.5924068689346313 / Valid loss: 6.510288115910122
Training loss: 0.17585602402687073 / Valid loss: 6.5544995625813804
Training loss: 0.23843979835510254 / Valid loss: 6.523374884469169
Training loss: 0.2622915208339691 / Valid loss: 6.464280266988845
Training loss: 0.2514393925666809 / Valid loss: 6.513839912414551

Epoch: 35
Training loss: 0.1398164927959442 / Valid loss: 6.534541581925891
Training loss: 0.23858940601348877 / Valid loss: 6.551083389918009
Training loss: 0.2720387279987335 / Valid loss: 6.608319695790609
Training loss: 0.17078348994255066 / Valid loss: 6.531014449255807
Training loss: 0.5329785943031311 / Valid loss: 6.5561517715454105

Epoch: 36
Training loss: 0.2029581516981125 / Valid loss: 6.527612486339751
Training loss: 0.4194025695323944 / Valid loss: 6.5568323317028225
Training loss: 0.26481515169143677 / Valid loss: 6.532453982035319
Training loss: 0.27702078223228455 / Valid loss: 6.532191601253691
Training loss: 0.25558939576148987 / Valid loss: 6.555987196876889

Epoch: 37
Training loss: 0.20501577854156494 / Valid loss: 6.599065989539737
Training loss: 0.21734437346458435 / Valid loss: 6.4774463154020765
Training loss: 0.5347420573234558 / Valid loss: 6.54412149247669
Training loss: 0.2255886048078537 / Valid loss: 6.522966048831031
Training loss: 0.32522982358932495 / Valid loss: 6.583573171070644

Epoch: 38
Training loss: 0.185095876455307 / Valid loss: 6.526794803710211
Training loss: 0.22719457745552063 / Valid loss: 6.507651079268682
Training loss: 0.2521217167377472 / Valid loss: 6.570317014058431
Training loss: 0.17851977050304413 / Valid loss: 6.5618193876175654
Training loss: 0.154308021068573 / Valid loss: 6.589817989440191

Epoch: 39
Training loss: 0.26257145404815674 / Valid loss: 6.532779141834804
Training loss: 0.26046887040138245 / Valid loss: 6.5549737748645605
Training loss: 0.14716210961341858 / Valid loss: 6.486575941812425
Training loss: 0.13739848136901855 / Valid loss: 6.488957777477446
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.620214945929391
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.313697814941406 / Valid loss: 15.745897629147484
Model is saved in epoch 0, overall batch: 0
Training loss: 8.684988975524902 / Valid loss: 9.49561504636492
Model is saved in epoch 0, overall batch: 100
Training loss: 4.908787727355957 / Valid loss: 6.179162275223505
Model is saved in epoch 0, overall batch: 200
Training loss: 3.5378198623657227 / Valid loss: 5.832422360919771
Model is saved in epoch 0, overall batch: 300
Training loss: 4.7306928634643555 / Valid loss: 5.803812651407151
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 2.8372890949249268 / Valid loss: 5.704135016032628
Model is saved in epoch 1, overall batch: 500
Training loss: 3.7006726264953613 / Valid loss: 6.021931262243362
Training loss: 4.61144495010376 / Valid loss: 5.934419132414318
Training loss: 3.6826870441436768 / Valid loss: 5.901263974961781
Training loss: 3.4629695415496826 / Valid loss: 5.997354820796422

Epoch: 2
Training loss: 1.2658900022506714 / Valid loss: 6.049722392218453
Training loss: 1.465818166732788 / Valid loss: 6.213417702629453
Training loss: 1.7295174598693848 / Valid loss: 6.15231842313494
Training loss: 2.207765579223633 / Valid loss: 6.211950892493839
Training loss: 2.833462715148926 / Valid loss: 6.204968570527576

Epoch: 3
Training loss: 1.615018606185913 / Valid loss: 6.307268483298166
Training loss: 1.5284675359725952 / Valid loss: 6.3918011710757305
Training loss: 1.8343451023101807 / Valid loss: 6.311211368015834
Training loss: 2.013132095336914 / Valid loss: 6.421137330645607
Training loss: 2.616546869277954 / Valid loss: 6.407960124242873

Epoch: 4
Training loss: 0.894239068031311 / Valid loss: 6.36410132362729
Training loss: 1.3359668254852295 / Valid loss: 6.348277314503988
Training loss: 1.0741404294967651 / Valid loss: 6.414147140866234
Training loss: 1.3340966701507568 / Valid loss: 6.448921496527536
Training loss: 1.2350059747695923 / Valid loss: 6.433647923242479

Epoch: 5
Training loss: 0.7173894047737122 / Valid loss: 6.348970095316569
Training loss: 1.167283535003662 / Valid loss: 6.384468918754941
Training loss: 1.5063858032226562 / Valid loss: 6.60608651297433
Training loss: 1.501814365386963 / Valid loss: 6.470526390983945
Training loss: 1.356440544128418 / Valid loss: 6.5586728027888705

Epoch: 6
Training loss: 0.9099092483520508 / Valid loss: 6.439469391959054
Training loss: 1.1255732774734497 / Valid loss: 6.500457800002325
Training loss: 0.9667467474937439 / Valid loss: 6.544581090836298
Training loss: 1.0650227069854736 / Valid loss: 6.583671111152285
Training loss: 0.7638564109802246 / Valid loss: 6.642902789797102

Epoch: 7
Training loss: 0.6583631634712219 / Valid loss: 6.532422438121977
Training loss: 0.44508597254753113 / Valid loss: 6.628567759195963
Training loss: 0.6579651832580566 / Valid loss: 6.56311579885937
Training loss: 0.5398718118667603 / Valid loss: 6.595710895175025
Training loss: 0.6912883520126343 / Valid loss: 6.6194314275469095

Epoch: 8
Training loss: 0.6974809169769287 / Valid loss: 6.503086505617414
Training loss: 0.7269664406776428 / Valid loss: 6.624625546591623
Training loss: 0.5872806906700134 / Valid loss: 6.581576915014358
Training loss: 0.5066336989402771 / Valid loss: 6.61157271521432
Training loss: 1.2461148500442505 / Valid loss: 6.523114844730922

Epoch: 9
Training loss: 0.4821762442588806 / Valid loss: 6.502066528229486
Training loss: 0.6635230779647827 / Valid loss: 6.5013633024124875
Training loss: 0.8526779413223267 / Valid loss: 6.535044710976737
Training loss: 1.4150654077529907 / Valid loss: 6.6493458021254765

Epoch: 10
Training loss: 0.6363362073898315 / Valid loss: 6.546749310266404
Training loss: 0.5917974710464478 / Valid loss: 6.490375119163876
Training loss: 0.6361744999885559 / Valid loss: 6.527628823689052
Training loss: 0.510942816734314 / Valid loss: 6.600630991799491
Training loss: 0.3405280113220215 / Valid loss: 6.640354969387963

Epoch: 11
Training loss: 0.5939878225326538 / Valid loss: 6.587566450663975
Training loss: 0.3698906898498535 / Valid loss: 6.520260070619129
Training loss: 0.6074413657188416 / Valid loss: 6.53062534786406
Training loss: 0.5874255895614624 / Valid loss: 6.590512684413365
Training loss: 1.1299018859863281 / Valid loss: 6.53587246622358

Epoch: 12
Training loss: 0.7502713203430176 / Valid loss: 6.475961862291609
Training loss: 0.45305365324020386 / Valid loss: 6.472553198678153
Training loss: 0.2625068128108978 / Valid loss: 6.603708362579345
Training loss: 0.7936733961105347 / Valid loss: 6.535207412356422
Training loss: 0.5003366470336914 / Valid loss: 6.576465502239409

Epoch: 13
Training loss: 0.8025192022323608 / Valid loss: 6.536697303681146
Training loss: 0.389568954706192 / Valid loss: 6.5797850586119155
Training loss: 0.421860933303833 / Valid loss: 6.587742439905802
Training loss: 0.3118355870246887 / Valid loss: 6.580432996295747
Training loss: 0.5759113430976868 / Valid loss: 6.660602583203997

Epoch: 14
Training loss: 0.353251188993454 / Valid loss: 6.4444993927365255
Training loss: 0.8745355010032654 / Valid loss: 6.528064137413388
Training loss: 0.4805101752281189 / Valid loss: 6.562660462515694
Training loss: 0.5129649639129639 / Valid loss: 6.700190848395938
Training loss: 0.24494796991348267 / Valid loss: 6.609800770169213

Epoch: 15
Training loss: 0.4693726897239685 / Valid loss: 6.508649449121385
Training loss: 0.23722973465919495 / Valid loss: 6.5207324209667386
Training loss: 1.2743778228759766 / Valid loss: 6.5133678981236045
Training loss: 0.37731385231018066 / Valid loss: 6.56175217628479
Training loss: 1.709514856338501 / Valid loss: 6.478470116569882

Epoch: 16
Training loss: 0.29940301179885864 / Valid loss: 6.470775978905814
Training loss: 0.6139556765556335 / Valid loss: 6.505754607064383
Training loss: 0.6553341150283813 / Valid loss: 6.585602385657174
Training loss: 0.931609034538269 / Valid loss: 6.6017825944083075
Training loss: 0.3906426429748535 / Valid loss: 6.609635580153692

Epoch: 17
Training loss: 0.39378488063812256 / Valid loss: 6.475527590797061
Training loss: 0.31744885444641113 / Valid loss: 6.465401685805547
Training loss: 0.3460504114627838 / Valid loss: 6.5508512746720085
Training loss: 0.6233891844749451 / Valid loss: 6.589701834179106
Training loss: 0.258596271276474 / Valid loss: 6.604754259472801

Epoch: 18
Training loss: 0.3133184313774109 / Valid loss: 6.403406188601539
Training loss: 0.4075828492641449 / Valid loss: 6.481780463173276
Training loss: 0.3420165181159973 / Valid loss: 6.4736455599466955
Training loss: 0.3445042371749878 / Valid loss: 6.594500089827038
Training loss: 0.3332347869873047 / Valid loss: 6.477781917935326

Epoch: 19
Training loss: 0.5060778260231018 / Valid loss: 6.513262026650565
Training loss: 0.47445929050445557 / Valid loss: 6.577573149544852
Training loss: 0.32881006598472595 / Valid loss: 6.538410945165725
Training loss: 0.2756175994873047 / Valid loss: 6.511911358152117

Epoch: 20
Training loss: 0.7196418046951294 / Valid loss: 6.609729494367327
Training loss: 0.3726348876953125 / Valid loss: 6.436045074462891
Training loss: 0.660146951675415 / Valid loss: 6.495520923251197
Training loss: 0.3930930495262146 / Valid loss: 6.491270499002366
Training loss: 0.31839317083358765 / Valid loss: 6.449209928512573

Epoch: 21
Training loss: 0.2786908745765686 / Valid loss: 6.457190908704486
Training loss: 0.39336973428726196 / Valid loss: 6.434904103052048
Training loss: 0.24793250858783722 / Valid loss: 6.368903636932373
Training loss: 0.21189998090267181 / Valid loss: 6.4606051354181195
Training loss: 0.24774593114852905 / Valid loss: 6.492416899544852

Epoch: 22
Training loss: 0.5395664572715759 / Valid loss: 6.476408799489339
Training loss: 0.17520925402641296 / Valid loss: 6.469224818547567
Training loss: 0.4087114930152893 / Valid loss: 6.38553763798305
Training loss: 0.4738818407058716 / Valid loss: 6.486860963276454
Training loss: 0.4789775013923645 / Valid loss: 6.54966337567284

Epoch: 23
Training loss: 0.34807974100112915 / Valid loss: 6.4529222374870665
Training loss: 0.43454766273498535 / Valid loss: 6.486228879292806
Training loss: 1.0768320560455322 / Valid loss: 6.437157726287841
Training loss: 0.30013740062713623 / Valid loss: 6.477739958536057
Training loss: 0.4840748906135559 / Valid loss: 6.54503904070173

Epoch: 24
Training loss: 0.355022668838501 / Valid loss: 6.415515393302554
Training loss: 0.27303561568260193 / Valid loss: 6.445001270657494
Training loss: 0.6353403329849243 / Valid loss: 6.464518138340542
Training loss: 0.3070358335971832 / Valid loss: 6.516793110257104
Training loss: 0.512339174747467 / Valid loss: 6.5560840742928645

Epoch: 25
Training loss: 0.5106174349784851 / Valid loss: 6.4628934633164175
Training loss: 0.3487192690372467 / Valid loss: 6.480559721447173
Training loss: 0.2129436731338501 / Valid loss: 6.44981506892613
Training loss: 0.36493566632270813 / Valid loss: 6.482892953781855
Training loss: 0.3965153694152832 / Valid loss: 6.443420078640893

Epoch: 26
Training loss: 0.29840919375419617 / Valid loss: 6.397738166082473
Training loss: 0.287007212638855 / Valid loss: 6.463116679872785
Training loss: 0.3344346880912781 / Valid loss: 6.494342367989677
Training loss: 0.2361464500427246 / Valid loss: 6.43222964150565
Training loss: 0.2708641290664673 / Valid loss: 6.44104875382923

Epoch: 27
Training loss: 0.28134775161743164 / Valid loss: 6.458008525485084
Training loss: 0.37940919399261475 / Valid loss: 6.4055256888979955
Training loss: 0.3141205906867981 / Valid loss: 6.457094197046189
Training loss: 0.2631910443305969 / Valid loss: 6.434181326911563
Training loss: 0.29406213760375977 / Valid loss: 6.449012256803966

Epoch: 28
Training loss: 0.2481173872947693 / Valid loss: 6.3808216367449075
Training loss: 0.18109279870986938 / Valid loss: 6.373530110858735
Training loss: 0.4942256510257721 / Valid loss: 6.380398193995158
Training loss: 0.3628624975681305 / Valid loss: 6.43394147327968
Training loss: 0.271908700466156 / Valid loss: 6.532962140582857

Epoch: 29
Training loss: 0.12312132120132446 / Valid loss: 6.463160428546724
Training loss: 0.2506713271141052 / Valid loss: 6.405507773444766
Training loss: 0.4330078065395355 / Valid loss: 6.4308076676868255
Training loss: 0.3137783408164978 / Valid loss: 6.483300935654413

Epoch: 30
Training loss: 0.45134055614471436 / Valid loss: 6.505454744611468
Training loss: 0.3815426826477051 / Valid loss: 6.382511184329078
Training loss: 0.2977233827114105 / Valid loss: 6.410803729011899
Training loss: 0.2796311378479004 / Valid loss: 6.437516128449213
Training loss: 0.46158868074417114 / Valid loss: 6.47126742317563

Epoch: 31
Training loss: 0.5077478885650635 / Valid loss: 6.375725614456903
Training loss: 0.18151894211769104 / Valid loss: 6.42792744182405
Training loss: 0.3490920066833496 / Valid loss: 6.406954093206497
Training loss: 0.30500662326812744 / Valid loss: 6.4478832017807735
Training loss: 0.24359849095344543 / Valid loss: 6.403703952970959

Epoch: 32
Training loss: 0.328731894493103 / Valid loss: 6.392922019958496
Training loss: 0.2789621353149414 / Valid loss: 6.404750783102853
Training loss: 0.36513423919677734 / Valid loss: 6.360852863675072
Training loss: 0.37065955996513367 / Valid loss: 6.43345574878511
Training loss: 0.21159721910953522 / Valid loss: 6.429827063424247

Epoch: 33
Training loss: 0.24468618631362915 / Valid loss: 6.361913513001942
Training loss: 0.21299463510513306 / Valid loss: 6.3756340253920785
Training loss: 0.37528520822525024 / Valid loss: 6.378321874709356
Training loss: 0.17590831220149994 / Valid loss: 6.3790869054340185
Training loss: 0.2514352798461914 / Valid loss: 6.403542945498512

Epoch: 34
Training loss: 0.5210361480712891 / Valid loss: 6.354113665081206
Training loss: 0.18633389472961426 / Valid loss: 6.388042894999186
Training loss: 0.33086326718330383 / Valid loss: 6.4202723276047475
Training loss: 0.30958354473114014 / Valid loss: 6.317562525612967
Training loss: 0.2051554173231125 / Valid loss: 6.43399596668425

Epoch: 35
Training loss: 0.22818215191364288 / Valid loss: 6.3739131087348575
Training loss: 0.25959312915802 / Valid loss: 6.429718410401117
Training loss: 0.31362250447273254 / Valid loss: 6.406144959586007
Training loss: 0.16690540313720703 / Valid loss: 6.371241801125663
Training loss: 0.4830375611782074 / Valid loss: 6.414421385810488

Epoch: 36
Training loss: 0.2265583872795105 / Valid loss: 6.406071136111305
Training loss: 0.5216261148452759 / Valid loss: 6.4054186730157765
Training loss: 0.2214403748512268 / Valid loss: 6.4166829336257205
Training loss: 0.3463539183139801 / Valid loss: 6.44429738180978
Training loss: 0.3433627188205719 / Valid loss: 6.431467494510469

Epoch: 37
Training loss: 0.18185314536094666 / Valid loss: 6.3876727626437235
Training loss: 0.21585118770599365 / Valid loss: 6.312241440727597
Training loss: 0.6151388883590698 / Valid loss: 6.406463900066558
Training loss: 0.18075349926948547 / Valid loss: 6.435305643081665
Training loss: 0.3025254011154175 / Valid loss: 6.494917267844791

Epoch: 38
Training loss: 0.24127066135406494 / Valid loss: 6.380795996529716
Training loss: 0.3191433548927307 / Valid loss: 6.365634370985485
Training loss: 0.23392736911773682 / Valid loss: 6.4271805990309945
Training loss: 0.1855630874633789 / Valid loss: 6.392951913107009
Training loss: 0.24130728840827942 / Valid loss: 6.440296148118518

Epoch: 39
Training loss: 0.2724155783653259 / Valid loss: 6.369026263554891
Training loss: 0.3128526508808136 / Valid loss: 6.378529768898374
Training loss: 0.17988580465316772 / Valid loss: 6.375946208408901
Training loss: 0.16236960887908936 / Valid loss: 6.404260031382242
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.6254023188636415
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.220863342285156 / Valid loss: 16.406339981442407
Model is saved in epoch 0, overall batch: 0
Training loss: 7.161015033721924 / Valid loss: 6.829741114661807
Model is saved in epoch 0, overall batch: 100
Training loss: 5.48242712020874 / Valid loss: 6.842602175757999
Training loss: 5.094162940979004 / Valid loss: 6.712977336701893
Model is saved in epoch 0, overall batch: 300
Training loss: 7.588825225830078 / Valid loss: 6.531738331204369
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.257430076599121 / Valid loss: 6.265892367135911
Model is saved in epoch 1, overall batch: 500
Training loss: 3.536505937576294 / Valid loss: 6.262178652627128
Model is saved in epoch 1, overall batch: 600
Training loss: 2.896556854248047 / Valid loss: 6.4458493073781336
Training loss: 4.152067184448242 / Valid loss: 6.363753613971529
Training loss: 4.274667739868164 / Valid loss: 6.353168934867496

Epoch: 2
Training loss: 1.7083919048309326 / Valid loss: 6.293895324071248
Training loss: 3.0117597579956055 / Valid loss: 6.63429609934489
Training loss: 2.2811644077301025 / Valid loss: 6.473323790232341
Training loss: 2.4323031902313232 / Valid loss: 6.482635622932797
Training loss: 2.1768102645874023 / Valid loss: 6.464260900588263

Epoch: 3
Training loss: 1.7394853830337524 / Valid loss: 6.577650206429618
Training loss: 1.9787509441375732 / Valid loss: 6.645025718779791
Training loss: 2.73429536819458 / Valid loss: 6.6776993115743
Training loss: 1.907366394996643 / Valid loss: 6.699671375183832
Training loss: 3.0475220680236816 / Valid loss: 6.722275824773879

Epoch: 4
Training loss: 1.6508030891418457 / Valid loss: 6.816239802042643
Training loss: 1.7953108549118042 / Valid loss: 6.858878063020252
Training loss: 2.113997459411621 / Valid loss: 6.844531981150309
Training loss: 2.4683265686035156 / Valid loss: 6.906167191550845
Training loss: 1.5686132907867432 / Valid loss: 6.93701050167992

Epoch: 5
Training loss: 1.3866811990737915 / Valid loss: 6.961434214455741
Training loss: 2.0947365760803223 / Valid loss: 6.942538735980079
Training loss: 1.1516486406326294 / Valid loss: 6.978683508010137
Training loss: 2.439023494720459 / Valid loss: 6.963300891149611
Training loss: 2.1955699920654297 / Valid loss: 6.935974339076451

Epoch: 6
Training loss: 1.0904830694198608 / Valid loss: 7.016772860572452
Training loss: 1.9534082412719727 / Valid loss: 6.909561593191964
Training loss: 2.1746134757995605 / Valid loss: 6.967873085112799
Training loss: 1.5537831783294678 / Valid loss: 7.033780418123517
Training loss: 2.8911101818084717 / Valid loss: 7.0342209498087565

Epoch: 7
Training loss: 1.6612199544906616 / Valid loss: 6.995692866189139
Training loss: 1.1583504676818848 / Valid loss: 7.105394640423003
Training loss: 1.3204238414764404 / Valid loss: 7.107662863958449
Training loss: 1.3471977710723877 / Valid loss: 7.173334346498762
Training loss: 1.521538496017456 / Valid loss: 7.235638768332345

Epoch: 8
Training loss: 1.0863385200500488 / Valid loss: 7.142251650492351
Training loss: 1.2083277702331543 / Valid loss: 7.166301418486095
Training loss: 1.1464766263961792 / Valid loss: 7.257538768223354
Training loss: 1.0640822649002075 / Valid loss: 7.300285909289405
Training loss: 1.1532434225082397 / Valid loss: 7.4000195821126304

Epoch: 9
Training loss: 0.9190365076065063 / Valid loss: 7.226581891377767
Training loss: 1.163602590560913 / Valid loss: 7.300419902801513
Training loss: 0.9489127993583679 / Valid loss: 7.294905194782076
Training loss: 0.9125964641571045 / Valid loss: 7.287169420151484

Epoch: 10
Training loss: 0.8415749669075012 / Valid loss: 7.316340296609061
Training loss: 0.787870466709137 / Valid loss: 7.3429105122884115
Training loss: 0.5133730173110962 / Valid loss: 7.177202551705497
Training loss: 1.587038278579712 / Valid loss: 7.335528455461774
Training loss: 1.3203620910644531 / Valid loss: 7.393686221894764

Epoch: 11
Training loss: 0.5217010974884033 / Valid loss: 7.403841568174816
Training loss: 0.6330167055130005 / Valid loss: 7.371284278233846
Training loss: 0.6165683269500732 / Valid loss: 7.410258465721494
Training loss: 0.8320856094360352 / Valid loss: 7.385245064326695
Training loss: 1.1556389331817627 / Valid loss: 7.48827688126337

Epoch: 12
Training loss: 0.8847454786300659 / Valid loss: 7.387408043089367
Training loss: 0.579796314239502 / Valid loss: 7.410102031344459
Training loss: 1.0434216260910034 / Valid loss: 7.477159827096122
Training loss: 0.8428254723548889 / Valid loss: 7.4443498066493445
Training loss: 0.7214807271957397 / Valid loss: 7.4211649940127415

Epoch: 13
Training loss: 0.8455866575241089 / Valid loss: 7.39572064990089
Training loss: 0.6811468601226807 / Valid loss: 7.40983220963251
Training loss: 0.6514315009117126 / Valid loss: 7.525778434390113
Training loss: 0.35091209411621094 / Valid loss: 7.519331198646909
Training loss: 1.0620653629302979 / Valid loss: 7.4719294093904045

Epoch: 14
Training loss: 0.7512069940567017 / Valid loss: 7.506759670802525
Training loss: 0.6037793159484863 / Valid loss: 7.441931288582938
Training loss: 0.6154974699020386 / Valid loss: 7.514321486155192
Training loss: 0.5960752964019775 / Valid loss: 7.452915645781017
Training loss: 0.600487470626831 / Valid loss: 7.530923875172933

Epoch: 15
Training loss: 0.7203777432441711 / Valid loss: 7.414179150263468
Training loss: 0.6944214105606079 / Valid loss: 7.4165420850118
Training loss: 0.6178058385848999 / Valid loss: 7.437822836921328
Training loss: 1.0864132642745972 / Valid loss: 7.535951210203625
Training loss: 0.4323809742927551 / Valid loss: 7.49625156493414

Epoch: 16
Training loss: 0.9036523699760437 / Valid loss: 7.586697047097342
Training loss: 0.6040761470794678 / Valid loss: 7.3765093757992695
Training loss: 0.4614761471748352 / Valid loss: 7.618459052131289
Training loss: 0.45828503370285034 / Valid loss: 7.460265622820173
Training loss: 1.0039186477661133 / Valid loss: 7.46705044791812

Epoch: 17
Training loss: 0.585974395275116 / Valid loss: 7.458680098397391
Training loss: 0.3186744153499603 / Valid loss: 7.424227310362316
Training loss: 0.6327090859413147 / Valid loss: 7.45323219753447
Training loss: 0.4785842299461365 / Valid loss: 7.53771922701881
Training loss: 0.523917555809021 / Valid loss: 7.414876765296572

Epoch: 18
Training loss: 0.8476210832595825 / Valid loss: 7.449094150179908
Training loss: 0.6828955411911011 / Valid loss: 7.381050346011207
Training loss: 0.7395994067192078 / Valid loss: 7.559427502041771
Training loss: 0.5592176914215088 / Valid loss: 7.545882202330089
Training loss: 0.6515995860099792 / Valid loss: 7.5362390018644785

Epoch: 19
Training loss: 0.630841076374054 / Valid loss: 7.45865280060541
Training loss: 0.520824670791626 / Valid loss: 7.455388868422736
Training loss: 0.8571003675460815 / Valid loss: 7.495466027941022
Training loss: 0.9296776652336121 / Valid loss: 7.466138249351864

Epoch: 20
Training loss: 0.3825340270996094 / Valid loss: 7.494428484780448
Training loss: 0.7067073583602905 / Valid loss: 7.434899702526274
Training loss: 0.4571859836578369 / Valid loss: 7.490297589983259
Training loss: 0.4461238384246826 / Valid loss: 7.512708005451021
Training loss: 0.443108469247818 / Valid loss: 7.487663655054002

Epoch: 21
Training loss: 0.3015860617160797 / Valid loss: 7.552160776229131
Training loss: 0.4499332904815674 / Valid loss: 7.588216200329009
Training loss: 0.576903223991394 / Valid loss: 7.542056192670549
Training loss: 0.6802764534950256 / Valid loss: 7.5032143365769155
Training loss: 0.43251168727874756 / Valid loss: 7.517552561987014

Epoch: 22
Training loss: 0.4837122857570648 / Valid loss: 7.483543609437488
Training loss: 0.31554239988327026 / Valid loss: 7.470666690099807
Training loss: 0.8010107278823853 / Valid loss: 7.579455552782331
Training loss: 0.4986330568790436 / Valid loss: 7.511228375207811
Training loss: 0.33561766147613525 / Valid loss: 7.5298532803853355

Epoch: 23
Training loss: 0.2757553458213806 / Valid loss: 7.584067117600214
Training loss: 0.28668496012687683 / Valid loss: 7.510986459822882
Training loss: 0.490731418132782 / Valid loss: 7.4345036915370395
Training loss: 0.8279364109039307 / Valid loss: 7.587390304747082
Training loss: 0.5678966045379639 / Valid loss: 7.611192909876506

Epoch: 24
Training loss: 0.3252212405204773 / Valid loss: 7.5097644306364515
Training loss: 0.5032528638839722 / Valid loss: 7.565078267597017
Training loss: 0.7749044299125671 / Valid loss: 7.5218780267806284
Training loss: 0.5245333909988403 / Valid loss: 7.559136504218692
Training loss: 0.3712834417819977 / Valid loss: 7.537616284688314

Epoch: 25
Training loss: 0.615050196647644 / Valid loss: 7.524170575823103
Training loss: 0.3435818552970886 / Valid loss: 7.496840213593982
Training loss: 0.30088385939598083 / Valid loss: 7.53740420568557
Training loss: 0.4630030393600464 / Valid loss: 7.549655773526147
Training loss: 0.3594546616077423 / Valid loss: 7.60859584354219

Epoch: 26
Training loss: 0.4353792667388916 / Valid loss: 7.52673749923706
Training loss: 0.4934999644756317 / Valid loss: 7.494487453642345
Training loss: 0.22679944336414337 / Valid loss: 7.522698173068819
Training loss: 0.4638335704803467 / Valid loss: 7.545256176449004
Training loss: 0.6083855628967285 / Valid loss: 7.567637207394554

Epoch: 27
Training loss: 0.6701663136482239 / Valid loss: 7.4583256131126765
Training loss: 0.5390537977218628 / Valid loss: 7.521209535144624
Training loss: 0.2844235897064209 / Valid loss: 7.462232344491142
Training loss: 0.34438037872314453 / Valid loss: 7.603735006423223
Training loss: 0.33576178550720215 / Valid loss: 7.533988657451811

Epoch: 28
Training loss: 0.2692705988883972 / Valid loss: 7.516566558111282
Training loss: 0.41644617915153503 / Valid loss: 7.481602814084008
Training loss: 0.5690602660179138 / Valid loss: 7.5001711027962825
Training loss: 0.40398871898651123 / Valid loss: 7.484897618066697
Training loss: 0.2919800281524658 / Valid loss: 7.494636122385661

Epoch: 29
Training loss: 0.7705183029174805 / Valid loss: 7.547650596073695
Training loss: 0.34090861678123474 / Valid loss: 7.537130342211042
Training loss: 0.6324901580810547 / Valid loss: 7.507266803014846
Training loss: 0.4092457890510559 / Valid loss: 7.5671137491861975

Epoch: 30
Training loss: 0.3007854223251343 / Valid loss: 7.617790063222249
Training loss: 0.4931265115737915 / Valid loss: 7.489413733709426
Training loss: 0.682086169719696 / Valid loss: 7.560986682346889
Training loss: 0.5913293361663818 / Valid loss: 7.4639285314650765
Training loss: 0.19301658868789673 / Valid loss: 7.526675501323882

Epoch: 31
Training loss: 0.21814697980880737 / Valid loss: 7.511706215994699
Training loss: 0.4779125452041626 / Valid loss: 7.496926148732503
Training loss: 0.20311670005321503 / Valid loss: 7.510593128204346
Training loss: 0.302141010761261 / Valid loss: 7.481096680959066
Training loss: 0.682315468788147 / Valid loss: 7.554557909284319

Epoch: 32
Training loss: 0.2578233480453491 / Valid loss: 7.475116666158041
Training loss: 0.2009304165840149 / Valid loss: 7.506206734975179
Training loss: 0.21460750699043274 / Valid loss: 7.47517891838437
Training loss: 0.21290312707424164 / Valid loss: 7.562351204100109
Training loss: 0.47888535261154175 / Valid loss: 7.508683404468354

Epoch: 33
Training loss: 0.7188746333122253 / Valid loss: 7.533263683319092
Training loss: 0.2708338797092438 / Valid loss: 7.5500054427555625
Training loss: 0.2689417600631714 / Valid loss: 7.5246732076009115
Training loss: 0.2327718585729599 / Valid loss: 7.477434548877534
Training loss: 0.35376250743865967 / Valid loss: 7.482684267134894

Epoch: 34
Training loss: 0.2426251620054245 / Valid loss: 7.536525376637777
Training loss: 0.33782556653022766 / Valid loss: 7.5809346834818525
Training loss: 0.6381131410598755 / Valid loss: 7.50515927814302
Training loss: 0.34093984961509705 / Valid loss: 7.439398997170585
Training loss: 0.24563480913639069 / Valid loss: 7.562099179767427

Epoch: 35
Training loss: 0.3083447217941284 / Valid loss: 7.539411962599981
Training loss: 0.40913450717926025 / Valid loss: 7.5941278412228534
Training loss: 0.2014058232307434 / Valid loss: 7.494923419044131
Training loss: 0.1881386786699295 / Valid loss: 7.460795334407261
Training loss: 0.4894140362739563 / Valid loss: 7.562849746431623

Epoch: 36
Training loss: 0.35134345293045044 / Valid loss: 7.53031218846639
Training loss: 0.38751012086868286 / Valid loss: 7.451153496333531
Training loss: 0.18006449937820435 / Valid loss: 7.479579416910807
Training loss: 0.5778640508651733 / Valid loss: 7.534171199798584
Training loss: 0.2704089879989624 / Valid loss: 7.534466316586449

Epoch: 37
Training loss: 0.3118932545185089 / Valid loss: 7.569976327532814
Training loss: 1.0193328857421875 / Valid loss: 7.5121498244149345
Training loss: 0.2784324884414673 / Valid loss: 7.511622465224493
Training loss: 0.23507395386695862 / Valid loss: 7.472448884873163
Training loss: 0.36432310938835144 / Valid loss: 7.506976331983294

Epoch: 38
Training loss: 0.2542332708835602 / Valid loss: 7.499194208780924
Training loss: 0.20743155479431152 / Valid loss: 7.535943807874407
Training loss: 0.36721593141555786 / Valid loss: 7.517326795487177
Training loss: 0.22970321774482727 / Valid loss: 7.514741134643555
Training loss: 0.37335270643234253 / Valid loss: 7.510649522145589

Epoch: 39
Training loss: 0.2201930284500122 / Valid loss: 7.45075209027245
Training loss: 0.2802746891975403 / Valid loss: 7.549128191811698
Training loss: 0.646377444267273 / Valid loss: 7.53011121749878
Training loss: 0.22829610109329224 / Valid loss: 7.540164543333508
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 600): 6.057986116409301
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.220863342285156 / Valid loss: 16.40694210415795
Model is saved in epoch 0, overall batch: 0
Training loss: 7.332840919494629 / Valid loss: 6.955939104443504
Model is saved in epoch 0, overall batch: 100
Training loss: 5.757575988769531 / Valid loss: 6.844838455745152
Model is saved in epoch 0, overall batch: 200
Training loss: 4.947753429412842 / Valid loss: 6.693674139749437
Model is saved in epoch 0, overall batch: 300
Training loss: 7.626798152923584 / Valid loss: 6.453283850351969
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.338624477386475 / Valid loss: 6.186670714332944
Model is saved in epoch 1, overall batch: 500
Training loss: 3.566880702972412 / Valid loss: 6.215447505315145
Training loss: 2.869546413421631 / Valid loss: 6.382504197529384
Training loss: 4.124619007110596 / Valid loss: 6.315593151819138
Training loss: 4.211917877197266 / Valid loss: 6.336466496331351

Epoch: 2
Training loss: 1.8071527481079102 / Valid loss: 6.273880157016572
Training loss: 3.124378204345703 / Valid loss: 6.589412203289213
Training loss: 2.309967517852783 / Valid loss: 6.457128236407326
Training loss: 2.3461289405822754 / Valid loss: 6.48585227557591
Training loss: 2.2351884841918945 / Valid loss: 6.459508468991234

Epoch: 3
Training loss: 1.747178554534912 / Valid loss: 6.571444754373459
Training loss: 1.9200334548950195 / Valid loss: 6.629461050033569
Training loss: 2.9335904121398926 / Valid loss: 6.672566781725203
Training loss: 1.9542776346206665 / Valid loss: 6.689701761518206
Training loss: 3.1881179809570312 / Valid loss: 6.688919975644066

Epoch: 4
Training loss: 1.6455917358398438 / Valid loss: 6.776827682767595
Training loss: 1.7176787853240967 / Valid loss: 6.84315215973627
Training loss: 2.0226895809173584 / Valid loss: 6.813142649332682
Training loss: 2.708500862121582 / Valid loss: 6.878172097887312
Training loss: 1.5773637294769287 / Valid loss: 6.9196965081351145

Epoch: 5
Training loss: 1.5105791091918945 / Valid loss: 6.952132797241211
Training loss: 2.089266061782837 / Valid loss: 6.92441467103504
Training loss: 1.1292757987976074 / Valid loss: 6.93488268171038
Training loss: 2.420779228210449 / Valid loss: 6.924571791149321
Training loss: 2.295712471008301 / Valid loss: 6.900079273042225

Epoch: 6
Training loss: 1.0254992246627808 / Valid loss: 7.015396304357619
Training loss: 1.8893115520477295 / Valid loss: 6.895164671398344
Training loss: 2.3699700832366943 / Valid loss: 6.9695525578090125
Training loss: 1.3423731327056885 / Valid loss: 7.016193893977574
Training loss: 3.1562581062316895 / Valid loss: 7.006037821088518

Epoch: 7
Training loss: 1.8206162452697754 / Valid loss: 6.96164584841047
Training loss: 1.156939148902893 / Valid loss: 7.085591793060303
Training loss: 1.3613431453704834 / Valid loss: 7.064221986134847
Training loss: 1.4285534620285034 / Valid loss: 7.1441752706255235
Training loss: 1.6092205047607422 / Valid loss: 7.2382696287972585

Epoch: 8
Training loss: 1.0693720579147339 / Valid loss: 7.130190336136591
Training loss: 1.2796107530593872 / Valid loss: 7.141752138591948
Training loss: 1.2408372163772583 / Valid loss: 7.209331712268647
Training loss: 1.1555755138397217 / Valid loss: 7.206210399809338
Training loss: 1.2812013626098633 / Valid loss: 7.30168324425107

Epoch: 9
Training loss: 0.94693523645401 / Valid loss: 7.160459781828381
Training loss: 1.1385046243667603 / Valid loss: 7.238620367504302
Training loss: 0.9746224880218506 / Valid loss: 7.228651519048782
Training loss: 0.8637897372245789 / Valid loss: 7.224891113099598

Epoch: 10
Training loss: 0.7867496013641357 / Valid loss: 7.31202682313465
Training loss: 0.812711238861084 / Valid loss: 7.3324967384338375
Training loss: 0.64998859167099 / Valid loss: 7.13713443392799
Training loss: 1.5327287912368774 / Valid loss: 7.248180071512858
Training loss: 1.4065735340118408 / Valid loss: 7.323012946900867

Epoch: 11
Training loss: 0.5971928834915161 / Valid loss: 7.306082739148821
Training loss: 0.4947788119316101 / Valid loss: 7.218540384655907
Training loss: 0.5287043452262878 / Valid loss: 7.332875587826684
Training loss: 0.9104170799255371 / Valid loss: 7.342578147706531
Training loss: 1.1184040307998657 / Valid loss: 7.446291673751104

Epoch: 12
Training loss: 1.0035176277160645 / Valid loss: 7.357453255426316
Training loss: 0.6163936853408813 / Valid loss: 7.38992285274324
Training loss: 1.0886280536651611 / Valid loss: 7.405302638099307
Training loss: 0.9109894037246704 / Valid loss: 7.376698898133777
Training loss: 0.8998491764068604 / Valid loss: 7.369267549968901

Epoch: 13
Training loss: 1.0328352451324463 / Valid loss: 7.358583663758777
Training loss: 0.6183690428733826 / Valid loss: 7.391956865219843
Training loss: 0.601495087146759 / Valid loss: 7.4389725957598
Training loss: 0.40663301944732666 / Valid loss: 7.421403780437651
Training loss: 1.1051301956176758 / Valid loss: 7.384255232129778

Epoch: 14
Training loss: 0.885636031627655 / Valid loss: 7.457582437424433
Training loss: 0.6281585693359375 / Valid loss: 7.353997008005778
Training loss: 0.5500185489654541 / Valid loss: 7.451459941409883
Training loss: 0.6970300674438477 / Valid loss: 7.327338672819591
Training loss: 0.6214227676391602 / Valid loss: 7.451633621397472

Epoch: 15
Training loss: 0.7460180521011353 / Valid loss: 7.3676010177249
Training loss: 0.9202675223350525 / Valid loss: 7.395239548456101
Training loss: 0.6568134427070618 / Valid loss: 7.458904416220529
Training loss: 0.9768468141555786 / Valid loss: 7.5013692038399835
Training loss: 0.5057603120803833 / Valid loss: 7.4485572224571595

Epoch: 16
Training loss: 0.7672379612922668 / Valid loss: 7.482055986495245
Training loss: 0.468906432390213 / Valid loss: 7.350558807736351
Training loss: 0.5508931875228882 / Valid loss: 7.559954125540597
Training loss: 0.5009629130363464 / Valid loss: 7.41647074109032
Training loss: 1.219552755355835 / Valid loss: 7.430785987490699

Epoch: 17
Training loss: 0.4425523281097412 / Valid loss: 7.411799835023426
Training loss: 0.48141759634017944 / Valid loss: 7.399000917162214
Training loss: 0.6042002439498901 / Valid loss: 7.383636147635324
Training loss: 0.5066386461257935 / Valid loss: 7.441520114172072
Training loss: 0.5281336903572083 / Valid loss: 7.366032064528692

Epoch: 18
Training loss: 0.8493144512176514 / Valid loss: 7.396607503436861
Training loss: 0.6339578628540039 / Valid loss: 7.29809586207072
Training loss: 0.8495855331420898 / Valid loss: 7.461405522482735
Training loss: 0.5374593734741211 / Valid loss: 7.473029949551537
Training loss: 0.6354572772979736 / Valid loss: 7.518947767076038

Epoch: 19
Training loss: 0.6931893825531006 / Valid loss: 7.4366272381373815
Training loss: 0.6001438498497009 / Valid loss: 7.4388955888294035
Training loss: 0.8653198480606079 / Valid loss: 7.473286669594901
Training loss: 0.8635534048080444 / Valid loss: 7.381021667662121

Epoch: 20
Training loss: 0.3497566878795624 / Valid loss: 7.439469460078648
Training loss: 0.7403311729431152 / Valid loss: 7.3907571043287
Training loss: 0.46933436393737793 / Valid loss: 7.43006204877581
Training loss: 0.5005275011062622 / Valid loss: 7.4874702726091655
Training loss: 0.34816986322402954 / Valid loss: 7.402209504445394

Epoch: 21
Training loss: 0.29298651218414307 / Valid loss: 7.49943185533796
Training loss: 0.436429500579834 / Valid loss: 7.501671305156889
Training loss: 0.7361910343170166 / Valid loss: 7.44650452931722
Training loss: 0.7731226682662964 / Valid loss: 7.442082863762265
Training loss: 0.4338786005973816 / Valid loss: 7.464861860729399

Epoch: 22
Training loss: 0.6211975812911987 / Valid loss: 7.398812734513056
Training loss: 0.3836590051651001 / Valid loss: 7.4011797405424575
Training loss: 0.9951054453849792 / Valid loss: 7.475204635801769
Training loss: 0.4555125832557678 / Valid loss: 7.407384590875535
Training loss: 0.3994736671447754 / Valid loss: 7.466460377829415

Epoch: 23
Training loss: 0.3255210220813751 / Valid loss: 7.472931748344784
Training loss: 0.3644348382949829 / Valid loss: 7.405966054825556
Training loss: 0.5313097238540649 / Valid loss: 7.369209030696324
Training loss: 0.9748379588127136 / Valid loss: 7.464880075908843
Training loss: 0.7490112781524658 / Valid loss: 7.505924256642659

Epoch: 24
Training loss: 0.34345269203186035 / Valid loss: 7.400012729281471
Training loss: 0.45703136920928955 / Valid loss: 7.4830867903573175
Training loss: 0.7242852449417114 / Valid loss: 7.478847580864316
Training loss: 0.5612310171127319 / Valid loss: 7.491263507661365
Training loss: 0.47422125935554504 / Valid loss: 7.454245317549932

Epoch: 25
Training loss: 0.6131254434585571 / Valid loss: 7.425100031353178
Training loss: 0.44847238063812256 / Valid loss: 7.402441992078509
Training loss: 0.33235618472099304 / Valid loss: 7.428572209676107
Training loss: 0.4807819724082947 / Valid loss: 7.463781415848505
Training loss: 0.5163370966911316 / Valid loss: 7.502998002370199

Epoch: 26
Training loss: 0.4928652048110962 / Valid loss: 7.4495164553324384
Training loss: 0.47649529576301575 / Valid loss: 7.421929227738153
Training loss: 0.2882917821407318 / Valid loss: 7.397345813115438
Training loss: 0.4248126149177551 / Valid loss: 7.45180792354402
Training loss: 0.6208218932151794 / Valid loss: 7.494688024975004

Epoch: 27
Training loss: 0.6779052019119263 / Valid loss: 7.403963070824033
Training loss: 0.5611658692359924 / Valid loss: 7.466934444790795
Training loss: 0.3527006506919861 / Valid loss: 7.397992883409772
Training loss: 0.299749493598938 / Valid loss: 7.503248503094627
Training loss: 0.37642258405685425 / Valid loss: 7.452880600520543

Epoch: 28
Training loss: 0.2722954750061035 / Valid loss: 7.464211641039167
Training loss: 0.49733391404151917 / Valid loss: 7.429123907997495
Training loss: 0.5640161037445068 / Valid loss: 7.442752992539178
Training loss: 0.3871992230415344 / Valid loss: 7.436497928982689
Training loss: 0.35446226596832275 / Valid loss: 7.436201649620419

Epoch: 29
Training loss: 0.8823655247688293 / Valid loss: 7.428062629699707
Training loss: 0.28309881687164307 / Valid loss: 7.4314613523937405
Training loss: 0.7039158344268799 / Valid loss: 7.42208144778297
Training loss: 0.368105947971344 / Valid loss: 7.437211068471273

Epoch: 30
Training loss: 0.36323249340057373 / Valid loss: 7.505929174877348
Training loss: 0.5660228133201599 / Valid loss: 7.407612628028506
Training loss: 0.6321051716804504 / Valid loss: 7.443852810632615
Training loss: 0.6425257921218872 / Valid loss: 7.382415176573254
Training loss: 0.20862427353858948 / Valid loss: 7.399098609742664

Epoch: 31
Training loss: 0.3128720819950104 / Valid loss: 7.475439471290225
Training loss: 0.5331929326057434 / Valid loss: 7.412096573057629
Training loss: 0.3022081255912781 / Valid loss: 7.430392085938227
Training loss: 0.3662494421005249 / Valid loss: 7.3753628912426175
Training loss: 0.6534914374351501 / Valid loss: 7.4665672302246096

Epoch: 32
Training loss: 0.22813758254051208 / Valid loss: 7.416878182547433
Training loss: 0.2197948694229126 / Valid loss: 7.411509046100435
Training loss: 0.2746577262878418 / Valid loss: 7.388597561064221
Training loss: 0.25138986110687256 / Valid loss: 7.500303936004639
Training loss: 0.48038116097450256 / Valid loss: 7.425578421638125

Epoch: 33
Training loss: 0.8272233605384827 / Valid loss: 7.426470683869861
Training loss: 0.21560171246528625 / Valid loss: 7.483260781424386
Training loss: 0.32141199707984924 / Valid loss: 7.499370638529459
Training loss: 0.22301027178764343 / Valid loss: 7.401840137300037
Training loss: 0.3516150116920471 / Valid loss: 7.439924462636312

Epoch: 34
Training loss: 0.29784995317459106 / Valid loss: 7.453798144204276
Training loss: 0.3633214831352234 / Valid loss: 7.4718276886712935
Training loss: 0.6535081267356873 / Valid loss: 7.425814957845779
Training loss: 0.3159060478210449 / Valid loss: 7.3574576014564155
Training loss: 0.3035619258880615 / Valid loss: 7.460658745538621

Epoch: 35
Training loss: 0.46175599098205566 / Valid loss: 7.453582332247779
Training loss: 0.39463311433792114 / Valid loss: 7.476929324013846
Training loss: 0.2507975697517395 / Valid loss: 7.420674542018346
Training loss: 0.2074441909790039 / Valid loss: 7.388771606626965
Training loss: 0.30497902631759644 / Valid loss: 7.513482037044707

Epoch: 36
Training loss: 0.3769516050815582 / Valid loss: 7.5031095686413
Training loss: 0.385606586933136 / Valid loss: 7.415899503798712
Training loss: 0.20575377345085144 / Valid loss: 7.41361775852385
Training loss: 0.5997200012207031 / Valid loss: 7.474612503960019
Training loss: 0.22607170045375824 / Valid loss: 7.439918182009742

Epoch: 37
Training loss: 0.3440192937850952 / Valid loss: 7.484620720999581
Training loss: 1.2334554195404053 / Valid loss: 7.436209383464995
Training loss: 0.3818822205066681 / Valid loss: 7.419439633687337
Training loss: 0.31169945001602173 / Valid loss: 7.393194897969564
Training loss: 0.45408427715301514 / Valid loss: 7.406611406235468

Epoch: 38
Training loss: 0.3675156831741333 / Valid loss: 7.4151132220313665
Training loss: 0.18434171378612518 / Valid loss: 7.436761011396135
Training loss: 0.42545539140701294 / Valid loss: 7.440089225769043
Training loss: 0.2642323672771454 / Valid loss: 7.439441163199288
Training loss: 0.39886003732681274 / Valid loss: 7.463645153953916

Epoch: 39
Training loss: 0.25371992588043213 / Valid loss: 7.422028264545259
Training loss: 0.3428855538368225 / Valid loss: 7.488713882082984
Training loss: 0.582263171672821 / Valid loss: 7.450160766783215
Training loss: 0.22840812802314758 / Valid loss: 7.50790086927868
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 6.017267088663011
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.986988067626953 / Valid loss: 16.471467853727795
Model is saved in epoch 0, overall batch: 0
Training loss: 18.63018035888672 / Valid loss: 15.407264627729143
Model is saved in epoch 0, overall batch: 100
Training loss: 9.775970458984375 / Valid loss: 14.426258850097657
Model is saved in epoch 0, overall batch: 200
Training loss: 14.232864379882812 / Valid loss: 13.518839972359794
Model is saved in epoch 0, overall batch: 300
Training loss: 10.275541305541992 / Valid loss: 12.678959583100818
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 9.813549041748047 / Valid loss: 11.911137335641044
Model is saved in epoch 1, overall batch: 500
Training loss: 8.737800598144531 / Valid loss: 11.236984107607887
Model is saved in epoch 1, overall batch: 600
Training loss: 10.666757583618164 / Valid loss: 10.617521808260964
Model is saved in epoch 1, overall batch: 700
Training loss: 10.168182373046875 / Valid loss: 10.048979196094331
Model is saved in epoch 1, overall batch: 800
Training loss: 13.487499237060547 / Valid loss: 9.520679791768393
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 7.51106071472168 / Valid loss: 9.050390645435877
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.945745944976807 / Valid loss: 8.648081552414666
Model is saved in epoch 2, overall batch: 1100
Training loss: 8.438943862915039 / Valid loss: 8.284702714284261
Model is saved in epoch 2, overall batch: 1200
Training loss: 6.492394924163818 / Valid loss: 7.949681014106387
Model is saved in epoch 2, overall batch: 1300
Training loss: 8.264232635498047 / Valid loss: 7.648545896439325
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.461374282836914 / Valid loss: 7.379040933790661
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.330970764160156 / Valid loss: 7.13654230208624
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.036120891571045 / Valid loss: 6.944165985924857
Model is saved in epoch 3, overall batch: 1700
Training loss: 7.556905269622803 / Valid loss: 6.774666450137183
Model is saved in epoch 3, overall batch: 1800
Training loss: 9.208368301391602 / Valid loss: 6.615230208351498
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 8.830127716064453 / Valid loss: 6.485729703449068
Model is saved in epoch 4, overall batch: 2000
Training loss: 7.469746112823486 / Valid loss: 6.355050073351179
Model is saved in epoch 4, overall batch: 2100
Training loss: 7.797349452972412 / Valid loss: 6.256985076268514
Model is saved in epoch 4, overall batch: 2200
Training loss: 7.23930025100708 / Valid loss: 6.165032670611427
Model is saved in epoch 4, overall batch: 2300
Training loss: 4.6738080978393555 / Valid loss: 6.0889358656747
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 3.5148346424102783 / Valid loss: 6.0190873622894285
Model is saved in epoch 5, overall batch: 2500
Training loss: 7.282408714294434 / Valid loss: 5.98071608543396
Model is saved in epoch 5, overall batch: 2600
Training loss: 4.551709175109863 / Valid loss: 5.931868180774507
Model is saved in epoch 5, overall batch: 2700
Training loss: 7.018116474151611 / Valid loss: 5.889771372931344
Model is saved in epoch 5, overall batch: 2800
Training loss: 3.8549957275390625 / Valid loss: 5.8466522307623
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 4.616265296936035 / Valid loss: 5.8137611979529975
Model is saved in epoch 6, overall batch: 3000
Training loss: 5.636221885681152 / Valid loss: 5.8003549462273005
Model is saved in epoch 6, overall batch: 3100
Training loss: 3.526322841644287 / Valid loss: 5.772770541054862
Model is saved in epoch 6, overall batch: 3200
Training loss: 5.155546188354492 / Valid loss: 5.748990079334804
Model is saved in epoch 6, overall batch: 3300
Training loss: 3.940946102142334 / Valid loss: 5.737064599990845
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 4.155340194702148 / Valid loss: 5.720840980893089
Model is saved in epoch 7, overall batch: 3500
Training loss: 6.737896919250488 / Valid loss: 5.7037204651605515
Model is saved in epoch 7, overall batch: 3600
Training loss: 3.382598638534546 / Valid loss: 5.686359469095866
Model is saved in epoch 7, overall batch: 3700
Training loss: 5.368721961975098 / Valid loss: 5.671949731735956
Model is saved in epoch 7, overall batch: 3800
Training loss: 5.057578086853027 / Valid loss: 5.6711028348831904
Model is saved in epoch 7, overall batch: 3900

Epoch: 8
Training loss: 4.709362506866455 / Valid loss: 5.658303771700178
Model is saved in epoch 8, overall batch: 4000
Training loss: 6.755249500274658 / Valid loss: 5.651263250623431
Model is saved in epoch 8, overall batch: 4100
Training loss: 5.235943794250488 / Valid loss: 5.648202575956073
Model is saved in epoch 8, overall batch: 4200
Training loss: 6.896762847900391 / Valid loss: 5.645067664555141
Model is saved in epoch 8, overall batch: 4300
Training loss: 5.341348171234131 / Valid loss: 5.629574787049067
Model is saved in epoch 8, overall batch: 4400

Epoch: 9
Training loss: 6.07421350479126 / Valid loss: 5.625497588657198
Model is saved in epoch 9, overall batch: 4500
Training loss: 5.768555641174316 / Valid loss: 5.620560525712513
Model is saved in epoch 9, overall batch: 4600
Training loss: 5.625482559204102 / Valid loss: 5.6066020965576175
Model is saved in epoch 9, overall batch: 4700
Training loss: 5.50877046585083 / Valid loss: 5.604776262101673
Model is saved in epoch 9, overall batch: 4800

Epoch: 10
Training loss: 5.38389253616333 / Valid loss: 5.60124729020255
Model is saved in epoch 10, overall batch: 4900
Training loss: 6.878102779388428 / Valid loss: 5.592987362543742
Model is saved in epoch 10, overall batch: 5000
Training loss: 6.6444549560546875 / Valid loss: 5.584481430053711
Model is saved in epoch 10, overall batch: 5100
Training loss: 4.63402795791626 / Valid loss: 5.582790465581985
Model is saved in epoch 10, overall batch: 5200
Training loss: 5.699923515319824 / Valid loss: 5.57356477237883
Model is saved in epoch 10, overall batch: 5300

Epoch: 11
Training loss: 5.739457130432129 / Valid loss: 5.569696467263358
Model is saved in epoch 11, overall batch: 5400
Training loss: 5.298558235168457 / Valid loss: 5.562433910369873
Model is saved in epoch 11, overall batch: 5500
Training loss: 6.251924514770508 / Valid loss: 5.565613935107277
Training loss: 5.0719404220581055 / Valid loss: 5.557139371690296
Model is saved in epoch 11, overall batch: 5700
Training loss: 3.952608585357666 / Valid loss: 5.541522911616734
Model is saved in epoch 11, overall batch: 5800

Epoch: 12
Training loss: 6.005978584289551 / Valid loss: 5.5536993208385645
Training loss: 3.7381834983825684 / Valid loss: 5.546840962909517
Training loss: 3.470838785171509 / Valid loss: 5.5433446906861805
Training loss: 6.545736312866211 / Valid loss: 5.538538587661017
Model is saved in epoch 12, overall batch: 6200
Training loss: 3.8931689262390137 / Valid loss: 5.528887083416893
Model is saved in epoch 12, overall batch: 6300

Epoch: 13
Training loss: 3.92470121383667 / Valid loss: 5.53395075343904
Training loss: 3.489238739013672 / Valid loss: 5.5299920649755565
Training loss: 6.522801399230957 / Valid loss: 5.526157131649199
Model is saved in epoch 13, overall batch: 6600
Training loss: 5.315229892730713 / Valid loss: 5.522433037984939
Model is saved in epoch 13, overall batch: 6700
Training loss: 5.406111717224121 / Valid loss: 5.52181879679362
Model is saved in epoch 13, overall batch: 6800

Epoch: 14
Training loss: 5.596658706665039 / Valid loss: 5.517273214885167
Model is saved in epoch 14, overall batch: 6900
Training loss: 3.982999801635742 / Valid loss: 5.506078792753674
Model is saved in epoch 14, overall batch: 7000
Training loss: 5.672061443328857 / Valid loss: 5.512235907145909
Training loss: 6.009856224060059 / Valid loss: 5.512065410614014
Training loss: 4.391214847564697 / Valid loss: 5.508027142570132

Epoch: 15
Training loss: 5.508995532989502 / Valid loss: 5.502175655819121
Model is saved in epoch 15, overall batch: 7400
Training loss: 5.6745219230651855 / Valid loss: 5.491270660218738
Model is saved in epoch 15, overall batch: 7500
Training loss: 3.1222219467163086 / Valid loss: 5.4989682379223055
Training loss: 4.454006195068359 / Valid loss: 5.496225236711048
Training loss: 6.341487884521484 / Valid loss: 5.494764089584351

Epoch: 16
Training loss: 3.4939026832580566 / Valid loss: 5.486854096821376
Model is saved in epoch 16, overall batch: 7900
Training loss: 5.857463836669922 / Valid loss: 5.479247865222749
Model is saved in epoch 16, overall batch: 8000
Training loss: 5.7376909255981445 / Valid loss: 5.48903869447254
Training loss: 4.922816276550293 / Valid loss: 5.479236457461402
Model is saved in epoch 16, overall batch: 8200
Training loss: 5.147218704223633 / Valid loss: 5.486211354391916

Epoch: 17
Training loss: 6.7169084548950195 / Valid loss: 5.482763898940314
Training loss: 4.572077751159668 / Valid loss: 5.474629352206276
Model is saved in epoch 17, overall batch: 8500
Training loss: 4.293863296508789 / Valid loss: 5.479684134892055
Training loss: 3.8922410011291504 / Valid loss: 5.47973282904852
Training loss: 5.4588942527771 / Valid loss: 5.473161218279884
Model is saved in epoch 17, overall batch: 8800

Epoch: 18
Training loss: 3.520796775817871 / Valid loss: 5.477810489563715
Training loss: 4.348184108734131 / Valid loss: 5.470775849478585
Model is saved in epoch 18, overall batch: 9000
Training loss: 4.205758094787598 / Valid loss: 5.472903574080695
Training loss: 4.5511908531188965 / Valid loss: 5.4734054406483965
Training loss: 5.027217864990234 / Valid loss: 5.474196272804623

Epoch: 19
Training loss: 3.8575048446655273 / Valid loss: 5.464369122187296
Model is saved in epoch 19, overall batch: 9400
Training loss: 6.779113292694092 / Valid loss: 5.462612660725911
Model is saved in epoch 19, overall batch: 9500
Training loss: 3.4621453285217285 / Valid loss: 5.467450017020816
Training loss: 6.038926601409912 / Valid loss: 5.461942895253499
Model is saved in epoch 19, overall batch: 9700

Epoch: 20
Training loss: 3.831066608428955 / Valid loss: 5.460612408320109
Model is saved in epoch 20, overall batch: 9800
Training loss: 5.324001312255859 / Valid loss: 5.4684534254528225
Training loss: 4.446750640869141 / Valid loss: 5.46638742855617
Training loss: 4.5989990234375 / Valid loss: 5.459184828258696
Model is saved in epoch 20, overall batch: 10100
Training loss: 4.198691368103027 / Valid loss: 5.460936099007016

Epoch: 21
Training loss: 5.774990081787109 / Valid loss: 5.463640494573684
Training loss: 3.729306697845459 / Valid loss: 5.465460695539202
Training loss: 4.4373064041137695 / Valid loss: 5.463199454262143
Training loss: 5.386929988861084 / Valid loss: 5.463839471907843
Training loss: 3.557811737060547 / Valid loss: 5.461174031666347

Epoch: 22
Training loss: 5.330171585083008 / Valid loss: 5.46182580221267
Training loss: 5.447303295135498 / Valid loss: 5.460245909009661
Training loss: 3.747264862060547 / Valid loss: 5.459955755869547
Training loss: 4.409695625305176 / Valid loss: 5.462120246887207
Training loss: 3.5266315937042236 / Valid loss: 5.459195972624279

Epoch: 23
Training loss: 4.412166595458984 / Valid loss: 5.459629917144776
Training loss: 5.923771381378174 / Valid loss: 5.462530449458531
Training loss: 4.794498443603516 / Valid loss: 5.4594519978477845
Training loss: 4.277701377868652 / Valid loss: 5.4631566865103585
Training loss: 3.39920711517334 / Valid loss: 5.460540678387597

Epoch: 24
Training loss: 3.986401319503784 / Valid loss: 5.461489632016137
Training loss: 4.655772686004639 / Valid loss: 5.4562210241953535
Model is saved in epoch 24, overall batch: 11900
Training loss: 2.764206886291504 / Valid loss: 5.458539497284662
Training loss: 4.550921440124512 / Valid loss: 5.458201120013283
Training loss: 4.368588924407959 / Valid loss: 5.458906963893345

Epoch: 25
Training loss: 4.102358818054199 / Valid loss: 5.454728451229277
Model is saved in epoch 25, overall batch: 12300
Training loss: 3.1712138652801514 / Valid loss: 5.462471909750075
Training loss: 5.878548622131348 / Valid loss: 5.464515390850249
Training loss: 4.762780666351318 / Valid loss: 5.461724079222906
Training loss: 4.97176456451416 / Valid loss: 5.457019946688697

Epoch: 26
Training loss: 3.4158682823181152 / Valid loss: 5.463481980278378
Training loss: 2.5887980461120605 / Valid loss: 5.462875466119676
Training loss: 3.9578447341918945 / Valid loss: 5.46315069652739
Training loss: 4.805314064025879 / Valid loss: 5.465556673776536
Training loss: 3.0926761627197266 / Valid loss: 5.464044427871704

Epoch: 27
Training loss: 4.249815940856934 / Valid loss: 5.465391229447865
Training loss: 3.9139084815979004 / Valid loss: 5.466057271049136
Training loss: 4.237305641174316 / Valid loss: 5.465354111081078
Training loss: 4.648406982421875 / Valid loss: 5.466040681657337
Training loss: 4.868549346923828 / Valid loss: 5.462231129691714

Epoch: 28
Training loss: 3.7042183876037598 / Valid loss: 5.467510445912679
Training loss: 3.237563133239746 / Valid loss: 5.462727092561268
Training loss: 3.533482551574707 / Valid loss: 5.470671199616932
Training loss: 4.426372528076172 / Valid loss: 5.468109223956153
Training loss: 3.2713396549224854 / Valid loss: 5.470784898031326

Epoch: 29
Training loss: 4.34877872467041 / Valid loss: 5.467292658487955
Training loss: 4.953998565673828 / Valid loss: 5.466768242063976
Training loss: 4.69462776184082 / Valid loss: 5.46694800286066
Training loss: 4.653944969177246 / Valid loss: 5.4733062290009995

Epoch: 30
Training loss: 5.436652183532715 / Valid loss: 5.471356276103428
Training loss: 4.259218215942383 / Valid loss: 5.476401240485055
Training loss: 3.3208744525909424 / Valid loss: 5.480070155007499
Training loss: 3.6347432136535645 / Valid loss: 5.479887628555298
Training loss: 3.8807125091552734 / Valid loss: 5.481768864677066

Epoch: 31
Training loss: 4.384098529815674 / Valid loss: 5.4811521371205645
Training loss: 5.077935695648193 / Valid loss: 5.476759143102736
Training loss: 4.34765100479126 / Valid loss: 5.479397217432658
Training loss: 5.206612586975098 / Valid loss: 5.48281569480896
Training loss: 4.906699180603027 / Valid loss: 5.476487009865897

Epoch: 32
Training loss: 2.665677070617676 / Valid loss: 5.483926196325393
Training loss: 4.790565490722656 / Valid loss: 5.485879421234131
Training loss: 4.158268928527832 / Valid loss: 5.485532283782959
Training loss: 4.1446990966796875 / Valid loss: 5.48585102217538
Training loss: 2.8604230880737305 / Valid loss: 5.489724191029866

Epoch: 33
Training loss: 5.027865409851074 / Valid loss: 5.490809102285476
Training loss: 2.909865379333496 / Valid loss: 5.489012529736473
Training loss: 4.623377799987793 / Valid loss: 5.493612559636434
Training loss: 3.740933418273926 / Valid loss: 5.486162889571417
Training loss: 5.635357856750488 / Valid loss: 5.496842082341512

Epoch: 34
Training loss: 3.541306972503662 / Valid loss: 5.495888832637242
Training loss: 3.288703680038452 / Valid loss: 5.4970024335952035
Training loss: 3.410468339920044 / Valid loss: 5.488862846011207
Training loss: 3.601592540740967 / Valid loss: 5.499371957778931
Training loss: 4.07620906829834 / Valid loss: 5.492313414528256

Epoch: 35
Training loss: 3.960672616958618 / Valid loss: 5.496681033997309
Training loss: 4.6308746337890625 / Valid loss: 5.503932653154646
Training loss: 4.73881196975708 / Valid loss: 5.504157652173723
Training loss: 5.873665809631348 / Valid loss: 5.500879251389277
Training loss: 5.6107072830200195 / Valid loss: 5.496418594178699

Epoch: 36
Training loss: 4.059154987335205 / Valid loss: 5.505883625575474
Training loss: 5.345928192138672 / Valid loss: 5.497515855516706
Training loss: 3.2264883518218994 / Valid loss: 5.50652684257144
Training loss: 3.550762414932251 / Valid loss: 5.513733995528448
Training loss: 2.8342270851135254 / Valid loss: 5.512877189545405

Epoch: 37
Training loss: 3.375135898590088 / Valid loss: 5.512912913731166
Training loss: 3.8723583221435547 / Valid loss: 5.515965949921381
Training loss: 3.3152122497558594 / Valid loss: 5.514694118499756
Training loss: 3.689467430114746 / Valid loss: 5.511787911823817
Training loss: 4.012165069580078 / Valid loss: 5.515170079185849

Epoch: 38
Training loss: 3.209845542907715 / Valid loss: 5.516621816725958
Training loss: 3.539217233657837 / Valid loss: 5.520804566428775
Training loss: 4.223217010498047 / Valid loss: 5.520545587085542
Training loss: 3.669124126434326 / Valid loss: 5.523608119147164
Training loss: 4.354582786560059 / Valid loss: 5.525421344666254

Epoch: 39
Training loss: 2.2911105155944824 / Valid loss: 5.525607056844802
Training loss: 3.8304402828216553 / Valid loss: 5.530759738740467
Training loss: 4.818692684173584 / Valid loss: 5.529716348648071
Training loss: 3.9178876876831055 / Valid loss: 5.533752554938907
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 12300): 5.356681946345738
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.986988067626953 / Valid loss: 16.471485319591704
Model is saved in epoch 0, overall batch: 0
Training loss: 18.62422752380371 / Valid loss: 15.407388896033877
Model is saved in epoch 0, overall batch: 100
Training loss: 9.775168418884277 / Valid loss: 14.4264066696167
Model is saved in epoch 0, overall batch: 200
Training loss: 14.232662200927734 / Valid loss: 13.519003831772578
Model is saved in epoch 0, overall batch: 300
Training loss: 10.273649215698242 / Valid loss: 12.679173932756697
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 9.815580368041992 / Valid loss: 11.911415990193685
Model is saved in epoch 1, overall batch: 500
Training loss: 8.736431121826172 / Valid loss: 11.237310804639543
Model is saved in epoch 1, overall batch: 600
Training loss: 10.664634704589844 / Valid loss: 10.61787927264259
Model is saved in epoch 1, overall batch: 700
Training loss: 10.169143676757812 / Valid loss: 10.049401256016322
Model is saved in epoch 1, overall batch: 800
Training loss: 13.490087509155273 / Valid loss: 9.521157087598528
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 7.508336067199707 / Valid loss: 9.050909846169608
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.949461460113525 / Valid loss: 8.648666972205753
Model is saved in epoch 2, overall batch: 1100
Training loss: 8.441496849060059 / Valid loss: 8.28530647187006
Model is saved in epoch 2, overall batch: 1200
Training loss: 6.492798805236816 / Valid loss: 7.950325375511532
Model is saved in epoch 2, overall batch: 1300
Training loss: 8.266548156738281 / Valid loss: 7.6492135706402005
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.462076187133789 / Valid loss: 7.379704071226574
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.334012985229492 / Valid loss: 7.137224061148507
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.03793478012085 / Valid loss: 6.9448509148189
Model is saved in epoch 3, overall batch: 1700
Training loss: 7.556719779968262 / Valid loss: 6.775339589800153
Model is saved in epoch 3, overall batch: 1800
Training loss: 9.20730972290039 / Valid loss: 6.615896976561773
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 8.831502914428711 / Valid loss: 6.486379096621559
Model is saved in epoch 4, overall batch: 2000
Training loss: 7.469795227050781 / Valid loss: 6.3556844484238395
Model is saved in epoch 4, overall batch: 2100
Training loss: 7.7973833084106445 / Valid loss: 6.257589544568743
Model is saved in epoch 4, overall batch: 2200
Training loss: 7.238829135894775 / Valid loss: 6.165608530952817
Model is saved in epoch 4, overall batch: 2300
Training loss: 4.670550346374512 / Valid loss: 6.089478367850894
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 3.5167486667633057 / Valid loss: 6.019614780516852
Model is saved in epoch 5, overall batch: 2500
Training loss: 7.286543846130371 / Valid loss: 5.981192286809286
Model is saved in epoch 5, overall batch: 2600
Training loss: 4.551792621612549 / Valid loss: 5.932322983514695
Model is saved in epoch 5, overall batch: 2700
Training loss: 7.020753860473633 / Valid loss: 5.890190192631313
Model is saved in epoch 5, overall batch: 2800
Training loss: 3.8560116291046143 / Valid loss: 5.847040389832996
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 4.618404388427734 / Valid loss: 5.814113803136916
Model is saved in epoch 6, overall batch: 3000
Training loss: 5.639842510223389 / Valid loss: 5.800701164063954
Model is saved in epoch 6, overall batch: 3100
Training loss: 3.527839183807373 / Valid loss: 5.773074733643305
Model is saved in epoch 6, overall batch: 3200
Training loss: 5.151665210723877 / Valid loss: 5.749265241622925
Model is saved in epoch 6, overall batch: 3300
Training loss: 3.940082550048828 / Valid loss: 5.737337857200986
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 4.156885623931885 / Valid loss: 5.721093059721447
Model is saved in epoch 7, overall batch: 3500
Training loss: 6.741113662719727 / Valid loss: 5.703955203010922
Model is saved in epoch 7, overall batch: 3600
Training loss: 3.3835718631744385 / Valid loss: 5.686567674364363
Model is saved in epoch 7, overall batch: 3700
Training loss: 5.370339870452881 / Valid loss: 5.672170754841396
Model is saved in epoch 7, overall batch: 3800
Training loss: 5.062630653381348 / Valid loss: 5.671327568235851
Model is saved in epoch 7, overall batch: 3900

Epoch: 8
Training loss: 4.710190296173096 / Valid loss: 5.65852023987543
Model is saved in epoch 8, overall batch: 4000
Training loss: 6.75522518157959 / Valid loss: 5.651457507269723
Model is saved in epoch 8, overall batch: 4100
Training loss: 5.23668909072876 / Valid loss: 5.648408624104091
Model is saved in epoch 8, overall batch: 4200
Training loss: 6.898776531219482 / Valid loss: 5.645268036070324
Model is saved in epoch 8, overall batch: 4300
Training loss: 5.344922065734863 / Valid loss: 5.629768943786621
Model is saved in epoch 8, overall batch: 4400

Epoch: 9
Training loss: 6.07792854309082 / Valid loss: 5.625689922060285
Model is saved in epoch 9, overall batch: 4500
Training loss: 5.770915508270264 / Valid loss: 5.620749870936076
Model is saved in epoch 9, overall batch: 4600
Training loss: 5.629438877105713 / Valid loss: 5.606780463173276
Model is saved in epoch 9, overall batch: 4700
Training loss: 5.508785247802734 / Valid loss: 5.604958752223424
Model is saved in epoch 9, overall batch: 4800

Epoch: 10
Training loss: 5.38722038269043 / Valid loss: 5.601431428818476
Model is saved in epoch 10, overall batch: 4900
Training loss: 6.877938270568848 / Valid loss: 5.593155127479917
Model is saved in epoch 10, overall batch: 5000
Training loss: 6.648453712463379 / Valid loss: 5.5846529234023325
Model is saved in epoch 10, overall batch: 5100
Training loss: 4.637917995452881 / Valid loss: 5.5829649198622935
Model is saved in epoch 10, overall batch: 5200
Training loss: 5.701563358306885 / Valid loss: 5.5737248852139425
Model is saved in epoch 10, overall batch: 5300

Epoch: 11
Training loss: 5.743037223815918 / Valid loss: 5.56987206141154
Model is saved in epoch 11, overall batch: 5400
Training loss: 5.296698570251465 / Valid loss: 5.562594095865886
Model is saved in epoch 11, overall batch: 5500
Training loss: 6.253935813903809 / Valid loss: 5.565773543857393
Training loss: 5.076759338378906 / Valid loss: 5.557286255700248
Model is saved in epoch 11, overall batch: 5700
Training loss: 3.958545446395874 / Valid loss: 5.541669636680966
Model is saved in epoch 11, overall batch: 5800

Epoch: 12
Training loss: 6.009912490844727 / Valid loss: 5.553832451502482
Training loss: 3.74214506149292 / Valid loss: 5.546961788904099
Training loss: 3.4764716625213623 / Valid loss: 5.54345824150812
Training loss: 6.552170753479004 / Valid loss: 5.538636654899234
Model is saved in epoch 12, overall batch: 6200
Training loss: 3.8990883827209473 / Valid loss: 5.528972484951928
Model is saved in epoch 12, overall batch: 6300

Epoch: 13
Training loss: 3.928645133972168 / Valid loss: 5.534032135918027
Training loss: 3.4929351806640625 / Valid loss: 5.5300695464724585
Training loss: 6.528385639190674 / Valid loss: 5.526212472007388
Model is saved in epoch 13, overall batch: 6600
Training loss: 5.323808670043945 / Valid loss: 5.5224812235151015
Model is saved in epoch 13, overall batch: 6700
Training loss: 5.408919811248779 / Valid loss: 5.521847252618699
Model is saved in epoch 13, overall batch: 6800

Epoch: 14
Training loss: 5.602349281311035 / Valid loss: 5.517270907901582
Model is saved in epoch 14, overall batch: 6900
Training loss: 3.9849839210510254 / Valid loss: 5.5060539654323035
Model is saved in epoch 14, overall batch: 7000
Training loss: 5.674663543701172 / Valid loss: 5.512203491301763
Training loss: 6.012394905090332 / Valid loss: 5.5120160693214055
Training loss: 4.395998001098633 / Valid loss: 5.507956261861892

Epoch: 15
Training loss: 5.512490749359131 / Valid loss: 5.502071339743478
Model is saved in epoch 15, overall batch: 7400
Training loss: 5.681952953338623 / Valid loss: 5.491162474950155
Model is saved in epoch 15, overall batch: 7500
Training loss: 3.129734992980957 / Valid loss: 5.49880094301133
Training loss: 4.4610185623168945 / Valid loss: 5.496039906002227
Training loss: 6.349005699157715 / Valid loss: 5.494543484279087

Epoch: 16
Training loss: 3.5026564598083496 / Valid loss: 5.4866321041470485
Model is saved in epoch 16, overall batch: 7900
Training loss: 5.867571830749512 / Valid loss: 5.479004780451457
Model is saved in epoch 16, overall batch: 8000
Training loss: 5.744171142578125 / Valid loss: 5.488745410101754
Training loss: 4.930498123168945 / Valid loss: 5.478908475240072
Model is saved in epoch 16, overall batch: 8200
Training loss: 5.1564412117004395 / Valid loss: 5.485844389597575

Epoch: 17
Training loss: 6.730345249176025 / Valid loss: 5.48235425494966
Training loss: 4.586308002471924 / Valid loss: 5.474174817403157
Model is saved in epoch 17, overall batch: 8500
Training loss: 4.304023265838623 / Valid loss: 5.479198666981288
Training loss: 3.9020519256591797 / Valid loss: 5.4792078018188475
Training loss: 5.471802711486816 / Valid loss: 5.4725724129449755
Model is saved in epoch 17, overall batch: 8800

Epoch: 18
Training loss: 3.5298361778259277 / Valid loss: 5.477198189780825
Training loss: 4.356614112854004 / Valid loss: 5.47011410849435
Model is saved in epoch 18, overall batch: 9000
Training loss: 4.21766471862793 / Valid loss: 5.47221101579212
Training loss: 4.553805351257324 / Valid loss: 5.472652308146158
Training loss: 5.039048194885254 / Valid loss: 5.4733726546877906

Epoch: 19
Training loss: 3.8700528144836426 / Valid loss: 5.463481496629261
Model is saved in epoch 19, overall batch: 9400
Training loss: 6.7884111404418945 / Valid loss: 5.4617010048457555
Model is saved in epoch 19, overall batch: 9500
Training loss: 3.475285291671753 / Valid loss: 5.4665438901810415
Training loss: 6.056950569152832 / Valid loss: 5.460843642552693
Model is saved in epoch 19, overall batch: 9700

Epoch: 20
Training loss: 3.8472158908843994 / Valid loss: 5.459486016773043
Model is saved in epoch 20, overall batch: 9800
Training loss: 5.3353400230407715 / Valid loss: 5.467244055157616
Training loss: 4.457357883453369 / Valid loss: 5.46513009752546
Training loss: 4.613487243652344 / Valid loss: 5.45787194115775
Model is saved in epoch 20, overall batch: 10100
Training loss: 4.211658477783203 / Valid loss: 5.459570453280494

Epoch: 21
Training loss: 5.785271644592285 / Valid loss: 5.462174304326376
Training loss: 3.7468175888061523 / Valid loss: 5.463928140912738
Training loss: 4.452763557434082 / Valid loss: 5.461595551172892
Training loss: 5.401202201843262 / Valid loss: 5.4621077810014995
Training loss: 3.5733540058135986 / Valid loss: 5.459412393115816

Epoch: 22
Training loss: 5.349349021911621 / Valid loss: 5.459979043688093
Training loss: 5.46153450012207 / Valid loss: 5.45831112634568
Training loss: 3.7591395378112793 / Valid loss: 5.4579503581637425
Training loss: 4.422696113586426 / Valid loss: 5.460025601159959
Training loss: 3.5458390712738037 / Valid loss: 5.457024796803792
Model is saved in epoch 22, overall batch: 11200

Epoch: 23
Training loss: 4.429222106933594 / Valid loss: 5.457394545418875
Training loss: 5.942568778991699 / Valid loss: 5.460158693222772
Training loss: 4.806405544281006 / Valid loss: 5.45698386373974
Model is saved in epoch 23, overall batch: 11500
Training loss: 4.293961524963379 / Valid loss: 5.460608448301043
Training loss: 3.4168267250061035 / Valid loss: 5.457908414659046

Epoch: 24
Training loss: 3.998537302017212 / Valid loss: 5.458716322126842
Training loss: 4.674107551574707 / Valid loss: 5.45333167938959
Model is saved in epoch 24, overall batch: 11900
Training loss: 2.7774085998535156 / Valid loss: 5.455580593290783
Training loss: 4.578322410583496 / Valid loss: 5.455108876455398
Training loss: 4.390563488006592 / Valid loss: 5.455703642254784

Epoch: 25
Training loss: 4.120656967163086 / Valid loss: 5.451450895127796
Model is saved in epoch 25, overall batch: 12300
Training loss: 3.1887006759643555 / Valid loss: 5.459069812865485
Training loss: 5.911221504211426 / Valid loss: 5.461045707975115
Training loss: 4.784975528717041 / Valid loss: 5.458039463134039
Training loss: 4.991654396057129 / Valid loss: 5.453219325201852

Epoch: 26
Training loss: 3.4392080307006836 / Valid loss: 5.459607424054827
Training loss: 2.618863105773926 / Valid loss: 5.458926437014625
Training loss: 3.980517864227295 / Valid loss: 5.459045323871431
Training loss: 4.829904079437256 / Valid loss: 5.461433113188971
Training loss: 3.1098384857177734 / Valid loss: 5.45964480127607

Epoch: 27
Training loss: 4.271839141845703 / Valid loss: 5.460880894888015
Training loss: 3.9417612552642822 / Valid loss: 5.461422479720342
Training loss: 4.264764785766602 / Valid loss: 5.46062571661813
Training loss: 4.6761474609375 / Valid loss: 5.461165151141938
Training loss: 4.899799346923828 / Valid loss: 5.457178390593755

Epoch: 28
Training loss: 3.731766700744629 / Valid loss: 5.462337453024728
Training loss: 3.2566120624542236 / Valid loss: 5.457318837302072
Training loss: 3.5515358448028564 / Valid loss: 5.465219638461158
Training loss: 4.447166442871094 / Valid loss: 5.462509695688883
Training loss: 3.296337127685547 / Valid loss: 5.465069802602132

Epoch: 29
Training loss: 4.382471084594727 / Valid loss: 5.461357586724418
Training loss: 4.975820541381836 / Valid loss: 5.460603119078137
Training loss: 4.724062919616699 / Valid loss: 5.460708590916225
Training loss: 4.684542179107666 / Valid loss: 5.466857880637759

Epoch: 30
Training loss: 5.46334171295166 / Valid loss: 5.464724577040899
Training loss: 4.285224914550781 / Valid loss: 5.469712050755819
Training loss: 3.350390911102295 / Valid loss: 5.473192955198742
Training loss: 3.6567559242248535 / Valid loss: 5.472817934127081
Training loss: 3.909605026245117 / Valid loss: 5.474607912699382

Epoch: 31
Training loss: 4.421286106109619 / Valid loss: 5.473762823286511
Training loss: 5.098689079284668 / Valid loss: 5.469135223116194
Training loss: 4.371725559234619 / Valid loss: 5.471580201103574
Training loss: 5.246979713439941 / Valid loss: 5.474852230435326
Training loss: 4.9393310546875 / Valid loss: 5.468327703930083

Epoch: 32
Training loss: 2.6893815994262695 / Valid loss: 5.475505020504906
Training loss: 4.8144731521606445 / Valid loss: 5.477412053516933
Training loss: 4.19667387008667 / Valid loss: 5.4768324511391775
Training loss: 4.170783519744873 / Valid loss: 5.476995386396136
Training loss: 2.8849000930786133 / Valid loss: 5.4804862726302375

Epoch: 33
Training loss: 5.060115814208984 / Valid loss: 5.48136682510376
Training loss: 2.9325428009033203 / Valid loss: 5.479420382635934
Training loss: 4.6576337814331055 / Valid loss: 5.48374316124689
Training loss: 3.770930290222168 / Valid loss: 5.476215160460699
Training loss: 5.663240909576416 / Valid loss: 5.486558525902884

Epoch: 34
Training loss: 3.57712459564209 / Valid loss: 5.48547111919948
Training loss: 3.318114995956421 / Valid loss: 5.486313095546904
Training loss: 3.4398274421691895 / Valid loss: 5.4781040146237325
Training loss: 3.6472079753875732 / Valid loss: 5.488411099570138
Training loss: 4.1252593994140625 / Valid loss: 5.481063981283278

Epoch: 35
Training loss: 3.996511220932007 / Valid loss: 5.4853177252269925
Training loss: 4.656855583190918 / Valid loss: 5.492266189484369
Training loss: 4.767539024353027 / Valid loss: 5.4922541255042665
Training loss: 5.917658805847168 / Valid loss: 5.488586098807199
Training loss: 5.650354385375977 / Valid loss: 5.483974647521973

Epoch: 36
Training loss: 4.092553615570068 / Valid loss: 5.493442980448405
Training loss: 5.393208026885986 / Valid loss: 5.484662560054234
Training loss: 3.2560737133026123 / Valid loss: 5.493287399836949
Training loss: 3.585742235183716 / Valid loss: 5.5004069623493015
Training loss: 2.872525691986084 / Valid loss: 5.499314787274315

Epoch: 37
Training loss: 3.402665615081787 / Valid loss: 5.499120133263724
Training loss: 3.911510705947876 / Valid loss: 5.502046855290731
Training loss: 3.351719856262207 / Valid loss: 5.50049341065543
Training loss: 3.7266218662261963 / Valid loss: 5.497279796146211
Training loss: 4.061398983001709 / Valid loss: 5.5003724234444755

Epoch: 38
Training loss: 3.236269950866699 / Valid loss: 5.50160824230739
Training loss: 3.573420524597168 / Valid loss: 5.505601385661534
Training loss: 4.264403820037842 / Valid loss: 5.5049280847821915
Training loss: 3.7087819576263428 / Valid loss: 5.507753115608579
Training loss: 4.401051998138428 / Valid loss: 5.509308708281744

Epoch: 39
Training loss: 2.338099479675293 / Valid loss: 5.509296340034122
Training loss: 3.877763271331787 / Valid loss: 5.51414335568746
Training loss: 4.858186721801758 / Valid loss: 5.512959798177083
Training loss: 3.9715676307678223 / Valid loss: 5.516570141201927
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 12300): 5.3533706437973745
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.8549747467041 / Valid loss: 16.989478783380417
Model is saved in epoch 0, overall batch: 0
Training loss: 18.9604549407959 / Valid loss: 15.762811751592727
Model is saved in epoch 0, overall batch: 100
Training loss: 14.243934631347656 / Valid loss: 14.771505800882975
Model is saved in epoch 0, overall batch: 200
Training loss: 13.741157531738281 / Valid loss: 14.510727628072102
Model is saved in epoch 0, overall batch: 300
Training loss: 15.681009292602539 / Valid loss: 13.818933096386138
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 11.804765701293945 / Valid loss: 13.503786159697032
Model is saved in epoch 1, overall batch: 500
Training loss: 13.750184059143066 / Valid loss: 12.843889495304653
Model is saved in epoch 1, overall batch: 600
Training loss: 9.983104705810547 / Valid loss: 12.513324351537795
Model is saved in epoch 1, overall batch: 700
Training loss: 8.259761810302734 / Valid loss: 11.970251405806769
Model is saved in epoch 1, overall batch: 800
Training loss: 13.36345100402832 / Valid loss: 11.645839890979586
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 7.839372634887695 / Valid loss: 11.188460418156215
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.438817024230957 / Valid loss: 10.850815664018903
Model is saved in epoch 2, overall batch: 1100
Training loss: 8.220318794250488 / Valid loss: 10.44327315830049
Model is saved in epoch 2, overall batch: 1200
Training loss: 3.4135053157806396 / Valid loss: 10.097323217846098
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.968689918518066 / Valid loss: 9.965783055623373
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.25254487991333 / Valid loss: 9.761306308564686
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.542266845703125 / Valid loss: 9.606659843808128
Model is saved in epoch 3, overall batch: 1600
Training loss: 3.2191877365112305 / Valid loss: 9.191124171302432
Model is saved in epoch 3, overall batch: 1700
Training loss: 2.6893715858459473 / Valid loss: 9.037983372097923
Model is saved in epoch 3, overall batch: 1800
Training loss: 3.2533884048461914 / Valid loss: 8.503107770284016
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.114194869995117 / Valid loss: 8.624888279324486
Training loss: 2.794037342071533 / Valid loss: 8.309217557452975
Model is saved in epoch 4, overall batch: 2100
Training loss: 2.236100673675537 / Valid loss: 8.56189973922003
Training loss: 2.0700531005859375 / Valid loss: 8.097637072063627
Model is saved in epoch 4, overall batch: 2300
Training loss: 2.303368330001831 / Valid loss: 7.930504085904076
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 1.4887430667877197 / Valid loss: 7.874084981282552
Model is saved in epoch 5, overall batch: 2500
Training loss: 1.6395235061645508 / Valid loss: 7.654250360670543
Model is saved in epoch 5, overall batch: 2600
Training loss: 2.4753787517547607 / Valid loss: 7.4716542561848955
Model is saved in epoch 5, overall batch: 2700
Training loss: 2.903928518295288 / Valid loss: 7.601965154920306
Training loss: 1.3593859672546387 / Valid loss: 7.332349877130418
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 1.2435369491577148 / Valid loss: 7.224972649982997
Model is saved in epoch 6, overall batch: 3000
Training loss: 1.0629594326019287 / Valid loss: 7.200286533719018
Model is saved in epoch 6, overall batch: 3100
Training loss: 0.6070163249969482 / Valid loss: 7.12367456299918
Model is saved in epoch 6, overall batch: 3200
Training loss: 0.8960704207420349 / Valid loss: 7.2148067701430545
Training loss: 0.8296201229095459 / Valid loss: 7.184710829598563

Epoch: 7
Training loss: 1.0243456363677979 / Valid loss: 6.9970308031354636
Model is saved in epoch 7, overall batch: 3500
Training loss: 1.323797345161438 / Valid loss: 7.070120816003708
Training loss: 0.37596988677978516 / Valid loss: 6.838238479977562
Model is saved in epoch 7, overall batch: 3700
Training loss: 0.6739439368247986 / Valid loss: 6.894528765905471
Training loss: 1.2239863872528076 / Valid loss: 6.917397549038841

Epoch: 8
Training loss: 0.5739498138427734 / Valid loss: 6.920224557604109
Training loss: 0.9101067781448364 / Valid loss: 6.876267950875419
Training loss: 0.6599305868148804 / Valid loss: 6.770483936582293
Model is saved in epoch 8, overall batch: 4200
Training loss: 0.8790944814682007 / Valid loss: 6.845487590063186
Training loss: 0.47576233744621277 / Valid loss: 6.91627949987139

Epoch: 9
Training loss: 0.33581334352493286 / Valid loss: 6.897675300779797
Training loss: 1.003495693206787 / Valid loss: 6.821571404593332
Training loss: 0.9544498920440674 / Valid loss: 6.771408530644008
Training loss: 0.3690384328365326 / Valid loss: 6.802124409448533

Epoch: 10
Training loss: 0.5431381464004517 / Valid loss: 6.902392123994373
Training loss: 0.45153895020484924 / Valid loss: 6.8260498637244815
Training loss: 0.46570736169815063 / Valid loss: 6.806181975773403
Training loss: 0.7411077618598938 / Valid loss: 6.87525247846331
Training loss: 0.39238643646240234 / Valid loss: 6.88486974352882

Epoch: 11
Training loss: 0.5609350204467773 / Valid loss: 6.815281466075352
Training loss: 0.5107731819152832 / Valid loss: 6.875526600792295
Training loss: 0.2644200325012207 / Valid loss: 6.844042448770432
Training loss: 0.606702983379364 / Valid loss: 6.778639443715414
Training loss: 0.5604170560836792 / Valid loss: 6.934774609974452

Epoch: 12
Training loss: 0.5473272800445557 / Valid loss: 6.803172888074602
Training loss: 0.39758217334747314 / Valid loss: 6.73669995807466
Model is saved in epoch 12, overall batch: 6000
Training loss: 0.3088725805282593 / Valid loss: 6.7955768630618145
Training loss: 0.4500817060470581 / Valid loss: 6.850722044990176
Training loss: 0.3779047727584839 / Valid loss: 6.832348405747187

Epoch: 13
Training loss: 0.5579949021339417 / Valid loss: 6.8155485471089685
Training loss: 0.2153196483850479 / Valid loss: 6.82216705594744
Training loss: 0.3301597833633423 / Valid loss: 6.836717868986584
Training loss: 0.44077762961387634 / Valid loss: 6.928799869900658
Training loss: 0.6700647473335266 / Valid loss: 6.8441530704498295

Epoch: 14
Training loss: 0.5292296409606934 / Valid loss: 6.785062812623524
Training loss: 0.5539875030517578 / Valid loss: 6.746500941685268
Training loss: 0.24252499639987946 / Valid loss: 6.766802706037249
Training loss: 0.3379790186882019 / Valid loss: 6.730667554764521
Model is saved in epoch 14, overall batch: 7200
Training loss: 0.3836494982242584 / Valid loss: 6.84228717031933

Epoch: 15
Training loss: 0.3520006537437439 / Valid loss: 6.792228062947591
Training loss: 0.3449285626411438 / Valid loss: 6.718585718245733
Model is saved in epoch 15, overall batch: 7500
Training loss: 0.2460244596004486 / Valid loss: 6.757302138918922
Training loss: 0.3163890838623047 / Valid loss: 6.797678958801996
Training loss: 0.2651504874229431 / Valid loss: 6.81274816876366

Epoch: 16
Training loss: 0.33195167779922485 / Valid loss: 6.783235858735584
Training loss: 0.7072224020957947 / Valid loss: 6.7611717542012535
Training loss: 0.33475959300994873 / Valid loss: 6.804534128734043
Training loss: 0.39051055908203125 / Valid loss: 6.815451136089506
Training loss: 0.5698040723800659 / Valid loss: 6.785856033506848

Epoch: 17
Training loss: 0.442054808139801 / Valid loss: 6.734289146604992
Training loss: 0.3011819124221802 / Valid loss: 6.765458869934082
Training loss: 0.2919681966304779 / Valid loss: 6.84344208581107
Training loss: 0.2771933376789093 / Valid loss: 6.807620212009975
Training loss: 0.42575299739837646 / Valid loss: 6.7521776063101635

Epoch: 18
Training loss: 0.37893521785736084 / Valid loss: 6.673490883055187
Model is saved in epoch 18, overall batch: 8900
Training loss: 0.343404620885849 / Valid loss: 6.690603238060361
Training loss: 0.3835693597793579 / Valid loss: 6.74746135075887
Training loss: 0.2721300721168518 / Valid loss: 6.8038202694484164
Training loss: 0.21042373776435852 / Valid loss: 6.837271081833612

Epoch: 19
Training loss: 0.2939293086528778 / Valid loss: 6.709373392377581
Training loss: 0.204054594039917 / Valid loss: 6.765358102889288
Training loss: 0.7189841270446777 / Valid loss: 6.8167487871079215
Training loss: 0.22793957591056824 / Valid loss: 6.74536295845395

Epoch: 20
Training loss: 0.27458927035331726 / Valid loss: 6.818800258636474
Training loss: 0.643502950668335 / Valid loss: 6.703916043326968
Training loss: 0.40919923782348633 / Valid loss: 6.860252203260149
Training loss: 0.39447689056396484 / Valid loss: 6.815677613303775
Training loss: 0.3174591660499573 / Valid loss: 6.841670899164109

Epoch: 21
Training loss: 0.70247483253479 / Valid loss: 6.753223936898368
Training loss: 0.42034056782722473 / Valid loss: 6.735323435919625
Training loss: 0.15986913442611694 / Valid loss: 6.7830505961463565
Training loss: 0.17195415496826172 / Valid loss: 6.727358089174543
Training loss: 0.20520547032356262 / Valid loss: 6.752349424362182

Epoch: 22
Training loss: 0.2752501368522644 / Valid loss: 6.821345449629284
Training loss: 0.49725693464279175 / Valid loss: 6.7384551911127
Training loss: 0.6506203413009644 / Valid loss: 6.756122847965785
Training loss: 0.20125806331634521 / Valid loss: 6.84979684920538
Training loss: 0.20270155370235443 / Valid loss: 6.796430587768555

Epoch: 23
Training loss: 0.6763166189193726 / Valid loss: 6.73906325385684
Training loss: 0.39856815338134766 / Valid loss: 6.718262577056885
Training loss: 0.34340327978134155 / Valid loss: 6.811808717818487
Training loss: 0.21485087275505066 / Valid loss: 6.761248506818499
Training loss: 0.342002809047699 / Valid loss: 6.889806552160354

Epoch: 24
Training loss: 0.23928409814834595 / Valid loss: 6.775109811056228
Training loss: 0.18139633536338806 / Valid loss: 6.735822184880575
Training loss: 0.27855145931243896 / Valid loss: 6.775829269772484
Training loss: 0.18065029382705688 / Valid loss: 6.8156948679969425
Training loss: 0.6007393002510071 / Valid loss: 6.805187166304815

Epoch: 25
Training loss: 0.21475335955619812 / Valid loss: 6.8273999304998485
Training loss: 0.27679356932640076 / Valid loss: 6.737676584152949
Training loss: 0.21858522295951843 / Valid loss: 6.737405558994838
Training loss: 0.22296936810016632 / Valid loss: 6.79356575012207
Training loss: 0.2844315767288208 / Valid loss: 6.820682012467158

Epoch: 26
Training loss: 0.23756247758865356 / Valid loss: 6.7038420427413214
Training loss: 0.7757533192634583 / Valid loss: 6.767373087292626
Training loss: 0.21082910895347595 / Valid loss: 6.689236502420335
Training loss: 0.2861737608909607 / Valid loss: 6.753827424276443
Training loss: 0.3284309506416321 / Valid loss: 6.760358390353975

Epoch: 27
Training loss: 0.138717383146286 / Valid loss: 6.750019486745199
Training loss: 0.28578293323516846 / Valid loss: 6.837595894223168
Training loss: 0.5274522304534912 / Valid loss: 6.871369820549375
Training loss: 0.18951624631881714 / Valid loss: 6.774052887871152
Training loss: 0.11294794082641602 / Valid loss: 6.738321147646222

Epoch: 28
Training loss: 0.24426952004432678 / Valid loss: 6.706620000657582
Training loss: 0.26223990321159363 / Valid loss: 6.759342861175537
Training loss: 0.4751245975494385 / Valid loss: 6.759523373558408
Training loss: 0.24936622381210327 / Valid loss: 6.707304162070865
Training loss: 0.23779667913913727 / Valid loss: 6.694358369282313

Epoch: 29
Training loss: 0.14186403155326843 / Valid loss: 6.750195753006708
Training loss: 0.2177591174840927 / Valid loss: 6.782495691662743
Training loss: 0.2107696235179901 / Valid loss: 6.81010008312407
Training loss: 0.3349123001098633 / Valid loss: 6.713498574211484

Epoch: 30
Training loss: 0.2155487984418869 / Valid loss: 6.745994713192895
Training loss: 0.5554934144020081 / Valid loss: 6.704826407205491
Training loss: 0.28896546363830566 / Valid loss: 6.69459121340797
Training loss: 0.2955531179904938 / Valid loss: 6.715953531719389
Training loss: 0.7332888841629028 / Valid loss: 6.737763291313534

Epoch: 31
Training loss: 0.2328016310930252 / Valid loss: 6.719991379692441
Training loss: 0.11600129306316376 / Valid loss: 6.8069935889471145
Training loss: 0.22605513036251068 / Valid loss: 6.7579528445289245
Training loss: 0.10836049914360046 / Valid loss: 6.786216295333135
Training loss: 0.13793343305587769 / Valid loss: 6.76075503939674

Epoch: 32
Training loss: 0.22300365567207336 / Valid loss: 6.725256161462693
Training loss: 0.2218417078256607 / Valid loss: 6.801505751836867
Training loss: 0.16937069594860077 / Valid loss: 6.742709788821992
Training loss: 0.25928357243537903 / Valid loss: 6.72525681086949
Training loss: 0.16320383548736572 / Valid loss: 6.764471483230591

Epoch: 33
Training loss: 0.15947052836418152 / Valid loss: 6.842311568487258
Training loss: 0.19248315691947937 / Valid loss: 6.770128157025292
Training loss: 0.20160116255283356 / Valid loss: 6.680939161209833
Training loss: 0.1611499935388565 / Valid loss: 6.748599274953206
Training loss: 0.1470526158809662 / Valid loss: 6.748466941288539

Epoch: 34
Training loss: 0.1859557181596756 / Valid loss: 6.772602047239031
Training loss: 0.1188066154718399 / Valid loss: 6.741887959979829
Training loss: 0.16587860882282257 / Valid loss: 6.772767019271851
Training loss: 0.17384807765483856 / Valid loss: 6.769509401775542
Training loss: 0.6138131022453308 / Valid loss: 6.71676801953997

Epoch: 35
Training loss: 0.21274614334106445 / Valid loss: 6.706857790265764
Training loss: 0.16994579136371613 / Valid loss: 6.7238447348276775
Training loss: 0.16680774092674255 / Valid loss: 6.765249983469645
Training loss: 0.2880183756351471 / Valid loss: 6.790684409368605
Training loss: 0.21892127394676208 / Valid loss: 6.734586397806804

Epoch: 36
Training loss: 0.2579687833786011 / Valid loss: 6.704917485373361
Training loss: 0.37556058168411255 / Valid loss: 6.791302245003837
Training loss: 0.5738528966903687 / Valid loss: 6.688610058739072
Training loss: 0.46552810072898865 / Valid loss: 6.729285140264602
Training loss: 0.10767881572246552 / Valid loss: 6.675568462553478

Epoch: 37
Training loss: 0.12111412733793259 / Valid loss: 6.675740264710926
Training loss: 0.14752483367919922 / Valid loss: 6.775943124861945
Training loss: 0.24527713656425476 / Valid loss: 6.719796846026466
Training loss: 0.7367719411849976 / Valid loss: 6.7095755781446185
Training loss: 0.4818275570869446 / Valid loss: 6.76403268178304

Epoch: 38
Training loss: 0.15818670392036438 / Valid loss: 6.773845404670352
Training loss: 0.08949186652898788 / Valid loss: 6.7507500625792005
Training loss: 0.27375492453575134 / Valid loss: 6.770172927493141
Training loss: 0.14869996905326843 / Valid loss: 6.74221848533267
Training loss: 0.3657069206237793 / Valid loss: 6.78048970812843

Epoch: 39
Training loss: 0.1467004269361496 / Valid loss: 6.806995189757574
Training loss: 0.3146520256996155 / Valid loss: 6.815779926663353
Training loss: 0.15105512738227844 / Valid loss: 6.6881218456086655
Training loss: 0.18484662473201752 / Valid loss: 6.710921205793109
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 8900): 6.630269036974226
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.8549747467041 / Valid loss: 16.989282317388625
Model is saved in epoch 0, overall batch: 0
Training loss: 18.71987533569336 / Valid loss: 16.577519216991607
Model is saved in epoch 0, overall batch: 100
Training loss: 13.944015502929688 / Valid loss: 14.50412517275129
Model is saved in epoch 0, overall batch: 200
Training loss: 13.494029998779297 / Valid loss: 14.268681671505883
Model is saved in epoch 0, overall batch: 300
Training loss: 15.386560440063477 / Valid loss: 13.671728506542387
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 11.904047012329102 / Valid loss: 13.323538067227318
Model is saved in epoch 1, overall batch: 500
Training loss: 13.417734146118164 / Valid loss: 12.927565134139288
Model is saved in epoch 1, overall batch: 600
Training loss: 9.91977596282959 / Valid loss: 12.602770778111049
Model is saved in epoch 1, overall batch: 700
Training loss: 8.506234169006348 / Valid loss: 11.98803600129627
Model is saved in epoch 1, overall batch: 800
Training loss: 13.644721031188965 / Valid loss: 11.505714262099493
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 7.764143943786621 / Valid loss: 11.475505878811791
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.562756061553955 / Valid loss: 11.100779792240688
Model is saved in epoch 2, overall batch: 1100
Training loss: 8.459943771362305 / Valid loss: 10.549263640812464
Model is saved in epoch 2, overall batch: 1200
Training loss: 3.26881742477417 / Valid loss: 10.382505516778854
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.733218193054199 / Valid loss: 10.26291362671625
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.153595924377441 / Valid loss: 9.931334472837902
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.667619705200195 / Valid loss: 9.840016342344738
Model is saved in epoch 3, overall batch: 1600
Training loss: 3.383429527282715 / Valid loss: 9.521135943276542
Model is saved in epoch 3, overall batch: 1700
Training loss: 2.5581955909729004 / Valid loss: 9.240983245486305
Model is saved in epoch 3, overall batch: 1800
Training loss: 3.4836530685424805 / Valid loss: 8.609454568227132
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.4373769760131836 / Valid loss: 8.773651563553583
Training loss: 2.6421360969543457 / Valid loss: 8.569875376565117
Model is saved in epoch 4, overall batch: 2100
Training loss: 2.1787071228027344 / Valid loss: 8.787059070950463
Training loss: 2.101890802383423 / Valid loss: 8.280126498994374
Model is saved in epoch 4, overall batch: 2300
Training loss: 2.1347594261169434 / Valid loss: 8.061016028267996
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 1.364093542098999 / Valid loss: 7.798166833605085
Model is saved in epoch 5, overall batch: 2500
Training loss: 1.6452736854553223 / Valid loss: 7.70816080229623
Model is saved in epoch 5, overall batch: 2600
Training loss: 2.3252832889556885 / Valid loss: 7.59639752705892
Model is saved in epoch 5, overall batch: 2700
Training loss: 2.6891839504241943 / Valid loss: 7.651763030460903
Training loss: 1.2720060348510742 / Valid loss: 7.462504613967169
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 1.2643533945083618 / Valid loss: 7.344875131334577
Model is saved in epoch 6, overall batch: 3000
Training loss: 1.406606912612915 / Valid loss: 7.184498205639067
Model is saved in epoch 6, overall batch: 3100
Training loss: 0.7279776334762573 / Valid loss: 7.064533083779471
Model is saved in epoch 6, overall batch: 3200
Training loss: 0.8416270017623901 / Valid loss: 7.1605260304042275
Training loss: 0.7767577171325684 / Valid loss: 7.177975123269217

Epoch: 7
Training loss: 1.13193678855896 / Valid loss: 6.976999378204345
Model is saved in epoch 7, overall batch: 3500
Training loss: 1.209247350692749 / Valid loss: 7.093536722092401
Training loss: 0.4909910559654236 / Valid loss: 6.804210240500314
Model is saved in epoch 7, overall batch: 3700
Training loss: 0.5719143152236938 / Valid loss: 6.865160160972959
Training loss: 0.9284185171127319 / Valid loss: 7.013742980502901

Epoch: 8
Training loss: 0.6801728010177612 / Valid loss: 6.848267141977946
Training loss: 0.7555689215660095 / Valid loss: 6.842142908913749
Training loss: 0.5068368911743164 / Valid loss: 6.717271078200567
Model is saved in epoch 8, overall batch: 4200
Training loss: 0.7770464420318604 / Valid loss: 6.763698711849394
Training loss: 0.502727210521698 / Valid loss: 6.842949499402727

Epoch: 9
Training loss: 0.4286852180957794 / Valid loss: 6.732717402776083
Training loss: 0.8577988743782043 / Valid loss: 6.773301605951218
Training loss: 0.9850816130638123 / Valid loss: 6.705267987932477
Model is saved in epoch 9, overall batch: 4700
Training loss: 0.5394054055213928 / Valid loss: 6.670256551106771
Model is saved in epoch 9, overall batch: 4800

Epoch: 10
Training loss: 0.5862762331962585 / Valid loss: 6.742739504859561
Training loss: 0.34523820877075195 / Valid loss: 6.68745722089495
Training loss: 0.4429130554199219 / Valid loss: 6.72593016851516
Training loss: 0.6204068660736084 / Valid loss: 6.683345476786296
Training loss: 0.41149473190307617 / Valid loss: 6.768543443225679

Epoch: 11
Training loss: 0.5282504558563232 / Valid loss: 6.7218024412790935
Training loss: 0.48470383882522583 / Valid loss: 6.736397731871832
Training loss: 0.37289854884147644 / Valid loss: 6.81979805174328
Training loss: 0.5843750238418579 / Valid loss: 6.800368860789708
Training loss: 0.4706014394760132 / Valid loss: 6.813744399661109

Epoch: 12
Training loss: 0.49520090222358704 / Valid loss: 6.762264823913574
Training loss: 0.4128824770450592 / Valid loss: 6.646165856860933
Model is saved in epoch 12, overall batch: 6000
Training loss: 0.28040221333503723 / Valid loss: 6.677178634916033
Training loss: 0.459370881319046 / Valid loss: 6.790663728259859
Training loss: 0.34900087118148804 / Valid loss: 6.7203828085036506

Epoch: 13
Training loss: 0.4819353222846985 / Valid loss: 6.78967338743664
Training loss: 0.3725898861885071 / Valid loss: 6.773336301531111
Training loss: 0.3024553060531616 / Valid loss: 6.725922670818511
Training loss: 0.4490850865840912 / Valid loss: 6.846538666316441
Training loss: 0.5934059619903564 / Valid loss: 6.765640031723749

Epoch: 14
Training loss: 0.5000863075256348 / Valid loss: 6.739931242806571
Training loss: 0.6868691444396973 / Valid loss: 6.664021201360793
Training loss: 0.2500246465206146 / Valid loss: 6.675752857753209
Training loss: 0.42007580399513245 / Valid loss: 6.670502510524932
Training loss: 0.37129640579223633 / Valid loss: 6.6894972619556246

Epoch: 15
Training loss: 0.299434095621109 / Valid loss: 6.6973636377425425
Training loss: 0.3185400366783142 / Valid loss: 6.6850707031431655
Training loss: 0.2874847948551178 / Valid loss: 6.677689924694243
Training loss: 0.39767348766326904 / Valid loss: 6.802081805183774
Training loss: 0.43265241384506226 / Valid loss: 6.74812878881182

Epoch: 16
Training loss: 0.3475312292575836 / Valid loss: 6.702721068972633
Training loss: 0.7865797281265259 / Valid loss: 6.6850600378853935
Training loss: 0.3443455100059509 / Valid loss: 6.795094757988339
Training loss: 0.4127221703529358 / Valid loss: 6.7751653739384246
Training loss: 0.7112647891044617 / Valid loss: 6.705691019694011

Epoch: 17
Training loss: 0.44200006127357483 / Valid loss: 6.739997763860793
Training loss: 0.29840922355651855 / Valid loss: 6.74240878423055
Training loss: 0.3475783169269562 / Valid loss: 6.785403142656599
Training loss: 0.28590667247772217 / Valid loss: 6.751890777406238
Training loss: 0.33361032605171204 / Valid loss: 6.694183217911493

Epoch: 18
Training loss: 0.3725857138633728 / Valid loss: 6.677005252384004
Training loss: 0.37475094199180603 / Valid loss: 6.661957872481573
Training loss: 0.32364317774772644 / Valid loss: 6.722908630825224
Training loss: 0.17889274656772614 / Valid loss: 6.70941401890346
Training loss: 0.21877601742744446 / Valid loss: 6.760486362093971

Epoch: 19
Training loss: 0.2754226326942444 / Valid loss: 6.64306636991955
Model is saved in epoch 19, overall batch: 9400
Training loss: 0.19528993964195251 / Valid loss: 6.790398089090983
Training loss: 0.5353250503540039 / Valid loss: 6.766988917759487
Training loss: 0.2694408893585205 / Valid loss: 6.794781294323149

Epoch: 20
Training loss: 0.16752399504184723 / Valid loss: 6.781945641835531
Training loss: 0.8105869293212891 / Valid loss: 6.648544220697312
Training loss: 0.3728843033313751 / Valid loss: 6.74079289209275
Training loss: 0.4710780084133148 / Valid loss: 6.716152309236072
Training loss: 0.38428759574890137 / Valid loss: 6.665066037859235

Epoch: 21
Training loss: 0.7552285194396973 / Valid loss: 6.617957519349598
Model is saved in epoch 21, overall batch: 10300
Training loss: 0.46489667892456055 / Valid loss: 6.629118892124721
Training loss: 0.23900571465492249 / Valid loss: 6.686562442779541
Training loss: 0.18893787264823914 / Valid loss: 6.674706717899867
Training loss: 0.23111961781978607 / Valid loss: 6.690763918558757

Epoch: 22
Training loss: 0.2673647701740265 / Valid loss: 6.769394522621518
Training loss: 0.5612499713897705 / Valid loss: 6.597681486038935
Model is saved in epoch 22, overall batch: 10900
Training loss: 0.6959216594696045 / Valid loss: 6.658321634928385
Training loss: 0.35564279556274414 / Valid loss: 6.706452022280012
Training loss: 0.2097715437412262 / Valid loss: 6.784806267420451

Epoch: 23
Training loss: 0.6602396368980408 / Valid loss: 6.747779205867222
Training loss: 0.3890725374221802 / Valid loss: 6.734426975250244
Training loss: 0.4910668134689331 / Valid loss: 6.70796690214248
Training loss: 0.16796354949474335 / Valid loss: 6.704385712033226
Training loss: 0.28232482075691223 / Valid loss: 6.813895550228301

Epoch: 24
Training loss: 0.1321716606616974 / Valid loss: 6.768900569279989
Training loss: 0.2738161087036133 / Valid loss: 6.6383367470332555
Training loss: 0.19910883903503418 / Valid loss: 6.74736115137736
Training loss: 0.11170187592506409 / Valid loss: 6.704115583783104
Training loss: 0.5065662860870361 / Valid loss: 6.7378291947501046

Epoch: 25
Training loss: 0.23719489574432373 / Valid loss: 6.733350681123279
Training loss: 0.2623150050640106 / Valid loss: 6.733605670928955
Training loss: 0.2095002681016922 / Valid loss: 6.785140205564953
Training loss: 0.1699630618095398 / Valid loss: 6.705953924996512
Training loss: 0.3889594078063965 / Valid loss: 6.74573418980553

Epoch: 26
Training loss: 0.18304666876792908 / Valid loss: 6.662538414909726
Training loss: 0.9133132696151733 / Valid loss: 6.70454295022147
Training loss: 0.19962051510810852 / Valid loss: 6.667833044415429
Training loss: 0.2383715808391571 / Valid loss: 6.803023992265974
Training loss: 0.42202335596084595 / Valid loss: 6.79796610559736

Epoch: 27
Training loss: 0.17379726469516754 / Valid loss: 6.739280319213867
Training loss: 0.22326001524925232 / Valid loss: 6.857460090092251
Training loss: 0.5138245224952698 / Valid loss: 6.926478917258127
Training loss: 0.16429556906223297 / Valid loss: 6.775235285077776
Training loss: 0.17645670473575592 / Valid loss: 6.704659080505371

Epoch: 28
Training loss: 0.16981542110443115 / Valid loss: 6.776238972800119
Training loss: 0.26019036769866943 / Valid loss: 6.804510048457554
Training loss: 0.38838323950767517 / Valid loss: 6.749985774358113
Training loss: 0.16254989802837372 / Valid loss: 6.665518810635521
Training loss: 0.2334136664867401 / Valid loss: 6.696768433707101

Epoch: 29
Training loss: 0.2238805592060089 / Valid loss: 6.789272826058524
Training loss: 0.23342421650886536 / Valid loss: 6.708801240012759
Training loss: 0.2285536825656891 / Valid loss: 6.818074873515537
Training loss: 0.4640672206878662 / Valid loss: 6.678847860154652

Epoch: 30
Training loss: 0.21683138608932495 / Valid loss: 6.766689055306571
Training loss: 0.6579495072364807 / Valid loss: 6.66342154003325
Training loss: 0.28832346200942993 / Valid loss: 6.607419999440511
Training loss: 0.34635329246520996 / Valid loss: 6.70533062616984
Training loss: 0.5651388764381409 / Valid loss: 6.650634477252052

Epoch: 31
Training loss: 0.2299732267856598 / Valid loss: 6.7853325162615095
Training loss: 0.2326451539993286 / Valid loss: 6.758329159872872
Training loss: 0.2007293403148651 / Valid loss: 6.658323728470576
Training loss: 0.16796937584877014 / Valid loss: 6.824034338905697
Training loss: 0.22146761417388916 / Valid loss: 6.726392073858352

Epoch: 32
Training loss: 0.18447725474834442 / Valid loss: 6.679752640497117
Training loss: 0.2199331670999527 / Valid loss: 6.804290249234154
Training loss: 0.23888830840587616 / Valid loss: 6.736733795347668
Training loss: 0.17121674120426178 / Valid loss: 6.68031496320452
Training loss: 0.19014078378677368 / Valid loss: 6.748306590034848

Epoch: 33
Training loss: 0.1786808967590332 / Valid loss: 6.8845411346072245
Training loss: 0.22846774756908417 / Valid loss: 6.719577537264143
Training loss: 0.18418198823928833 / Valid loss: 6.7017375809805735
Training loss: 0.20594076812267303 / Valid loss: 6.679080776941209
Training loss: 0.12029335647821426 / Valid loss: 6.791637025560651

Epoch: 34
Training loss: 0.2306288778781891 / Valid loss: 6.794495064871652
Training loss: 0.11985401809215546 / Valid loss: 6.75974101566133
Training loss: 0.11636669933795929 / Valid loss: 6.714536167326427
Training loss: 0.17072078585624695 / Valid loss: 6.733010457810901
Training loss: 0.6570292711257935 / Valid loss: 6.702500415983654

Epoch: 35
Training loss: 0.14736418426036835 / Valid loss: 6.72194090343657
Training loss: 0.15927928686141968 / Valid loss: 6.714669715790522
Training loss: 0.09433569014072418 / Valid loss: 6.772569324856713
Training loss: 0.29532212018966675 / Valid loss: 6.769309970310756
Training loss: 0.18204089999198914 / Valid loss: 6.73037702015468

Epoch: 36
Training loss: 0.3252701163291931 / Valid loss: 6.692923350561233
Training loss: 0.27342966198921204 / Valid loss: 6.811917532057989
Training loss: 0.6451894640922546 / Valid loss: 6.709807566234043
Training loss: 0.3899730443954468 / Valid loss: 6.7708136740184965
Training loss: 0.12454372644424438 / Valid loss: 6.7194355124518985

Epoch: 37
Training loss: 0.1315726637840271 / Valid loss: 6.683879239218576
Training loss: 0.2890070378780365 / Valid loss: 6.7375639733814054
Training loss: 0.3071745038032532 / Valid loss: 6.761617324465797
Training loss: 0.6559961438179016 / Valid loss: 6.719766115006946
Training loss: 0.36942577362060547 / Valid loss: 6.777876272655669

Epoch: 38
Training loss: 0.14132586121559143 / Valid loss: 6.8039473987761
Training loss: 0.16816166043281555 / Valid loss: 6.795019626617432
Training loss: 0.3103245198726654 / Valid loss: 6.833092221759615
Training loss: 0.16539284586906433 / Valid loss: 6.760000696636381
Training loss: 0.4077228605747223 / Valid loss: 6.808162857237316

Epoch: 39
Training loss: 0.1740405261516571 / Valid loss: 6.862719658442906
Training loss: 0.2878785729408264 / Valid loss: 6.829056194850376
Training loss: 0.15072932839393616 / Valid loss: 6.710040782746814
Training loss: 0.26403045654296875 / Valid loss: 6.686442441032046
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 10900): 6.540367880321685
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.313697814941406 / Valid loss: 15.753696487063454
Model is saved in epoch 0, overall batch: 0
Training loss: 14.843012809753418 / Valid loss: 15.198961430504209
Model is saved in epoch 0, overall batch: 100
Training loss: 12.712383270263672 / Valid loss: 13.58493992033459
Model is saved in epoch 0, overall batch: 200
Training loss: 7.54456901550293 / Valid loss: 12.540274093264625
Model is saved in epoch 0, overall batch: 300
Training loss: 8.390302658081055 / Valid loss: 11.754547341664631
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 7.468027114868164 / Valid loss: 11.292772497449603
Model is saved in epoch 1, overall batch: 500
Training loss: 7.5129194259643555 / Valid loss: 10.905619607652937
Model is saved in epoch 1, overall batch: 600
Training loss: 9.747920989990234 / Valid loss: 10.27510727019537
Model is saved in epoch 1, overall batch: 700
Training loss: 6.703456401824951 / Valid loss: 9.654329813094366
Model is saved in epoch 1, overall batch: 800
Training loss: 8.732757568359375 / Valid loss: 9.569907106672014
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.537815093994141 / Valid loss: 8.976354181198847
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.0763401985168457 / Valid loss: 8.747594433739073
Model is saved in epoch 2, overall batch: 1100
Training loss: 3.017594337463379 / Valid loss: 8.351310007912772
Model is saved in epoch 2, overall batch: 1200
Training loss: 2.6979565620422363 / Valid loss: 8.000102960495722
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.254835605621338 / Valid loss: 7.81107732682001
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 1.0351741313934326 / Valid loss: 7.278098776226952
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.1401891708374023 / Valid loss: 7.487479109991164
Training loss: 2.6856250762939453 / Valid loss: 7.228772081647601
Model is saved in epoch 3, overall batch: 1700
Training loss: 2.546393394470215 / Valid loss: 7.080998906635103
Model is saved in epoch 3, overall batch: 1800
Training loss: 3.2788634300231934 / Valid loss: 6.996843755812872
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 0.9358392953872681 / Valid loss: 6.847386791592553
Model is saved in epoch 4, overall batch: 2000
Training loss: 1.4807868003845215 / Valid loss: 6.854435625530424
Training loss: 1.246139645576477 / Valid loss: 6.784021718161447
Model is saved in epoch 4, overall batch: 2200
Training loss: 1.7047181129455566 / Valid loss: 6.722559842609224
Model is saved in epoch 4, overall batch: 2300
Training loss: 1.2468078136444092 / Valid loss: 6.743357533500308

Epoch: 5
Training loss: 0.7861952781677246 / Valid loss: 6.652912301108951
Model is saved in epoch 5, overall batch: 2500
Training loss: 1.3788580894470215 / Valid loss: 6.71295067469279
Training loss: 1.2559242248535156 / Valid loss: 6.750842341922579
Training loss: 1.1910043954849243 / Valid loss: 6.725674020676386
Training loss: 1.1267448663711548 / Valid loss: 6.761905029841832

Epoch: 6
Training loss: 0.6802124381065369 / Valid loss: 6.68127848534357
Training loss: 0.84412682056427 / Valid loss: 6.697375520070394
Training loss: 0.734642744064331 / Valid loss: 6.771144471849714
Training loss: 0.7603549957275391 / Valid loss: 6.815760178793044
Training loss: 0.7949017286300659 / Valid loss: 6.714059112185524

Epoch: 7
Training loss: 0.47565892338752747 / Valid loss: 6.742375369299026
Training loss: 0.5359765887260437 / Valid loss: 6.855217910948253
Training loss: 0.8194750547409058 / Valid loss: 6.792947828202021
Training loss: 0.6615052223205566 / Valid loss: 6.878000223068964
Training loss: 0.6660894751548767 / Valid loss: 6.8053489003862655

Epoch: 8
Training loss: 0.4774850010871887 / Valid loss: 6.709632614680699
Training loss: 0.6075410842895508 / Valid loss: 6.824116738637288
Training loss: 0.6927942037582397 / Valid loss: 6.77047917502267
Training loss: 0.29866868257522583 / Valid loss: 6.795749514443534
Training loss: 1.1487510204315186 / Valid loss: 6.7895909173148015

Epoch: 9
Training loss: 0.5340440273284912 / Valid loss: 6.76542702175322
Training loss: 0.5181180238723755 / Valid loss: 6.718706571488154
Training loss: 0.7883564233779907 / Valid loss: 6.776568212963286
Training loss: 1.1910607814788818 / Valid loss: 6.880406136739822

Epoch: 10
Training loss: 0.5802440047264099 / Valid loss: 6.911107676369803
Training loss: 0.49433040618896484 / Valid loss: 6.850507699875605
Training loss: 0.5767273902893066 / Valid loss: 6.848765709286645
Training loss: 0.4102814197540283 / Valid loss: 6.775330854597546
Training loss: 0.34525424242019653 / Valid loss: 6.830074909755162

Epoch: 11
Training loss: 0.5158448219299316 / Valid loss: 6.808866514478411
Training loss: 0.4288245439529419 / Valid loss: 6.787001646132696
Training loss: 0.5912654399871826 / Valid loss: 6.7744750431605745
Training loss: 0.4460943341255188 / Valid loss: 6.930423595791772
Training loss: 0.8701084852218628 / Valid loss: 6.779766144071306

Epoch: 12
Training loss: 0.5356260538101196 / Valid loss: 6.788479491642543
Training loss: 0.34373313188552856 / Valid loss: 6.816744032360258
Training loss: 0.3341811001300812 / Valid loss: 6.947468448820568
Training loss: 0.7505641579627991 / Valid loss: 6.854166653042748
Training loss: 0.5425981283187866 / Valid loss: 6.870962524414063

Epoch: 13
Training loss: 0.919853687286377 / Valid loss: 6.729379406429472
Training loss: 0.3413366675376892 / Valid loss: 6.814873727162679
Training loss: 0.4469653367996216 / Valid loss: 6.789065556299119
Training loss: 0.4784347116947174 / Valid loss: 6.890804354349772
Training loss: 0.45670580863952637 / Valid loss: 6.891915053413028

Epoch: 14
Training loss: 0.2549247741699219 / Valid loss: 6.789233475639707
Training loss: 0.5498886108398438 / Valid loss: 6.78699756349836
Training loss: 0.3895449936389923 / Valid loss: 6.791758705320812
Training loss: 0.42763280868530273 / Valid loss: 6.985291944231306
Training loss: 0.3501206338405609 / Valid loss: 6.84551659311567

Epoch: 15
Training loss: 0.5708348751068115 / Valid loss: 6.7094566345214846
Training loss: 0.23369169235229492 / Valid loss: 6.846040993645078
Training loss: 1.160614252090454 / Valid loss: 6.8173124177115305
Training loss: 0.5568310618400574 / Valid loss: 6.878842022305443
Training loss: 1.336945652961731 / Valid loss: 6.810163447970436

Epoch: 16
Training loss: 0.3346955478191376 / Valid loss: 6.856548091343471
Training loss: 0.6348859071731567 / Valid loss: 6.819860594613211
Training loss: 0.7172620892524719 / Valid loss: 6.894623701913016
Training loss: 0.6916841268539429 / Valid loss: 6.908578361783709
Training loss: 0.3409446179866791 / Valid loss: 6.882897472381591

Epoch: 17
Training loss: 0.2929670512676239 / Valid loss: 6.826634815761021
Training loss: 0.202987939119339 / Valid loss: 6.794696335565476
Training loss: 0.35849833488464355 / Valid loss: 6.8163499196370445
Training loss: 0.6253277659416199 / Valid loss: 6.876091425759451
Training loss: 0.24133768677711487 / Valid loss: 6.921649101802281

Epoch: 18
Training loss: 0.2207256704568863 / Valid loss: 6.766701934451149
Training loss: 0.5001040697097778 / Valid loss: 6.795575341724214
Training loss: 0.29343461990356445 / Valid loss: 6.778574080694289
Training loss: 0.2670270502567291 / Valid loss: 6.894643356686546
Training loss: 0.3846880793571472 / Valid loss: 6.791883595784506

Epoch: 19
Training loss: 0.36628246307373047 / Valid loss: 6.784356183097476
Training loss: 0.3995077311992645 / Valid loss: 6.838403574625651
Training loss: 0.21255606412887573 / Valid loss: 6.868103181748163
Training loss: 0.2618091106414795 / Valid loss: 6.946741705849057

Epoch: 20
Training loss: 0.3474332392215729 / Valid loss: 6.841365836915516
Training loss: 0.3138289451599121 / Valid loss: 6.873812548319498
Training loss: 0.48008957505226135 / Valid loss: 6.879916758764358
Training loss: 0.19026365876197815 / Valid loss: 6.812907377878825
Training loss: 0.3100920617580414 / Valid loss: 6.795332127525693

Epoch: 21
Training loss: 0.14189636707305908 / Valid loss: 6.806473616191319
Training loss: 0.4862443506717682 / Valid loss: 6.737730675651914
Training loss: 0.25240176916122437 / Valid loss: 6.793463221050444
Training loss: 0.20081225037574768 / Valid loss: 6.7995349974859325
Training loss: 0.2317204773426056 / Valid loss: 6.847449125562395

Epoch: 22
Training loss: 0.35564422607421875 / Valid loss: 6.794223690032959
Training loss: 0.15230701863765717 / Valid loss: 6.861483937218075
Training loss: 0.4719240069389343 / Valid loss: 6.817471190861293
Training loss: 0.5883740186691284 / Valid loss: 6.916577502659389
Training loss: 0.2592882513999939 / Valid loss: 6.854958284468878

Epoch: 23
Training loss: 0.1986827254295349 / Valid loss: 6.857266185397194
Training loss: 0.42163771390914917 / Valid loss: 6.87214548928397
Training loss: 0.959857165813446 / Valid loss: 6.774533012935094
Training loss: 0.13871058821678162 / Valid loss: 6.950332823253813
Training loss: 0.2565608322620392 / Valid loss: 6.872097074417841

Epoch: 24
Training loss: 0.32916420698165894 / Valid loss: 6.900554168791998
Training loss: 0.22103314101696014 / Valid loss: 6.77794710340954
Training loss: 0.4730081260204315 / Valid loss: 6.80828979128883
Training loss: 0.204532191157341 / Valid loss: 6.786112176804315
Training loss: 0.4216873347759247 / Valid loss: 6.853757095336914

Epoch: 25
Training loss: 0.41908323764801025 / Valid loss: 6.824651409330822
Training loss: 0.3060964047908783 / Valid loss: 6.8067235787709555
Training loss: 0.32903069257736206 / Valid loss: 6.814995359239124
Training loss: 0.29962989687919617 / Valid loss: 6.808039288293748
Training loss: 0.3225189447402954 / Valid loss: 6.914066487266904

Epoch: 26
Training loss: 0.26686227321624756 / Valid loss: 6.859159637632824
Training loss: 0.26777398586273193 / Valid loss: 6.868744007746378
Training loss: 0.2412346601486206 / Valid loss: 6.829711786905924
Training loss: 0.1644948124885559 / Valid loss: 6.826261885960897
Training loss: 0.3142988681793213 / Valid loss: 6.864504759652274

Epoch: 27
Training loss: 0.23770606517791748 / Valid loss: 6.866000370752244
Training loss: 0.3605474531650543 / Valid loss: 6.902607595352899
Training loss: 0.37771719694137573 / Valid loss: 6.844894238880703
Training loss: 0.20420202612876892 / Valid loss: 6.827095563071115
Training loss: 0.18738910555839539 / Valid loss: 6.752018483479818

Epoch: 28
Training loss: 0.21511459350585938 / Valid loss: 6.813782233283633
Training loss: 0.17459937930107117 / Valid loss: 6.762562156858898
Training loss: 0.40423524379730225 / Valid loss: 6.737609091259184
Training loss: 0.21373511850833893 / Valid loss: 6.818117214384533
Training loss: 0.15509693324565887 / Valid loss: 6.824519761403402

Epoch: 29
Training loss: 0.11955154687166214 / Valid loss: 6.879199886322022
Training loss: 0.20302066206932068 / Valid loss: 6.839168080829439
Training loss: 0.3694685697555542 / Valid loss: 6.821797411782401
Training loss: 0.26014038920402527 / Valid loss: 6.8133487837655204

Epoch: 30
Training loss: 0.49358242750167847 / Valid loss: 6.933948925563267
Training loss: 0.2455330342054367 / Valid loss: 6.747812189374652
Training loss: 0.1639866828918457 / Valid loss: 6.831049832843599
Training loss: 0.2253948152065277 / Valid loss: 6.851490506671724
Training loss: 0.3085760176181793 / Valid loss: 6.850447014399937

Epoch: 31
Training loss: 0.2867448329925537 / Valid loss: 6.917605972290039
Training loss: 0.18103376030921936 / Valid loss: 6.792305301484608
Training loss: 0.21167199313640594 / Valid loss: 6.871933178674607
Training loss: 0.17536482214927673 / Valid loss: 6.754181575775147
Training loss: 0.18958070874214172 / Valid loss: 6.836099883488246

Epoch: 32
Training loss: 0.26624056696891785 / Valid loss: 6.874772307986305
Training loss: 0.1895039677619934 / Valid loss: 6.805401134490967
Training loss: 0.22558808326721191 / Valid loss: 6.743262990315755
Training loss: 0.23573458194732666 / Valid loss: 6.8127100535801475
Training loss: 0.09613310545682907 / Valid loss: 6.783428323836554

Epoch: 33
Training loss: 0.22248247265815735 / Valid loss: 6.828184772673107
Training loss: 0.185958594083786 / Valid loss: 6.745828494571504
Training loss: 0.26854047179222107 / Valid loss: 6.805506629035587
Training loss: 0.14702536165714264 / Valid loss: 6.818772041229975
Training loss: 0.18452392518520355 / Valid loss: 6.790109911419097

Epoch: 34
Training loss: 0.5206811428070068 / Valid loss: 6.755184114547003
Training loss: 0.1464935541152954 / Valid loss: 6.77285430771964
Training loss: 0.18157371878623962 / Valid loss: 6.80527454557873
Training loss: 0.17695540189743042 / Valid loss: 6.808745665777297
Training loss: 0.21783064305782318 / Valid loss: 6.9476253713880265

Epoch: 35
Training loss: 0.20432379841804504 / Valid loss: 6.809744169598534
Training loss: 0.2624642848968506 / Valid loss: 6.805258714585078
Training loss: 0.2237538993358612 / Valid loss: 6.814376626695905
Training loss: 0.10667432844638824 / Valid loss: 6.807266884758359
Training loss: 0.5050758123397827 / Valid loss: 6.869486990429106

Epoch: 36
Training loss: 0.1526976227760315 / Valid loss: 6.812831538064139
Training loss: 0.27794861793518066 / Valid loss: 6.7927031925746375
Training loss: 0.20628014206886292 / Valid loss: 6.879074110303606
Training loss: 0.21822623908519745 / Valid loss: 6.814427900314331
Training loss: 0.25087013840675354 / Valid loss: 6.811988894144694

Epoch: 37
Training loss: 0.1574472337961197 / Valid loss: 6.869239291690644
Training loss: 0.14597828686237335 / Valid loss: 6.743290038335891
Training loss: 0.4632410407066345 / Valid loss: 6.821162725630261
Training loss: 0.2436566948890686 / Valid loss: 6.846349679856074
Training loss: 0.29798638820648193 / Valid loss: 6.9154951277233305

Epoch: 38
Training loss: 0.16338130831718445 / Valid loss: 6.813678573426746
Training loss: 0.15052619576454163 / Valid loss: 6.830858167012533
Training loss: 0.20106768608093262 / Valid loss: 6.786963403792608
Training loss: 0.17200103402137756 / Valid loss: 6.891637625013079
Training loss: 0.09449607878923416 / Valid loss: 6.834856596447173

Epoch: 39
Training loss: 0.15691275894641876 / Valid loss: 6.869220520201184
Training loss: 0.28705739974975586 / Valid loss: 6.801109572819301
Training loss: 0.16413509845733643 / Valid loss: 6.790891897110712
Training loss: 0.14062052965164185 / Valid loss: 6.799091077986217
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2500): 6.57307687486921
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.313697814941406 / Valid loss: 15.75428824651809
Model is saved in epoch 0, overall batch: 0
Training loss: 14.700357437133789 / Valid loss: 15.241077404930477
Model is saved in epoch 0, overall batch: 100
Training loss: 12.167218208312988 / Valid loss: 13.1383024034046
Model is saved in epoch 0, overall batch: 200
Training loss: 7.39070987701416 / Valid loss: 12.20773739587693
Model is saved in epoch 0, overall batch: 300
Training loss: 8.369834899902344 / Valid loss: 11.72161353883289
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 7.6263346672058105 / Valid loss: 11.303667368207659
Model is saved in epoch 1, overall batch: 500
Training loss: 7.615161418914795 / Valid loss: 10.977212615240187
Model is saved in epoch 1, overall batch: 600
Training loss: 9.68490219116211 / Valid loss: 10.42702176684425
Model is saved in epoch 1, overall batch: 700
Training loss: 6.548203468322754 / Valid loss: 9.694672920590355
Model is saved in epoch 1, overall batch: 800
Training loss: 8.813409805297852 / Valid loss: 9.560487706320627
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.877181529998779 / Valid loss: 8.98826534180414
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.0447773933410645 / Valid loss: 8.896401900336857
Model is saved in epoch 2, overall batch: 1100
Training loss: 3.052676200866699 / Valid loss: 8.427262615022205
Model is saved in epoch 2, overall batch: 1200
Training loss: 2.882483720779419 / Valid loss: 8.102327578408378
Model is saved in epoch 2, overall batch: 1300
Training loss: 3.9241809844970703 / Valid loss: 7.8662363688151045
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 0.8519588112831116 / Valid loss: 7.3530369531540645
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.0022130012512207 / Valid loss: 7.530597289403279
Training loss: 2.8279051780700684 / Valid loss: 7.285693520591373
Model is saved in epoch 3, overall batch: 1700
Training loss: 2.381460666656494 / Valid loss: 7.113954775674003
Model is saved in epoch 3, overall batch: 1800
Training loss: 3.345576763153076 / Valid loss: 7.016081169673375
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 0.9923201203346252 / Valid loss: 6.830339038939703
Model is saved in epoch 4, overall batch: 2000
Training loss: 1.774680733680725 / Valid loss: 6.881398055666969
Training loss: 1.2141221761703491 / Valid loss: 6.791256414140974
Model is saved in epoch 4, overall batch: 2200
Training loss: 1.4317817687988281 / Valid loss: 6.716959644499279
Model is saved in epoch 4, overall batch: 2300
Training loss: 1.2031633853912354 / Valid loss: 6.78033021291097

Epoch: 5
Training loss: 0.7435458898544312 / Valid loss: 6.6815687247685025
Model is saved in epoch 5, overall batch: 2500
Training loss: 1.1464109420776367 / Valid loss: 6.6983417147681825
Training loss: 1.2950735092163086 / Valid loss: 6.735611379714239
Training loss: 1.4203256368637085 / Valid loss: 6.73923575991676
Training loss: 1.2294740676879883 / Valid loss: 6.791013402030582

Epoch: 6
Training loss: 0.6050777435302734 / Valid loss: 6.727657143274943
Training loss: 0.9684150218963623 / Valid loss: 6.676746983755202
Model is saved in epoch 6, overall batch: 3100
Training loss: 0.7742184996604919 / Valid loss: 6.689106541588193
Training loss: 0.8080999851226807 / Valid loss: 6.809524318150111
Training loss: 0.8464338779449463 / Valid loss: 6.7255325090317495

Epoch: 7
Training loss: 0.721021294593811 / Valid loss: 6.716790112994966
Training loss: 0.4945122003555298 / Valid loss: 6.791015048254104
Training loss: 0.7292464971542358 / Valid loss: 6.785309164864676
Training loss: 0.6251881122589111 / Valid loss: 6.81250425974528
Training loss: 0.6554616689682007 / Valid loss: 6.761764326549712

Epoch: 8
Training loss: 0.44299423694610596 / Valid loss: 6.669622157868885
Model is saved in epoch 8, overall batch: 4000
Training loss: 0.6467598080635071 / Valid loss: 6.767721537181309
Training loss: 0.6769647002220154 / Valid loss: 6.760021931784493
Training loss: 0.3636866807937622 / Valid loss: 6.812616616203671
Training loss: 1.1223825216293335 / Valid loss: 6.843003908793132

Epoch: 9
Training loss: 0.5438838005065918 / Valid loss: 6.7231906731923425
Training loss: 0.4957156181335449 / Valid loss: 6.667118805930728
Model is saved in epoch 9, overall batch: 4600
Training loss: 0.8114796876907349 / Valid loss: 6.788304785319737
Training loss: 1.014798641204834 / Valid loss: 6.9464489982241675

Epoch: 10
Training loss: 0.702489972114563 / Valid loss: 6.948087587810698
Training loss: 0.5089366436004639 / Valid loss: 6.76922079267956
Training loss: 0.5368586778640747 / Valid loss: 6.784164067677089
Training loss: 0.31153208017349243 / Valid loss: 6.772459665934245
Training loss: 0.3239787220954895 / Valid loss: 6.8581655366080145

Epoch: 11
Training loss: 0.44543176889419556 / Valid loss: 6.7993586631048295
Training loss: 0.40456658601760864 / Valid loss: 6.714099039350237
Training loss: 0.44235944747924805 / Valid loss: 6.676967906951904
Training loss: 0.4263675808906555 / Valid loss: 6.908010968707857
Training loss: 0.9241876602172852 / Valid loss: 6.755384463355655

Epoch: 12
Training loss: 0.6188421845436096 / Valid loss: 6.7967232340858095
Training loss: 0.33039775490760803 / Valid loss: 6.771483512151809
Training loss: 0.31962886452674866 / Valid loss: 6.871581499917166
Training loss: 0.7043102979660034 / Valid loss: 6.821499517985752
Training loss: 0.43785011768341064 / Valid loss: 6.823501266751971

Epoch: 13
Training loss: 0.7106128931045532 / Valid loss: 6.68686155364627
Training loss: 0.2787329852581024 / Valid loss: 6.811497324988956
Training loss: 0.43069902062416077 / Valid loss: 6.787900377455212
Training loss: 0.3645370602607727 / Valid loss: 6.856642200833275
Training loss: 0.37924304604530334 / Valid loss: 6.798605432964506

Epoch: 14
Training loss: 0.3267066180706024 / Valid loss: 6.707119928087507
Training loss: 0.6593794822692871 / Valid loss: 6.70904377301534
Training loss: 0.3463822603225708 / Valid loss: 6.745715445563906
Training loss: 0.43635499477386475 / Valid loss: 6.977158746265229
Training loss: 0.4968603253364563 / Valid loss: 6.868100491024199

Epoch: 15
Training loss: 0.5390568375587463 / Valid loss: 6.72362551007952
Training loss: 0.3483544886112213 / Valid loss: 6.839548655918667
Training loss: 1.1725687980651855 / Valid loss: 6.736494852247692
Training loss: 0.413374125957489 / Valid loss: 6.817420033046178
Training loss: 1.3824995756149292 / Valid loss: 6.756327261243547

Epoch: 16
Training loss: 0.31437593698501587 / Valid loss: 6.776490122931344
Training loss: 0.6318480968475342 / Valid loss: 6.733231776101249
Training loss: 0.5335781574249268 / Valid loss: 6.898680282774426
Training loss: 0.4600346088409424 / Valid loss: 6.845720942815145
Training loss: 0.3615126609802246 / Valid loss: 6.835439836411249

Epoch: 17
Training loss: 0.30407798290252686 / Valid loss: 6.710075841631208
Training loss: 0.22849193215370178 / Valid loss: 6.697299680255708
Training loss: 0.40326789021492004 / Valid loss: 6.732454726809547
Training loss: 0.6178618669509888 / Valid loss: 6.751684552147275
Training loss: 0.22512850165367126 / Valid loss: 6.761595171973819

Epoch: 18
Training loss: 0.252219021320343 / Valid loss: 6.709966582343692
Training loss: 0.37929773330688477 / Valid loss: 6.67324618612017
Training loss: 0.351925253868103 / Valid loss: 6.732352697281611
Training loss: 0.2864583730697632 / Valid loss: 6.774190114793323
Training loss: 0.419711172580719 / Valid loss: 6.730570770445324

Epoch: 19
Training loss: 0.3099132180213928 / Valid loss: 6.662150353477115
Model is saved in epoch 19, overall batch: 9400
Training loss: 0.28000742197036743 / Valid loss: 6.7057957921709335
Training loss: 0.21573548018932343 / Valid loss: 6.787102980840774
Training loss: 0.2351524531841278 / Valid loss: 6.800758765992664

Epoch: 20
Training loss: 0.3713008463382721 / Valid loss: 6.75621843565078
Training loss: 0.31963542103767395 / Valid loss: 6.725274644579206
Training loss: 0.39611539244651794 / Valid loss: 6.760205300649007
Training loss: 0.15829311311244965 / Valid loss: 6.719385380972
Training loss: 0.32552462816238403 / Valid loss: 6.724449405216036

Epoch: 21
Training loss: 0.17540675401687622 / Valid loss: 6.803098101842971
Training loss: 0.4537932872772217 / Valid loss: 6.681482206072126
Training loss: 0.3055260181427002 / Valid loss: 6.695220981325422
Training loss: 0.18904900550842285 / Valid loss: 6.720729882376535
Training loss: 0.19720309972763062 / Valid loss: 6.780336611611503

Epoch: 22
Training loss: 0.49323174357414246 / Valid loss: 6.662388633546375
Training loss: 0.16491049528121948 / Valid loss: 6.710837850116548
Training loss: 0.455473393201828 / Valid loss: 6.7106675465901695
Training loss: 0.6489241123199463 / Valid loss: 6.877962884448824
Training loss: 0.2900295853614807 / Valid loss: 6.763016800653367

Epoch: 23
Training loss: 0.21892289817333221 / Valid loss: 6.760751840046474
Training loss: 0.38418328762054443 / Valid loss: 6.746086365836007
Training loss: 1.05743408203125 / Valid loss: 6.68273830867949
Training loss: 0.1929040551185608 / Valid loss: 6.84933211916969
Training loss: 0.3751794099807739 / Valid loss: 6.730176852998279

Epoch: 24
Training loss: 0.34716302156448364 / Valid loss: 6.733599617367699
Training loss: 0.25434836745262146 / Valid loss: 6.665255349023002
Training loss: 0.3806551396846771 / Valid loss: 6.733329277946836
Training loss: 0.2561848759651184 / Valid loss: 6.737579136803037
Training loss: 0.34676289558410645 / Valid loss: 6.81133857908703

Epoch: 25
Training loss: 0.3758891224861145 / Valid loss: 6.6466433161780945
Model is saved in epoch 25, overall batch: 12300
Training loss: 0.26629021763801575 / Valid loss: 6.706347401936849
Training loss: 0.22694101929664612 / Valid loss: 6.706559887386503
Training loss: 0.3221280574798584 / Valid loss: 6.703519609996251
Training loss: 0.32702237367630005 / Valid loss: 6.818383012499128

Epoch: 26
Training loss: 0.17594283819198608 / Valid loss: 6.741692388625372
Training loss: 0.2343595027923584 / Valid loss: 6.701957471030099
Training loss: 0.2416345477104187 / Valid loss: 6.765132658822196
Training loss: 0.23422963917255402 / Valid loss: 6.678740903309413
Training loss: 0.24581637978553772 / Valid loss: 6.692732690629505

Epoch: 27
Training loss: 0.2650783956050873 / Valid loss: 6.759425817217146
Training loss: 0.3293219208717346 / Valid loss: 6.710610984620594
Training loss: 0.2531958520412445 / Valid loss: 6.714628321783883
Training loss: 0.18647649884223938 / Valid loss: 6.657765452067057
Training loss: 0.14045420289039612 / Valid loss: 6.683470015298752

Epoch: 28
Training loss: 0.2420337200164795 / Valid loss: 6.706850687662761
Training loss: 0.19935452938079834 / Valid loss: 6.680670397622245
Training loss: 0.40499627590179443 / Valid loss: 6.604439431145078
Model is saved in epoch 28, overall batch: 14000
Training loss: 0.2814408540725708 / Valid loss: 6.680941388720558
Training loss: 0.1761697381734848 / Valid loss: 6.691017382485526

Epoch: 29
Training loss: 0.13214656710624695 / Valid loss: 6.669221821285429
Training loss: 0.1518564671278 / Valid loss: 6.640808014642625
Training loss: 0.32080700993537903 / Valid loss: 6.7195683842613585
Training loss: 0.2118193656206131 / Valid loss: 6.715493165879023

Epoch: 30
Training loss: 0.4302261471748352 / Valid loss: 6.772875706354777
Training loss: 0.2927830219268799 / Valid loss: 6.6197433698745005
Training loss: 0.18297654390335083 / Valid loss: 6.695922565460205
Training loss: 0.1475263386964798 / Valid loss: 6.705221207936605
Training loss: 0.23706749081611633 / Valid loss: 6.736545160838536

Epoch: 31
Training loss: 0.2503491938114166 / Valid loss: 6.756960753032139
Training loss: 0.14416912198066711 / Valid loss: 6.68495975676037
Training loss: 0.23662057518959045 / Valid loss: 6.713675299144927
Training loss: 0.17069396376609802 / Valid loss: 6.631795731044951
Training loss: 0.18449372053146362 / Valid loss: 6.6922591754368375

Epoch: 32
Training loss: 0.23490668833255768 / Valid loss: 6.699809685207549
Training loss: 0.13053856790065765 / Valid loss: 6.696951366606212
Training loss: 0.21243101358413696 / Valid loss: 6.582971731821696
Model is saved in epoch 32, overall batch: 15900
Training loss: 0.23507198691368103 / Valid loss: 6.708298960186187
Training loss: 0.1301240622997284 / Valid loss: 6.671631454286121

Epoch: 33
Training loss: 0.22592191398143768 / Valid loss: 6.693064040229434
Training loss: 0.2136864960193634 / Valid loss: 6.590091693969
Training loss: 0.2073308229446411 / Valid loss: 6.670116946810768
Training loss: 0.2187950611114502 / Valid loss: 6.686601897648402
Training loss: 0.20519939064979553 / Valid loss: 6.7020261764526365

Epoch: 34
Training loss: 0.4892866313457489 / Valid loss: 6.6437324569338845
Training loss: 0.10951098799705505 / Valid loss: 6.666143144880023
Training loss: 0.17005567252635956 / Valid loss: 6.660560630616688
Training loss: 0.2525032162666321 / Valid loss: 6.644841066996256
Training loss: 0.16934575140476227 / Valid loss: 6.756684482665289

Epoch: 35
Training loss: 0.1225663274526596 / Valid loss: 6.692506108965192
Training loss: 0.2927224338054657 / Valid loss: 6.663555197488694
Training loss: 0.18564529716968536 / Valid loss: 6.676468862806048
Training loss: 0.15074345469474792 / Valid loss: 6.654552316665649
Training loss: 0.4978533387184143 / Valid loss: 6.698899936676026

Epoch: 36
Training loss: 0.15041303634643555 / Valid loss: 6.689797115325928
Training loss: 0.27353066205978394 / Valid loss: 6.653942898341588
Training loss: 0.2256971001625061 / Valid loss: 6.698822026025681
Training loss: 0.21350109577178955 / Valid loss: 6.65013466108413
Training loss: 0.27252399921417236 / Valid loss: 6.682423312323434

Epoch: 37
Training loss: 0.19790208339691162 / Valid loss: 6.676094259534564
Training loss: 0.13532474637031555 / Valid loss: 6.598158591134208
Training loss: 0.6253286004066467 / Valid loss: 6.651372296469552
Training loss: 0.25592660903930664 / Valid loss: 6.702653648739769
Training loss: 0.25225022435188293 / Valid loss: 6.67304413659232

Epoch: 38
Training loss: 0.14594700932502747 / Valid loss: 6.642712257021949
Training loss: 0.16036449372768402 / Valid loss: 6.633828292574202
Training loss: 0.12153530865907669 / Valid loss: 6.612713573092506
Training loss: 0.17320966720581055 / Valid loss: 6.699041906992594
Training loss: 0.11387547850608826 / Valid loss: 6.646301755451021

Epoch: 39
Training loss: 0.12988945841789246 / Valid loss: 6.6366251241593135
Training loss: 0.23697862029075623 / Valid loss: 6.607373137701125
Training loss: 0.1639581024646759 / Valid loss: 6.615673966634841
Training loss: 0.1810261756181717 / Valid loss: 6.6369246392023
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 15900): 6.510488071895781
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.220863342285156 / Valid loss: 16.43889894031343
Model is saved in epoch 0, overall batch: 0
Training loss: 11.010725021362305 / Valid loss: 12.136854262579055
Model is saved in epoch 0, overall batch: 100
Training loss: 8.312519073486328 / Valid loss: 8.353009378342401
Model is saved in epoch 0, overall batch: 200
Training loss: 5.576720714569092 / Valid loss: 6.936966671262469
Model is saved in epoch 0, overall batch: 300
Training loss: 6.93902063369751 / Valid loss: 6.341448842911493
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.22274923324585 / Valid loss: 6.054472991398403
Model is saved in epoch 1, overall batch: 500
Training loss: 3.307562828063965 / Valid loss: 6.093241350991385
Training loss: 2.688591241836548 / Valid loss: 6.184382445471627
Training loss: 4.3806257247924805 / Valid loss: 6.255521887824649
Training loss: 4.116939544677734 / Valid loss: 6.284444234484718

Epoch: 2
Training loss: 1.685755729675293 / Valid loss: 6.288634895143055
Training loss: 2.2900938987731934 / Valid loss: 6.423663591203235
Training loss: 2.166539192199707 / Valid loss: 6.4515859899066745
Training loss: 2.069154977798462 / Valid loss: 6.514133371625628
Training loss: 2.034419298171997 / Valid loss: 6.5277907689412435

Epoch: 3
Training loss: 1.5604474544525146 / Valid loss: 6.612724202019828
Training loss: 1.8657437562942505 / Valid loss: 6.71597413562593
Training loss: 1.95290207862854 / Valid loss: 6.682222012111119
Training loss: 1.4603689908981323 / Valid loss: 6.678148719242641
Training loss: 2.54310941696167 / Valid loss: 6.682394018627348

Epoch: 4
Training loss: 1.3245253562927246 / Valid loss: 6.743814963386172
Training loss: 1.1290065050125122 / Valid loss: 6.782487524123419
Training loss: 1.4995176792144775 / Valid loss: 6.802688539595831
Training loss: 2.012632369995117 / Valid loss: 6.842526317778088
Training loss: 1.284424066543579 / Valid loss: 6.87516607329959

Epoch: 5
Training loss: 0.9802114963531494 / Valid loss: 6.823968914576939
Training loss: 1.753232479095459 / Valid loss: 6.8382567746298655
Training loss: 0.8567119836807251 / Valid loss: 6.865124902271089
Training loss: 1.9267489910125732 / Valid loss: 6.835303361075265
Training loss: 1.9004170894622803 / Valid loss: 6.860692264920189

Epoch: 6
Training loss: 1.047821283340454 / Valid loss: 6.876826290857224
Training loss: 1.488762617111206 / Valid loss: 6.860671856289819
Training loss: 1.6646640300750732 / Valid loss: 6.8857053211757115
Training loss: 1.0729615688323975 / Valid loss: 6.8943580082484655
Training loss: 2.4375357627868652 / Valid loss: 6.9643440927777975

Epoch: 7
Training loss: 1.3937321901321411 / Valid loss: 6.964436308542887
Training loss: 0.8741785287857056 / Valid loss: 6.962203146162487
Training loss: 1.0502454042434692 / Valid loss: 6.922637773695446
Training loss: 1.020350456237793 / Valid loss: 6.934606338682629
Training loss: 1.1639456748962402 / Valid loss: 7.001792762393043

Epoch: 8
Training loss: 0.8620957732200623 / Valid loss: 6.929139657247634
Training loss: 1.0681827068328857 / Valid loss: 6.95725231624785
Training loss: 1.189394235610962 / Valid loss: 7.0570085321153915
Training loss: 0.7450203895568848 / Valid loss: 7.094849123273577
Training loss: 0.9157299399375916 / Valid loss: 7.084551157270159

Epoch: 9
Training loss: 0.7911748886108398 / Valid loss: 7.027384108588809
Training loss: 0.9922532439231873 / Valid loss: 6.937702324276879
Training loss: 0.877464234828949 / Valid loss: 7.002670460655576
Training loss: 0.6232957243919373 / Valid loss: 7.023536718459356

Epoch: 10
Training loss: 0.5907033681869507 / Valid loss: 7.034772859300886
Training loss: 0.8060780167579651 / Valid loss: 7.0562429609752835
Training loss: 0.5145354270935059 / Valid loss: 7.069271673474993
Training loss: 1.2520129680633545 / Valid loss: 7.026525953837804
Training loss: 1.0947365760803223 / Valid loss: 7.13703396660941

Epoch: 11
Training loss: 0.5585460662841797 / Valid loss: 7.102332569303966
Training loss: 0.6277426481246948 / Valid loss: 7.0142974876222155
Training loss: 0.7668939232826233 / Valid loss: 7.139676598140172
Training loss: 0.9671870470046997 / Valid loss: 7.2437482379731675
Training loss: 0.8539898991584778 / Valid loss: 7.188925579616002

Epoch: 12
Training loss: 0.6566191911697388 / Valid loss: 7.204005077907017
Training loss: 0.4326581358909607 / Valid loss: 7.159751587822324
Training loss: 1.179897427558899 / Valid loss: 7.146103363945371
Training loss: 0.7157042026519775 / Valid loss: 7.073645671208699
Training loss: 0.6124178171157837 / Valid loss: 7.103249627067929

Epoch: 13
Training loss: 0.7674095630645752 / Valid loss: 7.069810553959438
Training loss: 0.5332539081573486 / Valid loss: 7.089443283989316
Training loss: 0.5923007726669312 / Valid loss: 7.076223941076369
Training loss: 0.46528324484825134 / Valid loss: 7.16174685160319
Training loss: 0.8815881609916687 / Valid loss: 7.067097155253093

Epoch: 14
Training loss: 0.7154781818389893 / Valid loss: 7.095803256261917
Training loss: 0.5875641107559204 / Valid loss: 7.063563905443464
Training loss: 0.4630761444568634 / Valid loss: 7.0647342500232515
Training loss: 0.4945969879627228 / Valid loss: 7.106763421921503
Training loss: 0.6335958242416382 / Valid loss: 7.1230066208612355

Epoch: 15
Training loss: 0.6444014310836792 / Valid loss: 7.170005466824486
Training loss: 0.6619549989700317 / Valid loss: 7.159081390925816
Training loss: 0.8930954337120056 / Valid loss: 7.154626796359108
Training loss: 0.85792475938797 / Valid loss: 7.1142310028984435
Training loss: 0.3975512683391571 / Valid loss: 7.159545053754534

Epoch: 16
Training loss: 0.7336669564247131 / Valid loss: 7.145486232212612
Training loss: 0.4641796946525574 / Valid loss: 7.078643866947719
Training loss: 0.44675225019454956 / Valid loss: 7.065433547610328
Training loss: 0.7043405771255493 / Valid loss: 7.176013628641765
Training loss: 0.9182149171829224 / Valid loss: 7.041791343688965

Epoch: 17
Training loss: 0.4471115171909332 / Valid loss: 7.079055563608805
Training loss: 0.3570058047771454 / Valid loss: 7.117388634454636
Training loss: 0.5481861233711243 / Valid loss: 7.097369807107108
Training loss: 0.4013429582118988 / Valid loss: 7.110388083685012
Training loss: 0.6108609437942505 / Valid loss: 7.110741424560547

Epoch: 18
Training loss: 0.8557604551315308 / Valid loss: 7.079570652189709
Training loss: 0.9366413354873657 / Valid loss: 7.07907817023141
Training loss: 0.8217254281044006 / Valid loss: 7.120234530312675
Training loss: 0.43516111373901367 / Valid loss: 7.151746245792934
Training loss: 0.6407977938652039 / Valid loss: 7.201108210427421

Epoch: 19
Training loss: 0.778720498085022 / Valid loss: 7.080169187273298
Training loss: 0.5393562316894531 / Valid loss: 7.076718543824695
Training loss: 0.7087148427963257 / Valid loss: 7.111634417942592
Training loss: 0.5702794194221497 / Valid loss: 7.101467786516462

Epoch: 20
Training loss: 0.3731014132499695 / Valid loss: 7.16519505182902
Training loss: 0.6948340535163879 / Valid loss: 7.0201416537875225
Training loss: 0.30771175026893616 / Valid loss: 7.068975126175653
Training loss: 0.5479652285575867 / Valid loss: 7.129569589524042
Training loss: 0.4792598485946655 / Valid loss: 7.049782053629557

Epoch: 21
Training loss: 0.17904892563819885 / Valid loss: 7.144348909741356
Training loss: 0.48197507858276367 / Valid loss: 7.089768931979225
Training loss: 0.6363959312438965 / Valid loss: 7.042125043414888
Training loss: 0.48837414383888245 / Valid loss: 7.131077443985712
Training loss: 0.48628175258636475 / Valid loss: 7.1404013951619465

Epoch: 22
Training loss: 0.40938469767570496 / Valid loss: 7.1121663547697525
Training loss: 0.2545161247253418 / Valid loss: 7.144327754066104
Training loss: 0.6882036924362183 / Valid loss: 7.09191370010376
Training loss: 0.4939751923084259 / Valid loss: 7.1419182277861095
Training loss: 0.28418537974357605 / Valid loss: 7.045908877963112

Epoch: 23
Training loss: 0.37706416845321655 / Valid loss: 7.051175458090646
Training loss: 0.3789159655570984 / Valid loss: 7.083803671882266
Training loss: 0.46872764825820923 / Valid loss: 7.025963765098935
Training loss: 0.862682580947876 / Valid loss: 7.085049992515927
Training loss: 0.48877209424972534 / Valid loss: 7.099100103832426

Epoch: 24
Training loss: 0.3509310781955719 / Valid loss: 7.067668578738258
Training loss: 0.561310887336731 / Valid loss: 7.088577175140381
Training loss: 0.7400058507919312 / Valid loss: 7.078513059161958
Training loss: 0.5029176473617554 / Valid loss: 7.147199703398205
Training loss: 0.340705931186676 / Valid loss: 7.117360069638207

Epoch: 25
Training loss: 0.5662655234336853 / Valid loss: 7.072266431081863
Training loss: 0.5900287628173828 / Valid loss: 7.07653507959275
Training loss: 0.3347401022911072 / Valid loss: 7.104861659095401
Training loss: 0.3967958390712738 / Valid loss: 7.064677960532052
Training loss: 0.37938374280929565 / Valid loss: 7.049376855577742

Epoch: 26
Training loss: 0.396867960691452 / Valid loss: 7.04867825735183
Training loss: 0.39075055718421936 / Valid loss: 6.990539668855213
Training loss: 0.34383922815322876 / Valid loss: 7.049469893319266
Training loss: 0.42446470260620117 / Valid loss: 7.070613804317656
Training loss: 0.5729614496231079 / Valid loss: 7.04766366141183

Epoch: 27
Training loss: 0.6845331192016602 / Valid loss: 7.012137376694453
Training loss: 0.6675896644592285 / Valid loss: 6.997539202372233
Training loss: 0.27437567710876465 / Valid loss: 7.025667063395182
Training loss: 0.2925542891025543 / Valid loss: 7.10383715856643
Training loss: 0.33306729793548584 / Valid loss: 7.094251918792724

Epoch: 28
Training loss: 0.2840394675731659 / Valid loss: 7.044048270725068
Training loss: 0.43845871090888977 / Valid loss: 7.088881138392857
Training loss: 0.5152723789215088 / Valid loss: 7.041694028036935
Training loss: 0.3260875344276428 / Valid loss: 7.061306817190988
Training loss: 0.28037145733833313 / Valid loss: 7.092274420601981

Epoch: 29
Training loss: 0.788262128829956 / Valid loss: 7.079811918167841
Training loss: 0.24343901872634888 / Valid loss: 7.101370202927362
Training loss: 0.44496044516563416 / Valid loss: 7.010643477666946
Training loss: 0.40854793787002563 / Valid loss: 7.054571056365967

Epoch: 30
Training loss: 0.3794634938240051 / Valid loss: 7.1077138083321705
Training loss: 0.5003958344459534 / Valid loss: 7.008455644335066
Training loss: 0.5343549251556396 / Valid loss: 7.012238466171992
Training loss: 0.5936756134033203 / Valid loss: 7.010988735017323
Training loss: 0.2212054580450058 / Valid loss: 7.00769544328962

Epoch: 31
Training loss: 0.2620561718940735 / Valid loss: 7.012795988718668
Training loss: 0.4561142921447754 / Valid loss: 7.013898917606899
Training loss: 0.3807978928089142 / Valid loss: 7.086037311099824
Training loss: 0.4204857051372528 / Valid loss: 7.010263272694179
Training loss: 0.6750738620758057 / Valid loss: 7.058216483252389

Epoch: 32
Training loss: 0.1812475621700287 / Valid loss: 7.026829492478144
Training loss: 0.2207207977771759 / Valid loss: 7.037157848903111
Training loss: 0.28058120608329773 / Valid loss: 6.989345255352202
Training loss: 0.2040015310049057 / Valid loss: 7.132482560475667
Training loss: 0.4374743103981018 / Valid loss: 7.080489626384916

Epoch: 33
Training loss: 0.6396987438201904 / Valid loss: 7.023339889163062
Training loss: 0.3338611423969269 / Valid loss: 7.050342850458055
Training loss: 0.27859586477279663 / Valid loss: 7.0945637339637395
Training loss: 0.17618171870708466 / Valid loss: 7.026346751621792
Training loss: 0.36097070574760437 / Valid loss: 7.0755668594723655

Epoch: 34
Training loss: 0.20651331543922424 / Valid loss: 7.034980474199568
Training loss: 0.29477259516716003 / Valid loss: 7.058251292364938
Training loss: 0.49960994720458984 / Valid loss: 7.05553750764756
Training loss: 0.35413411259651184 / Valid loss: 6.996748529161725
Training loss: 0.19278869032859802 / Valid loss: 7.078712131863549

Epoch: 35
Training loss: 0.4168831706047058 / Valid loss: 7.064978313446045
Training loss: 0.3366396725177765 / Valid loss: 7.055232286453247
Training loss: 0.25345128774642944 / Valid loss: 6.984307420821417
Training loss: 0.26596546173095703 / Valid loss: 7.01649508249192
Training loss: 0.389209121465683 / Valid loss: 7.099722231002081

Epoch: 36
Training loss: 0.32646968960762024 / Valid loss: 7.04905195917402
Training loss: 0.2592369019985199 / Valid loss: 7.0226911998930435
Training loss: 0.18870854377746582 / Valid loss: 7.028390044257755
Training loss: 0.34274089336395264 / Valid loss: 7.085550916762579
Training loss: 0.236776202917099 / Valid loss: 7.068241112572807

Epoch: 37
Training loss: 0.28310078382492065 / Valid loss: 7.041920475732713
Training loss: 0.9691177606582642 / Valid loss: 7.007404761087327
Training loss: 0.25092998147010803 / Valid loss: 7.028324613117037
Training loss: 0.3354108929634094 / Valid loss: 6.984066059475853
Training loss: 0.30190712213516235 / Valid loss: 6.989691436858404

Epoch: 38
Training loss: 0.25419148802757263 / Valid loss: 6.983464009421212
Training loss: 0.2177290916442871 / Valid loss: 7.000724547249931
Training loss: 0.42470794916152954 / Valid loss: 7.069397490365165
Training loss: 0.22574634850025177 / Valid loss: 7.056407694589524
Training loss: 0.392309308052063 / Valid loss: 7.045142759595598

Epoch: 39
Training loss: 0.21129339933395386 / Valid loss: 6.970331855047316
Training loss: 0.2097775638103485 / Valid loss: 7.072067396981375
Training loss: 0.4827858805656433 / Valid loss: 6.980125023069836
Training loss: 0.27364131808280945 / Valid loss: 7.0419281959533695
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.84931263923645
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.220863342285156 / Valid loss: 16.43895381745838
Model is saved in epoch 0, overall batch: 0
Training loss: 11.050359725952148 / Valid loss: 12.184982159024193
Model is saved in epoch 0, overall batch: 100
Training loss: 7.873411178588867 / Valid loss: 8.350096234821137
Model is saved in epoch 0, overall batch: 200
Training loss: 5.4994916915893555 / Valid loss: 7.015137438547043
Model is saved in epoch 0, overall batch: 300
Training loss: 6.790398120880127 / Valid loss: 6.392471722194126
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.770482540130615 / Valid loss: 6.092098867325555
Model is saved in epoch 1, overall batch: 500
Training loss: 3.3677635192871094 / Valid loss: 6.144465451013474
Training loss: 2.9563300609588623 / Valid loss: 6.230835876010713
Training loss: 4.314071178436279 / Valid loss: 6.287359514690581
Training loss: 4.481784820556641 / Valid loss: 6.313443930943807

Epoch: 2
Training loss: 1.8106579780578613 / Valid loss: 6.280772558848063
Training loss: 2.5419511795043945 / Valid loss: 6.3946084204174225
Training loss: 2.44286847114563 / Valid loss: 6.419098561150687
Training loss: 2.1684412956237793 / Valid loss: 6.480762272789365
Training loss: 2.1939430236816406 / Valid loss: 6.4963813963390535

Epoch: 3
Training loss: 1.616542100906372 / Valid loss: 6.578102425166539
Training loss: 1.701889991760254 / Valid loss: 6.722367359343029
Training loss: 2.277080535888672 / Valid loss: 6.680244336809431
Training loss: 1.650471568107605 / Valid loss: 6.655703998747326
Training loss: 2.745997428894043 / Valid loss: 6.655808099110922

Epoch: 4
Training loss: 1.412966251373291 / Valid loss: 6.722581824802217
Training loss: 1.193418264389038 / Valid loss: 6.74810828708467
Training loss: 1.6819379329681396 / Valid loss: 6.797752684638613
Training loss: 1.9618887901306152 / Valid loss: 6.779436156863258
Training loss: 1.1734669208526611 / Valid loss: 6.847134776342482

Epoch: 5
Training loss: 1.1378703117370605 / Valid loss: 6.83216286159697
Training loss: 1.8183531761169434 / Valid loss: 6.822720386868432
Training loss: 0.8237992525100708 / Valid loss: 6.848549965449742
Training loss: 2.018979787826538 / Valid loss: 6.848285507020496
Training loss: 1.9750910997390747 / Valid loss: 6.864056655338833

Epoch: 6
Training loss: 1.038299322128296 / Valid loss: 6.843287143253145
Training loss: 1.556504487991333 / Valid loss: 6.862160001482282
Training loss: 1.7857705354690552 / Valid loss: 6.8785109701610745
Training loss: 1.0205072164535522 / Valid loss: 6.912669508797782
Training loss: 2.4222278594970703 / Valid loss: 6.956383132934571

Epoch: 7
Training loss: 1.3834971189498901 / Valid loss: 6.93044064839681
Training loss: 0.9672678709030151 / Valid loss: 6.974673121316092
Training loss: 1.0683197975158691 / Valid loss: 6.920140411740258
Training loss: 1.0922211408615112 / Valid loss: 6.925855234691075
Training loss: 1.168898105621338 / Valid loss: 6.994749577840169

Epoch: 8
Training loss: 0.9102085828781128 / Valid loss: 6.91872893288022
Training loss: 1.0715776681900024 / Valid loss: 6.953028674352737
Training loss: 1.1034302711486816 / Valid loss: 7.0440203598567415
Training loss: 0.7822631597518921 / Valid loss: 7.109475040435791
Training loss: 0.8957607746124268 / Valid loss: 7.095157489322481

Epoch: 9
Training loss: 0.8082765340805054 / Valid loss: 7.071073087056478
Training loss: 1.0062429904937744 / Valid loss: 6.97640643801008
Training loss: 0.9675000905990601 / Valid loss: 7.046033718472436
Training loss: 0.6868442296981812 / Valid loss: 7.042691040039062

Epoch: 10
Training loss: 0.6447619199752808 / Valid loss: 7.058056041172573
Training loss: 0.8922879695892334 / Valid loss: 7.055028938111805
Training loss: 0.5124119520187378 / Valid loss: 7.084618316377912
Training loss: 1.3584787845611572 / Valid loss: 7.0650660514831545
Training loss: 1.0531527996063232 / Valid loss: 7.1581273305983775

Epoch: 11
Training loss: 0.611049473285675 / Valid loss: 7.133138933635894
Training loss: 0.6410094499588013 / Valid loss: 7.03564262617202
Training loss: 0.6530574560165405 / Valid loss: 7.086877627599807
Training loss: 0.9462870359420776 / Valid loss: 7.22508468173799
Training loss: 1.0626052618026733 / Valid loss: 7.152636296408517

Epoch: 12
Training loss: 0.6753737330436707 / Valid loss: 7.189292757851737
Training loss: 0.5042706727981567 / Valid loss: 7.128251543499174
Training loss: 1.1491787433624268 / Valid loss: 7.143007564544678
Training loss: 0.6642658710479736 / Valid loss: 7.102899814787365
Training loss: 0.5693150162696838 / Valid loss: 7.136288983481271

Epoch: 13
Training loss: 0.77406907081604 / Valid loss: 7.120288317544119
Training loss: 0.5298619270324707 / Valid loss: 7.086505195072719
Training loss: 0.5193688869476318 / Valid loss: 7.051634883880615
Training loss: 0.44510960578918457 / Valid loss: 7.163291172754197
Training loss: 0.9590827226638794 / Valid loss: 7.03885433560326

Epoch: 14
Training loss: 0.666624903678894 / Valid loss: 7.111247530437651
Training loss: 0.5500413179397583 / Valid loss: 7.089057109469459
Training loss: 0.6633180975914001 / Valid loss: 7.059419014340356
Training loss: 0.5785059928894043 / Valid loss: 7.08934886114938
Training loss: 0.4538428485393524 / Valid loss: 7.128458895002093

Epoch: 15
Training loss: 0.621849775314331 / Valid loss: 7.141819590613956
Training loss: 0.62745201587677 / Valid loss: 7.143951761154901
Training loss: 0.6818337440490723 / Valid loss: 7.16513550167992
Training loss: 0.8927078247070312 / Valid loss: 7.119355347042992
Training loss: 0.5601021647453308 / Valid loss: 7.142303389594669

Epoch: 16
Training loss: 0.7443926334381104 / Valid loss: 7.132763803572882
Training loss: 0.44874995946884155 / Valid loss: 7.0838547116234185
Training loss: 0.3146708607673645 / Valid loss: 7.10397515978132
Training loss: 0.629832923412323 / Valid loss: 7.198906980242048
Training loss: 0.963298499584198 / Valid loss: 7.061019343421573

Epoch: 17
Training loss: 0.44825509190559387 / Valid loss: 7.104328246343703
Training loss: 0.3950747549533844 / Valid loss: 7.168088912963867
Training loss: 0.49669522047042847 / Valid loss: 7.115127038955689
Training loss: 0.5170600414276123 / Valid loss: 7.101657136281331
Training loss: 0.6330254673957825 / Valid loss: 7.11448069072905

Epoch: 18
Training loss: 0.8447965383529663 / Valid loss: 7.045893773578462
Training loss: 0.9258676767349243 / Valid loss: 7.102406488146101
Training loss: 0.8848156929016113 / Valid loss: 7.129456788017636
Training loss: 0.6199177503585815 / Valid loss: 7.1504750796726775
Training loss: 0.5547745227813721 / Valid loss: 7.182634508042108

Epoch: 19
Training loss: 0.7639714479446411 / Valid loss: 7.096246019999186
Training loss: 0.5881044864654541 / Valid loss: 7.099508558000837
Training loss: 0.6363106369972229 / Valid loss: 7.159577396937779
Training loss: 0.8073764443397522 / Valid loss: 7.11865382875715

Epoch: 20
Training loss: 0.4235217571258545 / Valid loss: 7.171316973368326
Training loss: 0.8191365003585815 / Valid loss: 7.022628302801223
Training loss: 0.32659393548965454 / Valid loss: 7.085392425173805
Training loss: 0.5552588701248169 / Valid loss: 7.150969691503615
Training loss: 0.45911145210266113 / Valid loss: 7.093217913309733

Epoch: 21
Training loss: 0.29435935616493225 / Valid loss: 7.185388397035145
Training loss: 0.45149141550064087 / Valid loss: 7.0851699692862375
Training loss: 0.5516669750213623 / Valid loss: 7.059004184177944
Training loss: 0.3864182233810425 / Valid loss: 7.118141478583926
Training loss: 0.42312487959861755 / Valid loss: 7.176608578364054

Epoch: 22
Training loss: 0.5007209777832031 / Valid loss: 7.089622302282424
Training loss: 0.35189902782440186 / Valid loss: 7.134150777544294
Training loss: 0.631227970123291 / Valid loss: 7.052121221451532
Training loss: 0.3941984474658966 / Valid loss: 7.155439245133173
Training loss: 0.2865602374076843 / Valid loss: 7.044826562064035

Epoch: 23
Training loss: 0.35063475370407104 / Valid loss: 7.034013757251558
Training loss: 0.32967084646224976 / Valid loss: 7.085966528029669
Training loss: 0.4150483012199402 / Valid loss: 7.0368693215506415
Training loss: 0.8584598302841187 / Valid loss: 7.08991942632766
Training loss: 0.4681990444660187 / Valid loss: 7.1220512889680405

Epoch: 24
Training loss: 0.31464827060699463 / Valid loss: 7.050613385155088
Training loss: 0.52687668800354 / Valid loss: 7.113310591379801
Training loss: 0.7400136590003967 / Valid loss: 7.1069904327392575
Training loss: 0.5133841037750244 / Valid loss: 7.151656075886318
Training loss: 0.27792394161224365 / Valid loss: 7.0979236739022395

Epoch: 25
Training loss: 0.573836088180542 / Valid loss: 7.058542015438988
Training loss: 0.3565758168697357 / Valid loss: 7.0761853763035365
Training loss: 0.2644515931606293 / Valid loss: 7.098684601556688
Training loss: 0.39894768595695496 / Valid loss: 7.115412861960275
Training loss: 0.33896684646606445 / Valid loss: 7.125125471750895

Epoch: 26
Training loss: 0.46628499031066895 / Valid loss: 7.090333570752825
Training loss: 0.4030907154083252 / Valid loss: 6.986102149600074
Training loss: 0.35319891571998596 / Valid loss: 7.080308208011446
Training loss: 0.43311119079589844 / Valid loss: 7.10951702935355
Training loss: 0.6579936742782593 / Valid loss: 7.07668521517799

Epoch: 27
Training loss: 0.7869093418121338 / Valid loss: 7.02738717397054
Training loss: 0.8027764558792114 / Valid loss: 7.016379247392927
Training loss: 0.36206263303756714 / Valid loss: 7.038132435934884
Training loss: 0.26615336537361145 / Valid loss: 7.068806082861764
Training loss: 0.2625137269496918 / Valid loss: 7.100684002467564

Epoch: 28
Training loss: 0.26881104707717896 / Valid loss: 7.05189376331511
Training loss: 0.4397967457771301 / Valid loss: 7.08271027292524
Training loss: 0.5688682794570923 / Valid loss: 7.033768390473865
Training loss: 0.3549933433532715 / Valid loss: 7.058065913972401
Training loss: 0.23291629552841187 / Valid loss: 7.0513456707909

Epoch: 29
Training loss: 0.7972508668899536 / Valid loss: 7.049371060870943
Training loss: 0.3227710425853729 / Valid loss: 7.06372868674142
Training loss: 0.5106690526008606 / Valid loss: 7.005655724661691
Training loss: 0.39121749997138977 / Valid loss: 7.050131625220889

Epoch: 30
Training loss: 0.39276427030563354 / Valid loss: 7.113816733587356
Training loss: 0.4824640154838562 / Valid loss: 7.01658688499814
Training loss: 0.5207738876342773 / Valid loss: 7.0385692551022485
Training loss: 0.6291157007217407 / Valid loss: 7.065138101577759
Training loss: 0.24660325050354004 / Valid loss: 7.008991155170259

Epoch: 31
Training loss: 0.31082823872566223 / Valid loss: 7.034767019181024
Training loss: 0.4501149654388428 / Valid loss: 7.009148561386835
Training loss: 0.29580533504486084 / Valid loss: 7.076856059119815
Training loss: 0.4170054793357849 / Valid loss: 6.974825055258615
Training loss: 0.6154143810272217 / Valid loss: 7.035065355755034

Epoch: 32
Training loss: 0.1664276421070099 / Valid loss: 6.9936797278268
Training loss: 0.22281086444854736 / Valid loss: 6.996086206890288
Training loss: 0.28229883313179016 / Valid loss: 6.975087983267648
Training loss: 0.1958424299955368 / Valid loss: 7.127225821358817
Training loss: 0.39310958981513977 / Valid loss: 7.080291507357643

Epoch: 33
Training loss: 0.5396478176116943 / Valid loss: 7.022969114212763
Training loss: 0.37288883328437805 / Valid loss: 7.061489961260841
Training loss: 0.2302255928516388 / Valid loss: 7.103980750129336
Training loss: 0.20414718985557556 / Valid loss: 7.009327052888416
Training loss: 0.3280695676803589 / Valid loss: 7.046269471304758

Epoch: 34
Training loss: 0.2761704921722412 / Valid loss: 7.011970056806292
Training loss: 0.2996987998485565 / Valid loss: 7.045422192982265
Training loss: 0.5281656980514526 / Valid loss: 7.0238183498382565
Training loss: 0.3491397500038147 / Valid loss: 6.978222542717344
Training loss: 0.18727561831474304 / Valid loss: 7.029807422274635

Epoch: 35
Training loss: 0.4025786519050598 / Valid loss: 7.039890879676456
Training loss: 0.4527624249458313 / Valid loss: 7.069488091695876
Training loss: 0.2132987082004547 / Valid loss: 6.98295849164327
Training loss: 0.28518831729888916 / Valid loss: 7.024515506199428
Training loss: 0.40550124645233154 / Valid loss: 7.101156402769543

Epoch: 36
Training loss: 0.26344528794288635 / Valid loss: 7.033080932072231
Training loss: 0.24671797454357147 / Valid loss: 6.979381543114072
Training loss: 0.22176986932754517 / Valid loss: 7.008659367334275
Training loss: 0.40528780221939087 / Valid loss: 7.046750218527658
Training loss: 0.25552448630332947 / Valid loss: 7.0254611378624325

Epoch: 37
Training loss: 0.22732585668563843 / Valid loss: 7.015706293923515
Training loss: 0.9936307668685913 / Valid loss: 7.001225453331357
Training loss: 0.2394944429397583 / Valid loss: 7.020884041559128
Training loss: 0.2498418241739273 / Valid loss: 6.973259326389858
Training loss: 0.27477261424064636 / Valid loss: 6.965822301592145

Epoch: 38
Training loss: 0.2496117353439331 / Valid loss: 6.942622407277425
Training loss: 0.21302402019500732 / Valid loss: 6.9569858528318855
Training loss: 0.38957107067108154 / Valid loss: 7.063503206343878
Training loss: 0.2585069537162781 / Valid loss: 7.041388643355597
Training loss: 0.32308682799339294 / Valid loss: 7.018507062821161

Epoch: 39
Training loss: 0.23034611344337463 / Valid loss: 6.925047599701655
Training loss: 0.27615225315093994 / Valid loss: 7.035564849490211
Training loss: 0.4086982309818268 / Valid loss: 6.942387667156401
Training loss: 0.2604493200778961 / Valid loss: 7.057644894009544
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.905091444651286
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.986988067626953 / Valid loss: 16.48066234588623
Model is saved in epoch 0, overall batch: 0
Training loss: 19.682048797607422 / Valid loss: 16.37194080352783
Model is saved in epoch 0, overall batch: 100
Training loss: 11.277936935424805 / Valid loss: 16.270790345328194
Model is saved in epoch 0, overall batch: 200
Training loss: 17.17060089111328 / Valid loss: 16.158159146990094
Model is saved in epoch 0, overall batch: 300
Training loss: 13.421060562133789 / Valid loss: 16.04465164002918
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 13.482011795043945 / Valid loss: 15.918952969142369
Model is saved in epoch 1, overall batch: 500
Training loss: 12.709381103515625 / Valid loss: 15.825508480980282
Model is saved in epoch 1, overall batch: 600
Training loss: 15.81386947631836 / Valid loss: 15.729942212785993
Model is saved in epoch 1, overall batch: 700
Training loss: 16.324106216430664 / Valid loss: 15.625586237226214
Model is saved in epoch 1, overall batch: 800
Training loss: 20.26406478881836 / Valid loss: 15.500804764883858
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 13.794792175292969 / Valid loss: 15.392302703857421
Model is saved in epoch 2, overall batch: 1000
Training loss: 11.086048126220703 / Valid loss: 15.314764358883812
Model is saved in epoch 2, overall batch: 1100
Training loss: 15.955490112304688 / Valid loss: 15.208577719188872
Model is saved in epoch 2, overall batch: 1200
Training loss: 12.489133834838867 / Valid loss: 15.111477406819661
Model is saved in epoch 2, overall batch: 1300
Training loss: 16.036945343017578 / Valid loss: 15.00173830304827
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 12.296623229980469 / Valid loss: 14.889990234375
Model is saved in epoch 3, overall batch: 1500
Training loss: 10.433905601501465 / Valid loss: 14.789945511590867
Model is saved in epoch 3, overall batch: 1600
Training loss: 11.790611267089844 / Valid loss: 14.703085063752674
Model is saved in epoch 3, overall batch: 1700
Training loss: 15.266878128051758 / Valid loss: 14.622550274076916
Model is saved in epoch 3, overall batch: 1800
Training loss: 19.18914222717285 / Valid loss: 14.523673284621466
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 19.98931884765625 / Valid loss: 14.428661201113746
Model is saved in epoch 4, overall batch: 2000
Training loss: 17.70398712158203 / Valid loss: 14.321386673336937
Model is saved in epoch 4, overall batch: 2100
Training loss: 17.032913208007812 / Valid loss: 14.23578216916039
Model is saved in epoch 4, overall batch: 2200
Training loss: 16.628955841064453 / Valid loss: 14.136079533894856
Model is saved in epoch 4, overall batch: 2300
Training loss: 8.939642906188965 / Valid loss: 14.029433023361932
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 8.300857543945312 / Valid loss: 13.927495683942523
Model is saved in epoch 5, overall batch: 2500
Training loss: 17.888978958129883 / Valid loss: 13.863056092035203
Model is saved in epoch 5, overall batch: 2600
Training loss: 11.465845108032227 / Valid loss: 13.779152992793492
Model is saved in epoch 5, overall batch: 2700
Training loss: 15.246354103088379 / Valid loss: 13.688334732963925
Model is saved in epoch 5, overall batch: 2800
Training loss: 9.759997367858887 / Valid loss: 13.58138785589309
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 10.536222457885742 / Valid loss: 13.494659396580287
Model is saved in epoch 6, overall batch: 3000
Training loss: 15.552804946899414 / Valid loss: 13.420817724863689
Model is saved in epoch 6, overall batch: 3100
Training loss: 10.575103759765625 / Valid loss: 13.32546554747082
Model is saved in epoch 6, overall batch: 3200
Training loss: 12.887259483337402 / Valid loss: 13.232887976510185
Model is saved in epoch 6, overall batch: 3300
Training loss: 7.42832088470459 / Valid loss: 13.15969780967349
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 12.315414428710938 / Valid loss: 13.067579119546073
Model is saved in epoch 7, overall batch: 3500
Training loss: 14.849267959594727 / Valid loss: 12.9857957976205
Model is saved in epoch 7, overall batch: 3600
Training loss: 10.279006958007812 / Valid loss: 12.89237067812965
Model is saved in epoch 7, overall batch: 3700
Training loss: 12.9608736038208 / Valid loss: 12.794570750281924
Model is saved in epoch 7, overall batch: 3800
Training loss: 12.382295608520508 / Valid loss: 12.730896059672038
Model is saved in epoch 7, overall batch: 3900

Epoch: 8
Training loss: 12.953064918518066 / Valid loss: 12.630402428763253
Model is saved in epoch 8, overall batch: 4000
Training loss: 14.429489135742188 / Valid loss: 12.546014281681606
Model is saved in epoch 8, overall batch: 4100
Training loss: 12.974165916442871 / Valid loss: 12.485242471240817
Model is saved in epoch 8, overall batch: 4200
Training loss: 17.060237884521484 / Valid loss: 12.405861618405297
Model is saved in epoch 8, overall batch: 4300
Training loss: 13.49267864227295 / Valid loss: 12.329460566384451
Model is saved in epoch 8, overall batch: 4400

Epoch: 9
Training loss: 12.661523818969727 / Valid loss: 12.237885257175991
Model is saved in epoch 9, overall batch: 4500
Training loss: 12.848800659179688 / Valid loss: 12.165480186825707
Model is saved in epoch 9, overall batch: 4600
Training loss: 14.488607406616211 / Valid loss: 12.07058756692069
Model is saved in epoch 9, overall batch: 4700
Training loss: 12.78664779663086 / Valid loss: 12.006422742207844
Model is saved in epoch 9, overall batch: 4800

Epoch: 10
Training loss: 11.318477630615234 / Valid loss: 11.934741347176688
Model is saved in epoch 10, overall batch: 4900
Training loss: 14.250657081604004 / Valid loss: 11.849716894967216
Model is saved in epoch 10, overall batch: 5000
Training loss: 15.35003662109375 / Valid loss: 11.760051768166678
Model is saved in epoch 10, overall batch: 5100
Training loss: 10.402201652526855 / Valid loss: 11.700620596749442
Model is saved in epoch 10, overall batch: 5200
Training loss: 9.59667682647705 / Valid loss: 11.61595755985805
Model is saved in epoch 10, overall batch: 5300

Epoch: 11
Training loss: 13.91616153717041 / Valid loss: 11.545271264939082
Model is saved in epoch 11, overall batch: 5400
Training loss: 12.028694152832031 / Valid loss: 11.46419475646246
Model is saved in epoch 11, overall batch: 5500
Training loss: 15.863500595092773 / Valid loss: 11.411583691551572
Model is saved in epoch 11, overall batch: 5600
Training loss: 9.365795135498047 / Valid loss: 11.331166875930059
Model is saved in epoch 11, overall batch: 5700
Training loss: 8.788071632385254 / Valid loss: 11.235160514286585
Model is saved in epoch 11, overall batch: 5800

Epoch: 12
Training loss: 12.931285858154297 / Valid loss: 11.192734627496629
Model is saved in epoch 12, overall batch: 5900
Training loss: 9.14463996887207 / Valid loss: 11.12009361357916
Model is saved in epoch 12, overall batch: 6000
Training loss: 7.943866729736328 / Valid loss: 11.053012747991653
Model is saved in epoch 12, overall batch: 6100
Training loss: 12.71893310546875 / Valid loss: 10.97653767267863
Model is saved in epoch 12, overall batch: 6200
Training loss: 9.803539276123047 / Valid loss: 10.89538856688
Model is saved in epoch 12, overall batch: 6300

Epoch: 13
Training loss: 8.23149299621582 / Valid loss: 10.841041483197893
Model is saved in epoch 13, overall batch: 6400
Training loss: 5.316556930541992 / Valid loss: 10.773876285552978
Model is saved in epoch 13, overall batch: 6500
Training loss: 16.398771286010742 / Valid loss: 10.705930637177968
Model is saved in epoch 13, overall batch: 6600
Training loss: 10.863359451293945 / Valid loss: 10.639790235246931
Model is saved in epoch 13, overall batch: 6700
Training loss: 11.081249237060547 / Valid loss: 10.573303590502057
Model is saved in epoch 13, overall batch: 6800

Epoch: 14
Training loss: 12.457226753234863 / Valid loss: 10.506310031527565
Model is saved in epoch 14, overall batch: 6900
Training loss: 8.325041770935059 / Valid loss: 10.430002916426886
Model is saved in epoch 14, overall batch: 7000
Training loss: 11.232620239257812 / Valid loss: 10.378535370599655
Model is saved in epoch 14, overall batch: 7100
Training loss: 12.521027565002441 / Valid loss: 10.31368628456479
Model is saved in epoch 14, overall batch: 7200
Training loss: 9.738624572753906 / Valid loss: 10.248642335619245
Model is saved in epoch 14, overall batch: 7300

Epoch: 15
Training loss: 9.12507152557373 / Valid loss: 10.172648025694347
Model is saved in epoch 15, overall batch: 7400
Training loss: 11.46051025390625 / Valid loss: 10.112477733975364
Model is saved in epoch 15, overall batch: 7500
Training loss: 7.849015712738037 / Valid loss: 10.056317565554664
Model is saved in epoch 15, overall batch: 7600
Training loss: 8.746415138244629 / Valid loss: 10.000544697897775
Model is saved in epoch 15, overall batch: 7700
Training loss: 12.457427978515625 / Valid loss: 9.938714967455184
Model is saved in epoch 15, overall batch: 7800

Epoch: 16
Training loss: 8.21353530883789 / Valid loss: 9.863726779392787
Model is saved in epoch 16, overall batch: 7900
Training loss: 13.074492454528809 / Valid loss: 9.802010926746187
Model is saved in epoch 16, overall batch: 8000
Training loss: 9.916379928588867 / Valid loss: 9.757700198037284
Model is saved in epoch 16, overall batch: 8100
Training loss: 10.219854354858398 / Valid loss: 9.684298056647892
Model is saved in epoch 16, overall batch: 8200
Training loss: 11.502340316772461 / Valid loss: 9.640258166903541
Model is saved in epoch 16, overall batch: 8300

Epoch: 17
Training loss: 14.204569816589355 / Valid loss: 9.581542455582392
Model is saved in epoch 17, overall batch: 8400
Training loss: 10.009955406188965 / Valid loss: 9.51403964360555
Model is saved in epoch 17, overall batch: 8500
Training loss: 7.5635786056518555 / Valid loss: 9.470443525768461
Model is saved in epoch 17, overall batch: 8600
Training loss: 7.332355499267578 / Valid loss: 9.41474609375
Model is saved in epoch 17, overall batch: 8700
Training loss: 10.604107856750488 / Valid loss: 9.344839105151948
Model is saved in epoch 17, overall batch: 8800

Epoch: 18
Training loss: 7.74551248550415 / Valid loss: 9.304518254597982
Model is saved in epoch 18, overall batch: 8900
Training loss: 8.721367835998535 / Valid loss: 9.24378812880743
Model is saved in epoch 18, overall batch: 9000
Training loss: 8.48185920715332 / Valid loss: 9.194694832393102
Model is saved in epoch 18, overall batch: 9100
Training loss: 9.893543243408203 / Valid loss: 9.139756279899961
Model is saved in epoch 18, overall batch: 9200
Training loss: 10.01772689819336 / Valid loss: 9.08765535808745
Model is saved in epoch 18, overall batch: 9300

Epoch: 19
Training loss: 7.984832286834717 / Valid loss: 9.021559020451138
Model is saved in epoch 19, overall batch: 9400
Training loss: 13.705082893371582 / Valid loss: 8.97275292078654
Model is saved in epoch 19, overall batch: 9500
Training loss: 7.8443145751953125 / Valid loss: 8.927644161950974
Model is saved in epoch 19, overall batch: 9600
Training loss: 9.692012786865234 / Valid loss: 8.867765644618443
Model is saved in epoch 19, overall batch: 9700

Epoch: 20
Training loss: 8.243289947509766 / Valid loss: 8.817563833509173
Model is saved in epoch 20, overall batch: 9800
Training loss: 7.973526954650879 / Valid loss: 8.776530567804972
Model is saved in epoch 20, overall batch: 9900
Training loss: 7.908421993255615 / Valid loss: 8.727069859277634
Model is saved in epoch 20, overall batch: 10000
Training loss: 8.625310897827148 / Valid loss: 8.665111042204357
Model is saved in epoch 20, overall batch: 10100
Training loss: 7.779120922088623 / Valid loss: 8.62550997052874
Model is saved in epoch 20, overall batch: 10200

Epoch: 21
Training loss: 10.446858406066895 / Valid loss: 8.578724761236282
Model is saved in epoch 21, overall batch: 10300
Training loss: 7.084285259246826 / Valid loss: 8.534106838135493
Model is saved in epoch 21, overall batch: 10400
Training loss: 8.033493041992188 / Valid loss: 8.487863436199369
Model is saved in epoch 21, overall batch: 10500
Training loss: 10.207932472229004 / Valid loss: 8.44068979535784
Model is saved in epoch 21, overall batch: 10600
Training loss: 6.014873504638672 / Valid loss: 8.397699660346621
Model is saved in epoch 21, overall batch: 10700

Epoch: 22
Training loss: 9.762716293334961 / Valid loss: 8.34499880472819
Model is saved in epoch 22, overall batch: 10800
Training loss: 10.322514533996582 / Valid loss: 8.306488509405227
Model is saved in epoch 22, overall batch: 10900
Training loss: 7.075475692749023 / Valid loss: 8.258304155440557
Model is saved in epoch 22, overall batch: 11000
Training loss: 8.624849319458008 / Valid loss: 8.220180802118211
Model is saved in epoch 22, overall batch: 11100
Training loss: 7.0339179039001465 / Valid loss: 8.166706067039852
Model is saved in epoch 22, overall batch: 11200

Epoch: 23
Training loss: 6.749889373779297 / Valid loss: 8.132677409762428
Model is saved in epoch 23, overall batch: 11300
Training loss: 9.434531211853027 / Valid loss: 8.089775189899264
Model is saved in epoch 23, overall batch: 11400
Training loss: 9.242650985717773 / Valid loss: 8.047398857843309
Model is saved in epoch 23, overall batch: 11500
Training loss: 9.20905876159668 / Valid loss: 8.007768108731224
Model is saved in epoch 23, overall batch: 11600
Training loss: 7.030669212341309 / Valid loss: 7.962444042024158
Model is saved in epoch 23, overall batch: 11700

Epoch: 24
Training loss: 6.561295986175537 / Valid loss: 7.921996470860073
Model is saved in epoch 24, overall batch: 11800
Training loss: 9.040871620178223 / Valid loss: 7.872059672219413
Model is saved in epoch 24, overall batch: 11900
Training loss: 5.018951416015625 / Valid loss: 7.844508071172805
Model is saved in epoch 24, overall batch: 12000
Training loss: 7.180352210998535 / Valid loss: 7.799436373937698
Model is saved in epoch 24, overall batch: 12100
Training loss: 7.522808074951172 / Valid loss: 7.76118221282959
Model is saved in epoch 24, overall batch: 12200

Epoch: 25
Training loss: 7.213128089904785 / Valid loss: 7.716519859858922
Model is saved in epoch 25, overall batch: 12300
Training loss: 5.065730094909668 / Valid loss: 7.692800982793172
Model is saved in epoch 25, overall batch: 12400
Training loss: 9.215853691101074 / Valid loss: 7.6548626945132305
Model is saved in epoch 25, overall batch: 12500
Training loss: 7.34841251373291 / Valid loss: 7.619838383084252
Model is saved in epoch 25, overall batch: 12600
Training loss: 7.990118980407715 / Valid loss: 7.573426494144258
Model is saved in epoch 25, overall batch: 12700

Epoch: 26
Training loss: 4.871264457702637 / Valid loss: 7.5420619510468985
Model is saved in epoch 26, overall batch: 12800
Training loss: 5.153250217437744 / Valid loss: 7.50938822882516
Model is saved in epoch 26, overall batch: 12900
Training loss: 6.817732810974121 / Valid loss: 7.477307678404308
Model is saved in epoch 26, overall batch: 13000
Training loss: 6.640079021453857 / Valid loss: 7.4408416748046875
Model is saved in epoch 26, overall batch: 13100
Training loss: 5.4359917640686035 / Valid loss: 7.403074010213216
Model is saved in epoch 26, overall batch: 13200

Epoch: 27
Training loss: 6.719500541687012 / Valid loss: 7.373971684773763
Model is saved in epoch 27, overall batch: 13300
Training loss: 7.678191184997559 / Valid loss: 7.339787469591413
Model is saved in epoch 27, overall batch: 13400
Training loss: 5.690126419067383 / Valid loss: 7.308266948518299
Model is saved in epoch 27, overall batch: 13500
Training loss: 9.939260482788086 / Valid loss: 7.275042838142031
Model is saved in epoch 27, overall batch: 13600
Training loss: 7.206747055053711 / Valid loss: 7.235150080635434
Model is saved in epoch 27, overall batch: 13700

Epoch: 28
Training loss: 6.5599846839904785 / Valid loss: 7.209531379881359
Model is saved in epoch 28, overall batch: 13800
Training loss: 5.544612407684326 / Valid loss: 7.170056638263521
Model is saved in epoch 28, overall batch: 13900
Training loss: 5.580269813537598 / Valid loss: 7.149353769847325
Model is saved in epoch 28, overall batch: 14000
Training loss: 7.545458793640137 / Valid loss: 7.1128284431639175
Model is saved in epoch 28, overall batch: 14100
Training loss: 6.597137451171875 / Valid loss: 7.091689468565441
Model is saved in epoch 28, overall batch: 14200

Epoch: 29
Training loss: 7.114513397216797 / Valid loss: 7.056382515316918
Model is saved in epoch 29, overall batch: 14300
Training loss: 7.104617118835449 / Valid loss: 7.023388862609863
Model is saved in epoch 29, overall batch: 14400
Training loss: 7.799118995666504 / Valid loss: 7.000736050378709
Model is saved in epoch 29, overall batch: 14500
Training loss: 7.6495361328125 / Valid loss: 6.972735500335693
Model is saved in epoch 29, overall batch: 14600

Epoch: 30
Training loss: 7.701648235321045 / Valid loss: 6.94017999058678
Model is saved in epoch 30, overall batch: 14700
Training loss: 6.902754306793213 / Valid loss: 6.920930757976714
Model is saved in epoch 30, overall batch: 14800
Training loss: 6.556793212890625 / Valid loss: 6.896700607027326
Model is saved in epoch 30, overall batch: 14900
Training loss: 5.587644577026367 / Valid loss: 6.868607836677914
Model is saved in epoch 30, overall batch: 15000
Training loss: 7.208433151245117 / Valid loss: 6.844891702561151
Model is saved in epoch 30, overall batch: 15100

Epoch: 31
Training loss: 7.565220832824707 / Valid loss: 6.818242786044166
Model is saved in epoch 31, overall batch: 15200
Training loss: 7.458193302154541 / Valid loss: 6.7858726592290965
Model is saved in epoch 31, overall batch: 15300
Training loss: 6.709178924560547 / Valid loss: 6.764234797159831
Model is saved in epoch 31, overall batch: 15400
Training loss: 8.398345947265625 / Valid loss: 6.744727473031907
Model is saved in epoch 31, overall batch: 15500
Training loss: 8.66853141784668 / Valid loss: 6.712681720370338
Model is saved in epoch 31, overall batch: 15600

Epoch: 32
Training loss: 5.211962699890137 / Valid loss: 6.693080847603934
Model is saved in epoch 32, overall batch: 15700
Training loss: 7.3792009353637695 / Valid loss: 6.673259167444138
Model is saved in epoch 32, overall batch: 15800
Training loss: 6.8103556632995605 / Valid loss: 6.64864908400036
Model is saved in epoch 32, overall batch: 15900
Training loss: 6.148314476013184 / Valid loss: 6.625681829452515
Model is saved in epoch 32, overall batch: 16000
Training loss: 3.468625545501709 / Valid loss: 6.606570098513648
Model is saved in epoch 32, overall batch: 16100

Epoch: 33
Training loss: 8.989485740661621 / Valid loss: 6.583173817679995
Model is saved in epoch 33, overall batch: 16200
Training loss: 3.968885898590088 / Valid loss: 6.560458421707153
Model is saved in epoch 33, overall batch: 16300
Training loss: 7.75716495513916 / Valid loss: 6.543986172903152
Model is saved in epoch 33, overall batch: 16400
Training loss: 5.627066135406494 / Valid loss: 6.514040481476557
Model is saved in epoch 33, overall batch: 16500
Training loss: 8.754674911499023 / Valid loss: 6.503159048443749
Model is saved in epoch 33, overall batch: 16600

Epoch: 34
Training loss: 6.3698649406433105 / Valid loss: 6.481677650270008
Model is saved in epoch 34, overall batch: 16700
Training loss: 5.686793327331543 / Valid loss: 6.461179492587135
Model is saved in epoch 34, overall batch: 16800
Training loss: 5.849918365478516 / Valid loss: 6.436648498262678
Model is saved in epoch 34, overall batch: 16900
Training loss: 5.365469932556152 / Valid loss: 6.4259866510118755
Model is saved in epoch 34, overall batch: 17000
Training loss: 6.180820465087891 / Valid loss: 6.3955019723801385
Model is saved in epoch 34, overall batch: 17100

Epoch: 35
Training loss: 6.420785903930664 / Valid loss: 6.383819089617048
Model is saved in epoch 35, overall batch: 17200
Training loss: 6.0384321212768555 / Valid loss: 6.371484820048014
Model is saved in epoch 35, overall batch: 17300
Training loss: 5.9723286628723145 / Valid loss: 6.355618608565558
Model is saved in epoch 35, overall batch: 17400
Training loss: 8.753804206848145 / Valid loss: 6.32885768981207
Model is saved in epoch 35, overall batch: 17500
Training loss: 9.091300964355469 / Valid loss: 6.304590620313372
Model is saved in epoch 35, overall batch: 17600

Epoch: 36
Training loss: 5.635075569152832 / Valid loss: 6.302248750414167
Model is saved in epoch 36, overall batch: 17700
Training loss: 8.582216262817383 / Valid loss: 6.271927947089786
Model is saved in epoch 36, overall batch: 17800
Training loss: 4.723543643951416 / Valid loss: 6.265770008450462
Model is saved in epoch 36, overall batch: 17900
Training loss: 5.019966125488281 / Valid loss: 6.256271764210292
Model is saved in epoch 36, overall batch: 18000
Training loss: 5.156325817108154 / Valid loss: 6.241504837217785
Model is saved in epoch 36, overall batch: 18100

Epoch: 37
Training loss: 4.257423400878906 / Valid loss: 6.223622683116368
Model is saved in epoch 37, overall batch: 18200
Training loss: 6.48952579498291 / Valid loss: 6.213091046469552
Model is saved in epoch 37, overall batch: 18300
Training loss: 5.173551559448242 / Valid loss: 6.192239604677472
Model is saved in epoch 37, overall batch: 18400
Training loss: 5.929388999938965 / Valid loss: 6.173491711843582
Model is saved in epoch 37, overall batch: 18500
Training loss: 5.362987518310547 / Valid loss: 6.163554432278588
Model is saved in epoch 37, overall batch: 18600

Epoch: 38
Training loss: 4.333440780639648 / Valid loss: 6.150973619733538
Model is saved in epoch 38, overall batch: 18700
Training loss: 6.75394344329834 / Valid loss: 6.143286798113868
Model is saved in epoch 38, overall batch: 18800
Training loss: 6.75734281539917 / Valid loss: 6.126579550334386
Model is saved in epoch 38, overall batch: 18900
Training loss: 5.579655170440674 / Valid loss: 6.118428305217198
Model is saved in epoch 38, overall batch: 19000
Training loss: 7.495403289794922 / Valid loss: 6.104158097221738
Model is saved in epoch 38, overall batch: 19100

Epoch: 39
Training loss: 4.2290778160095215 / Valid loss: 6.093478754588536
Model is saved in epoch 39, overall batch: 19200
Training loss: 6.333347797393799 / Valid loss: 6.083985576175508
Model is saved in epoch 39, overall batch: 19300
Training loss: 7.369989395141602 / Valid loss: 6.0706811496189665
Model is saved in epoch 39, overall batch: 19400
Training loss: 6.377228736877441 / Valid loss: 6.06127055258978
Model is saved in epoch 39, overall batch: 19500
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 19500): 5.881724416641962
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.986988067626953 / Valid loss: 16.480664116995676
Model is saved in epoch 0, overall batch: 0
Training loss: 19.677352905273438 / Valid loss: 16.37182796114967
Model is saved in epoch 0, overall batch: 100
Training loss: 11.277108192443848 / Valid loss: 16.270686304001583
Model is saved in epoch 0, overall batch: 200
Training loss: 17.17035484313965 / Valid loss: 16.15804943811326
Model is saved in epoch 0, overall batch: 300
Training loss: 13.419815063476562 / Valid loss: 16.04455093202137
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 13.483314514160156 / Valid loss: 15.918846729823521
Model is saved in epoch 1, overall batch: 500
Training loss: 12.707671165466309 / Valid loss: 15.825412259783064
Model is saved in epoch 1, overall batch: 600
Training loss: 15.812392234802246 / Valid loss: 15.729827608381
Model is saved in epoch 1, overall batch: 700
Training loss: 16.32420539855957 / Valid loss: 15.625482441130139
Model is saved in epoch 1, overall batch: 800
Training loss: 20.266319274902344 / Valid loss: 15.500701486496698
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 13.793365478515625 / Valid loss: 15.3921903973534
Model is saved in epoch 2, overall batch: 1000
Training loss: 11.086446762084961 / Valid loss: 15.314666784377325
Model is saved in epoch 2, overall batch: 1100
Training loss: 15.95673942565918 / Valid loss: 15.208471116565523
Model is saved in epoch 2, overall batch: 1200
Training loss: 12.488374710083008 / Valid loss: 15.111382012140183
Model is saved in epoch 2, overall batch: 1300
Training loss: 16.035301208496094 / Valid loss: 15.001646568661645
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 12.296142578125 / Valid loss: 14.889883649916875
Model is saved in epoch 3, overall batch: 1500
Training loss: 10.437994003295898 / Valid loss: 14.789862996055966
Model is saved in epoch 3, overall batch: 1600
Training loss: 11.790931701660156 / Valid loss: 14.702999469212124
Model is saved in epoch 3, overall batch: 1700
Training loss: 15.268013954162598 / Valid loss: 14.62246283576602
Model is saved in epoch 3, overall batch: 1800
Training loss: 19.18791389465332 / Valid loss: 14.523591886247907
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 19.991132736206055 / Valid loss: 14.428579466683525
Model is saved in epoch 4, overall batch: 2000
Training loss: 17.702125549316406 / Valid loss: 14.321311523800805
Model is saved in epoch 4, overall batch: 2100
Training loss: 17.029682159423828 / Valid loss: 14.235703631809779
Model is saved in epoch 4, overall batch: 2200
Training loss: 16.6248836517334 / Valid loss: 14.136004720415388
Model is saved in epoch 4, overall batch: 2300
Training loss: 8.936918258666992 / Valid loss: 14.029360544113885
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 8.302671432495117 / Valid loss: 13.927443150111607
Model is saved in epoch 5, overall batch: 2500
Training loss: 17.89111328125 / Valid loss: 13.862990261259533
Model is saved in epoch 5, overall batch: 2600
Training loss: 11.464764595031738 / Valid loss: 13.779095472608294
Model is saved in epoch 5, overall batch: 2700
Training loss: 15.246381759643555 / Valid loss: 13.688282040187291
Model is saved in epoch 5, overall batch: 2800
Training loss: 9.759899139404297 / Valid loss: 13.581337156749907
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 10.537437438964844 / Valid loss: 13.49460362933931
Model is saved in epoch 6, overall batch: 3000
Training loss: 15.554609298706055 / Valid loss: 13.42077922821045
Model is saved in epoch 6, overall batch: 3100
Training loss: 10.575742721557617 / Valid loss: 13.325420161655972
Model is saved in epoch 6, overall batch: 3200
Training loss: 12.88115406036377 / Valid loss: 13.232850138346354
Model is saved in epoch 6, overall batch: 3300
Training loss: 7.4280242919921875 / Valid loss: 13.159662546430315
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 12.313913345336914 / Valid loss: 13.067552303132556
Model is saved in epoch 7, overall batch: 3500
Training loss: 14.85020637512207 / Valid loss: 12.985771651495071
Model is saved in epoch 7, overall batch: 3600
Training loss: 10.279264450073242 / Valid loss: 12.892348239535377
Model is saved in epoch 7, overall batch: 3700
Training loss: 12.96088981628418 / Valid loss: 12.794571649460565
Model is saved in epoch 7, overall batch: 3800
Training loss: 12.382774353027344 / Valid loss: 12.730890523819696
Model is saved in epoch 7, overall batch: 3900

Epoch: 8
Training loss: 12.952590942382812 / Valid loss: 12.630397778465634
Model is saved in epoch 8, overall batch: 4000
Training loss: 14.42795181274414 / Valid loss: 12.546009522392636
Model is saved in epoch 8, overall batch: 4100
Training loss: 12.97240161895752 / Valid loss: 12.485247194199335
Model is saved in epoch 8, overall batch: 4200
Training loss: 17.05814552307129 / Valid loss: 12.405877894446963
Model is saved in epoch 8, overall batch: 4300
Training loss: 13.496763229370117 / Valid loss: 12.3294798714774
Model is saved in epoch 8, overall batch: 4400

Epoch: 9
Training loss: 12.66133975982666 / Valid loss: 12.237902096339635
Model is saved in epoch 9, overall batch: 4500
Training loss: 12.84781551361084 / Valid loss: 12.16550471896217
Model is saved in epoch 9, overall batch: 4600
Training loss: 14.489997863769531 / Valid loss: 12.070613057272775
Model is saved in epoch 9, overall batch: 4700
Training loss: 12.785116195678711 / Valid loss: 12.00647105262393
Model is saved in epoch 9, overall batch: 4800

Epoch: 10
Training loss: 11.319220542907715 / Valid loss: 11.934787586757116
Model is saved in epoch 10, overall batch: 4900
Training loss: 14.249738693237305 / Valid loss: 11.849767880212694
Model is saved in epoch 10, overall batch: 5000
Training loss: 15.348673820495605 / Valid loss: 11.76011265345982
Model is saved in epoch 10, overall batch: 5100
Training loss: 10.402387619018555 / Valid loss: 11.700684969765799
Model is saved in epoch 10, overall batch: 5200
Training loss: 9.59557056427002 / Valid loss: 11.61602638335455
Model is saved in epoch 10, overall batch: 5300

Epoch: 11
Training loss: 13.915672302246094 / Valid loss: 11.545356400807698
Model is saved in epoch 11, overall batch: 5400
Training loss: 12.025884628295898 / Valid loss: 11.464276699792771
Model is saved in epoch 11, overall batch: 5500
Training loss: 15.86079216003418 / Valid loss: 11.411666870117188
Model is saved in epoch 11, overall batch: 5600
Training loss: 9.366896629333496 / Valid loss: 11.331265594845727
Model is saved in epoch 11, overall batch: 5700
Training loss: 8.78748893737793 / Valid loss: 11.23527009146554
Model is saved in epoch 11, overall batch: 5800

Epoch: 12
Training loss: 12.932413101196289 / Valid loss: 11.192842120216007
Model is saved in epoch 12, overall batch: 5900
Training loss: 9.145257949829102 / Valid loss: 11.120214671180362
Model is saved in epoch 12, overall batch: 6000
Training loss: 7.9460129737854 / Valid loss: 11.053129232497442
Model is saved in epoch 12, overall batch: 6100
Training loss: 12.717744827270508 / Valid loss: 10.976662186213902
Model is saved in epoch 12, overall batch: 6200
Training loss: 9.804675102233887 / Valid loss: 10.89551248550415
Model is saved in epoch 12, overall batch: 6300

Epoch: 13
Training loss: 8.229656219482422 / Valid loss: 10.841177531651088
Model is saved in epoch 13, overall batch: 6400
Training loss: 5.3165283203125 / Valid loss: 10.774024845304943
Model is saved in epoch 13, overall batch: 6500
Training loss: 16.396827697753906 / Valid loss: 10.706083688281831
Model is saved in epoch 13, overall batch: 6600
Training loss: 10.865030288696289 / Valid loss: 10.639952723185221
Model is saved in epoch 13, overall batch: 6700
Training loss: 11.080229759216309 / Valid loss: 10.57346868060884
Model is saved in epoch 13, overall batch: 6800

Epoch: 14
Training loss: 12.459484100341797 / Valid loss: 10.506481329600016
Model is saved in epoch 14, overall batch: 6900
Training loss: 8.323531150817871 / Valid loss: 10.430177288963682
Model is saved in epoch 14, overall batch: 7000
Training loss: 11.22993278503418 / Valid loss: 10.3787231082008
Model is saved in epoch 14, overall batch: 7100
Training loss: 12.518169403076172 / Valid loss: 10.31387864067441
Model is saved in epoch 14, overall batch: 7200
Training loss: 9.737617492675781 / Valid loss: 10.248844083150228
Model is saved in epoch 14, overall batch: 7300

Epoch: 15
Training loss: 9.124177932739258 / Valid loss: 10.172853874024891
Model is saved in epoch 15, overall batch: 7400
Training loss: 11.461158752441406 / Valid loss: 10.112701624915713
Model is saved in epoch 15, overall batch: 7500
Training loss: 7.848908424377441 / Valid loss: 10.056529749007453
Model is saved in epoch 15, overall batch: 7600
Training loss: 8.748259544372559 / Valid loss: 10.00077589579991
Model is saved in epoch 15, overall batch: 7700
Training loss: 12.455785751342773 / Valid loss: 9.938952718462263
Model is saved in epoch 15, overall batch: 7800

Epoch: 16
Training loss: 8.210550308227539 / Valid loss: 9.86398233232044
Model is saved in epoch 16, overall batch: 7900
Training loss: 13.074759483337402 / Valid loss: 9.802250630514962
Model is saved in epoch 16, overall batch: 8000
Training loss: 9.917193412780762 / Valid loss: 9.757963534763881
Model is saved in epoch 16, overall batch: 8100
Training loss: 10.22091007232666 / Valid loss: 9.68457290558588
Model is saved in epoch 16, overall batch: 8200
Training loss: 11.499995231628418 / Valid loss: 9.640529732477097
Model is saved in epoch 16, overall batch: 8300

Epoch: 17
Training loss: 14.204838752746582 / Valid loss: 9.581819752284458
Model is saved in epoch 17, overall batch: 8400
Training loss: 10.012857437133789 / Valid loss: 9.514329569680351
Model is saved in epoch 17, overall batch: 8500
Training loss: 7.564882278442383 / Valid loss: 9.470739991324288
Model is saved in epoch 17, overall batch: 8600
Training loss: 7.329903602600098 / Valid loss: 9.41505248660133
Model is saved in epoch 17, overall batch: 8700
Training loss: 10.605506896972656 / Valid loss: 9.345150416237967
Model is saved in epoch 17, overall batch: 8800

Epoch: 18
Training loss: 7.746281623840332 / Valid loss: 9.304840687343052
Model is saved in epoch 18, overall batch: 8900
Training loss: 8.720105171203613 / Valid loss: 9.244118504297166
Model is saved in epoch 18, overall batch: 9000
Training loss: 8.482166290283203 / Valid loss: 9.195024549393427
Model is saved in epoch 18, overall batch: 9100
Training loss: 9.892004013061523 / Valid loss: 9.140098094940186
Model is saved in epoch 18, overall batch: 9200
Training loss: 10.018410682678223 / Valid loss: 9.087998812539237
Model is saved in epoch 18, overall batch: 9300

Epoch: 19
Training loss: 7.985472679138184 / Valid loss: 9.02191196169172
Model is saved in epoch 19, overall batch: 9400
Training loss: 13.702970504760742 / Valid loss: 8.97311756043207
Model is saved in epoch 19, overall batch: 9500
Training loss: 7.846212387084961 / Valid loss: 8.92800516855149
Model is saved in epoch 19, overall batch: 9600
Training loss: 9.693824768066406 / Valid loss: 8.868135602133615
Model is saved in epoch 19, overall batch: 9700

Epoch: 20
Training loss: 8.244790077209473 / Valid loss: 8.81793828691755
Model is saved in epoch 20, overall batch: 9800
Training loss: 7.972582817077637 / Valid loss: 8.776911535717192
Model is saved in epoch 20, overall batch: 9900
Training loss: 7.907534122467041 / Valid loss: 8.727455734071278
Model is saved in epoch 20, overall batch: 10000
Training loss: 8.624664306640625 / Valid loss: 8.66550323395502
Model is saved in epoch 20, overall batch: 10100
Training loss: 7.777900695800781 / Valid loss: 8.625898497445244
Model is saved in epoch 20, overall batch: 10200

Epoch: 21
Training loss: 10.446353912353516 / Valid loss: 8.579130390712193
Model is saved in epoch 21, overall batch: 10300
Training loss: 7.085441589355469 / Valid loss: 8.534510442188807
Model is saved in epoch 21, overall batch: 10400
Training loss: 8.034834861755371 / Valid loss: 8.488278284527006
Model is saved in epoch 21, overall batch: 10500
Training loss: 10.210451126098633 / Valid loss: 8.441103708176385
Model is saved in epoch 21, overall batch: 10600
Training loss: 6.014551162719727 / Valid loss: 8.398117723919096
Model is saved in epoch 21, overall batch: 10700

Epoch: 22
Training loss: 9.76284408569336 / Valid loss: 8.345427558535622
Model is saved in epoch 22, overall batch: 10800
Training loss: 10.319765090942383 / Valid loss: 8.306921252750215
Model is saved in epoch 22, overall batch: 10900
Training loss: 7.074702262878418 / Valid loss: 8.25874125616891
Model is saved in epoch 22, overall batch: 11000
Training loss: 8.622663497924805 / Valid loss: 8.220622171674457
Model is saved in epoch 22, overall batch: 11100
Training loss: 7.034607410430908 / Valid loss: 8.167158810297648
Model is saved in epoch 22, overall batch: 11200

Epoch: 23
Training loss: 6.750048637390137 / Valid loss: 8.133130000886464
Model is saved in epoch 23, overall batch: 11300
Training loss: 9.433219909667969 / Valid loss: 8.09023573739188
Model is saved in epoch 23, overall batch: 11400
Training loss: 9.242837905883789 / Valid loss: 8.047853197370257
Model is saved in epoch 23, overall batch: 11500
Training loss: 9.209817886352539 / Valid loss: 8.00822929200672
Model is saved in epoch 23, overall batch: 11600
Training loss: 7.030826568603516 / Valid loss: 7.962917500450498
Model is saved in epoch 23, overall batch: 11700

Epoch: 24
Training loss: 6.559779644012451 / Valid loss: 7.92246310370309
Model is saved in epoch 24, overall batch: 11800
Training loss: 9.041868209838867 / Valid loss: 7.872533067067464
Model is saved in epoch 24, overall batch: 11900
Training loss: 5.018991947174072 / Valid loss: 7.844984263465518
Model is saved in epoch 24, overall batch: 12000
Training loss: 7.179923057556152 / Valid loss: 7.799917003086635
Model is saved in epoch 24, overall batch: 12100
Training loss: 7.521563529968262 / Valid loss: 7.761662206195649
Model is saved in epoch 24, overall batch: 12200

Epoch: 25
Training loss: 7.213030815124512 / Valid loss: 7.717008915401641
Model is saved in epoch 25, overall batch: 12300
Training loss: 5.065887451171875 / Valid loss: 7.69329347156343
Model is saved in epoch 25, overall batch: 12400
Training loss: 9.21848201751709 / Valid loss: 7.655360049293154
Model is saved in epoch 25, overall batch: 12500
Training loss: 7.349845886230469 / Valid loss: 7.620333864575341
Model is saved in epoch 25, overall batch: 12600
Training loss: 7.990910530090332 / Valid loss: 7.5739182903653095
Model is saved in epoch 25, overall batch: 12700

Epoch: 26
Training loss: 4.87119722366333 / Valid loss: 7.542555436633882
Model is saved in epoch 26, overall batch: 12800
Training loss: 5.15377140045166 / Valid loss: 7.509897586277553
Model is saved in epoch 26, overall batch: 12900
Training loss: 6.818230628967285 / Valid loss: 7.477816188903082
Model is saved in epoch 26, overall batch: 13000
Training loss: 6.640239715576172 / Valid loss: 7.441350151243664
Model is saved in epoch 26, overall batch: 13100
Training loss: 5.434355735778809 / Valid loss: 7.403587763650076
Model is saved in epoch 26, overall batch: 13200

Epoch: 27
Training loss: 6.720257759094238 / Valid loss: 7.374485565367199
Model is saved in epoch 27, overall batch: 13300
Training loss: 7.677760124206543 / Valid loss: 7.340300805228097
Model is saved in epoch 27, overall batch: 13400
Training loss: 5.691406726837158 / Valid loss: 7.30878230276562
Model is saved in epoch 27, overall batch: 13500
Training loss: 9.940574645996094 / Valid loss: 7.2755623658498125
Model is saved in epoch 27, overall batch: 13600
Training loss: 7.206713676452637 / Valid loss: 7.235675250916254
Model is saved in epoch 27, overall batch: 13700

Epoch: 28
Training loss: 6.5592875480651855 / Valid loss: 7.210044311341785
Model is saved in epoch 28, overall batch: 13800
Training loss: 5.544672966003418 / Valid loss: 7.1705777531578425
Model is saved in epoch 28, overall batch: 13900
Training loss: 5.579814910888672 / Valid loss: 7.149878896985736
Model is saved in epoch 28, overall batch: 14000
Training loss: 7.545835494995117 / Valid loss: 7.113352294195266
Model is saved in epoch 28, overall batch: 14100
Training loss: 6.5961103439331055 / Valid loss: 7.0922111329578215
Model is saved in epoch 28, overall batch: 14200

Epoch: 29
Training loss: 7.119235992431641 / Valid loss: 7.056902063460577
Model is saved in epoch 29, overall batch: 14300
Training loss: 7.104818344116211 / Valid loss: 7.023910136449905
Model is saved in epoch 29, overall batch: 14400
Training loss: 7.7990899085998535 / Valid loss: 7.0012581961495535
Model is saved in epoch 29, overall batch: 14500
Training loss: 7.650061130523682 / Valid loss: 6.973257478078207
Model is saved in epoch 29, overall batch: 14600

Epoch: 30
Training loss: 7.699419975280762 / Valid loss: 6.9407045409792945
Model is saved in epoch 30, overall batch: 14700
Training loss: 6.903687953948975 / Valid loss: 6.921463189806257
Model is saved in epoch 30, overall batch: 14800
Training loss: 6.558467388153076 / Valid loss: 6.8972252686818445
Model is saved in epoch 30, overall batch: 14900
Training loss: 5.5889997482299805 / Valid loss: 6.869132325762794
Model is saved in epoch 30, overall batch: 15000
Training loss: 7.207929611206055 / Valid loss: 6.845410549072993
Model is saved in epoch 30, overall batch: 15100

Epoch: 31
Training loss: 7.568089485168457 / Valid loss: 6.818761348724365
Model is saved in epoch 31, overall batch: 15200
Training loss: 7.45900297164917 / Valid loss: 6.786389845893496
Model is saved in epoch 31, overall batch: 15300
Training loss: 6.707639217376709 / Valid loss: 6.764751936140515
Model is saved in epoch 31, overall batch: 15400
Training loss: 8.399425506591797 / Valid loss: 6.745245404470534
Model is saved in epoch 31, overall batch: 15500
Training loss: 8.6719331741333 / Valid loss: 6.713202962421236
Model is saved in epoch 31, overall batch: 15600

Epoch: 32
Training loss: 5.2133893966674805 / Valid loss: 6.69359488033113
Model is saved in epoch 32, overall batch: 15700
Training loss: 7.379868507385254 / Valid loss: 6.673777237392607
Model is saved in epoch 32, overall batch: 15800
Training loss: 6.811100959777832 / Valid loss: 6.649161549976894
Model is saved in epoch 32, overall batch: 15900
Training loss: 6.149275302886963 / Valid loss: 6.626192231405349
Model is saved in epoch 32, overall batch: 16000
Training loss: 3.4682486057281494 / Valid loss: 6.607080100831531
Model is saved in epoch 32, overall batch: 16100

Epoch: 33
Training loss: 8.990398406982422 / Valid loss: 6.583680214200701
Model is saved in epoch 33, overall batch: 16200
Training loss: 3.9685611724853516 / Valid loss: 6.56096408707755
Model is saved in epoch 33, overall batch: 16300
Training loss: 7.757145881652832 / Valid loss: 6.544485326040359
Model is saved in epoch 33, overall batch: 16400
Training loss: 5.6274309158325195 / Valid loss: 6.514537513823736
Model is saved in epoch 33, overall batch: 16500
Training loss: 8.755319595336914 / Valid loss: 6.50365666207813
Model is saved in epoch 33, overall batch: 16600

Epoch: 34
Training loss: 6.372257232666016 / Valid loss: 6.482172296160743
Model is saved in epoch 34, overall batch: 16700
Training loss: 5.687057971954346 / Valid loss: 6.461667819250198
Model is saved in epoch 34, overall batch: 16800
Training loss: 5.849520683288574 / Valid loss: 6.437143223626273
Model is saved in epoch 34, overall batch: 16900
Training loss: 5.365017414093018 / Valid loss: 6.4264695394606814
Model is saved in epoch 34, overall batch: 17000
Training loss: 6.1817240715026855 / Valid loss: 6.395980219613938
Model is saved in epoch 34, overall batch: 17100

Epoch: 35
Training loss: 6.421302795410156 / Valid loss: 6.384296517145066
Model is saved in epoch 35, overall batch: 17200
Training loss: 6.0397233963012695 / Valid loss: 6.371952667690459
Model is saved in epoch 35, overall batch: 17300
Training loss: 5.9718122482299805 / Valid loss: 6.356092675526937
Model is saved in epoch 35, overall batch: 17400
Training loss: 8.755292892456055 / Valid loss: 6.329311384473528
Model is saved in epoch 35, overall batch: 17500
Training loss: 9.089969635009766 / Valid loss: 6.30505511647179
Model is saved in epoch 35, overall batch: 17600

Epoch: 36
Training loss: 5.635422229766846 / Valid loss: 6.302705235708327
Model is saved in epoch 36, overall batch: 17700
Training loss: 8.582940101623535 / Valid loss: 6.27238097190857
Model is saved in epoch 36, overall batch: 17800
Training loss: 4.722360610961914 / Valid loss: 6.266219659078689
Model is saved in epoch 36, overall batch: 17900
Training loss: 5.020790100097656 / Valid loss: 6.25671648979187
Model is saved in epoch 36, overall batch: 18000
Training loss: 5.157421112060547 / Valid loss: 6.241949689955939
Model is saved in epoch 36, overall batch: 18100

Epoch: 37
Training loss: 4.25868034362793 / Valid loss: 6.224057079496838
Model is saved in epoch 37, overall batch: 18200
Training loss: 6.493109703063965 / Valid loss: 6.213524005526588
Model is saved in epoch 37, overall batch: 18300
Training loss: 5.1733222007751465 / Valid loss: 6.192666716802687
Model is saved in epoch 37, overall batch: 18400
Training loss: 5.930492401123047 / Valid loss: 6.173915915262132
Model is saved in epoch 37, overall batch: 18500
Training loss: 5.361602783203125 / Valid loss: 6.163972037179129
Model is saved in epoch 37, overall batch: 18600

Epoch: 38
Training loss: 4.3343963623046875 / Valid loss: 6.151373767852784
Model is saved in epoch 38, overall batch: 18700
Training loss: 6.753810882568359 / Valid loss: 6.143693276814052
Model is saved in epoch 38, overall batch: 18800
Training loss: 6.756619453430176 / Valid loss: 6.126982089451381
Model is saved in epoch 38, overall batch: 18900
Training loss: 5.579838752746582 / Valid loss: 6.118819881620861
Model is saved in epoch 38, overall batch: 19000
Training loss: 7.497264862060547 / Valid loss: 6.104549719038464
Model is saved in epoch 38, overall batch: 19100

Epoch: 39
Training loss: 4.229279041290283 / Valid loss: 6.093863151186988
Model is saved in epoch 39, overall batch: 19200
Training loss: 6.332876682281494 / Valid loss: 6.084366594042097
Model is saved in epoch 39, overall batch: 19300
Training loss: 7.369969367980957 / Valid loss: 6.07105644089835
Model is saved in epoch 39, overall batch: 19400
Training loss: 6.377756118774414 / Valid loss: 6.061640510104952
Model is saved in epoch 39, overall batch: 19500
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 19500): 5.882078559058053
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.266402771359399
Model is saved in epoch 0, overall batch: 0
Training loss: 5.042430877685547 / Valid loss: 7.500886340368361
Model is saved in epoch 0, overall batch: 100
Training loss: 4.23737907409668 / Valid loss: 5.921776885078067
Model is saved in epoch 0, overall batch: 200
Training loss: 6.29364538192749 / Valid loss: 5.682949795041766
Model is saved in epoch 0, overall batch: 300
Training loss: 5.842200756072998 / Valid loss: 5.548471437181745
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.756719589233398 / Valid loss: 5.553807235899425
Training loss: 4.298105239868164 / Valid loss: 5.569606599353609
Training loss: 5.326928615570068 / Valid loss: 5.642630109332857
Training loss: 5.017754554748535 / Valid loss: 5.6339343093690415
Training loss: 3.9678406715393066 / Valid loss: 5.5637674763089136

Epoch: 2
Training loss: 4.209566593170166 / Valid loss: 5.612356049673898
Training loss: 4.178608417510986 / Valid loss: 5.750446242377872
Training loss: 3.3077023029327393 / Valid loss: 5.74691725685483
Training loss: 4.88202428817749 / Valid loss: 5.984495680672782
Training loss: 4.264095306396484 / Valid loss: 5.786848156792777

Epoch: 3
Training loss: 2.5928397178649902 / Valid loss: 5.875179701759702
Training loss: 2.498826503753662 / Valid loss: 6.106087437130156
Training loss: 4.6850714683532715 / Valid loss: 5.97977549235026
Training loss: 2.186169385910034 / Valid loss: 6.105248587472098
Training loss: 3.4255576133728027 / Valid loss: 6.111393374488467

Epoch: 4
Training loss: 2.261228322982788 / Valid loss: 6.212632283710298
Training loss: 2.7856407165527344 / Valid loss: 6.367695467812674
Training loss: 2.8565831184387207 / Valid loss: 6.320631045386905
Training loss: 2.643772602081299 / Valid loss: 6.169245856148856
Training loss: 2.3476099967956543 / Valid loss: 6.288671822774978

Epoch: 5
Training loss: 2.6007585525512695 / Valid loss: 6.993596685500372
Training loss: 1.937721848487854 / Valid loss: 6.489501063028971
Training loss: 2.0332107543945312 / Valid loss: 6.682902040935698
Training loss: 2.2377610206604004 / Valid loss: 6.4714922450837635
Training loss: 2.2933297157287598 / Valid loss: 6.610230096181234

Epoch: 6
Training loss: 2.3704824447631836 / Valid loss: 6.5890911692664735
Training loss: 1.2237951755523682 / Valid loss: 6.530026122501918
Training loss: 2.6668028831481934 / Valid loss: 6.618755563100179
Training loss: 1.8128156661987305 / Valid loss: 6.598240836461385
Training loss: 1.754177212715149 / Valid loss: 6.67875330334618

Epoch: 7
Training loss: 1.6314806938171387 / Valid loss: 6.549886939639137
Training loss: 1.7161669731140137 / Valid loss: 6.626883702051072
Training loss: 1.825516939163208 / Valid loss: 6.685345942633493
Training loss: 1.6144566535949707 / Valid loss: 6.730530461810884
Training loss: 1.254022240638733 / Valid loss: 6.626742994217645

Epoch: 8
Training loss: 0.8628376722335815 / Valid loss: 6.59803987003508
Training loss: 0.840111494064331 / Valid loss: 6.550634683881487
Training loss: 1.0600968599319458 / Valid loss: 6.688631010055542
Training loss: 1.804614543914795 / Valid loss: 6.621865404219855
Training loss: 1.3723456859588623 / Valid loss: 6.63860752923148

Epoch: 9
Training loss: 0.6908299922943115 / Valid loss: 6.583039256504604
Training loss: 0.7573602795600891 / Valid loss: 6.690453978947231
Training loss: 1.4202158451080322 / Valid loss: 6.676934541974749
Training loss: 1.119789958000183 / Valid loss: 6.6478013401939755

Epoch: 10
Training loss: 0.9327704906463623 / Valid loss: 6.61181899252392
Training loss: 0.7600219249725342 / Valid loss: 6.525962102980841
Training loss: 0.5637192726135254 / Valid loss: 6.57061973299299
Training loss: 1.3438125848770142 / Valid loss: 6.6073793547494075
Training loss: 0.7735837697982788 / Valid loss: 6.65690032641093

Epoch: 11
Training loss: 0.5911161303520203 / Valid loss: 6.639800630296979
Training loss: 0.9196419715881348 / Valid loss: 6.714557536443075
Training loss: 0.7024827003479004 / Valid loss: 6.790416422344389
Training loss: 0.6993299722671509 / Valid loss: 6.754666791643415
Training loss: 0.7075324058532715 / Valid loss: 6.7032001586187455

Epoch: 12
Training loss: 0.39375412464141846 / Valid loss: 6.579132511502221
Training loss: 0.5301622748374939 / Valid loss: 6.604428423018683
Training loss: 0.6285430192947388 / Valid loss: 6.513908277239119
Training loss: 0.6034932732582092 / Valid loss: 6.645499206724621
Training loss: 0.6196821331977844 / Valid loss: 6.61227723757426

Epoch: 13
Training loss: 0.37680479884147644 / Valid loss: 6.681903859547206
Training loss: 0.526980996131897 / Valid loss: 6.640049271356492
Training loss: 0.4539257884025574 / Valid loss: 6.595271096910749
Training loss: 0.5621163845062256 / Valid loss: 6.582649800890968
Training loss: 0.4460565745830536 / Valid loss: 6.5734669299352735

Epoch: 14
Training loss: 0.5914333462715149 / Valid loss: 6.621801598866781
Training loss: 0.49070507287979126 / Valid loss: 6.560744562603179
Training loss: 0.5259989500045776 / Valid loss: 6.555673088346209
Training loss: 0.25948449969291687 / Valid loss: 6.586678527650379
Training loss: 0.6754760146141052 / Valid loss: 6.649354369299752

Epoch: 15
Training loss: 0.46882298588752747 / Valid loss: 6.6811547869727725
Training loss: 0.6348530054092407 / Valid loss: 6.670148454393659
Training loss: 0.46523556113243103 / Valid loss: 6.6584804807390485
Training loss: 0.45887142419815063 / Valid loss: 6.60891052427746
Training loss: 0.48496049642562866 / Valid loss: 6.634208256857736

Epoch: 16
Training loss: 0.3049743175506592 / Valid loss: 6.565555495307559
Training loss: 0.45435047149658203 / Valid loss: 6.612872582390195
Training loss: 0.36655542254447937 / Valid loss: 6.5486997104826425
Training loss: 0.3891465663909912 / Valid loss: 6.6117689904712496
Training loss: 0.39975547790527344 / Valid loss: 6.563732653572446

Epoch: 17
Training loss: 0.7990560531616211 / Valid loss: 6.569073933646792
Training loss: 0.7692824602127075 / Valid loss: 6.557745831353324
Training loss: 0.6669824123382568 / Valid loss: 6.614656439281645
Training loss: 0.27177250385284424 / Valid loss: 6.601601418994722
Training loss: 0.3779035210609436 / Valid loss: 6.620827372868856

Epoch: 18
Training loss: 0.4896887242794037 / Valid loss: 6.604000136965797
Training loss: 0.42007529735565186 / Valid loss: 6.5563198248545325
Training loss: 0.420172780752182 / Valid loss: 6.566674936385382
Training loss: 0.45496243238449097 / Valid loss: 6.598615662256877
Training loss: 0.6350970268249512 / Valid loss: 6.5631274904523575

Epoch: 19
Training loss: 0.5448774099349976 / Valid loss: 6.654344436100551
Training loss: 0.8887042999267578 / Valid loss: 6.577063755762009
Training loss: 0.4142709970474243 / Valid loss: 6.5773750532241095
Training loss: 0.2850325107574463 / Valid loss: 6.547097835086641

Epoch: 20
Training loss: 0.32555830478668213 / Valid loss: 6.4868345578511555
Training loss: 0.6206218004226685 / Valid loss: 6.558145187014625
Training loss: 0.21500426530838013 / Valid loss: 6.539990411485944
Training loss: 0.3254860043525696 / Valid loss: 6.58965566725958
Training loss: 0.7169045209884644 / Valid loss: 6.543543238866897

Epoch: 21
Training loss: 0.3072414696216583 / Valid loss: 6.5334583850133985
Training loss: 0.4154866337776184 / Valid loss: 6.569000938960484
Training loss: 0.32305556535720825 / Valid loss: 6.564593673887707
Training loss: 0.27794021368026733 / Valid loss: 6.5852838834126795
Training loss: 0.3463420569896698 / Valid loss: 6.5662931692032585

Epoch: 22
Training loss: 0.3098863363265991 / Valid loss: 6.557641692388625
Training loss: 0.253909707069397 / Valid loss: 6.467937930425008
Training loss: 0.38885200023651123 / Valid loss: 6.539727826345534
Training loss: 1.0445916652679443 / Valid loss: 6.594146610441662
Training loss: 0.4033229351043701 / Valid loss: 6.584196276891799

Epoch: 23
Training loss: 0.24921837449073792 / Valid loss: 6.556321732203165
Training loss: 0.7468335032463074 / Valid loss: 6.482907140822638
Training loss: 0.2993065118789673 / Valid loss: 6.507833896364485
Training loss: 0.20183780789375305 / Valid loss: 6.515956374577113
Training loss: 0.436636745929718 / Valid loss: 6.586847171329317

Epoch: 24
Training loss: 0.558465301990509 / Valid loss: 6.580224627540225
Training loss: 0.396455854177475 / Valid loss: 6.5163773445856
Training loss: 0.25740212202072144 / Valid loss: 6.585533464522589
Training loss: 0.5150564908981323 / Valid loss: 6.575876276833671
Training loss: 0.8670803904533386 / Valid loss: 6.589196180161975

Epoch: 25
Training loss: 0.6435341238975525 / Valid loss: 6.579582589013236
Training loss: 0.29623180627822876 / Valid loss: 6.517912360600063
Training loss: 0.41823065280914307 / Valid loss: 6.528584094274612
Training loss: 0.2676159739494324 / Valid loss: 6.514958645048596
Training loss: 0.24382954835891724 / Valid loss: 6.54841746148609

Epoch: 26
Training loss: 0.3725477457046509 / Valid loss: 6.54251918338594
Training loss: 0.504570484161377 / Valid loss: 6.555820183526902
Training loss: 0.20895236730575562 / Valid loss: 6.497135160082863
Training loss: 0.2649601101875305 / Valid loss: 6.582287747519357
Training loss: 0.3682362735271454 / Valid loss: 6.565891969771612

Epoch: 27
Training loss: 0.26271557807922363 / Valid loss: 6.516990770612444
Training loss: 0.39834481477737427 / Valid loss: 6.5540310064951575
Training loss: 0.39193862676620483 / Valid loss: 6.584957041059222
Training loss: 0.36901986598968506 / Valid loss: 6.53929474467323
Training loss: 0.3372167944908142 / Valid loss: 6.527570056915283

Epoch: 28
Training loss: 0.2596457004547119 / Valid loss: 6.523834460122245
Training loss: 0.4018998146057129 / Valid loss: 6.519163869676136
Training loss: 0.5498902201652527 / Valid loss: 6.553589929853167
Training loss: 0.3178008794784546 / Valid loss: 6.489233925229027
Training loss: 0.29510974884033203 / Valid loss: 6.527069623129709

Epoch: 29
Training loss: 0.3009474575519562 / Valid loss: 6.478154214223226
Training loss: 0.2926444709300995 / Valid loss: 6.515493987855457
Training loss: 0.12174738943576813 / Valid loss: 6.493541413261777
Training loss: 0.20460005104541779 / Valid loss: 6.508174405779157

Epoch: 30
Training loss: 0.22724877297878265 / Valid loss: 6.498072901226226
Training loss: 0.2598232626914978 / Valid loss: 6.4871627081008185
Training loss: 0.25000786781311035 / Valid loss: 6.531401722771781
Training loss: 0.315096914768219 / Valid loss: 6.53367938768296
Training loss: 0.30825871229171753 / Valid loss: 6.511435715357463

Epoch: 31
Training loss: 0.2798534631729126 / Valid loss: 6.433579987571353
Training loss: 0.263679563999176 / Valid loss: 6.473984209696452
Training loss: 0.24449357390403748 / Valid loss: 6.47363756497701
Training loss: 0.282825767993927 / Valid loss: 6.511305145990281
Training loss: 0.18286991119384766 / Valid loss: 6.539030633653913

Epoch: 32
Training loss: 0.3272579312324524 / Valid loss: 6.530714384714762
Training loss: 0.31074586510658264 / Valid loss: 6.512446162814186
Training loss: 0.20961004495620728 / Valid loss: 6.572762714113508
Training loss: 0.1993136703968048 / Valid loss: 6.476679697490874
Training loss: 0.32529497146606445 / Valid loss: 6.516221082778204

Epoch: 33
Training loss: 0.2471737265586853 / Valid loss: 6.4616634573255265
Training loss: 0.3477966785430908 / Valid loss: 6.5341354188464935
Training loss: 0.20642465353012085 / Valid loss: 6.461787173861549
Training loss: 0.1547841727733612 / Valid loss: 6.487263852074033
Training loss: 0.28536760807037354 / Valid loss: 6.497025898524693

Epoch: 34
Training loss: 0.5804500579833984 / Valid loss: 6.44485284941537
Training loss: 0.25263845920562744 / Valid loss: 6.464764149983724
Training loss: 0.22393028438091278 / Valid loss: 6.53330568586077
Training loss: 0.21801960468292236 / Valid loss: 6.5301706541152225
Training loss: 0.19783607125282288 / Valid loss: 6.4900689147767565

Epoch: 35
Training loss: 0.7059658765792847 / Valid loss: 6.470491100492931
Training loss: 0.2884620428085327 / Valid loss: 6.443025284721738
Training loss: 0.36435389518737793 / Valid loss: 6.5528745923723495
Training loss: 0.16806453466415405 / Valid loss: 6.506603879020328
Training loss: 0.19943425059318542 / Valid loss: 6.555985153289068

Epoch: 36
Training loss: 0.24202662706375122 / Valid loss: 6.500633044469924
Training loss: 0.4951649308204651 / Valid loss: 6.4992023468017575
Training loss: 0.16354691982269287 / Valid loss: 6.4916447730291456
Training loss: 0.21823997795581818 / Valid loss: 6.5242975779942105
Training loss: 0.17191584408283234 / Valid loss: 6.5625038237798785

Epoch: 37
Training loss: 0.33796337246894836 / Valid loss: 6.470151140576317
Training loss: 0.18709143996238708 / Valid loss: 6.49595235869998
Training loss: 0.38278505206108093 / Valid loss: 6.496720563797724
Training loss: 0.6351658701896667 / Valid loss: 6.4828751791091195
Training loss: 0.36936014890670776 / Valid loss: 6.478620590482439

Epoch: 38
Training loss: 0.2900775074958801 / Valid loss: 6.45573145094372
Training loss: 0.230264350771904 / Valid loss: 6.47436508224124
Training loss: 0.24393035471439362 / Valid loss: 6.47273131779262
Training loss: 0.15568336844444275 / Valid loss: 6.479994260697138
Training loss: 0.24576780200004578 / Valid loss: 6.552403949555897

Epoch: 39
Training loss: 0.251237154006958 / Valid loss: 6.4952503749302455
Training loss: 0.17403073608875275 / Valid loss: 6.496827915736607
Training loss: 0.2253260612487793 / Valid loss: 6.455013774690174
Training loss: 0.20019690692424774 / Valid loss: 6.4839876243046355
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 400): 5.374859732673282
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.266402789524623
Model is saved in epoch 0, overall batch: 0
Training loss: 5.0348968505859375 / Valid loss: 7.47481499626523
Model is saved in epoch 0, overall batch: 100
Training loss: 4.278203964233398 / Valid loss: 5.917708172116961
Model is saved in epoch 0, overall batch: 200
Training loss: 6.392219543457031 / Valid loss: 5.677431431270781
Model is saved in epoch 0, overall batch: 300
Training loss: 6.007085800170898 / Valid loss: 5.5386393433525445
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.696971416473389 / Valid loss: 5.540275285357521
Training loss: 4.237388610839844 / Valid loss: 5.567803428286598
Training loss: 5.027786731719971 / Valid loss: 5.65067686353411
Training loss: 5.022459983825684 / Valid loss: 5.634092905407861
Training loss: 3.9659364223480225 / Valid loss: 5.558868664786929

Epoch: 2
Training loss: 4.508965015411377 / Valid loss: 5.628752027239118
Training loss: 3.9401917457580566 / Valid loss: 5.765438767841884
Training loss: 3.4681451320648193 / Valid loss: 5.75125862076169
Training loss: 4.984563827514648 / Valid loss: 5.964262755711873
Training loss: 4.242550849914551 / Valid loss: 5.77800734156654

Epoch: 3
Training loss: 2.4835681915283203 / Valid loss: 5.812028748648507
Training loss: 2.458322525024414 / Valid loss: 6.1505091213044665
Training loss: 4.316514015197754 / Valid loss: 6.0256315571921215
Training loss: 1.893205165863037 / Valid loss: 6.034047969182333
Training loss: 3.199185371398926 / Valid loss: 6.031539176759265

Epoch: 4
Training loss: 2.299433469772339 / Valid loss: 6.10342698097229
Training loss: 2.4336204528808594 / Valid loss: 6.240989952995664
Training loss: 2.5901546478271484 / Valid loss: 6.2627257710411435
Training loss: 2.6146111488342285 / Valid loss: 6.187214490345546
Training loss: 2.537074089050293 / Valid loss: 6.291764504568917

Epoch: 5
Training loss: 2.6793100833892822 / Valid loss: 6.884952876681373
Training loss: 2.243591070175171 / Valid loss: 6.339245105925061
Training loss: 1.5069260597229004 / Valid loss: 6.45727025440761
Training loss: 2.4386744499206543 / Valid loss: 6.390168103717622
Training loss: 2.573669910430908 / Valid loss: 6.541066587538946

Epoch: 6
Training loss: 2.15779972076416 / Valid loss: 6.731491406758626
Training loss: 1.4528312683105469 / Valid loss: 6.505715713046846
Training loss: 2.776496648788452 / Valid loss: 6.491622343517485
Training loss: 1.7689704895019531 / Valid loss: 6.4776995068504695
Training loss: 1.6979490518569946 / Valid loss: 6.634709165209816

Epoch: 7
Training loss: 1.650230884552002 / Valid loss: 6.459455876123338
Training loss: 1.2438864707946777 / Valid loss: 6.553743194398426
Training loss: 1.9892454147338867 / Valid loss: 6.697380025046212
Training loss: 1.6026618480682373 / Valid loss: 6.575922053200858
Training loss: 1.3023016452789307 / Valid loss: 6.71209842818124

Epoch: 8
Training loss: 0.8102307915687561 / Valid loss: 6.476334342502413
Training loss: 0.734260082244873 / Valid loss: 6.465129756927491
Training loss: 1.0132819414138794 / Valid loss: 6.596251453672137
Training loss: 1.3281888961791992 / Valid loss: 6.626150231134324
Training loss: 1.3172154426574707 / Valid loss: 6.670802227656046

Epoch: 9
Training loss: 0.5756001472473145 / Valid loss: 6.462303345543997
Training loss: 1.1924129724502563 / Valid loss: 6.495929686228434
Training loss: 1.11946702003479 / Valid loss: 6.565565810884748
Training loss: 0.8002127408981323 / Valid loss: 6.524544318517049

Epoch: 10
Training loss: 0.7365152835845947 / Valid loss: 6.507903975532169
Training loss: 0.91594398021698 / Valid loss: 6.51995047614688
Training loss: 0.5958519577980042 / Valid loss: 6.592866393498012
Training loss: 1.4167180061340332 / Valid loss: 6.581923720950172
Training loss: 0.8149796724319458 / Valid loss: 6.600782871246338

Epoch: 11
Training loss: 0.7654539346694946 / Valid loss: 6.575266897110712
Training loss: 0.7735563516616821 / Valid loss: 6.5692592666262675
Training loss: 0.6295149326324463 / Valid loss: 6.54686918258667
Training loss: 0.5938568711280823 / Valid loss: 6.540421099889846
Training loss: 0.5370181798934937 / Valid loss: 6.557337179638091

Epoch: 12
Training loss: 0.42631858587265015 / Valid loss: 6.5071257909139
Training loss: 0.6319937109947205 / Valid loss: 6.499536670957293
Training loss: 0.5465025901794434 / Valid loss: 6.418217452367147
Training loss: 0.37303048372268677 / Valid loss: 6.548176933470226
Training loss: 0.6724017858505249 / Valid loss: 6.51566351935977

Epoch: 13
Training loss: 0.5111322999000549 / Valid loss: 6.54190019652957
Training loss: 0.5522328615188599 / Valid loss: 6.614793727511452
Training loss: 0.5887030363082886 / Valid loss: 6.561393492562431
Training loss: 0.5592777729034424 / Valid loss: 6.519777770269485
Training loss: 0.4665380120277405 / Valid loss: 6.5261842795780725

Epoch: 14
Training loss: 0.6023033857345581 / Valid loss: 6.562039141427903
Training loss: 0.6128227710723877 / Valid loss: 6.5011584826878135
Training loss: 0.5396109819412231 / Valid loss: 6.512892993291219
Training loss: 0.344563364982605 / Valid loss: 6.460542865026564
Training loss: 0.5834737420082092 / Valid loss: 6.564156768435524

Epoch: 15
Training loss: 0.5400057435035706 / Valid loss: 6.515019877751668
Training loss: 0.565315306186676 / Valid loss: 6.539307984851655
Training loss: 0.6293042302131653 / Valid loss: 6.534702811922346
Training loss: 0.6625697612762451 / Valid loss: 6.577039668673561
Training loss: 0.47096800804138184 / Valid loss: 6.583028568540301

Epoch: 16
Training loss: 0.4596961736679077 / Valid loss: 6.550441335496449
Training loss: 0.46581926941871643 / Valid loss: 6.529880382901147
Training loss: 0.39427492022514343 / Valid loss: 6.548042710622152
Training loss: 0.5199037790298462 / Valid loss: 6.493231982276553
Training loss: 0.4860115647315979 / Valid loss: 6.537580989655995

Epoch: 17
Training loss: 0.6821193099021912 / Valid loss: 6.5526975154876705
Training loss: 0.6909685134887695 / Valid loss: 6.528517468770345
Training loss: 0.5270192623138428 / Valid loss: 6.457200681595575
Training loss: 0.28720623254776 / Valid loss: 6.496563341504052
Training loss: 0.29934728145599365 / Valid loss: 6.551672967274984

Epoch: 18
Training loss: 0.383402556180954 / Valid loss: 6.5269191946302145
Training loss: 0.4466308057308197 / Valid loss: 6.484842781793503
Training loss: 0.42015179991722107 / Valid loss: 6.5055634294237406
Training loss: 0.42114192247390747 / Valid loss: 6.522176715305873
Training loss: 0.5377851128578186 / Valid loss: 6.49520746866862

Epoch: 19
Training loss: 0.42952191829681396 / Valid loss: 6.57371719678243
Training loss: 0.9881994724273682 / Valid loss: 6.515864299592518
Training loss: 0.3723157048225403 / Valid loss: 6.552016135624477
Training loss: 0.3103775978088379 / Valid loss: 6.450966971261161

Epoch: 20
Training loss: 0.23725049197673798 / Valid loss: 6.505363230478196
Training loss: 0.6895585060119629 / Valid loss: 6.543797197796049
Training loss: 0.30061328411102295 / Valid loss: 6.4720673424857
Training loss: 0.339847207069397 / Valid loss: 6.512610503605434
Training loss: 0.6854440569877625 / Valid loss: 6.509680484590076

Epoch: 21
Training loss: 0.3401038944721222 / Valid loss: 6.46756763458252
Training loss: 0.387711763381958 / Valid loss: 6.492637652442569
Training loss: 0.30057546496391296 / Valid loss: 6.505109832400367
Training loss: 0.33666253089904785 / Valid loss: 6.513793977101644
Training loss: 0.3081446588039398 / Valid loss: 6.511566693442209

Epoch: 22
Training loss: 0.2795468270778656 / Valid loss: 6.497236428941999
Training loss: 0.2837406396865845 / Valid loss: 6.3908663091205415
Training loss: 0.35473963618278503 / Valid loss: 6.446072850908552
Training loss: 0.9486091136932373 / Valid loss: 6.529360044570196
Training loss: 0.46186792850494385 / Valid loss: 6.502528444925944

Epoch: 23
Training loss: 0.24262391030788422 / Valid loss: 6.4616212504250665
Training loss: 0.727955162525177 / Valid loss: 6.442583000092279
Training loss: 0.3822314739227295 / Valid loss: 6.442696357908703
Training loss: 0.2636413872241974 / Valid loss: 6.489624983923775
Training loss: 0.5582002401351929 / Valid loss: 6.52730389095488

Epoch: 24
Training loss: 0.4048806130886078 / Valid loss: 6.5472746849060055
Training loss: 0.29930034279823303 / Valid loss: 6.440744985852922
Training loss: 0.2571537494659424 / Valid loss: 6.475989845820836
Training loss: 0.4734644293785095 / Valid loss: 6.539719297772362
Training loss: 0.8165249824523926 / Valid loss: 6.5129205567496165

Epoch: 25
Training loss: 0.48353132605552673 / Valid loss: 6.527647445315407
Training loss: 0.24215605854988098 / Valid loss: 6.451170996257237
Training loss: 0.47205013036727905 / Valid loss: 6.480919333866664
Training loss: 0.26764291524887085 / Valid loss: 6.434264364696684
Training loss: 0.23677797615528107 / Valid loss: 6.4507637727828255

Epoch: 26
Training loss: 0.3952586054801941 / Valid loss: 6.499194497153873
Training loss: 0.4491904079914093 / Valid loss: 6.450542579378401
Training loss: 0.18843623995780945 / Valid loss: 6.379413890838623
Training loss: 0.3279113173484802 / Valid loss: 6.4754982130868095
Training loss: 0.36079180240631104 / Valid loss: 6.490797964731852

Epoch: 27
Training loss: 0.1793329119682312 / Valid loss: 6.4513331980932325
Training loss: 0.30772897601127625 / Valid loss: 6.459061284292312
Training loss: 0.5283432006835938 / Valid loss: 6.459866401127407
Training loss: 0.35652971267700195 / Valid loss: 6.453861865543184
Training loss: 0.3413856029510498 / Valid loss: 6.444100089300246

Epoch: 28
Training loss: 0.31813541054725647 / Valid loss: 6.462337525685628
Training loss: 0.35898739099502563 / Valid loss: 6.455819193522135
Training loss: 0.5721069574356079 / Valid loss: 6.462548707780384
Training loss: 0.3438342213630676 / Valid loss: 6.431100695473807
Training loss: 0.26832789182662964 / Valid loss: 6.501377478100005

Epoch: 29
Training loss: 0.3744592070579529 / Valid loss: 6.422243747257051
Training loss: 0.22974848747253418 / Valid loss: 6.476159381866455
Training loss: 0.15967592597007751 / Valid loss: 6.418132239296323
Training loss: 0.2085072100162506 / Valid loss: 6.432739616575695

Epoch: 30
Training loss: 0.32070156931877136 / Valid loss: 6.475896240416027
Training loss: 0.34570568799972534 / Valid loss: 6.404546474275135
Training loss: 0.232929065823555 / Valid loss: 6.4616605667840865
Training loss: 0.392833411693573 / Valid loss: 6.500903604144142
Training loss: 0.27296900749206543 / Valid loss: 6.467305741991315

Epoch: 31
Training loss: 0.29999464750289917 / Valid loss: 6.423832223528907
Training loss: 0.33649760484695435 / Valid loss: 6.447372368403843
Training loss: 0.2321651428937912 / Valid loss: 6.409229467028664
Training loss: 0.38927924633026123 / Valid loss: 6.41981086730957
Training loss: 0.2182241976261139 / Valid loss: 6.4525373072851275

Epoch: 32
Training loss: 0.38010045886039734 / Valid loss: 6.5856338864281065
Training loss: 0.22486503422260284 / Valid loss: 6.451058841886974
Training loss: 0.20196646451950073 / Valid loss: 6.444505609784808
Training loss: 0.21109649538993835 / Valid loss: 6.417668805803571
Training loss: 0.40685343742370605 / Valid loss: 6.499940395355225

Epoch: 33
Training loss: 0.24972881376743317 / Valid loss: 6.4504298346383235
Training loss: 0.2925358712673187 / Valid loss: 6.476021017347064
Training loss: 0.16844651103019714 / Valid loss: 6.445469368071783
Training loss: 0.1946973204612732 / Valid loss: 6.392596203940255
Training loss: 0.296455979347229 / Valid loss: 6.4227887040092835

Epoch: 34
Training loss: 0.5938770771026611 / Valid loss: 6.371063096182687
Training loss: 0.30178046226501465 / Valid loss: 6.398475933074951
Training loss: 0.2487935721874237 / Valid loss: 6.4410978635152185
Training loss: 0.14579907059669495 / Valid loss: 6.4399718579791845
Training loss: 0.19115525484085083 / Valid loss: 6.3866198471614295

Epoch: 35
Training loss: 0.6441836357116699 / Valid loss: 6.437875906626384
Training loss: 0.29716020822525024 / Valid loss: 6.371323185875302
Training loss: 0.25839173793792725 / Valid loss: 6.46465333529881
Training loss: 0.21764326095581055 / Valid loss: 6.417549714587984
Training loss: 0.21889513731002808 / Valid loss: 6.42706740016029

Epoch: 36
Training loss: 0.21696606278419495 / Valid loss: 6.435967474892026
Training loss: 0.5462894439697266 / Valid loss: 6.434009656451997
Training loss: 0.2580625116825104 / Valid loss: 6.425433081672305
Training loss: 0.27159518003463745 / Valid loss: 6.41914592016311
Training loss: 0.23081353306770325 / Valid loss: 6.463167108808245

Epoch: 37
Training loss: 0.26708292961120605 / Valid loss: 6.41830000650315
Training loss: 0.16647924482822418 / Valid loss: 6.420569612866356
Training loss: 0.33794739842414856 / Valid loss: 6.423394550595964
Training loss: 0.6423270106315613 / Valid loss: 6.417838380450294
Training loss: 0.3337506651878357 / Valid loss: 6.4523162932623

Epoch: 38
Training loss: 0.2621821165084839 / Valid loss: 6.399573773429507
Training loss: 0.1612401306629181 / Valid loss: 6.4338075637817385
Training loss: 0.17192141711711884 / Valid loss: 6.412833268301828
Training loss: 0.24484652280807495 / Valid loss: 6.375636632101877
Training loss: 0.33680886030197144 / Valid loss: 6.437216656548636

Epoch: 39
Training loss: 0.29015496373176575 / Valid loss: 6.383399972461519
Training loss: 0.20729759335517883 / Valid loss: 6.389603928157261
Training loss: 0.1614130735397339 / Valid loss: 6.3682797886076425
Training loss: 0.24913795292377472 / Valid loss: 6.381860001881917
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 400): 5.372985880715507
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.485506057739258 / Valid loss: 16.577342787243072
Model is saved in epoch 0, overall batch: 0
Training loss: 3.4570741653442383 / Valid loss: 6.530275013333275
Model is saved in epoch 0, overall batch: 100
Training loss: 4.844887733459473 / Valid loss: 5.776760314759754
Model is saved in epoch 0, overall batch: 200
Training loss: 5.132018566131592 / Valid loss: 5.72529575030009
Model is saved in epoch 0, overall batch: 300
Training loss: 4.305407524108887 / Valid loss: 5.633358435403733
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.884970664978027 / Valid loss: 5.566636818931216
Model is saved in epoch 1, overall batch: 500
Training loss: 5.1891303062438965 / Valid loss: 5.602497368767148
Training loss: 3.8000714778900146 / Valid loss: 5.707312974475679
Training loss: 4.112241744995117 / Valid loss: 5.657078302474249
Training loss: 5.247347354888916 / Valid loss: 5.5525198187146865
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.111993789672852 / Valid loss: 5.595766122000558
Training loss: 3.9510116577148438 / Valid loss: 5.692064555486043
Training loss: 3.8972678184509277 / Valid loss: 5.667489344733102
Training loss: 5.479437828063965 / Valid loss: 5.915282499222529
Training loss: 5.177994728088379 / Valid loss: 5.575221906389509

Epoch: 3
Training loss: 4.20750617980957 / Valid loss: 5.896482197443644
Training loss: 3.4518933296203613 / Valid loss: 5.716273580278669
Training loss: 4.298651218414307 / Valid loss: 5.912095598947435
Training loss: 4.46403694152832 / Valid loss: 5.741578815096901
Training loss: 4.143260955810547 / Valid loss: 6.06236081804548

Epoch: 4
Training loss: 2.6088037490844727 / Valid loss: 5.830150197801136
Training loss: 3.865724802017212 / Valid loss: 5.851315382548741
Training loss: 3.7868330478668213 / Valid loss: 5.917695302054995
Training loss: 3.905289888381958 / Valid loss: 5.934809950419835
Training loss: 4.568512916564941 / Valid loss: 5.88766549428304

Epoch: 5
Training loss: 3.5554354190826416 / Valid loss: 5.917294075375511
Training loss: 3.8144683837890625 / Valid loss: 5.989838205065046
Training loss: 3.678898334503174 / Valid loss: 6.031517623719715
Training loss: 2.5668509006500244 / Valid loss: 5.980013252439953
Training loss: 4.721310615539551 / Valid loss: 6.088733409699939

Epoch: 6
Training loss: 2.87192440032959 / Valid loss: 6.058384257271176
Training loss: 1.8016934394836426 / Valid loss: 6.220289938790458
Training loss: 2.665989398956299 / Valid loss: 6.188613348915464
Training loss: 3.1001064777374268 / Valid loss: 6.203480577468872
Training loss: 2.820021629333496 / Valid loss: 6.087038507915678

Epoch: 7
Training loss: 2.6387758255004883 / Valid loss: 6.357588972364153
Training loss: 2.1998581886291504 / Valid loss: 6.406004174550374
Training loss: 2.270864486694336 / Valid loss: 7.398605414799282
Training loss: 2.562973976135254 / Valid loss: 6.724879460107712
Training loss: 2.236323833465576 / Valid loss: 6.488356029419672

Epoch: 8
Training loss: 1.8100686073303223 / Valid loss: 6.445573756808327
Training loss: 2.258617401123047 / Valid loss: 6.673467567988804
Training loss: 2.0214877128601074 / Valid loss: 6.5934448877970375
Training loss: 2.462266445159912 / Valid loss: 7.267830934978667
Training loss: 2.563457489013672 / Valid loss: 6.461018587294079

Epoch: 9
Training loss: 1.3175965547561646 / Valid loss: 6.435939078103928
Training loss: 2.1292898654937744 / Valid loss: 6.956590173358009
Training loss: 2.799058675765991 / Valid loss: 6.646304534730457
Training loss: 2.6456470489501953 / Valid loss: 6.550208405085972

Epoch: 10
Training loss: 1.6224770545959473 / Valid loss: 6.492935030800956
Training loss: 1.6329761743545532 / Valid loss: 7.0931503886268255
Training loss: 1.8883934020996094 / Valid loss: 6.659072778338477
Training loss: 1.784714937210083 / Valid loss: 7.168391180038452
Training loss: 1.790762186050415 / Valid loss: 6.7750151929401214

Epoch: 11
Training loss: 1.8431358337402344 / Valid loss: 6.7256905124301
Training loss: 1.9447927474975586 / Valid loss: 6.632198683420817
Training loss: 1.2166545391082764 / Valid loss: 6.877502968197777
Training loss: 2.1264216899871826 / Valid loss: 6.832256121862502
Training loss: 1.7260408401489258 / Valid loss: 7.26327375230335

Epoch: 12
Training loss: 1.176666021347046 / Valid loss: 6.885088312058222
Training loss: 1.3779499530792236 / Valid loss: 6.728337637583414
Training loss: 1.2002198696136475 / Valid loss: 7.039281427292597
Training loss: 1.5668423175811768 / Valid loss: 6.752382596333821
Training loss: 1.8814347982406616 / Valid loss: 6.692215367725917

Epoch: 13
Training loss: 1.2199516296386719 / Valid loss: 6.742926795142037
Training loss: 1.1335384845733643 / Valid loss: 6.7964219502040315
Training loss: 1.2104263305664062 / Valid loss: 6.768533261617025
Training loss: 1.1344892978668213 / Valid loss: 7.009672246660505
Training loss: 1.4516105651855469 / Valid loss: 6.804419231414795

Epoch: 14
Training loss: 1.2707455158233643 / Valid loss: 7.212129987989154
Training loss: 0.8627619743347168 / Valid loss: 6.828333209809803
Training loss: 0.9298070073127747 / Valid loss: 6.801197917120797
Training loss: 0.8657584190368652 / Valid loss: 6.816377607981364
Training loss: 1.3180333375930786 / Valid loss: 7.471163345518566

Epoch: 15
Training loss: 1.0745412111282349 / Valid loss: 6.762745925358364
Training loss: 1.3267104625701904 / Valid loss: 7.1384644644601005
Training loss: 1.034844994544983 / Valid loss: 6.7811291195097425
Training loss: 1.0370997190475464 / Valid loss: 7.182143370310466
Training loss: 1.357581377029419 / Valid loss: 6.920540973118373

Epoch: 16
Training loss: 1.7190804481506348 / Valid loss: 6.917948164258685
Training loss: 0.6100587248802185 / Valid loss: 6.892762883504232
Training loss: 1.2082757949829102 / Valid loss: 6.944518870399111
Training loss: 0.7216235995292664 / Valid loss: 6.867903309776669
Training loss: 0.8370188474655151 / Valid loss: 6.878616778055827

Epoch: 17
Training loss: 0.752098560333252 / Valid loss: 6.857291975475493
Training loss: 1.2123687267303467 / Valid loss: 7.2286987667992
Training loss: 0.6045585870742798 / Valid loss: 7.475333486284528
Training loss: 1.0052251815795898 / Valid loss: 6.851885977245512
Training loss: 1.1565428972244263 / Valid loss: 6.808535008203416

Epoch: 18
Training loss: 0.7613834142684937 / Valid loss: 6.852655269986108
Training loss: 1.292846441268921 / Valid loss: 6.96835067385719
Training loss: 0.7163707613945007 / Valid loss: 6.87259638877142
Training loss: 0.839563250541687 / Valid loss: 6.833955060868036
Training loss: 0.8825705051422119 / Valid loss: 6.9994033995128815

Epoch: 19
Training loss: 0.7163336277008057 / Valid loss: 6.899087905883789
Training loss: 1.2209455966949463 / Valid loss: 6.7125652880895705
Training loss: 1.3012529611587524 / Valid loss: 6.962290300641741
Training loss: 0.3375643491744995 / Valid loss: 6.789933545248849

Epoch: 20
Training loss: 0.45929422974586487 / Valid loss: 6.865611712137858
Training loss: 0.4780704379081726 / Valid loss: 6.831105046045213
Training loss: 0.5596227049827576 / Valid loss: 6.9172822271074565
Training loss: 0.6627935171127319 / Valid loss: 6.8773961339678085
Training loss: 0.8231115341186523 / Valid loss: 7.212409920919509

Epoch: 21
Training loss: 0.7881560921669006 / Valid loss: 7.091179970332554
Training loss: 0.7351148128509521 / Valid loss: 6.877445731844221
Training loss: 0.8956986665725708 / Valid loss: 6.910907259441557
Training loss: 1.0391745567321777 / Valid loss: 7.406857672191801
Training loss: 0.9281609058380127 / Valid loss: 6.821814820879982

Epoch: 22
Training loss: 0.5004188418388367 / Valid loss: 6.764326735905239
Training loss: 0.9319153428077698 / Valid loss: 7.078857340131488
Training loss: 0.7585253119468689 / Valid loss: 6.8439528783162435
Training loss: 0.5953716039657593 / Valid loss: 7.252071648552304
Training loss: 0.6517243385314941 / Valid loss: 6.888495951607114

Epoch: 23
Training loss: 0.5677701830863953 / Valid loss: 6.847617673873901
Training loss: 0.6247624158859253 / Valid loss: 6.9428961435953775
Training loss: 0.8995022773742676 / Valid loss: 6.984660312107631
Training loss: 0.6495487689971924 / Valid loss: 7.082498754773821
Training loss: 0.5745131969451904 / Valid loss: 6.89308998017084

Epoch: 24
Training loss: 0.45167461037635803 / Valid loss: 6.830669748215448
Training loss: 0.917344868183136 / Valid loss: 6.984068993159703
Training loss: 0.4396199584007263 / Valid loss: 6.822426755087716
Training loss: 0.6662071347236633 / Valid loss: 6.7646410896664575
Training loss: 0.4931257963180542 / Valid loss: 7.136090051560175

Epoch: 25
Training loss: 0.6583238840103149 / Valid loss: 6.851349653516497
Training loss: 0.7597413659095764 / Valid loss: 6.956604507991246
Training loss: 0.6999838352203369 / Valid loss: 6.866417803083148
Training loss: 0.572652280330658 / Valid loss: 6.835244351341611
Training loss: 0.45869213342666626 / Valid loss: 6.908564867292132

Epoch: 26
Training loss: 0.3454555571079254 / Valid loss: 6.812876719520205
Training loss: 0.4125431180000305 / Valid loss: 6.986858819779895
Training loss: 0.5950696468353271 / Valid loss: 6.912083970932733
Training loss: 0.5943369269371033 / Valid loss: 6.872687671298072
Training loss: 0.716728687286377 / Valid loss: 6.928625186284383

Epoch: 27
Training loss: 0.5915087461471558 / Valid loss: 6.940908777146112
Training loss: 0.36498063802719116 / Valid loss: 6.8118483861287435
Training loss: 0.7431935667991638 / Valid loss: 6.9018452371869765
Training loss: 0.5283437967300415 / Valid loss: 6.969739346277146
Training loss: 0.4297175407409668 / Valid loss: 6.806534744444347

Epoch: 28
Training loss: 0.33937326073646545 / Valid loss: 6.83280538604373
Training loss: 0.3737083077430725 / Valid loss: 6.854984494618007
Training loss: 0.26269668340682983 / Valid loss: 6.967533100219
Training loss: 0.4124552011489868 / Valid loss: 6.9173486664181665
Training loss: 0.4830261170864105 / Valid loss: 6.859507358641851

Epoch: 29
Training loss: 0.34671393036842346 / Valid loss: 6.869463141759237
Training loss: 0.286737322807312 / Valid loss: 6.795862652006603
Training loss: 0.49604904651641846 / Valid loss: 6.87221466700236
Training loss: 0.6726782321929932 / Valid loss: 6.998738997323173

Epoch: 30
Training loss: 0.4162137508392334 / Valid loss: 6.851128405616397
Training loss: 0.6545424461364746 / Valid loss: 6.968309615907215
Training loss: 0.40414363145828247 / Valid loss: 6.897067642211914
Training loss: 0.34227877855300903 / Valid loss: 6.861683141617548
Training loss: 0.7106068134307861 / Valid loss: 6.9125278927031015

Epoch: 31
Training loss: 0.44585347175598145 / Valid loss: 6.91486941746303
Training loss: 1.203381061553955 / Valid loss: 6.839730544317336
Training loss: 0.3731040060520172 / Valid loss: 6.945900031498501
Training loss: 0.506367564201355 / Valid loss: 6.840452503022694
Training loss: 0.49862658977508545 / Valid loss: 6.893275837671189

Epoch: 32
Training loss: 0.3162592649459839 / Valid loss: 6.8682549294971285
Training loss: 0.7038939595222473 / Valid loss: 6.879982662200928
Training loss: 0.4051509201526642 / Valid loss: 6.799691985902332
Training loss: 0.3026164472103119 / Valid loss: 6.879541224525088
Training loss: 0.4407041668891907 / Valid loss: 6.844918178376697

Epoch: 33
Training loss: 0.2967285215854645 / Valid loss: 7.205709534599667
Training loss: 0.4596618711948395 / Valid loss: 7.021758792513893
Training loss: 0.5868988037109375 / Valid loss: 6.865783984320505
Training loss: 0.4256957173347473 / Valid loss: 6.881491661071777
Training loss: 0.4544009566307068 / Valid loss: 7.244366804758708

Epoch: 34
Training loss: 0.8901783227920532 / Valid loss: 7.051511387597947
Training loss: 0.33228033781051636 / Valid loss: 6.781505866277786
Training loss: 0.28158384561538696 / Valid loss: 6.913215192159017
Training loss: 0.4508061408996582 / Valid loss: 6.810369936625163
Training loss: 0.6336835026741028 / Valid loss: 6.896123282114664

Epoch: 35
Training loss: 0.4027222990989685 / Valid loss: 6.9023961839221775
Training loss: 0.276203989982605 / Valid loss: 6.857167966025216
Training loss: 0.47990211844444275 / Valid loss: 6.918976438613165
Training loss: 0.19990865886211395 / Valid loss: 6.863313804353987
Training loss: 0.25360673666000366 / Valid loss: 6.863024806976318

Epoch: 36
Training loss: 0.3287600874900818 / Valid loss: 6.81584381375994
Training loss: 0.33990025520324707 / Valid loss: 6.82461028780256
Training loss: 0.5760876536369324 / Valid loss: 7.333871096656436
Training loss: 0.6256959438323975 / Valid loss: 6.988913245428176
Training loss: 0.3855710029602051 / Valid loss: 6.858831460135324

Epoch: 37
Training loss: 0.27400994300842285 / Valid loss: 6.823809669131324
Training loss: 0.23926402628421783 / Valid loss: 6.861449836549305
Training loss: 0.40206068754196167 / Valid loss: 6.789379019964309
Training loss: 0.3797105848789215 / Valid loss: 7.14957917985462
Training loss: 0.42887791991233826 / Valid loss: 6.799765305292039

Epoch: 38
Training loss: 0.36443787813186646 / Valid loss: 6.8943546158926825
Training loss: 0.30907538533210754 / Valid loss: 6.8422865958440875
Training loss: 0.3774983584880829 / Valid loss: 6.80353649684361
Training loss: 0.40939417481422424 / Valid loss: 6.848960806074596
Training loss: 0.6456255912780762 / Valid loss: 7.088196036929176

Epoch: 39
Training loss: 0.3082059919834137 / Valid loss: 6.823976916358585
Training loss: 0.38445335626602173 / Valid loss: 6.80925840650286
Training loss: 0.34243080019950867 / Valid loss: 6.774687004089356
Training loss: 0.2808007001876831 / Valid loss: 6.809128983815511
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 900): 5.452095892315819
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.485506057739258 / Valid loss: 16.577342787243072
Model is saved in epoch 0, overall batch: 0
Training loss: 3.457078218460083 / Valid loss: 6.529801007679531
Model is saved in epoch 0, overall batch: 100
Training loss: 4.903274059295654 / Valid loss: 5.776838897523426
Model is saved in epoch 0, overall batch: 200
Training loss: 5.24964714050293 / Valid loss: 5.712273786181496
Model is saved in epoch 0, overall batch: 300
Training loss: 4.254009246826172 / Valid loss: 5.629505007607596
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.0249223709106445 / Valid loss: 5.572055871146066
Model is saved in epoch 1, overall batch: 500
Training loss: 5.227969169616699 / Valid loss: 5.596946580069406
Training loss: 3.775056838989258 / Valid loss: 5.714872339793614
Training loss: 4.196728229522705 / Valid loss: 5.6329489458174935
Training loss: 5.078917503356934 / Valid loss: 5.566747946966262
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 4.957021713256836 / Valid loss: 5.5803651469094415
Training loss: 3.8963165283203125 / Valid loss: 5.668133460907709
Training loss: 3.9299278259277344 / Valid loss: 5.68123714810326
Training loss: 5.591420650482178 / Valid loss: 5.893374086561657
Training loss: 5.251518249511719 / Valid loss: 5.575754955836705

Epoch: 3
Training loss: 4.368344783782959 / Valid loss: 5.823444729759579
Training loss: 3.4472665786743164 / Valid loss: 5.7113502525147934
Training loss: 4.424086570739746 / Valid loss: 5.835715981892177
Training loss: 4.264437675476074 / Valid loss: 5.8107684657687235
Training loss: 4.065210819244385 / Valid loss: 6.037955513454619

Epoch: 4
Training loss: 2.4725255966186523 / Valid loss: 5.829659913835071
Training loss: 3.793936014175415 / Valid loss: 5.839409119742257
Training loss: 3.8041515350341797 / Valid loss: 5.86050990649632
Training loss: 3.9484071731567383 / Valid loss: 5.934009872164045
Training loss: 4.7301716804504395 / Valid loss: 5.941055232002622

Epoch: 5
Training loss: 3.689127206802368 / Valid loss: 5.879416027523223
Training loss: 3.7081031799316406 / Valid loss: 5.968183392570133
Training loss: 3.4849307537078857 / Valid loss: 6.03199683143979
Training loss: 2.515143871307373 / Valid loss: 5.996185847691128
Training loss: 4.823802947998047 / Valid loss: 6.059535662333171

Epoch: 6
Training loss: 3.114943504333496 / Valid loss: 6.09376129422869
Training loss: 1.7014141082763672 / Valid loss: 6.295024567558652
Training loss: 2.703395128250122 / Valid loss: 6.182360104152134
Training loss: 3.523953914642334 / Valid loss: 6.195398044586182
Training loss: 2.7278242111206055 / Valid loss: 6.163804274513608

Epoch: 7
Training loss: 2.804462432861328 / Valid loss: 6.313504212243217
Training loss: 2.050025463104248 / Valid loss: 6.267701980045864
Training loss: 1.9688502550125122 / Valid loss: 7.607316525777181
Training loss: 2.674919843673706 / Valid loss: 6.708087430681501
Training loss: 2.320472002029419 / Valid loss: 6.418960351035708

Epoch: 8
Training loss: 1.8835711479187012 / Valid loss: 6.381057952699207
Training loss: 1.7934260368347168 / Valid loss: 6.527355307624454
Training loss: 2.003281831741333 / Valid loss: 6.634474504561651
Training loss: 2.5673251152038574 / Valid loss: 7.249630773635138
Training loss: 2.225661277770996 / Valid loss: 6.561159742446173

Epoch: 9
Training loss: 1.355329990386963 / Valid loss: 6.445539569854736
Training loss: 2.0668885707855225 / Valid loss: 6.710856701078869
Training loss: 2.8149709701538086 / Valid loss: 6.529599757421584
Training loss: 2.5321617126464844 / Valid loss: 6.549903653916859

Epoch: 10
Training loss: 1.8134782314300537 / Valid loss: 6.477495688483828
Training loss: 1.8487379550933838 / Valid loss: 7.3067466758546376
Training loss: 1.5799219608306885 / Valid loss: 6.715597570510138
Training loss: 1.9633514881134033 / Valid loss: 7.388222676231748
Training loss: 1.9528708457946777 / Valid loss: 6.769683183942522

Epoch: 11
Training loss: 1.6593023538589478 / Valid loss: 7.0909818921770364
Training loss: 1.871406078338623 / Valid loss: 6.65293942406064
Training loss: 1.4234390258789062 / Valid loss: 7.090945534479051
Training loss: 1.7982196807861328 / Valid loss: 6.8029402732849125
Training loss: 1.7193052768707275 / Valid loss: 7.798814673650832

Epoch: 12
Training loss: 1.2765811681747437 / Valid loss: 6.956452047257196
Training loss: 1.4259328842163086 / Valid loss: 6.878681214650472
Training loss: 1.2659380435943604 / Valid loss: 6.879937017531622
Training loss: 1.3270783424377441 / Valid loss: 6.793448375520252
Training loss: 1.6433799266815186 / Valid loss: 6.721674251556396

Epoch: 13
Training loss: 1.2459845542907715 / Valid loss: 6.7394427413032165
Training loss: 1.2193936109542847 / Valid loss: 6.7030546097528365
Training loss: 1.167187213897705 / Valid loss: 6.728918105080014
Training loss: 0.9918975830078125 / Valid loss: 6.9229244550069176
Training loss: 1.2805886268615723 / Valid loss: 6.753319081806001

Epoch: 14
Training loss: 1.0811058282852173 / Valid loss: 6.9730480148678735
Training loss: 1.1204426288604736 / Valid loss: 6.812748981657482
Training loss: 0.9950368404388428 / Valid loss: 6.808526965550014
Training loss: 1.2930935621261597 / Valid loss: 7.099529629661923
Training loss: 1.3272945880889893 / Valid loss: 7.190703687213716

Epoch: 15
Training loss: 0.8640760779380798 / Valid loss: 6.80994660059611
Training loss: 1.3229689598083496 / Valid loss: 6.983769226074219
Training loss: 0.8309289216995239 / Valid loss: 6.871503017062232
Training loss: 0.9126944541931152 / Valid loss: 6.893044290088472
Training loss: 1.442546010017395 / Valid loss: 6.914877283005487

Epoch: 16
Training loss: 1.831380844116211 / Valid loss: 7.071582276480538
Training loss: 0.661740779876709 / Valid loss: 6.858134653454735
Training loss: 1.2450730800628662 / Valid loss: 7.150740918659029
Training loss: 0.9564782381057739 / Valid loss: 7.1121045612153555
Training loss: 0.9379270076751709 / Valid loss: 6.9296857992808025

Epoch: 17
Training loss: 0.80303555727005 / Valid loss: 6.889118646440052
Training loss: 1.3934917449951172 / Valid loss: 6.937965141023908
Training loss: 0.7662994861602783 / Valid loss: 7.080066667284284
Training loss: 0.9924865961074829 / Valid loss: 7.1136927241370795
Training loss: 1.1381220817565918 / Valid loss: 6.8611907913571315

Epoch: 18
Training loss: 0.5728482007980347 / Valid loss: 6.872178620383853
Training loss: 1.0971341133117676 / Valid loss: 7.102431905837286
Training loss: 0.6738294959068298 / Valid loss: 7.185974511646089
Training loss: 0.9953924417495728 / Valid loss: 7.094902102152506
Training loss: 0.8422345519065857 / Valid loss: 6.989444528307233

Epoch: 19
Training loss: 0.6585409045219421 / Valid loss: 7.259522006625221
Training loss: 1.283768653869629 / Valid loss: 6.901655487787156
Training loss: 1.0570359230041504 / Valid loss: 7.050513994126093
Training loss: 0.41520220041275024 / Valid loss: 6.913929498763311

Epoch: 20
Training loss: 0.5090558528900146 / Valid loss: 6.966407585144043
Training loss: 0.5043242573738098 / Valid loss: 7.017505336943127
Training loss: 0.6910997629165649 / Valid loss: 6.897238038835072
Training loss: 0.7945942878723145 / Valid loss: 6.951887498583113
Training loss: 0.6619163751602173 / Valid loss: 7.126078378586542

Epoch: 21
Training loss: 0.6472759246826172 / Valid loss: 7.0661819594247
Training loss: 0.5935914516448975 / Valid loss: 6.895619687579927
Training loss: 0.8492886424064636 / Valid loss: 7.005775333586193
Training loss: 0.8460538387298584 / Valid loss: 6.96570718174889
Training loss: 0.9981487989425659 / Valid loss: 6.902836261476789

Epoch: 22
Training loss: 0.5943883657455444 / Valid loss: 6.937026246388753
Training loss: 0.7727178335189819 / Valid loss: 7.001781445457822
Training loss: 0.7795895934104919 / Valid loss: 6.907544231414795
Training loss: 0.6584877371788025 / Valid loss: 6.958041824613299
Training loss: 0.4378513693809509 / Valid loss: 7.009244882492792

Epoch: 23
Training loss: 0.6647024154663086 / Valid loss: 6.908148111615862
Training loss: 0.6206005811691284 / Valid loss: 6.907786805289132
Training loss: 0.8952193260192871 / Valid loss: 7.1289108367193315
Training loss: 0.7080087065696716 / Valid loss: 6.985342568442935
Training loss: 0.45752406120300293 / Valid loss: 7.015857324146089

Epoch: 24
Training loss: 0.46706894040107727 / Valid loss: 6.864413674672445
Training loss: 0.8854964971542358 / Valid loss: 6.920893394379389
Training loss: 0.5515551567077637 / Valid loss: 6.935793626876104
Training loss: 0.755024254322052 / Valid loss: 6.926880636669341
Training loss: 0.5239611864089966 / Valid loss: 7.037851628803072

Epoch: 25
Training loss: 0.4871308505535126 / Valid loss: 7.115468388512021
Training loss: 1.0336873531341553 / Valid loss: 6.991028045472644
Training loss: 0.6540994048118591 / Valid loss: 6.953837794349307
Training loss: 0.5797979831695557 / Valid loss: 6.9320159367152625
Training loss: 0.5241333842277527 / Valid loss: 6.935137249174572

Epoch: 26
Training loss: 0.37405145168304443 / Valid loss: 6.864972972869873
Training loss: 0.6069234609603882 / Valid loss: 7.051710065205892
Training loss: 0.6727738380432129 / Valid loss: 6.985199251629058
Training loss: 0.723617672920227 / Valid loss: 6.960308901468912
Training loss: 0.8047763109207153 / Valid loss: 7.165904717218308

Epoch: 27
Training loss: 0.47623133659362793 / Valid loss: 6.85762794585455
Training loss: 0.42902112007141113 / Valid loss: 6.838637070428757
Training loss: 0.5495664477348328 / Valid loss: 6.978038311004639
Training loss: 0.6719669103622437 / Valid loss: 7.05523087637765
Training loss: 0.33895838260650635 / Valid loss: 6.879837608337402

Epoch: 28
Training loss: 0.30973678827285767 / Valid loss: 6.958047133400327
Training loss: 0.2495696246623993 / Valid loss: 6.901720439820062
Training loss: 0.400736540555954 / Valid loss: 7.328949269794283
Training loss: 0.5703094601631165 / Valid loss: 6.989046087719145
Training loss: 0.6628759503364563 / Valid loss: 7.031058733803885

Epoch: 29
Training loss: 0.3077479898929596 / Valid loss: 6.90930393082755
Training loss: 0.19452568888664246 / Valid loss: 6.990406047730219
Training loss: 0.5045199990272522 / Valid loss: 6.909203415825253
Training loss: 0.7959120273590088 / Valid loss: 6.96449833824521

Epoch: 30
Training loss: 0.34426912665367126 / Valid loss: 6.866918266387213
Training loss: 0.7310848236083984 / Valid loss: 7.061628845759801
Training loss: 0.26152023673057556 / Valid loss: 6.958064469837007
Training loss: 0.37470656633377075 / Valid loss: 6.921448816571917
Training loss: 0.8306399583816528 / Valid loss: 6.981259554908389

Epoch: 31
Training loss: 0.4202406108379364 / Valid loss: 7.158872393199376
Training loss: 0.9354183673858643 / Valid loss: 6.877529548463367
Training loss: 0.4070549011230469 / Valid loss: 6.9821786698840915
Training loss: 0.48457175493240356 / Valid loss: 6.840339706057594
Training loss: 0.5195673704147339 / Valid loss: 6.86444335210891

Epoch: 32
Training loss: 0.22988927364349365 / Valid loss: 6.883358628409249
Training loss: 0.6464457511901855 / Valid loss: 6.827281452360607
Training loss: 0.36872613430023193 / Valid loss: 6.917693480991182
Training loss: 0.34448814392089844 / Valid loss: 7.076045940035866
Training loss: 0.5470592975616455 / Valid loss: 6.863318584078834

Epoch: 33
Training loss: 0.31299933791160583 / Valid loss: 6.990965488978794
Training loss: 0.4442322254180908 / Valid loss: 6.985515240260533
Training loss: 0.5362575054168701 / Valid loss: 6.928737386067708
Training loss: 0.5632263422012329 / Valid loss: 6.89925624756586
Training loss: 0.37766942381858826 / Valid loss: 6.926023356119791

Epoch: 34
Training loss: 1.082247018814087 / Valid loss: 6.89746759505499
Training loss: 0.5059753656387329 / Valid loss: 6.974391144797916
Training loss: 0.304455429315567 / Valid loss: 7.014540159134638
Training loss: 0.4042292833328247 / Valid loss: 6.8372020971207395
Training loss: 0.7308337688446045 / Valid loss: 7.188690943945022

Epoch: 35
Training loss: 0.3702801764011383 / Valid loss: 6.805372283572242
Training loss: 0.37456732988357544 / Valid loss: 7.039246309371221
Training loss: 0.45405080914497375 / Valid loss: 6.948231965019589
Training loss: 0.38725489377975464 / Valid loss: 6.876561005910237
Training loss: 0.3556004762649536 / Valid loss: 7.00182865687779

Epoch: 36
Training loss: 0.38017114996910095 / Valid loss: 6.890349597022647
Training loss: 0.36959660053253174 / Valid loss: 6.895889132363456
Training loss: 0.3882257342338562 / Valid loss: 6.839681956881568
Training loss: 0.6402799487113953 / Valid loss: 6.932602659861247
Training loss: 0.3274349868297577 / Valid loss: 6.87552683694022

Epoch: 37
Training loss: 0.3282313942909241 / Valid loss: 6.867753778185163
Training loss: 0.3362838327884674 / Valid loss: 6.944752992902483
Training loss: 0.34709882736206055 / Valid loss: 6.957557719094413
Training loss: 0.3530508875846863 / Valid loss: 6.965642179761614
Training loss: 0.32260841131210327 / Valid loss: 7.011216281709217

Epoch: 38
Training loss: 0.493783175945282 / Valid loss: 7.268724573226202
Training loss: 0.35758015513420105 / Valid loss: 6.842532589322045
Training loss: 0.39458173513412476 / Valid loss: 6.829485332398187
Training loss: 0.4816513955593109 / Valid loss: 6.983888047082083
Training loss: 0.8191705942153931 / Valid loss: 6.971751276652018

Epoch: 39
Training loss: 0.38350802659988403 / Valid loss: 6.903936590467181
Training loss: 0.4124823212623596 / Valid loss: 6.857743762788319
Training loss: 0.35658079385757446 / Valid loss: 6.8633064973922
Training loss: 0.3654819130897522 / Valid loss: 6.845270343053908
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 900): 5.484912738345918
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.62260627746582 / Valid loss: 15.821076865423294
Model is saved in epoch 0, overall batch: 0
Training loss: 6.312438488006592 / Valid loss: 5.782402011326381
Model is saved in epoch 0, overall batch: 100
Training loss: 6.626265525817871 / Valid loss: 5.648898072469802
Model is saved in epoch 0, overall batch: 200
Training loss: 5.999965667724609 / Valid loss: 5.561895890462966
Model is saved in epoch 0, overall batch: 300
Training loss: 8.334207534790039 / Valid loss: 5.654933245976766

Epoch: 1
Training loss: 5.244283199310303 / Valid loss: 5.5207379522777735
Model is saved in epoch 1, overall batch: 500
Training loss: 5.461071491241455 / Valid loss: 5.537272419248309
Training loss: 7.309114456176758 / Valid loss: 5.678013149897257
Training loss: 5.803442001342773 / Valid loss: 5.587360309419178
Training loss: 5.411737442016602 / Valid loss: 5.5323139145260765

Epoch: 2
Training loss: 4.822872638702393 / Valid loss: 5.5481255372365315
Training loss: 4.553755760192871 / Valid loss: 5.557839439028785
Training loss: 5.0963897705078125 / Valid loss: 5.595197818392799
Training loss: 5.403703212738037 / Valid loss: 5.525537722451347
Training loss: 5.436009407043457 / Valid loss: 5.5749356542314805

Epoch: 3
Training loss: 3.486049175262451 / Valid loss: 5.572971319016956
Training loss: 4.630762100219727 / Valid loss: 5.5767019022078745
Training loss: 3.9973928928375244 / Valid loss: 5.655842749277751
Training loss: 4.906824111938477 / Valid loss: 5.889032793045044
Training loss: 4.649782180786133 / Valid loss: 5.592046848932902

Epoch: 4
Training loss: 3.3244166374206543 / Valid loss: 5.830181137720744
Training loss: 4.0159101486206055 / Valid loss: 5.651510890324911
Training loss: 3.8812828063964844 / Valid loss: 5.937579842976161
Training loss: 4.830534934997559 / Valid loss: 5.643080879393078
Training loss: 6.1269001960754395 / Valid loss: 5.781846918378558

Epoch: 5
Training loss: 3.6648192405700684 / Valid loss: 5.7482916968209405
Training loss: 3.3266990184783936 / Valid loss: 5.882529313223703
Training loss: 5.620205402374268 / Valid loss: 5.7094084194728305
Training loss: 4.914240837097168 / Valid loss: 5.789184275127592
Training loss: 6.35711669921875 / Valid loss: 6.270100180308024

Epoch: 6
Training loss: 3.645369529724121 / Valid loss: 5.879225120090303
Training loss: 2.953738212585449 / Valid loss: 5.943836882000878
Training loss: 3.931783676147461 / Valid loss: 5.917436100187755
Training loss: 3.622002363204956 / Valid loss: 5.902938413619995
Training loss: 2.798304796218872 / Valid loss: 5.894418432599022

Epoch: 7
Training loss: 2.320878505706787 / Valid loss: 5.918871970403762
Training loss: 3.0377392768859863 / Valid loss: 5.980479269935971
Training loss: 3.617147922515869 / Valid loss: 6.126538190387544
Training loss: 4.319531440734863 / Valid loss: 5.912345754532587
Training loss: 3.8508827686309814 / Valid loss: 6.351355257488432

Epoch: 8
Training loss: 3.076977491378784 / Valid loss: 6.022086393265497
Training loss: 3.0622143745422363 / Valid loss: 6.3435213679359075
Training loss: 4.180960178375244 / Valid loss: 6.006243662607102
Training loss: 3.341827630996704 / Valid loss: 6.064748278118315
Training loss: 3.111532211303711 / Valid loss: 6.848010771615165

Epoch: 9
Training loss: 3.1600849628448486 / Valid loss: 6.1069217273167204
Training loss: 3.25532865524292 / Valid loss: 6.224871147246588
Training loss: 3.521022319793701 / Valid loss: 6.4632301421392535
Training loss: 2.1903724670410156 / Valid loss: 7.069436259496779

Epoch: 10
Training loss: 2.719900608062744 / Valid loss: 6.134010169619605
Training loss: 3.373067855834961 / Valid loss: 6.351782678422474
Training loss: 2.8975272178649902 / Valid loss: 6.2298657462710425
Training loss: 3.1403613090515137 / Valid loss: 6.178451824188232
Training loss: 3.001293420791626 / Valid loss: 6.432161299387614

Epoch: 11
Training loss: 3.0253567695617676 / Valid loss: 6.282483296167283
Training loss: 2.8200957775115967 / Valid loss: 6.195561431703114
Training loss: 2.7768054008483887 / Valid loss: 6.4379486538115005
Training loss: 2.7201359272003174 / Valid loss: 6.158489215941656
Training loss: 2.620471954345703 / Valid loss: 6.407860905783517

Epoch: 12
Training loss: 2.8014986515045166 / Valid loss: 7.080790151868547
Training loss: 2.336186408996582 / Valid loss: 6.7314493905930295
Training loss: 3.2585458755493164 / Valid loss: 6.449056661696661
Training loss: 2.6679506301879883 / Valid loss: 7.324375193459647
Training loss: 1.8373174667358398 / Valid loss: 6.676259735652379

Epoch: 13
Training loss: 1.9504060745239258 / Valid loss: 6.487425234204247
Training loss: 2.126605749130249 / Valid loss: 6.4027812321980795
Training loss: 1.6486432552337646 / Valid loss: 6.591701693761916
Training loss: 1.8099974393844604 / Valid loss: 6.768780345008487
Training loss: 2.268421173095703 / Valid loss: 6.5504781450544085

Epoch: 14
Training loss: 1.6912081241607666 / Valid loss: 6.6034802754720054
Training loss: 2.398416757583618 / Valid loss: 6.584603241511753
Training loss: 4.172308921813965 / Valid loss: 7.601777558099656
Training loss: 2.4657371044158936 / Valid loss: 6.716397962116059
Training loss: 2.6717379093170166 / Valid loss: 6.542923836481004

Epoch: 15
Training loss: 1.5136778354644775 / Valid loss: 7.112515413193476
Training loss: 2.703660488128662 / Valid loss: 6.570807150432041
Training loss: 2.40661358833313 / Valid loss: 6.7601741018749415
Training loss: 2.3519058227539062 / Valid loss: 6.601628968829201
Training loss: 2.0764241218566895 / Valid loss: 6.735687024252755

Epoch: 16
Training loss: 2.7509775161743164 / Valid loss: 6.65966256459554
Training loss: 1.93463933467865 / Valid loss: 6.828725590024676
Training loss: 1.3701145648956299 / Valid loss: 6.686442384265718
Training loss: 2.2532691955566406 / Valid loss: 6.675660242353167
Training loss: 2.2510194778442383 / Valid loss: 7.822255234491258

Epoch: 17
Training loss: 1.9451680183410645 / Valid loss: 6.646299239567348
Training loss: 1.7607523202896118 / Valid loss: 6.724984146299816
Training loss: 1.5927481651306152 / Valid loss: 7.046758231662569
Training loss: 3.6873903274536133 / Valid loss: 6.807383019583566
Training loss: 1.653456211090088 / Valid loss: 6.947065453302293

Epoch: 18
Training loss: 2.5527515411376953 / Valid loss: 7.228678685142881
Training loss: 1.4423255920410156 / Valid loss: 8.694548516046433
Training loss: 2.3361072540283203 / Valid loss: 8.179304740542458
Training loss: 1.7511323690414429 / Valid loss: 7.235463346753802
Training loss: 1.7928860187530518 / Valid loss: 6.790559201013474

Epoch: 19
Training loss: 1.2379469871520996 / Valid loss: 7.175202501387823
Training loss: 1.890880823135376 / Valid loss: 7.303953938257127
Training loss: 1.7326329946517944 / Valid loss: 7.304204641069685
Training loss: 1.251649260520935 / Valid loss: 7.8710838862827845

Epoch: 20
Training loss: 2.158975124359131 / Valid loss: 7.161475395020984
Training loss: 1.4121849536895752 / Valid loss: 6.779689044044131
Training loss: 1.400665283203125 / Valid loss: 7.563430854252407
Training loss: 1.27761971950531 / Valid loss: 7.924210330418178
Training loss: 1.31663978099823 / Valid loss: 9.345052482968285

Epoch: 21
Training loss: 1.0083339214324951 / Valid loss: 7.273468639737084
Training loss: 1.620606780052185 / Valid loss: 6.977515175229027
Training loss: 1.5522946119308472 / Valid loss: 7.655810778481619
Training loss: 2.152210235595703 / Valid loss: 7.658027471814837
Training loss: 1.626043677330017 / Valid loss: 7.413818313961937

Epoch: 22
Training loss: 1.1321399211883545 / Valid loss: 6.9571509906223845
Training loss: 1.2778651714324951 / Valid loss: 7.425983824048724
Training loss: 1.9657901525497437 / Valid loss: 7.52528167452131
Training loss: 1.4729115962982178 / Valid loss: 7.606688790094285
Training loss: 1.1462655067443848 / Valid loss: 6.891003231775193

Epoch: 23
Training loss: 0.7695568799972534 / Valid loss: 6.7873677730560305
Training loss: 1.6960642337799072 / Valid loss: 9.775730160304478
Training loss: 1.6201090812683105 / Valid loss: 6.899731436229888
Training loss: 1.0499831438064575 / Valid loss: 7.60558530716669
Training loss: 1.2725582122802734 / Valid loss: 6.927141389392671

Epoch: 24
Training loss: 0.9235942959785461 / Valid loss: 7.263785244169689
Training loss: 1.4577627182006836 / Valid loss: 7.833597196851458
Training loss: 1.2676076889038086 / Valid loss: 6.996071034386045
Training loss: 0.931744396686554 / Valid loss: 7.527961526598249
Training loss: 1.2709816694259644 / Valid loss: 6.951789887746175

Epoch: 25
Training loss: 0.8715004324913025 / Valid loss: 6.983160364060175
Training loss: 0.9061920642852783 / Valid loss: 7.042179357437861
Training loss: 1.3260633945465088 / Valid loss: 7.073294680459159
Training loss: 1.2035713195800781 / Valid loss: 7.153155844552177
Training loss: 1.1284788846969604 / Valid loss: 6.976862103598458

Epoch: 26
Training loss: 0.565925121307373 / Valid loss: 6.959483818780808
Training loss: 1.0841190814971924 / Valid loss: 7.117844168345133
Training loss: 0.7412866353988647 / Valid loss: 7.248322327931722
Training loss: 0.8270851373672485 / Valid loss: 7.827180549076625
Training loss: 0.8841052651405334 / Valid loss: 7.3820248876299175

Epoch: 27
Training loss: 1.1943004131317139 / Valid loss: 6.976496998469035
Training loss: 0.9046428203582764 / Valid loss: 7.032549481164842
Training loss: 1.2672779560089111 / Valid loss: 12.832330939883278
Training loss: 1.0461781024932861 / Valid loss: 6.938991469428653
Training loss: 1.6026496887207031 / Valid loss: 7.022827738807315

Epoch: 28
Training loss: 0.6578078269958496 / Valid loss: 7.52043848945981
Training loss: 1.5707437992095947 / Valid loss: 12.310394704909552
Training loss: 0.7806028723716736 / Valid loss: 7.259953884851365
Training loss: 1.5970242023468018 / Valid loss: 8.663047200157528
Training loss: 1.291646957397461 / Valid loss: 7.058036695207869

Epoch: 29
Training loss: 0.6785643100738525 / Valid loss: 7.063009330204555
Training loss: 0.998985767364502 / Valid loss: 10.35784786769322
Training loss: 1.0889352560043335 / Valid loss: 7.0853437968662805
Training loss: 0.8913177251815796 / Valid loss: 6.986072077069964

Epoch: 30
Training loss: 0.5871413946151733 / Valid loss: 7.235504781632196
Training loss: 0.7231724262237549 / Valid loss: 7.019511141095843
Training loss: 0.6910883784294128 / Valid loss: 7.088810566493443
Training loss: 0.8817110061645508 / Valid loss: 7.367452457972935
Training loss: 0.7224568128585815 / Valid loss: 7.075583580562046

Epoch: 31
Training loss: 0.7816085815429688 / Valid loss: 7.564044293903169
Training loss: 0.9432468414306641 / Valid loss: 7.723707283110846
Training loss: 0.8293903470039368 / Valid loss: 7.192809529531569
Training loss: 0.7992712259292603 / Valid loss: 7.151208269028436
Training loss: 0.7845082879066467 / Valid loss: 7.270709923335484

Epoch: 32
Training loss: 0.5720974206924438 / Valid loss: 7.577519112541562
Training loss: 0.44194406270980835 / Valid loss: 7.04359023684547
Training loss: 1.0357846021652222 / Valid loss: 7.398456537155878
Training loss: 0.5651695132255554 / Valid loss: 7.123799428485689
Training loss: 0.9525454044342041 / Valid loss: 7.092059203556606

Epoch: 33
Training loss: 0.7870014905929565 / Valid loss: 8.142335460299538
Training loss: 1.0066142082214355 / Valid loss: 7.608128833770752
Training loss: 0.7811516523361206 / Valid loss: 7.20413864680699
Training loss: 0.9908014535903931 / Valid loss: 7.487918072655088
Training loss: 1.0367634296417236 / Valid loss: 8.047446986607143

Epoch: 34
Training loss: 0.5266765356063843 / Valid loss: 7.444060702550979
Training loss: 1.180842638015747 / Valid loss: 8.290840907323927
Training loss: 0.4769260883331299 / Valid loss: 7.021278299604144
Training loss: 0.7135266065597534 / Valid loss: 7.164872737157912
Training loss: 0.5368931293487549 / Valid loss: 7.11102382569086

Epoch: 35
Training loss: 0.7694202065467834 / Valid loss: 8.698550110771542
Training loss: 0.6952452063560486 / Valid loss: 7.212755235036214
Training loss: 1.1983537673950195 / Valid loss: 7.8438841819763185
Training loss: 0.7231157422065735 / Valid loss: 7.88514164515904
Training loss: 1.3573447465896606 / Valid loss: 9.16420685450236

Epoch: 36
Training loss: 0.6967319250106812 / Valid loss: 7.096651703970773
Training loss: 0.8207906484603882 / Valid loss: 7.356746448789324
Training loss: 0.5043139457702637 / Valid loss: 7.232939270564488
Training loss: 0.7085784673690796 / Valid loss: 7.837363070533389
Training loss: 0.8565972447395325 / Valid loss: 7.333512015569777

Epoch: 37
Training loss: 1.248915433883667 / Valid loss: 8.128690619695755
Training loss: 0.5872550010681152 / Valid loss: 7.867724550338019
Training loss: 0.6185203790664673 / Valid loss: 7.298463430858794
Training loss: 0.38984963297843933 / Valid loss: 7.115603106362479
Training loss: 0.8182886838912964 / Valid loss: 7.572126606532506

Epoch: 38
Training loss: 0.4790794849395752 / Valid loss: 7.352297973632813
Training loss: 0.6044061183929443 / Valid loss: 7.3300574711390905
Training loss: 0.5140707492828369 / Valid loss: 7.319137387048631
Training loss: 0.9603894352912903 / Valid loss: 7.286004475184849
Training loss: 1.1746454238891602 / Valid loss: 9.81565389179048

Epoch: 39
Training loss: 0.527906060218811 / Valid loss: 7.531298564729236
Training loss: 0.7206075191497803 / Valid loss: 7.785276403881254
Training loss: 0.5597670078277588 / Valid loss: 7.477237151917957
Training loss: 0.4318363666534424 / Valid loss: 8.000659815470378
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.382941675186157
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.62260627746582 / Valid loss: 15.821076974414645
Model is saved in epoch 0, overall batch: 0
Training loss: 6.31243896484375 / Valid loss: 5.7824024404798235
Model is saved in epoch 0, overall batch: 100
Training loss: 6.626266002655029 / Valid loss: 5.648897781826201
Model is saved in epoch 0, overall batch: 200
Training loss: 5.999965667724609 / Valid loss: 5.561895397731236
Model is saved in epoch 0, overall batch: 300
Training loss: 8.334207534790039 / Valid loss: 5.654932614735195

Epoch: 1
Training loss: 5.244284629821777 / Valid loss: 5.520737654822213
Model is saved in epoch 1, overall batch: 500
Training loss: 5.462059020996094 / Valid loss: 5.5372261160895935
Training loss: 7.3090739250183105 / Valid loss: 5.676754815237863
Training loss: 5.80606746673584 / Valid loss: 5.585774503435407
Training loss: 5.407125473022461 / Valid loss: 5.532695620400565

Epoch: 2
Training loss: 4.833785533905029 / Valid loss: 5.546330617723011
Training loss: 4.555326461791992 / Valid loss: 5.554695481345767
Training loss: 5.087904930114746 / Valid loss: 5.595905989692325
Training loss: 5.387564182281494 / Valid loss: 5.526919478461856
Training loss: 5.443344593048096 / Valid loss: 5.578560070764451

Epoch: 3
Training loss: 3.5083868503570557 / Valid loss: 5.574336671829224
Training loss: 4.620428085327148 / Valid loss: 5.579485575358073
Training loss: 3.9703869819641113 / Valid loss: 5.6526000113714305
Training loss: 4.909674167633057 / Valid loss: 5.874191182000296
Training loss: 4.643283843994141 / Valid loss: 5.591592938559396

Epoch: 4
Training loss: 3.3259363174438477 / Valid loss: 5.831957764852614
Training loss: 4.010941505432129 / Valid loss: 5.646722057887486
Training loss: 3.856532096862793 / Valid loss: 5.918198578698295
Training loss: 4.813253402709961 / Valid loss: 5.638240932282947
Training loss: 6.140199661254883 / Valid loss: 5.798773524874733

Epoch: 5
Training loss: 3.689687728881836 / Valid loss: 5.742455223628453
Training loss: 3.340498924255371 / Valid loss: 5.886964159920102
Training loss: 5.560077667236328 / Valid loss: 5.706720747266497
Training loss: 4.941481590270996 / Valid loss: 5.8022171361105785
Training loss: 6.383753776550293 / Valid loss: 6.248243238812401

Epoch: 6
Training loss: 3.6439926624298096 / Valid loss: 5.883503611882528
Training loss: 2.9077582359313965 / Valid loss: 5.944308085668655
Training loss: 3.980340003967285 / Valid loss: 5.913109702155704
Training loss: 3.6173577308654785 / Valid loss: 5.904100422632126
Training loss: 2.823758602142334 / Valid loss: 5.909613423120408

Epoch: 7
Training loss: 2.321413516998291 / Valid loss: 5.911711109252203
Training loss: 2.968226194381714 / Valid loss: 5.998061752319336
Training loss: 3.65836763381958 / Valid loss: 6.097094415483021
Training loss: 4.287047863006592 / Valid loss: 5.9154892262958345
Training loss: 3.803001880645752 / Valid loss: 6.371848249435425

Epoch: 8
Training loss: 3.11281156539917 / Valid loss: 6.020056218192691
Training loss: 3.045865535736084 / Valid loss: 6.321726721809024
Training loss: 4.192165374755859 / Valid loss: 6.017969172341483
Training loss: 3.4005215167999268 / Valid loss: 6.059210729598999
Training loss: 3.093311071395874 / Valid loss: 6.900964128403436

Epoch: 9
Training loss: 3.104792594909668 / Valid loss: 6.105531070345924
Training loss: 3.258655548095703 / Valid loss: 6.266294651939756
Training loss: 3.362649917602539 / Valid loss: 6.39157843816848
Training loss: 2.2792887687683105 / Valid loss: 7.0688867705208915

Epoch: 10
Training loss: 2.689032554626465 / Valid loss: 6.146721594674247
Training loss: 3.309814453125 / Valid loss: 6.38696354230245
Training loss: 2.8514819145202637 / Valid loss: 6.230966168358212
Training loss: 3.0969700813293457 / Valid loss: 6.2286807151067825
Training loss: 2.929556131362915 / Valid loss: 6.4119097573416575

Epoch: 11
Training loss: 3.080461025238037 / Valid loss: 6.242519953137353
Training loss: 2.7610795497894287 / Valid loss: 6.2186444418770925
Training loss: 2.7843778133392334 / Valid loss: 6.412383043198359
Training loss: 2.6989643573760986 / Valid loss: 6.18273777280535
Training loss: 2.4770541191101074 / Valid loss: 6.479943457103911

Epoch: 12
Training loss: 2.771414279937744 / Valid loss: 7.226985568091983
Training loss: 2.3835771083831787 / Valid loss: 6.6688410214015414
Training loss: 3.261402130126953 / Valid loss: 6.452148800804501
Training loss: 2.6217164993286133 / Valid loss: 7.388186500186012
Training loss: 1.8026814460754395 / Valid loss: 6.656590548015776

Epoch: 13
Training loss: 1.8817307949066162 / Valid loss: 6.48957074710301
Training loss: 2.1299333572387695 / Valid loss: 6.376529511951265
Training loss: 1.7274590730667114 / Valid loss: 6.5119788646698
Training loss: 1.8665964603424072 / Valid loss: 6.664610930851528
Training loss: 2.2637782096862793 / Valid loss: 6.517276178087507

Epoch: 14
Training loss: 1.6971553564071655 / Valid loss: 6.577278337024507
Training loss: 2.34810733795166 / Valid loss: 6.562672415233794
Training loss: 4.149113178253174 / Valid loss: 7.469465705326625
Training loss: 2.520022392272949 / Valid loss: 6.722053566433135
Training loss: 2.727428436279297 / Valid loss: 6.499142322086152

Epoch: 15
Training loss: 1.5508792400360107 / Valid loss: 7.241015945162092
Training loss: 2.7481045722961426 / Valid loss: 6.6677068369729176
Training loss: 2.2249112129211426 / Valid loss: 6.695356614249093
Training loss: 2.293153762817383 / Valid loss: 6.706963402884347
Training loss: 2.1345391273498535 / Valid loss: 6.811610807691301

Epoch: 16
Training loss: 2.6918158531188965 / Valid loss: 6.581891731988816
Training loss: 1.8986806869506836 / Valid loss: 6.706567977723621
Training loss: 1.4511815309524536 / Valid loss: 6.691025089082264
Training loss: 2.1966559886932373 / Valid loss: 6.668829852058774
Training loss: 2.183642864227295 / Valid loss: 7.701995817820231

Epoch: 17
Training loss: 2.0298519134521484 / Valid loss: 6.6475225403195335
Training loss: 1.7539548873901367 / Valid loss: 6.825080297106788
Training loss: 1.5968385934829712 / Valid loss: 7.03096745581854
Training loss: 3.9687514305114746 / Valid loss: 6.824669356573196
Training loss: 1.7052044868469238 / Valid loss: 6.829312810443697

Epoch: 18
Training loss: 2.538565158843994 / Valid loss: 7.053721627734956
Training loss: 1.5092694759368896 / Valid loss: 9.296861857459659
Training loss: 2.129129409790039 / Valid loss: 8.02834209714617
Training loss: 1.7020279169082642 / Valid loss: 7.4519518307277135
Training loss: 1.7365037202835083 / Valid loss: 6.88466367267427

Epoch: 19
Training loss: 1.207040786743164 / Valid loss: 6.81933145977202
Training loss: 2.0812935829162598 / Valid loss: 7.975995922088623
Training loss: 1.7513854503631592 / Valid loss: 7.0925973074776785
Training loss: 1.1875872611999512 / Valid loss: 7.794662620907738

Epoch: 20
Training loss: 2.0841450691223145 / Valid loss: 7.168278417133149
Training loss: 1.3866589069366455 / Valid loss: 6.785233397710891
Training loss: 1.3807766437530518 / Valid loss: 7.4171606563386465
Training loss: 1.440195083618164 / Valid loss: 8.310466516585578
Training loss: 1.2114789485931396 / Valid loss: 9.295926230294365

Epoch: 21
Training loss: 1.0266900062561035 / Valid loss: 7.23318597702753
Training loss: 1.6182866096496582 / Valid loss: 7.047046298072452
Training loss: 1.7190327644348145 / Valid loss: 8.220656022571383
Training loss: 2.230229616165161 / Valid loss: 7.312781402042934
Training loss: 1.5576648712158203 / Valid loss: 7.064283062162853

Epoch: 22
Training loss: 1.0997918844223022 / Valid loss: 7.173328059060233
Training loss: 1.2417696714401245 / Valid loss: 7.196157977694557
Training loss: 1.915820598602295 / Valid loss: 7.539146900177002
Training loss: 1.3906974792480469 / Valid loss: 7.308007403782436
Training loss: 1.0754450559616089 / Valid loss: 6.900415820167178

Epoch: 23
Training loss: 0.7041946649551392 / Valid loss: 6.7855814207167855
Training loss: 1.7005648612976074 / Valid loss: 9.902993806203206
Training loss: 1.6506311893463135 / Valid loss: 6.9099836394900365
Training loss: 1.1286873817443848 / Valid loss: 8.097960740044003
Training loss: 1.254583477973938 / Valid loss: 7.120745213826497

Epoch: 24
Training loss: 0.8993397355079651 / Valid loss: 7.172510623931885
Training loss: 1.295778512954712 / Valid loss: 7.542505482264928
Training loss: 1.269913911819458 / Valid loss: 6.964164243425642
Training loss: 0.8386960625648499 / Valid loss: 7.415717869713193
Training loss: 1.289052128791809 / Valid loss: 7.62537263688587

Epoch: 25
Training loss: 0.8139933347702026 / Valid loss: 7.134596334184919
Training loss: 0.9631361365318298 / Valid loss: 7.012451076507569
Training loss: 1.3837833404541016 / Valid loss: 7.1438060601552325
Training loss: 1.2297329902648926 / Valid loss: 7.329778653099424
Training loss: 1.0975967645645142 / Valid loss: 6.980490752628872

Epoch: 26
Training loss: 0.5703245401382446 / Valid loss: 6.997206310998826
Training loss: 1.152716040611267 / Valid loss: 6.9638249306451705
Training loss: 0.7338632345199585 / Valid loss: 7.162733059837705
Training loss: 0.8388539552688599 / Valid loss: 8.116879712967645
Training loss: 0.8494301438331604 / Valid loss: 7.491306105114165

Epoch: 27
Training loss: 1.0984034538269043 / Valid loss: 6.946869164421445
Training loss: 0.9892629384994507 / Valid loss: 7.003455875033424
Training loss: 1.3462573289871216 / Valid loss: 12.419645731789725
Training loss: 0.9844245314598083 / Valid loss: 6.939306254613967
Training loss: 1.4207839965820312 / Valid loss: 7.031429177238827

Epoch: 28
Training loss: 0.742802619934082 / Valid loss: 7.415110760643369
Training loss: 1.5606937408447266 / Valid loss: 11.121278040749687
Training loss: 0.8140002489089966 / Valid loss: 7.5219850176856635
Training loss: 1.5303759574890137 / Valid loss: 9.230445325942267
Training loss: 1.2394132614135742 / Valid loss: 7.149847325824556

Epoch: 29
Training loss: 0.5340394973754883 / Valid loss: 7.108301185426258
Training loss: 1.0494718551635742 / Valid loss: 11.813137476784842
Training loss: 1.144063949584961 / Valid loss: 7.144426940736317
Training loss: 0.7896578311920166 / Valid loss: 7.081945264907111

Epoch: 30
Training loss: 0.5933140516281128 / Valid loss: 7.259557492392403
Training loss: 0.7612113356590271 / Valid loss: 7.083998621077765
Training loss: 0.8331606388092041 / Valid loss: 7.2221052306038995
Training loss: 0.8774034380912781 / Valid loss: 7.405998833974203
Training loss: 0.9062743186950684 / Valid loss: 7.158683281853086

Epoch: 31
Training loss: 0.809799313545227 / Valid loss: 7.82545504342942
Training loss: 0.9501198530197144 / Valid loss: 7.719833010718936
Training loss: 0.7758182287216187 / Valid loss: 7.17489641734532
Training loss: 0.8644152879714966 / Valid loss: 7.086654013679141
Training loss: 0.7499524354934692 / Valid loss: 7.254643499283564

Epoch: 32
Training loss: 0.5805127620697021 / Valid loss: 7.6395826566787
Training loss: 0.5383208990097046 / Valid loss: 7.129545570555187
Training loss: 0.9965425133705139 / Valid loss: 7.553420035044352
Training loss: 0.6468126773834229 / Valid loss: 7.304976526896159
Training loss: 0.8502696752548218 / Valid loss: 7.072486391521635

Epoch: 33
Training loss: 0.6288979053497314 / Valid loss: 7.354139200846354
Training loss: 0.8930057287216187 / Valid loss: 7.687717315128872
Training loss: 0.8091106414794922 / Valid loss: 7.287354614621117
Training loss: 1.1167277097702026 / Valid loss: 7.691113826206752
Training loss: 0.9401094913482666 / Valid loss: 7.9909705025809155

Epoch: 34
Training loss: 0.477641224861145 / Valid loss: 7.112370958782377
Training loss: 1.0839052200317383 / Valid loss: 7.890890126001267
Training loss: 0.4528771638870239 / Valid loss: 7.022931475866408
Training loss: 0.7280521392822266 / Valid loss: 7.212556085132417
Training loss: 0.5958542227745056 / Valid loss: 7.206522859845843

Epoch: 35
Training loss: 0.6807767152786255 / Valid loss: 8.414259574526833
Training loss: 0.7233390808105469 / Valid loss: 7.10902715410505
Training loss: 1.0911281108856201 / Valid loss: 7.860506507328578
Training loss: 0.6700342297554016 / Valid loss: 7.528869915008545
Training loss: 1.1868035793304443 / Valid loss: 8.535571843101865

Epoch: 36
Training loss: 0.7682154178619385 / Valid loss: 7.161772809709821
Training loss: 0.7640564441680908 / Valid loss: 7.157017889476958
Training loss: 0.4826373755931854 / Valid loss: 7.215681403023856
Training loss: 0.6128567457199097 / Valid loss: 7.51322614124843
Training loss: 0.9025854468345642 / Valid loss: 7.680446647462391

Epoch: 37
Training loss: 0.9881739616394043 / Valid loss: 7.313333665756952
Training loss: 0.6574206352233887 / Valid loss: 8.03755862372262
Training loss: 0.7089319825172424 / Valid loss: 7.294422649201893
Training loss: 0.34212997555732727 / Valid loss: 7.094467487789336
Training loss: 0.8537850975990295 / Valid loss: 7.74343977428618

Epoch: 38
Training loss: 0.5141117572784424 / Valid loss: 7.241118494669596
Training loss: 0.6086757183074951 / Valid loss: 7.285800565992083
Training loss: 0.4511145353317261 / Valid loss: 7.369636649177188
Training loss: 1.0732344388961792 / Valid loss: 7.543777697426933
Training loss: 1.3650381565093994 / Valid loss: 10.43611971537272

Epoch: 39
Training loss: 0.6048872470855713 / Valid loss: 8.463032908666701
Training loss: 0.9196205735206604 / Valid loss: 9.685310032254174
Training loss: 0.6146384477615356 / Valid loss: 7.6159821782793315
Training loss: 0.4612047076225281 / Valid loss: 7.315154602414085
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.382941259656634
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)

Epoch: 0
Training loss: 19.044994354248047 / Valid loss: 11.605803235371907
Model is saved in epoch 0, overall batch: 0
Training loss: 6.233587265014648 / Valid loss: 5.6877159527369905
Model is saved in epoch 0, overall batch: 100
Training loss: 8.956886291503906 / Valid loss: 6.622720209757487
Training loss: 6.104818344116211 / Valid loss: 5.5687253974732895
Model is saved in epoch 0, overall batch: 300
Training loss: 5.278043746948242 / Valid loss: 7.2043254261925105

Epoch: 1
Training loss: 6.830215930938721 / Valid loss: 5.590337753295898
Training loss: 6.109194755554199 / Valid loss: 6.903700687771752
Training loss: 7.234245777130127 / Valid loss: 5.82455054918925
Training loss: 4.501651763916016 / Valid loss: 5.536232619058518
Model is saved in epoch 1, overall batch: 800
Training loss: 4.593646049499512 / Valid loss: 5.567711214792161

Epoch: 2
Training loss: 5.865692138671875 / Valid loss: 5.658770677021572
Training loss: 4.678683280944824 / Valid loss: 5.758566286450341
Training loss: 5.74685525894165 / Valid loss: 5.608671220143636
Training loss: 3.720409631729126 / Valid loss: 5.516275882720947
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.305973052978516 / Valid loss: 5.584481568563552

Epoch: 3
Training loss: 6.397364616394043 / Valid loss: 7.340221745627267
Training loss: 3.5680248737335205 / Valid loss: 5.522664431163243
Training loss: 7.928363800048828 / Valid loss: 5.6984667936960856
Training loss: 4.823384761810303 / Valid loss: 5.579473150344122
Training loss: 6.852466106414795 / Valid loss: 5.866765460513887

Epoch: 4
Training loss: 6.533152103424072 / Valid loss: 5.8073187283107215
Training loss: 5.8464202880859375 / Valid loss: 6.906640818005516
Training loss: 8.197744369506836 / Valid loss: 6.6357269332522435
Training loss: 4.234875202178955 / Valid loss: 5.558915267671858
Training loss: 6.440201759338379 / Valid loss: 5.8595650718325665

Epoch: 5
Training loss: 8.43641471862793 / Valid loss: 6.7196505274091445
Training loss: 4.9477996826171875 / Valid loss: 6.170750904083252
Training loss: 5.98702335357666 / Valid loss: 6.21905092284793
Training loss: 6.48671817779541 / Valid loss: 6.58290175256275
Training loss: 6.5802388191223145 / Valid loss: 6.555798825763521

Epoch: 6
Training loss: 5.147722244262695 / Valid loss: 5.786156191144671
Training loss: 5.493581771850586 / Valid loss: 5.59947992960612
Training loss: 5.899710655212402 / Valid loss: 6.493518298012869
Training loss: 4.31746768951416 / Valid loss: 5.642876550129482
Training loss: 7.172349452972412 / Valid loss: 5.7266990752447215

Epoch: 7
Training loss: 5.477851867675781 / Valid loss: 6.393417921520415
Training loss: 4.094450950622559 / Valid loss: 5.570412215732393
Training loss: 6.077883720397949 / Valid loss: 5.5666077636537095
Training loss: 5.927037715911865 / Valid loss: 5.662841665177118
Training loss: 5.029651641845703 / Valid loss: 5.547769526072911

Epoch: 8
Training loss: 5.203245162963867 / Valid loss: 5.651392848151071
Training loss: 4.287456035614014 / Valid loss: 5.856799416314988
Training loss: 4.227967262268066 / Valid loss: 5.927859131495158
Training loss: 5.034282684326172 / Valid loss: 5.5950026671091715
Training loss: 2.7079224586486816 / Valid loss: 5.573429670787993

Epoch: 9
Training loss: 3.8609728813171387 / Valid loss: 5.623851596741449
Training loss: 5.114484786987305 / Valid loss: 5.532989917482649
Training loss: 6.079976558685303 / Valid loss: 7.0948711803981235
Training loss: 6.05258846282959 / Valid loss: 5.587150682721819

Epoch: 10
Training loss: 3.675558090209961 / Valid loss: 5.826813525245303
Training loss: 4.768142223358154 / Valid loss: 5.652700907843453
Training loss: 5.309962272644043 / Valid loss: 5.741624539239066
Training loss: 5.626169204711914 / Valid loss: 5.659780000504993
Training loss: 4.5319504737854 / Valid loss: 5.56618481363569

Epoch: 11
Training loss: 5.256104469299316 / Valid loss: 5.5605287302108035
Training loss: 5.382481575012207 / Valid loss: 5.631904988061814
Training loss: 4.7153730392456055 / Valid loss: 6.05427519934518
Training loss: 8.08288288116455 / Valid loss: 5.966754967825754
Training loss: 5.055109977722168 / Valid loss: 5.608443544024513

Epoch: 12
Training loss: 5.645113945007324 / Valid loss: 5.90388555980864
Training loss: 6.477214813232422 / Valid loss: 5.725001425970168
Training loss: 3.318206310272217 / Valid loss: 5.566337483269828
Training loss: 7.468264102935791 / Valid loss: 6.632022199176607
Training loss: 4.171628952026367 / Valid loss: 5.678161332720802

Epoch: 13
Training loss: 4.677181243896484 / Valid loss: 5.572214401335943
Training loss: 4.708996772766113 / Valid loss: 6.083573809124174
Training loss: 5.760586261749268 / Valid loss: 6.1308925901140485
Training loss: 5.77506685256958 / Valid loss: 5.923007181712559
Training loss: 5.067523002624512 / Valid loss: 5.6042783737182615

Epoch: 14
Training loss: 6.0380635261535645 / Valid loss: 5.607392034076509
Training loss: 5.212264060974121 / Valid loss: 6.096261206127348
Training loss: 6.4772491455078125 / Valid loss: 6.143990484873454
Training loss: 3.5006978511810303 / Valid loss: 5.600411451430547
Training loss: 5.698729991912842 / Valid loss: 6.181932235899426

Epoch: 15
Training loss: 7.542665481567383 / Valid loss: 6.313969761984689
Training loss: 5.362405776977539 / Valid loss: 5.7362406617119195
Training loss: 4.465013027191162 / Valid loss: 5.784387922286987
Training loss: 5.467430114746094 / Valid loss: 6.099666926974342
Training loss: 5.061066627502441 / Valid loss: 5.7661211286272325

Epoch: 16
Training loss: 4.883539199829102 / Valid loss: 5.6205765133812315
Training loss: 3.9475603103637695 / Valid loss: 5.789505617959159
Training loss: 5.191761016845703 / Valid loss: 5.673306617282686
Training loss: 6.204493999481201 / Valid loss: 5.9248096466064455
Training loss: 4.815020561218262 / Valid loss: 5.733182294028146

Epoch: 17
Training loss: 7.310279369354248 / Valid loss: 6.609887649899437
Training loss: 5.048857688903809 / Valid loss: 5.626484453110468
Training loss: 4.90383243560791 / Valid loss: 6.623116177604312
Training loss: 4.7015838623046875 / Valid loss: 5.904196123849778
Training loss: 6.708926200866699 / Valid loss: 6.206314436594645

Epoch: 18
Training loss: 5.302820205688477 / Valid loss: 5.72102515129816
Training loss: 4.712104320526123 / Valid loss: 5.919375896453857
Training loss: 6.733667373657227 / Valid loss: 6.007576486042567
Training loss: 4.087976455688477 / Valid loss: 5.879420257750011
Training loss: 5.991967678070068 / Valid loss: 6.400280096417382

Epoch: 19
Training loss: 4.020490646362305 / Valid loss: 5.600616902396792
Training loss: 4.263517379760742 / Valid loss: 5.692392515000843
Training loss: 4.7462568283081055 / Valid loss: 5.913887180600848
Training loss: 4.60718297958374 / Valid loss: 5.654750885282244

Epoch: 20
Training loss: 4.818239688873291 / Valid loss: 5.706610906691778
Training loss: 4.9601030349731445 / Valid loss: 5.605382760365804
Training loss: 3.135228157043457 / Valid loss: 5.804007089705694
Training loss: 3.841580390930176 / Valid loss: 5.826805353164673
Training loss: 5.641953468322754 / Valid loss: 6.198301051911853

Epoch: 21
Training loss: 4.688629150390625 / Valid loss: 5.63499421846299
Training loss: 6.4580230712890625 / Valid loss: 5.681576002211798
Training loss: 5.643385887145996 / Valid loss: 5.591377017611549
Training loss: 7.5972442626953125 / Valid loss: 6.19975110008603
Training loss: 6.441099166870117 / Valid loss: 5.616251718430292

Epoch: 22
Training loss: 5.397235870361328 / Valid loss: 5.595166785376413
Training loss: 4.263177871704102 / Valid loss: 5.987125478472028
Training loss: 6.3736467361450195 / Valid loss: 6.479333080564227
Training loss: 5.637017250061035 / Valid loss: 5.754922167460124
Training loss: 4.293376922607422 / Valid loss: 5.596246262959071

Epoch: 23
Training loss: 4.202808380126953 / Valid loss: 5.914879303886777
Training loss: 6.216846466064453 / Valid loss: 5.6223649955931165
Training loss: 6.93431282043457 / Valid loss: 6.794202845437186
Training loss: 4.8869309425354 / Valid loss: 6.304131862095424
Training loss: 7.144604682922363 / Valid loss: 5.854468620391119

Epoch: 24
Training loss: 6.222475051879883 / Valid loss: 5.614241781688872
Training loss: 5.109199523925781 / Valid loss: 5.656102700460525
Training loss: 5.8730149269104 / Valid loss: 5.903124868302118
Training loss: 7.265566825866699 / Valid loss: 5.6949914387294225
Training loss: 3.1198339462280273 / Valid loss: 5.939079068955921

Epoch: 25
Training loss: 3.6499381065368652 / Valid loss: 5.619322079703921
Training loss: 4.7351765632629395 / Valid loss: 6.090116950443813
Training loss: 3.295076608657837 / Valid loss: 5.808775048028855
Training loss: 7.251196384429932 / Valid loss: 5.765037874948411
Training loss: 3.5082905292510986 / Valid loss: 5.692767622357323

Epoch: 26
Training loss: 5.520445823669434 / Valid loss: 5.857924931389945
Training loss: 5.945540428161621 / Valid loss: 5.627878990627471
Training loss: 6.008176326751709 / Valid loss: 6.013500481560117
Training loss: 5.612238883972168 / Valid loss: 5.593208387919835
Training loss: 5.895040035247803 / Valid loss: 6.169143642698016

Epoch: 27
Training loss: 7.878345966339111 / Valid loss: 5.771604533422561
Training loss: 3.800182342529297 / Valid loss: 5.797807579948788
Training loss: 5.039762496948242 / Valid loss: 5.588474457604544
Training loss: 6.272525787353516 / Valid loss: 5.904419835408529
Training loss: 5.91379976272583 / Valid loss: 5.6839205923534575

Epoch: 28
Training loss: 4.336751937866211 / Valid loss: 5.643369438534691
Training loss: 3.8723649978637695 / Valid loss: 5.758279589244298
Training loss: 4.889076232910156 / Valid loss: 5.794171710241408
Training loss: 4.564959526062012 / Valid loss: 5.774090460368565
Training loss: 4.856571197509766 / Valid loss: 5.805958573023478

Epoch: 29
Training loss: 6.126382827758789 / Valid loss: 6.325887512025379
Training loss: 4.7741546630859375 / Valid loss: 5.800983978453137
Training loss: 6.333512306213379 / Valid loss: 5.763782973516555
Training loss: 6.408913612365723 / Valid loss: 5.638013950983683

Epoch: 30
Training loss: 5.8819966316223145 / Valid loss: 5.751700078873407
Training loss: 5.228111743927002 / Valid loss: 5.68864936601548
Training loss: 4.56832218170166 / Valid loss: 5.907743874050322
Training loss: 5.918269157409668 / Valid loss: 6.506429962884813
Training loss: 4.50120735168457 / Valid loss: 5.704903781981695

Epoch: 31
Training loss: 5.2229509353637695 / Valid loss: 5.665068335760207
Training loss: 5.248996734619141 / Valid loss: 5.636965451921736
Training loss: 4.419496536254883 / Valid loss: 5.633541804268247
Training loss: 4.910571575164795 / Valid loss: 5.673246712911697
Training loss: 6.287589073181152 / Valid loss: 5.718127641223726

Epoch: 32
Training loss: 5.862462043762207 / Valid loss: 5.653569246473767
Training loss: 5.908127784729004 / Valid loss: 5.88056366784232
Training loss: 4.933133125305176 / Valid loss: 5.738030442737398
Training loss: 4.100588321685791 / Valid loss: 5.66922185080392
Training loss: 7.525920867919922 / Valid loss: 5.622197230656941

Epoch: 33
Training loss: 3.8701677322387695 / Valid loss: 5.625374798547654
Training loss: 6.629280090332031 / Valid loss: 5.795474486123948
Training loss: 7.193123817443848 / Valid loss: 7.377545502072289
Training loss: 5.5168538093566895 / Valid loss: 5.660670028414045
Training loss: 6.612575531005859 / Valid loss: 5.750549479893276

Epoch: 34
Training loss: 5.894913673400879 / Valid loss: 6.1731927258627755
Training loss: 6.4904656410217285 / Valid loss: 6.001151616232736
Training loss: 6.136376857757568 / Valid loss: 7.050502940586635
Training loss: 5.878281593322754 / Valid loss: 6.182113938104539
Training loss: 3.811739444732666 / Valid loss: 5.971574365525019

Epoch: 35
Training loss: 5.4219970703125 / Valid loss: 5.714378711155483
Training loss: 4.389300346374512 / Valid loss: 5.740226282392229
Training loss: 5.78460693359375 / Valid loss: 5.766556694394066
Training loss: 4.696505546569824 / Valid loss: 5.64801504271371
Training loss: 4.478883743286133 / Valid loss: 5.642313652946836

Epoch: 36
Training loss: 3.875303268432617 / Valid loss: 6.733918417067755
Training loss: 6.571816444396973 / Valid loss: 5.6376824401673815
Training loss: 3.894845962524414 / Valid loss: 6.0234915551685155
Training loss: 6.470118522644043 / Valid loss: 6.454912065324329
Training loss: 4.92977237701416 / Valid loss: 6.425132578895205

Epoch: 37
Training loss: 5.505114555358887 / Valid loss: 6.179086814607893
Training loss: 3.8298258781433105 / Valid loss: 5.665137288683937
Training loss: 4.687688827514648 / Valid loss: 5.906090295882452
Training loss: 6.1500563621521 / Valid loss: 5.815752740133377
Training loss: 5.639581680297852 / Valid loss: 5.6493039948599675

Epoch: 38
Training loss: 5.3923139572143555 / Valid loss: 5.681745828901018
Training loss: 6.763485908508301 / Valid loss: 6.500005926404681
Training loss: 5.479387283325195 / Valid loss: 5.659573441460019
Training loss: 4.815489292144775 / Valid loss: 5.799389237449283
Training loss: 5.212503433227539 / Valid loss: 6.276879608063471

Epoch: 39
Training loss: 4.415060043334961 / Valid loss: 5.833248504002889
Training loss: 5.258335113525391 / Valid loss: 6.557717904590425
Training loss: 6.478341579437256 / Valid loss: 6.194249139513288
Training loss: 4.929792404174805 / Valid loss: 5.757008704685029
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1300): 5.359580868766422
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)

Epoch: 0
Training loss: 19.044994354248047 / Valid loss: 11.605803217206683
Model is saved in epoch 0, overall batch: 0
Training loss: 6.233587265014648 / Valid loss: 5.68771600269136
Model is saved in epoch 0, overall batch: 100
Training loss: 8.95688533782959 / Valid loss: 6.622719991774786
Training loss: 6.104818344116211 / Valid loss: 5.568725454239618
Model is saved in epoch 0, overall batch: 300
Training loss: 5.278044700622559 / Valid loss: 7.20432580993289

Epoch: 1
Training loss: 6.830216407775879 / Valid loss: 5.590337730589367
Training loss: 6.10919713973999 / Valid loss: 6.903702177320208
Training loss: 7.234245777130127 / Valid loss: 5.824550819396973
Training loss: 4.501652717590332 / Valid loss: 5.536232662200928
Model is saved in epoch 1, overall batch: 800
Training loss: 4.59364652633667 / Valid loss: 5.567711096718198

Epoch: 2
Training loss: 5.865693092346191 / Valid loss: 5.658770899545579
Training loss: 4.678684234619141 / Valid loss: 5.7585673332214355
Training loss: 5.746857643127441 / Valid loss: 5.608671049844651
Training loss: 3.720409870147705 / Valid loss: 5.5162756670089
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.305972099304199 / Valid loss: 5.584481332415626

Epoch: 3
Training loss: 6.39736795425415 / Valid loss: 7.3402229536147345
Training loss: 3.5680251121520996 / Valid loss: 5.522664158684867
Training loss: 7.928360462188721 / Valid loss: 5.69846556527274
Training loss: 4.823387622833252 / Valid loss: 5.579471992311023
Training loss: 6.852465629577637 / Valid loss: 5.866764272962298

Epoch: 4
Training loss: 6.533155918121338 / Valid loss: 5.807318585259574
Training loss: 5.8464202880859375 / Valid loss: 6.906637196313767
Training loss: 8.197744369506836 / Valid loss: 6.635725947788783
Training loss: 4.234875679016113 / Valid loss: 5.558914366222563
Training loss: 6.440204620361328 / Valid loss: 5.8595659528459825

Epoch: 5
Training loss: 8.436423301696777 / Valid loss: 6.719653747195289
Training loss: 4.947800636291504 / Valid loss: 6.170751385461717
Training loss: 5.987026214599609 / Valid loss: 6.219048515955607
Training loss: 6.486722469329834 / Valid loss: 6.582904942830404
Training loss: 6.580243110656738 / Valid loss: 6.555799783979143

Epoch: 6
Training loss: 5.147727012634277 / Valid loss: 5.786154326938448
Training loss: 5.493593692779541 / Valid loss: 5.5994788237980435
Training loss: 5.899714469909668 / Valid loss: 6.493516054607573
Training loss: 4.31746768951416 / Valid loss: 5.642874011539278
Training loss: 7.172344207763672 / Valid loss: 5.726695194698515

Epoch: 7
Training loss: 5.47785758972168 / Valid loss: 6.393416895185198
Training loss: 4.094455718994141 / Valid loss: 5.570410841987247
Training loss: 6.077879905700684 / Valid loss: 5.566605279559181
Training loss: 5.927042484283447 / Valid loss: 5.66283966700236
Training loss: 5.029657363891602 / Valid loss: 5.547766703651065

Epoch: 8
Training loss: 5.203247547149658 / Valid loss: 5.651391887664795
Training loss: 4.2874555587768555 / Valid loss: 5.856800254185995
Training loss: 4.227963447570801 / Valid loss: 5.927857591992333
Training loss: 5.034285545349121 / Valid loss: 5.594999737966628
Training loss: 2.707923650741577 / Valid loss: 5.573426596323649

Epoch: 9
Training loss: 3.8609724044799805 / Valid loss: 5.6238506862095425
Training loss: 5.1144914627075195 / Valid loss: 5.532987272171747
Training loss: 6.079977989196777 / Valid loss: 7.094862910679408
Training loss: 6.052596092224121 / Valid loss: 5.587146404811314

Epoch: 10
Training loss: 3.675564765930176 / Valid loss: 5.826810891287668
Training loss: 4.768149375915527 / Valid loss: 5.652697837920416
Training loss: 5.30996561050415 / Valid loss: 5.741619017010644
Training loss: 5.626172065734863 / Valid loss: 5.659776349294753
Training loss: 4.5319504737854 / Valid loss: 5.566180712836129

Epoch: 11
Training loss: 5.256111145019531 / Valid loss: 5.560525308336531
Training loss: 5.38248348236084 / Valid loss: 5.631901974905105
Training loss: 4.715381622314453 / Valid loss: 6.05427067166283
Training loss: 8.082886695861816 / Valid loss: 5.966748637244815
Training loss: 5.055108070373535 / Valid loss: 5.60843825340271

Epoch: 12
Training loss: 5.645116329193115 / Valid loss: 5.903879765101841
Training loss: 6.477218151092529 / Valid loss: 5.724996796108427
Training loss: 3.318208694458008 / Valid loss: 5.566332671755836
Training loss: 7.468267440795898 / Valid loss: 6.632013861338297
Training loss: 4.171638488769531 / Valid loss: 5.678158812295823

Epoch: 13
Training loss: 4.677188396453857 / Valid loss: 5.57220923332941
Training loss: 4.709011077880859 / Valid loss: 6.083568702425276
Training loss: 5.760602951049805 / Valid loss: 6.130892708188012
Training loss: 5.7750654220581055 / Valid loss: 5.923003362473987
Training loss: 5.067526340484619 / Valid loss: 5.60427363486517

Epoch: 14
Training loss: 6.038066864013672 / Valid loss: 5.607386248452323
Training loss: 5.212271690368652 / Valid loss: 6.096251356034052
Training loss: 6.477280139923096 / Valid loss: 6.143996211460658
Training loss: 3.500702381134033 / Valid loss: 5.600405545461745
Training loss: 5.698763847351074 / Valid loss: 6.181931373051235

Epoch: 15
Training loss: 7.542675971984863 / Valid loss: 6.313964859644572
Training loss: 5.362399101257324 / Valid loss: 5.73623169263204
Training loss: 4.46502161026001 / Valid loss: 5.7843803178696405
Training loss: 5.467462062835693 / Valid loss: 6.099670873369489
Training loss: 5.061073303222656 / Valid loss: 5.766115134102957

Epoch: 16
Training loss: 4.8835344314575195 / Valid loss: 5.6205677259536015
Training loss: 3.9475643634796143 / Valid loss: 5.789497632072085
Training loss: 5.191768646240234 / Valid loss: 5.67330170131865
Training loss: 6.204511642456055 / Valid loss: 5.924804521742321
Training loss: 4.815033435821533 / Valid loss: 5.733175032479423

Epoch: 17
Training loss: 7.310285568237305 / Valid loss: 6.609876818883986
Training loss: 5.048862457275391 / Valid loss: 5.626474807375954
Training loss: 4.903851509094238 / Valid loss: 6.623115282966977
Training loss: 4.701584815979004 / Valid loss: 5.9041900657472155
Training loss: 6.708946228027344 / Valid loss: 6.206309863499233

Epoch: 18
Training loss: 5.30283784866333 / Valid loss: 5.721015977859497
Training loss: 4.712128639221191 / Valid loss: 5.919366159893218
Training loss: 6.733675003051758 / Valid loss: 6.007566093263172
Training loss: 4.087984561920166 / Valid loss: 5.879410534813291
Training loss: 5.991982460021973 / Valid loss: 6.400281361171177

Epoch: 19
Training loss: 4.020491600036621 / Valid loss: 5.600607683545067
Training loss: 4.263535499572754 / Valid loss: 5.692378802526565
Training loss: 4.746255874633789 / Valid loss: 5.913876885459537
Training loss: 4.607193470001221 / Valid loss: 5.6547408194769

Epoch: 20
Training loss: 4.818246364593506 / Valid loss: 5.7066025529588975
Training loss: 4.9601030349731445 / Valid loss: 5.605372880754016
Training loss: 3.1352245807647705 / Valid loss: 5.803998057047526
Training loss: 3.8415796756744385 / Valid loss: 5.8267940952664325
Training loss: 5.641980171203613 / Valid loss: 6.198297082810175

Epoch: 21
Training loss: 4.688637733459473 / Valid loss: 5.634983619054158
Training loss: 6.458018779754639 / Valid loss: 5.681566438220796
Training loss: 5.643411636352539 / Valid loss: 5.59136602083842
Training loss: 7.597234725952148 / Valid loss: 6.199747439793178
Training loss: 6.441114902496338 / Valid loss: 5.616239297957647

Epoch: 22
Training loss: 5.397256374359131 / Valid loss: 5.5951538676307315
Training loss: 4.263194561004639 / Valid loss: 5.987122755958921
Training loss: 6.373666763305664 / Valid loss: 6.479323178245908
Training loss: 5.637028694152832 / Valid loss: 5.7549131393432615
Training loss: 4.293374061584473 / Valid loss: 5.596234094528925

Epoch: 23
Training loss: 4.202826499938965 / Valid loss: 5.9148724692208425
Training loss: 6.216841220855713 / Valid loss: 5.622352840786888
Training loss: 6.934323310852051 / Valid loss: 6.794198860440935
Training loss: 4.886955738067627 / Valid loss: 6.304124593734741
Training loss: 7.144615173339844 / Valid loss: 5.854458704448882

Epoch: 24
Training loss: 6.222524642944336 / Valid loss: 5.614229079655239
Training loss: 5.109203338623047 / Valid loss: 5.656089780444191
Training loss: 5.873028755187988 / Valid loss: 5.9031134060450965
Training loss: 7.2655839920043945 / Valid loss: 5.694975787117368
Training loss: 3.1198370456695557 / Valid loss: 5.939073746544974

Epoch: 25
Training loss: 3.649962902069092 / Valid loss: 5.619309482120332
Training loss: 4.735198020935059 / Valid loss: 6.090113117581322
Training loss: 3.2951064109802246 / Valid loss: 5.808765804199945
Training loss: 7.251197814941406 / Valid loss: 5.765018331436884
Training loss: 3.50830078125 / Valid loss: 5.692754654657273

Epoch: 26
Training loss: 5.520443916320801 / Valid loss: 5.857910889670962
Training loss: 5.945550918579102 / Valid loss: 5.627864288148426
Training loss: 6.008199691772461 / Valid loss: 6.013489303134737
Training loss: 5.612242221832275 / Valid loss: 5.593193031492687
Training loss: 5.895049095153809 / Valid loss: 6.16914063181196

Epoch: 27
Training loss: 7.878322601318359 / Valid loss: 5.77158145904541
Training loss: 3.8001959323883057 / Valid loss: 5.797796217600505
Training loss: 5.0397725105285645 / Valid loss: 5.5884591397785
Training loss: 6.272524833679199 / Valid loss: 5.904396297818138
Training loss: 5.913836479187012 / Valid loss: 5.683909332184564

Epoch: 28
Training loss: 4.336744785308838 / Valid loss: 5.643352117992583
Training loss: 3.872373342514038 / Valid loss: 5.75825693720863
Training loss: 4.8890814781188965 / Valid loss: 5.794154664448329
Training loss: 4.56497859954834 / Valid loss: 5.774078725633167
Training loss: 4.856569766998291 / Valid loss: 5.805937857854934

Epoch: 29
Training loss: 6.1263957023620605 / Valid loss: 6.325880468459356
Training loss: 4.774148464202881 / Valid loss: 5.800971224194481
Training loss: 6.333532333374023 / Valid loss: 5.763772623879569
Training loss: 6.408919334411621 / Valid loss: 5.637997897466024

Epoch: 30
Training loss: 5.882004261016846 / Valid loss: 5.751685274214972
Training loss: 5.22812032699585 / Valid loss: 5.688633128574916
Training loss: 4.568360805511475 / Valid loss: 5.9077258291698636
Training loss: 5.918262481689453 / Valid loss: 6.506422001974923
Training loss: 4.501229763031006 / Valid loss: 5.7048835254850845

Epoch: 31
Training loss: 5.2229437828063965 / Valid loss: 5.665050116039458
Training loss: 5.249007225036621 / Valid loss: 5.636945272627331
Training loss: 4.419498443603516 / Valid loss: 5.6335228783743725
Training loss: 4.910574913024902 / Valid loss: 5.673227964128767
Training loss: 6.287613868713379 / Valid loss: 5.718108678999402

Epoch: 32
Training loss: 5.862471103668213 / Valid loss: 5.6535499595460434
Training loss: 5.908181190490723 / Valid loss: 5.880543870017642
Training loss: 4.933132171630859 / Valid loss: 5.738008165359497
Training loss: 4.1006083488464355 / Valid loss: 5.669202121098836
Training loss: 7.525910377502441 / Valid loss: 5.62217705363319

Epoch: 33
Training loss: 3.870176315307617 / Valid loss: 5.625354909896851
Training loss: 6.629340648651123 / Valid loss: 5.795459129696801
Training loss: 7.193102836608887 / Valid loss: 7.377512132553827
Training loss: 5.516879081726074 / Valid loss: 5.660651331856137
Training loss: 6.612584114074707 / Valid loss: 5.750526453199841

Epoch: 34
Training loss: 5.894989490509033 / Valid loss: 6.173178184600103
Training loss: 6.490460395812988 / Valid loss: 6.001124352500552
Training loss: 6.13638973236084 / Valid loss: 7.050466187795004
Training loss: 5.878287315368652 / Valid loss: 6.182084219796317
Training loss: 3.8117408752441406 / Valid loss: 5.971548307509649

Epoch: 35
Training loss: 5.422003746032715 / Valid loss: 5.714364085878644
Training loss: 4.389334201812744 / Valid loss: 5.74020414352417
Training loss: 5.784628868103027 / Valid loss: 5.766539028712681
Training loss: 4.696538925170898 / Valid loss: 5.64799253372919
Training loss: 4.478877544403076 / Valid loss: 5.642290421894619

Epoch: 36
Training loss: 3.8753178119659424 / Valid loss: 6.733904711405436
Training loss: 6.571828842163086 / Valid loss: 5.637658984320504
Training loss: 3.8948447704315186 / Valid loss: 6.023469804582142
Training loss: 6.470149040222168 / Valid loss: 6.454883461906796
Training loss: 4.929774284362793 / Valid loss: 6.425124000367664

Epoch: 37
Training loss: 5.505101203918457 / Valid loss: 6.179056776137579
Training loss: 3.829845905303955 / Valid loss: 5.66511394182841
Training loss: 4.687721252441406 / Valid loss: 5.90606358391898
Training loss: 6.1500444412231445 / Valid loss: 5.815722045444307
Training loss: 5.639596939086914 / Valid loss: 5.649280021304176

Epoch: 38
Training loss: 5.392325401306152 / Valid loss: 5.68172090167091
Training loss: 6.763522624969482 / Valid loss: 6.499992061796642
Training loss: 5.479423522949219 / Valid loss: 5.659549647285825
Training loss: 4.815499305725098 / Valid loss: 5.799363070442563
Training loss: 5.212509632110596 / Valid loss: 6.276862317039853

Epoch: 39
Training loss: 4.415077209472656 / Valid loss: 5.833226610365368
Training loss: 5.258366584777832 / Valid loss: 6.557692618597121
Training loss: 6.478342533111572 / Valid loss: 6.1942347799028665
Training loss: 4.929831027984619 / Valid loss: 5.756988107590448
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1300): 5.359580739339193
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.326356043134417
Model is saved in epoch 0, overall batch: 0
Training loss: 9.031549453735352 / Valid loss: 12.925361442565919
Model is saved in epoch 0, overall batch: 100
Training loss: 9.649495124816895 / Valid loss: 11.93502668199085
Model is saved in epoch 0, overall batch: 200
Training loss: 14.655001640319824 / Valid loss: 11.029834052494595
Model is saved in epoch 0, overall batch: 300
Training loss: 10.413329124450684 / Valid loss: 10.265079829806373
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 8.910350799560547 / Valid loss: 9.613765380496071
Model is saved in epoch 1, overall batch: 500
Training loss: 7.872617721557617 / Valid loss: 8.97424146107265
Model is saved in epoch 1, overall batch: 600
Training loss: 7.332974433898926 / Valid loss: 8.527550057002477
Model is saved in epoch 1, overall batch: 700
Training loss: 7.229619026184082 / Valid loss: 8.04958698181879
Model is saved in epoch 1, overall batch: 800
Training loss: 5.423467636108398 / Valid loss: 7.773928292592367
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 7.810892105102539 / Valid loss: 7.546292895362491
Model is saved in epoch 2, overall batch: 1000
Training loss: 7.799148082733154 / Valid loss: 7.289633210500082
Model is saved in epoch 2, overall batch: 1100
Training loss: 7.037626266479492 / Valid loss: 7.038455272856213
Model is saved in epoch 2, overall batch: 1200
Training loss: 8.314278602600098 / Valid loss: 6.658316196714129
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.37082576751709 / Valid loss: 6.479670063654582
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.279825687408447 / Valid loss: 6.456323323931013
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.787858724594116 / Valid loss: 6.2811033793858115
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.567384719848633 / Valid loss: 6.132880061013358
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.074599266052246 / Valid loss: 6.027197063536871
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.374438285827637 / Valid loss: 5.917620524905977
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 5.37590217590332 / Valid loss: 6.066183928080967
Training loss: 3.7718729972839355 / Valid loss: 6.084708586193266
Training loss: 4.1693549156188965 / Valid loss: 5.980021408626011
Training loss: 4.449491500854492 / Valid loss: 5.9436943780808225
Training loss: 3.1457338333129883 / Valid loss: 5.8985951628003805
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 6.034307479858398 / Valid loss: 5.8190965062096005
Model is saved in epoch 5, overall batch: 2500
Training loss: 4.65392541885376 / Valid loss: 5.883803662799654
Training loss: 3.245314359664917 / Valid loss: 5.882777790796189
Training loss: 3.8647522926330566 / Valid loss: 5.8596199580601285
Training loss: 3.6135709285736084 / Valid loss: 5.8016817705971855
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 5.956195831298828 / Valid loss: 5.851204438436599
Training loss: 3.4913218021392822 / Valid loss: 5.922347840808687
Training loss: 3.9809350967407227 / Valid loss: 5.927971462976365
Training loss: 3.54435396194458 / Valid loss: 5.909689396903628
Training loss: 4.117196559906006 / Valid loss: 6.0334273542676655

Epoch: 7
Training loss: 3.502049446105957 / Valid loss: 5.933380254109701
Training loss: 3.139695644378662 / Valid loss: 5.993251664297921
Training loss: 2.697209358215332 / Valid loss: 5.977835546221052
Training loss: 4.136932373046875 / Valid loss: 6.063802925745646
Training loss: 3.038693428039551 / Valid loss: 6.007130838575817

Epoch: 8
Training loss: 2.8324384689331055 / Valid loss: 6.0750229494912285
Training loss: 1.8436771631240845 / Valid loss: 6.078082086926415
Training loss: 3.234579086303711 / Valid loss: 6.108758088520595
Training loss: 4.641075134277344 / Valid loss: 6.153123925981068
Training loss: 2.8398232460021973 / Valid loss: 6.134172952742803

Epoch: 9
Training loss: 1.5098446607589722 / Valid loss: 6.158533652623494
Training loss: 2.5671896934509277 / Valid loss: 6.18920902070545
Training loss: 3.6185734272003174 / Valid loss: 6.201976576305571
Training loss: 2.732630968093872 / Valid loss: 6.195091059094383

Epoch: 10
Training loss: 1.832048773765564 / Valid loss: 6.237085744312831
Training loss: 2.661381244659424 / Valid loss: 6.234330043338594
Training loss: 1.560402512550354 / Valid loss: 6.398729760306222
Training loss: 2.305189609527588 / Valid loss: 6.391875509988694
Training loss: 2.492584705352783 / Valid loss: 6.298088019234793

Epoch: 11
Training loss: 1.2297296524047852 / Valid loss: 6.3910364877609975
Training loss: 1.4975640773773193 / Valid loss: 6.3959820860908145
Training loss: 2.548607349395752 / Valid loss: 6.465189738500686
Training loss: 1.4185914993286133 / Valid loss: 6.6187856447129025
Training loss: 2.128636360168457 / Valid loss: 6.666912732805525

Epoch: 12
Training loss: 1.1633267402648926 / Valid loss: 6.452874792189825
Training loss: 1.2297829389572144 / Valid loss: 6.56710113797869
Training loss: 1.5909547805786133 / Valid loss: 6.575278670447213
Training loss: 1.3911828994750977 / Valid loss: 6.523057824089413
Training loss: 1.3432307243347168 / Valid loss: 6.534570619038173

Epoch: 13
Training loss: 0.9551684260368347 / Valid loss: 6.706716859908331
Training loss: 0.8917296528816223 / Valid loss: 6.6589717774164106
Training loss: 1.1735572814941406 / Valid loss: 6.546581147965931
Training loss: 1.4201452732086182 / Valid loss: 6.681150949568975
Training loss: 1.3012744188308716 / Valid loss: 6.600960706529163

Epoch: 14
Training loss: 1.0165108442306519 / Valid loss: 6.741644266673497
Training loss: 1.0852172374725342 / Valid loss: 6.643154287338257
Training loss: 1.4932785034179688 / Valid loss: 6.633780184246245
Training loss: 1.1285059452056885 / Valid loss: 6.6558440480913434
Training loss: 1.6491352319717407 / Valid loss: 6.664465704418364

Epoch: 15
Training loss: 1.0540403127670288 / Valid loss: 6.7938585939861476
Training loss: 1.0658385753631592 / Valid loss: 6.817523004895165
Training loss: 1.0568095445632935 / Valid loss: 6.705037425813221
Training loss: 1.0983721017837524 / Valid loss: 6.749984929675148
Training loss: 1.1242650747299194 / Valid loss: 6.689923824582781

Epoch: 16
Training loss: 0.5892389416694641 / Valid loss: 6.810284650893438
Training loss: 0.7803249359130859 / Valid loss: 7.188371404012044
Training loss: 1.1032828092575073 / Valid loss: 6.764204202379499
Training loss: 1.0612361431121826 / Valid loss: 6.95405265490214
Training loss: 1.173661708831787 / Valid loss: 6.885861097063337

Epoch: 17
Training loss: 1.0085021257400513 / Valid loss: 6.822495294752575
Training loss: 0.9435634613037109 / Valid loss: 6.956481792813256
Training loss: 0.9202849864959717 / Valid loss: 6.930529340108236
Training loss: 0.666002094745636 / Valid loss: 6.919105189187186
Training loss: 1.0445382595062256 / Valid loss: 6.757153756277901

Epoch: 18
Training loss: 0.6763856410980225 / Valid loss: 6.79201413109189
Training loss: 0.8552510738372803 / Valid loss: 6.800828341075352
Training loss: 0.8908299207687378 / Valid loss: 6.922042628696986
Training loss: 0.9965186715126038 / Valid loss: 7.065425046284994
Training loss: 0.863675594329834 / Valid loss: 6.810019663402012

Epoch: 19
Training loss: 0.8063485026359558 / Valid loss: 6.918242499941871
Training loss: 1.2864010334014893 / Valid loss: 6.813279499326433
Training loss: 0.6465586423873901 / Valid loss: 6.807423300970168
Training loss: 0.6824859380722046 / Valid loss: 6.840832583109537

Epoch: 20
Training loss: 0.40538251399993896 / Valid loss: 6.810974334535144
Training loss: 1.036321997642517 / Valid loss: 6.8397407032194595
Training loss: 0.4680996537208557 / Valid loss: 6.826008274441674
Training loss: 0.6702053546905518 / Valid loss: 6.848900013878232
Training loss: 1.099815845489502 / Valid loss: 6.829094416754586

Epoch: 21
Training loss: 0.5849310159683228 / Valid loss: 6.897969468434652
Training loss: 0.7242369651794434 / Valid loss: 6.8522044408889045
Training loss: 0.6205954551696777 / Valid loss: 6.849351374308268
Training loss: 0.492944598197937 / Valid loss: 6.910346090225946
Training loss: 0.671619176864624 / Valid loss: 6.842398089454288

Epoch: 22
Training loss: 0.7165206670761108 / Valid loss: 6.86344078154791
Training loss: 0.554999589920044 / Valid loss: 6.95038419905163
Training loss: 0.4596119523048401 / Valid loss: 6.781331836609613
Training loss: 1.3692537546157837 / Valid loss: 6.841743144534883
Training loss: 0.7559306621551514 / Valid loss: 6.840181078229632

Epoch: 23
Training loss: 0.7283676862716675 / Valid loss: 6.892171362468175
Training loss: 0.7919235229492188 / Valid loss: 6.783128611246744
Training loss: 0.8206788897514343 / Valid loss: 6.887018896284557
Training loss: 0.5897453427314758 / Valid loss: 6.852277633122036
Training loss: 1.0101314783096313 / Valid loss: 6.9078188987005325

Epoch: 24
Training loss: 0.8423129320144653 / Valid loss: 6.823648264294579
Training loss: 0.6432785987854004 / Valid loss: 6.800510660807292
Training loss: 0.5377112030982971 / Valid loss: 6.923416818891253
Training loss: 0.9146564602851868 / Valid loss: 6.848580855414981
Training loss: 1.0273497104644775 / Valid loss: 6.892161977858771

Epoch: 25
Training loss: 1.0057035684585571 / Valid loss: 6.882301693870907
Training loss: 0.3921237587928772 / Valid loss: 6.856995741526286
Training loss: 0.8008273839950562 / Valid loss: 6.7691147622608
Training loss: 0.3524518609046936 / Valid loss: 6.795892313548497
Training loss: 0.5134373903274536 / Valid loss: 6.9135886056082585

Epoch: 26
Training loss: 0.5141182541847229 / Valid loss: 6.811796706063407
Training loss: 0.5204722881317139 / Valid loss: 6.847500975926717
Training loss: 0.3656955361366272 / Valid loss: 6.839329501560756
Training loss: 0.43640822172164917 / Valid loss: 6.982221017565046
Training loss: 0.4070833921432495 / Valid loss: 6.97004113424392

Epoch: 27
Training loss: 0.3686547875404358 / Valid loss: 6.825541260128929
Training loss: 0.47597256302833557 / Valid loss: 6.963418935594104
Training loss: 0.6055011749267578 / Valid loss: 6.956779216584705
Training loss: 0.4820709824562073 / Valid loss: 6.935784280867804
Training loss: 0.6080188751220703 / Valid loss: 6.914210024334135

Epoch: 28
Training loss: 0.4018572270870209 / Valid loss: 6.922458239964077
Training loss: 0.37014228105545044 / Valid loss: 6.837387693495978
Training loss: 0.7190433144569397 / Valid loss: 6.880425839197068
Training loss: 0.57194584608078 / Valid loss: 6.869017782665434
Training loss: 0.4527132511138916 / Valid loss: 6.875129699707031

Epoch: 29
Training loss: 0.6052722930908203 / Valid loss: 6.821631667727516
Training loss: 0.46156978607177734 / Valid loss: 6.808241335550944
Training loss: 0.31890395283699036 / Valid loss: 6.8673095703125
Training loss: 0.4430583119392395 / Valid loss: 6.832905601319813

Epoch: 30
Training loss: 0.4817635715007782 / Valid loss: 6.803235054016113
Training loss: 0.4514594078063965 / Valid loss: 6.817978398005168
Training loss: 0.3631715178489685 / Valid loss: 6.807428577968053
Training loss: 0.5134695768356323 / Valid loss: 6.876563944135394
Training loss: 0.4263817369937897 / Valid loss: 6.7823568571181525

Epoch: 31
Training loss: 0.5117906332015991 / Valid loss: 6.86391129266648
Training loss: 0.4331490993499756 / Valid loss: 6.831766886938186
Training loss: 0.4823755621910095 / Valid loss: 6.872081515902565
Training loss: 0.33831608295440674 / Valid loss: 6.8909860020592095
Training loss: 0.43682053685188293 / Valid loss: 6.999074477241153

Epoch: 32
Training loss: 0.5587068796157837 / Valid loss: 6.849571125847953
Training loss: 0.4467211365699768 / Valid loss: 6.856874209358579
Training loss: 0.4933033585548401 / Valid loss: 6.851555061340332
Training loss: 0.41836023330688477 / Valid loss: 6.806804557073684
Training loss: 0.5701733231544495 / Valid loss: 6.752812017713274

Epoch: 33
Training loss: 0.48938149213790894 / Valid loss: 6.830871200561523
Training loss: 0.5299113392829895 / Valid loss: 6.78384948912121
Training loss: 0.3791443407535553 / Valid loss: 6.854760578700474
Training loss: 0.3959607183933258 / Valid loss: 6.874238688605172
Training loss: 0.5073026418685913 / Valid loss: 6.915350973038446

Epoch: 34
Training loss: 0.84784996509552 / Valid loss: 6.810558641524542
Training loss: 0.3780098557472229 / Valid loss: 6.870923362459455
Training loss: 0.5260709524154663 / Valid loss: 6.858384286789667
Training loss: 0.3105500042438507 / Valid loss: 6.847722543988909
Training loss: 0.37165871262550354 / Valid loss: 6.790072752180554

Epoch: 35
Training loss: 0.8120191097259521 / Valid loss: 6.947482231685093
Training loss: 0.40832674503326416 / Valid loss: 6.791333491461618
Training loss: 0.2575708031654358 / Valid loss: 6.839314024788993
Training loss: 0.32510441541671753 / Valid loss: 6.917860930306571
Training loss: 0.3456174433231354 / Valid loss: 6.814979321616036

Epoch: 36
Training loss: 0.36534979939460754 / Valid loss: 6.851553079060146
Training loss: 0.7617155909538269 / Valid loss: 6.811295768192836
Training loss: 0.31516551971435547 / Valid loss: 6.866854703994024
Training loss: 0.33060356974601746 / Valid loss: 6.8456908725556875
Training loss: 0.41588684916496277 / Valid loss: 6.8495058332170755

Epoch: 37
Training loss: 0.42353859543800354 / Valid loss: 6.873068968454997
Training loss: 0.3280651867389679 / Valid loss: 6.7587628818693615
Training loss: 0.4312743842601776 / Valid loss: 6.870133681524368
Training loss: 0.7213282585144043 / Valid loss: 6.8087525526682535
Training loss: 0.48998257517814636 / Valid loss: 6.8538070451645625

Epoch: 38
Training loss: 0.3917848765850067 / Valid loss: 6.765246945335751
Training loss: 0.357817679643631 / Valid loss: 6.76726332846142
Training loss: 0.41595977544784546 / Valid loss: 6.762485940115792
Training loss: 0.3384285271167755 / Valid loss: 6.8026490415845595
Training loss: 0.32240891456604004 / Valid loss: 6.9024126370747885

Epoch: 39
Training loss: 0.5113993287086487 / Valid loss: 6.83139526049296
Training loss: 0.3025866746902466 / Valid loss: 6.79451983315604
Training loss: 0.3561829924583435 / Valid loss: 6.755090431939988
Training loss: 0.45146700739860535 / Valid loss: 6.957076218014672
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2900): 5.636741495132446
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.326356034051804
Model is saved in epoch 0, overall batch: 0
Training loss: 9.031548500061035 / Valid loss: 12.92536077045259
Model is saved in epoch 0, overall batch: 100
Training loss: 9.650577545166016 / Valid loss: 11.934316607883998
Model is saved in epoch 0, overall batch: 200
Training loss: 14.655125617980957 / Valid loss: 11.028078238169352
Model is saved in epoch 0, overall batch: 300
Training loss: 10.40846061706543 / Valid loss: 10.262705943697975
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 8.91086196899414 / Valid loss: 9.611239714849562
Model is saved in epoch 1, overall batch: 500
Training loss: 7.879703521728516 / Valid loss: 8.974875931512742
Model is saved in epoch 1, overall batch: 600
Training loss: 7.321297645568848 / Valid loss: 8.525929201216925
Model is saved in epoch 1, overall batch: 700
Training loss: 7.225223541259766 / Valid loss: 8.045377140953427
Model is saved in epoch 1, overall batch: 800
Training loss: 5.413168907165527 / Valid loss: 7.770183940160842
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 7.828465938568115 / Valid loss: 7.544379722504389
Model is saved in epoch 2, overall batch: 1000
Training loss: 7.749826431274414 / Valid loss: 7.302296188899449
Model is saved in epoch 2, overall batch: 1100
Training loss: 7.042751312255859 / Valid loss: 7.0416650862920855
Model is saved in epoch 2, overall batch: 1200
Training loss: 8.316519737243652 / Valid loss: 6.648540721620832
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.371641159057617 / Valid loss: 6.476367083049956
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.284959316253662 / Valid loss: 6.460156508854458
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.781747341156006 / Valid loss: 6.276855607259841
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.559985160827637 / Valid loss: 6.137360041482108
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.0682876110076904 / Valid loss: 6.021827697753906
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.350690841674805 / Valid loss: 5.928299247650873
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 5.402994632720947 / Valid loss: 6.064223670959473
Training loss: 3.738124370574951 / Valid loss: 6.080743063063848
Training loss: 4.206275939941406 / Valid loss: 5.983329782031832
Training loss: 4.434745788574219 / Valid loss: 5.93656526747204
Training loss: 3.146935224533081 / Valid loss: 5.895251966658092
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 6.009397506713867 / Valid loss: 5.817267370223999
Model is saved in epoch 5, overall batch: 2500
Training loss: 4.677887439727783 / Valid loss: 5.876708811805361
Training loss: 3.275315761566162 / Valid loss: 5.88124091511681
Training loss: 3.8898580074310303 / Valid loss: 5.861418301718576
Training loss: 3.5959010124206543 / Valid loss: 5.809454629534767
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 5.99899959564209 / Valid loss: 5.851160944075811
Training loss: 3.506591320037842 / Valid loss: 5.92088539032709
Training loss: 3.9559953212738037 / Valid loss: 5.93734595207941
Training loss: 3.555493116378784 / Valid loss: 5.9089801856449675
Training loss: 4.089184284210205 / Valid loss: 6.018996486209688

Epoch: 7
Training loss: 3.5011703968048096 / Valid loss: 5.944055223464966
Training loss: 3.1968743801116943 / Valid loss: 5.993115318389166
Training loss: 2.7663772106170654 / Valid loss: 5.964052209399996
Training loss: 4.100218772888184 / Valid loss: 6.0673075199127195
Training loss: 3.015259265899658 / Valid loss: 6.011249991825649

Epoch: 8
Training loss: 2.755281448364258 / Valid loss: 6.069712368647258
Training loss: 1.9843294620513916 / Valid loss: 6.072485819317046
Training loss: 3.2295515537261963 / Valid loss: 6.109292831875029
Training loss: 4.648856163024902 / Valid loss: 6.127446174621582
Training loss: 2.892242431640625 / Valid loss: 6.1030380839393255

Epoch: 9
Training loss: 1.4914815425872803 / Valid loss: 6.1537242639632455
Training loss: 2.489710807800293 / Valid loss: 6.199421417145502
Training loss: 3.566265344619751 / Valid loss: 6.195499143146333
Training loss: 2.7511167526245117 / Valid loss: 6.18905751591637

Epoch: 10
Training loss: 1.839134931564331 / Valid loss: 6.228775374094645
Training loss: 2.6482255458831787 / Valid loss: 6.2506217161814375
Training loss: 1.5138589143753052 / Valid loss: 6.438051977611724
Training loss: 2.2387495040893555 / Valid loss: 6.401592072986421
Training loss: 2.425676107406616 / Valid loss: 6.298973764692034

Epoch: 11
Training loss: 1.1876357793807983 / Valid loss: 6.395075421106248
Training loss: 1.5594196319580078 / Valid loss: 6.3646170320965
Training loss: 2.5467448234558105 / Valid loss: 6.481342837924049
Training loss: 1.4884140491485596 / Valid loss: 6.661142117636544
Training loss: 2.1558566093444824 / Valid loss: 6.657272164026896

Epoch: 12
Training loss: 1.2064846754074097 / Valid loss: 6.435868260973976
Training loss: 1.264400601387024 / Valid loss: 6.566326999664307
Training loss: 1.5710415840148926 / Valid loss: 6.5735858508518765
Training loss: 1.4516798257827759 / Valid loss: 6.525573553357805
Training loss: 1.3536533117294312 / Valid loss: 6.5523589429401214

Epoch: 13
Training loss: 1.0192068815231323 / Valid loss: 6.738755557650611
Training loss: 1.0026063919067383 / Valid loss: 6.64287315096174
Training loss: 1.2080039978027344 / Valid loss: 6.560579068320138
Training loss: 1.4119534492492676 / Valid loss: 6.668180511111305
Training loss: 1.255929708480835 / Valid loss: 6.570403348831904

Epoch: 14
Training loss: 1.031406283378601 / Valid loss: 6.700459132875715
Training loss: 1.080609679222107 / Valid loss: 6.647205518540882
Training loss: 1.597045660018921 / Valid loss: 6.631706666946411
Training loss: 1.2095985412597656 / Valid loss: 6.6440823373340425
Training loss: 1.703953742980957 / Valid loss: 6.658649671645391

Epoch: 15
Training loss: 1.066464900970459 / Valid loss: 6.7676938465663365
Training loss: 1.0616669654846191 / Valid loss: 6.83087146622794
Training loss: 1.003649353981018 / Valid loss: 6.718657902308872
Training loss: 1.0799423456192017 / Valid loss: 6.742418974921817
Training loss: 1.0794179439544678 / Valid loss: 6.709110078357515

Epoch: 16
Training loss: 0.5974401831626892 / Valid loss: 6.798269408089774
Training loss: 0.757427453994751 / Valid loss: 7.1034065473647345
Training loss: 1.1887694597244263 / Valid loss: 6.765722983224052
Training loss: 1.092512607574463 / Valid loss: 6.942908516384306
Training loss: 1.1694416999816895 / Valid loss: 6.835650825500489

Epoch: 17
Training loss: 1.015536904335022 / Valid loss: 6.7497282891046435
Training loss: 1.0122051239013672 / Valid loss: 6.965714363824754
Training loss: 0.8982645273208618 / Valid loss: 6.899111402602423
Training loss: 0.715094804763794 / Valid loss: 6.919436627342588
Training loss: 1.056077003479004 / Valid loss: 6.758084955669585

Epoch: 18
Training loss: 0.7074944972991943 / Valid loss: 6.798213522774833
Training loss: 0.9015824794769287 / Valid loss: 6.797460260845366
Training loss: 0.8927057981491089 / Valid loss: 6.883557215191069
Training loss: 1.0717439651489258 / Valid loss: 7.045577190035865
Training loss: 0.8471208810806274 / Valid loss: 6.826673975444975

Epoch: 19
Training loss: 0.8230034112930298 / Valid loss: 6.890216577620733
Training loss: 1.2581554651260376 / Valid loss: 6.795543820517404
Training loss: 0.6443927884101868 / Valid loss: 6.827182792481922
Training loss: 0.6667264699935913 / Valid loss: 6.841987875529698

Epoch: 20
Training loss: 0.41839301586151123 / Valid loss: 6.800807503291539
Training loss: 1.0358597040176392 / Valid loss: 6.829166475931803
Training loss: 0.4798966348171234 / Valid loss: 6.832661419823056
Training loss: 0.6247392892837524 / Valid loss: 6.869507760093326
Training loss: 1.1649787425994873 / Valid loss: 6.892397578557333

Epoch: 21
Training loss: 0.5873231887817383 / Valid loss: 6.901598051616124
Training loss: 0.7229886651039124 / Valid loss: 6.860448832738967
Training loss: 0.5745152235031128 / Valid loss: 6.840251114254906
Training loss: 0.45379287004470825 / Valid loss: 6.909703717912946
Training loss: 0.593487024307251 / Valid loss: 6.819607130686442

Epoch: 22
Training loss: 0.7090750932693481 / Valid loss: 6.82711505435762
Training loss: 0.597029983997345 / Valid loss: 6.935643713814872
Training loss: 0.44458886981010437 / Valid loss: 6.777128287724087
Training loss: 1.3978371620178223 / Valid loss: 6.854293655213856
Training loss: 0.8003615140914917 / Valid loss: 6.8489130565098355

Epoch: 23
Training loss: 0.6976690292358398 / Valid loss: 6.877043801262265
Training loss: 0.8862740397453308 / Valid loss: 6.77059927440825
Training loss: 0.8803057670593262 / Valid loss: 6.89048569543021
Training loss: 0.6237207651138306 / Valid loss: 6.856757547741845
Training loss: 0.9747884273529053 / Valid loss: 6.908300422486805

Epoch: 24
Training loss: 0.8045611381530762 / Valid loss: 6.795974313645136
Training loss: 0.6097349524497986 / Valid loss: 6.800194985525948
Training loss: 0.5122262239456177 / Valid loss: 6.899501559847877
Training loss: 0.9576490521430969 / Valid loss: 6.834919611612956
Training loss: 0.9836373329162598 / Valid loss: 6.878724109558832

Epoch: 25
Training loss: 1.0119378566741943 / Valid loss: 6.880266507466634
Training loss: 0.40436214208602905 / Valid loss: 6.835707946050735
Training loss: 0.7766107320785522 / Valid loss: 6.745097219376337
Training loss: 0.3845329284667969 / Valid loss: 6.786855334327335
Training loss: 0.5562484264373779 / Valid loss: 6.907934792836508

Epoch: 26
Training loss: 0.5886632204055786 / Valid loss: 6.785281535557338
Training loss: 0.5262834429740906 / Valid loss: 6.837546757289341
Training loss: 0.35839301347732544 / Valid loss: 6.841853105454218
Training loss: 0.4926944971084595 / Valid loss: 6.996424470629011
Training loss: 0.3917546272277832 / Valid loss: 6.953425103142148

Epoch: 27
Training loss: 0.3654957711696625 / Valid loss: 6.811961278461275
Training loss: 0.4774388074874878 / Valid loss: 6.938746098109654
Training loss: 0.6667436957359314 / Valid loss: 6.975712776184082
Training loss: 0.5132372975349426 / Valid loss: 6.92077559970674
Training loss: 0.6443368196487427 / Valid loss: 6.897524583907354

Epoch: 28
Training loss: 0.3950005769729614 / Valid loss: 6.907257288978213
Training loss: 0.39783793687820435 / Valid loss: 6.821173304603214
Training loss: 0.6547859907150269 / Valid loss: 6.876206752232143
Training loss: 0.581788957118988 / Valid loss: 6.860197253454299
Training loss: 0.4888908863067627 / Valid loss: 6.896412699563163

Epoch: 29
Training loss: 0.5982891917228699 / Valid loss: 6.804635772250948
Training loss: 0.45078080892562866 / Valid loss: 6.810063421158564
Training loss: 0.3173828721046448 / Valid loss: 6.877296488625663
Training loss: 0.49544286727905273 / Valid loss: 6.808203456515358

Epoch: 30
Training loss: 0.5895927548408508 / Valid loss: 6.77880581901187
Training loss: 0.42640045285224915 / Valid loss: 6.79390670685541
Training loss: 0.37921905517578125 / Valid loss: 6.80463304973784
Training loss: 0.5117740035057068 / Valid loss: 6.87928139368693
Training loss: 0.4131211042404175 / Valid loss: 6.774644406636556

Epoch: 31
Training loss: 0.45472264289855957 / Valid loss: 6.861858131771996
Training loss: 0.48314860463142395 / Valid loss: 6.828687676929293
Training loss: 0.436541885137558 / Valid loss: 6.859236744471959
Training loss: 0.37717169523239136 / Valid loss: 6.882453141893659
Training loss: 0.4310706853866577 / Valid loss: 7.000587731315976

Epoch: 32
Training loss: 0.5668940544128418 / Valid loss: 6.845642811911446
Training loss: 0.435222864151001 / Valid loss: 6.867036238170805
Training loss: 0.5009066462516785 / Valid loss: 6.816389499391828
Training loss: 0.39305293560028076 / Valid loss: 6.800718661717006
Training loss: 0.5567759275436401 / Valid loss: 6.750890386672247

Epoch: 33
Training loss: 0.40785157680511475 / Valid loss: 6.8406016031901045
Training loss: 0.5160688161849976 / Valid loss: 6.804271025884719
Training loss: 0.4679940938949585 / Valid loss: 6.872405249731881
Training loss: 0.39820197224617004 / Valid loss: 6.890127695174444
Training loss: 0.5685959458351135 / Valid loss: 6.894368189857119

Epoch: 34
Training loss: 0.7828094959259033 / Valid loss: 6.804283087594168
Training loss: 0.3315759301185608 / Valid loss: 6.912725450879051
Training loss: 0.5125002861022949 / Valid loss: 6.881532855260939
Training loss: 0.3393252491950989 / Valid loss: 6.853461256481352
Training loss: 0.387925922870636 / Valid loss: 6.782809686660767

Epoch: 35
Training loss: 0.7703346610069275 / Valid loss: 6.913750889187767
Training loss: 0.3672916889190674 / Valid loss: 6.789346717652821
Training loss: 0.23744839429855347 / Valid loss: 6.845078454698835
Training loss: 0.30010679364204407 / Valid loss: 6.902506978171212
Training loss: 0.3484196960926056 / Valid loss: 6.800491526013329

Epoch: 36
Training loss: 0.3306175470352173 / Valid loss: 6.837956312724522
Training loss: 0.7334025502204895 / Valid loss: 6.81175719669887
Training loss: 0.2723846137523651 / Valid loss: 6.870183463323684
Training loss: 0.34169328212738037 / Valid loss: 6.821066288720994
Training loss: 0.3836182951927185 / Valid loss: 6.833472052074614

Epoch: 37
Training loss: 0.3566010594367981 / Valid loss: 6.889367584955124
Training loss: 0.32501405477523804 / Valid loss: 6.7725972652435305
Training loss: 0.4351050853729248 / Valid loss: 6.874697197051275
Training loss: 0.7412177324295044 / Valid loss: 6.821455297015962
Training loss: 0.5079891085624695 / Valid loss: 6.860224474044073

Epoch: 38
Training loss: 0.4239080548286438 / Valid loss: 6.773592574255807
Training loss: 0.3806225061416626 / Valid loss: 6.768887083871024
Training loss: 0.4063343405723572 / Valid loss: 6.805022239685059
Training loss: 0.32901692390441895 / Valid loss: 6.813697269984654
Training loss: 0.3178476095199585 / Valid loss: 6.909545480637323

Epoch: 39
Training loss: 0.5296711921691895 / Valid loss: 6.816830673671904
Training loss: 0.3437304198741913 / Valid loss: 6.789612722396851
Training loss: 0.32042980194091797 / Valid loss: 6.749990563165574
Training loss: 0.45757991075515747 / Valid loss: 6.985154054278419
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 2900): 5.648548750650315
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.485506057739258 / Valid loss: 16.65686526525588
Model is saved in epoch 0, overall batch: 0
Training loss: 8.353205680847168 / Valid loss: 13.521798624311174
Model is saved in epoch 0, overall batch: 100
Training loss: 9.878501892089844 / Valid loss: 11.759728340875535
Model is saved in epoch 0, overall batch: 200
Training loss: 8.94137954711914 / Valid loss: 10.662508110772995
Model is saved in epoch 0, overall batch: 300
Training loss: 7.284103870391846 / Valid loss: 9.390273657299224
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 10.038227081298828 / Valid loss: 8.6975235303243
Model is saved in epoch 1, overall batch: 500
Training loss: 7.003228187561035 / Valid loss: 8.028719107309977
Model is saved in epoch 1, overall batch: 600
Training loss: 5.50408411026001 / Valid loss: 7.721278290521531
Model is saved in epoch 1, overall batch: 700
Training loss: 4.810075283050537 / Valid loss: 7.310529688426427
Model is saved in epoch 1, overall batch: 800
Training loss: 7.264642238616943 / Valid loss: 6.510662315005348
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.978977203369141 / Valid loss: 6.450229251952399
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.520969390869141 / Valid loss: 6.250886642365229
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.182527542114258 / Valid loss: 6.037901083628337
Model is saved in epoch 2, overall batch: 1200
Training loss: 6.5769758224487305 / Valid loss: 6.223790320895967
Training loss: 5.607414722442627 / Valid loss: 5.8855375494275775
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.964859485626221 / Valid loss: 5.842487902868362
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.32572078704834 / Valid loss: 5.7728999228704545
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.642495155334473 / Valid loss: 5.807491749808902
Training loss: 5.040497303009033 / Valid loss: 5.72698795681908
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.986632823944092 / Valid loss: 5.871810997100104

Epoch: 4
Training loss: 3.0767107009887695 / Valid loss: 5.708779026213146
Model is saved in epoch 4, overall batch: 2000
Training loss: 4.436709880828857 / Valid loss: 5.7278491973876955
Training loss: 4.455597877502441 / Valid loss: 5.7002104532150994
Model is saved in epoch 4, overall batch: 2200
Training loss: 5.7150774002075195 / Valid loss: 5.651837548755464
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.348583698272705 / Valid loss: 5.647511498133341
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 5.756872177124023 / Valid loss: 5.642830601192656
Model is saved in epoch 5, overall batch: 2500
Training loss: 5.546241760253906 / Valid loss: 5.66232902208964
Training loss: 4.737227916717529 / Valid loss: 5.674365799767631
Training loss: 3.1277337074279785 / Valid loss: 5.669475580397107
Training loss: 5.894631862640381 / Valid loss: 5.663637163525536

Epoch: 6
Training loss: 4.596457481384277 / Valid loss: 5.6173127673921135
Model is saved in epoch 6, overall batch: 3000
Training loss: 2.6523051261901855 / Valid loss: 5.65349889709836
Training loss: 4.984997749328613 / Valid loss: 5.6713565190633135
Training loss: 3.9716835021972656 / Valid loss: 5.639438958395095
Training loss: 4.596701622009277 / Valid loss: 5.643691081092471

Epoch: 7
Training loss: 4.185222625732422 / Valid loss: 5.691637854349046
Training loss: 3.424835681915283 / Valid loss: 5.647243397576468
Training loss: 3.2519397735595703 / Valid loss: 5.69198731240772
Training loss: 5.034510612487793 / Valid loss: 5.671269428162348
Training loss: 3.414945363998413 / Valid loss: 5.699990901492891

Epoch: 8
Training loss: 3.4447197914123535 / Valid loss: 5.69239448365711
Training loss: 3.854642629623413 / Valid loss: 5.7410568986620225
Training loss: 3.331362247467041 / Valid loss: 5.732310102099464
Training loss: 4.362034797668457 / Valid loss: 5.726867348807199
Training loss: 4.993036270141602 / Valid loss: 5.706485827763875

Epoch: 9
Training loss: 2.586872100830078 / Valid loss: 5.720364257267543
Training loss: 4.44199275970459 / Valid loss: 5.7706506751832505
Training loss: 4.526033401489258 / Valid loss: 5.725092429206485
Training loss: 3.9691691398620605 / Valid loss: 5.755699171338763

Epoch: 10
Training loss: 3.5760233402252197 / Valid loss: 5.749398222423735
Training loss: 2.579925537109375 / Valid loss: 5.875948824201312
Training loss: 3.984163999557495 / Valid loss: 5.825910754430861
Training loss: 3.3292977809906006 / Valid loss: 5.797548997969854
Training loss: 4.5648193359375 / Valid loss: 5.832621547154018

Epoch: 11
Training loss: 3.8599345684051514 / Valid loss: 5.79313554082598
Training loss: 4.02146053314209 / Valid loss: 5.866930189586821
Training loss: 3.8362345695495605 / Valid loss: 5.879468513670422
Training loss: 3.6321663856506348 / Valid loss: 5.821375058946155
Training loss: 3.412081718444824 / Valid loss: 5.884131111417498

Epoch: 12
Training loss: 2.9512975215911865 / Valid loss: 5.84042340687343
Training loss: 4.117049694061279 / Valid loss: 5.87895462853568
Training loss: 3.4439003467559814 / Valid loss: 5.9965362526121595
Training loss: 3.089524269104004 / Valid loss: 5.9381985846019925
Training loss: 4.246267318725586 / Valid loss: 5.860563316799346

Epoch: 13
Training loss: 2.8052940368652344 / Valid loss: 5.870312316077096
Training loss: 3.571504592895508 / Valid loss: 5.856103815351214
Training loss: 2.1911802291870117 / Valid loss: 5.915453415825254
Training loss: 2.8195762634277344 / Valid loss: 5.922555010659354
Training loss: 3.802422523498535 / Valid loss: 5.891080197833833

Epoch: 14
Training loss: 2.9747562408447266 / Valid loss: 6.021978114900135
Training loss: 2.6204116344451904 / Valid loss: 5.961826111021496
Training loss: 2.182865619659424 / Valid loss: 5.938886092957996
Training loss: 2.9145400524139404 / Valid loss: 5.961219313031151
Training loss: 3.4904520511627197 / Valid loss: 6.100813052767799

Epoch: 15
Training loss: 2.9508118629455566 / Valid loss: 6.0236552102225165
Training loss: 3.2023191452026367 / Valid loss: 6.0836275259653725
Training loss: 2.3326539993286133 / Valid loss: 5.9511540935153056
Training loss: 2.9784626960754395 / Valid loss: 6.066595869972592
Training loss: 2.7173678874969482 / Valid loss: 6.282965900784447

Epoch: 16
Training loss: 3.688462495803833 / Valid loss: 6.079001499357678
Training loss: 2.3898093700408936 / Valid loss: 6.143254697890509
Training loss: 3.362879753112793 / Valid loss: 6.108857173011416
Training loss: 2.760833740234375 / Valid loss: 6.097749108359928
Training loss: 2.6331684589385986 / Valid loss: 6.14865266936166

Epoch: 17
Training loss: 2.2222723960876465 / Valid loss: 6.12653394880749
Training loss: 2.5854673385620117 / Valid loss: 6.106607112430391
Training loss: 2.506639242172241 / Valid loss: 6.116756187166486
Training loss: 2.6066365242004395 / Valid loss: 6.205545650209699
Training loss: 2.5980377197265625 / Valid loss: 6.1271946907043455

Epoch: 18
Training loss: 1.8954410552978516 / Valid loss: 6.135683402561006
Training loss: 3.0427050590515137 / Valid loss: 6.441155769711449
Training loss: 2.2443931102752686 / Valid loss: 6.28295712243943
Training loss: 2.4863104820251465 / Valid loss: 6.234489272889637
Training loss: 2.6563732624053955 / Valid loss: 6.144725009373256

Epoch: 19
Training loss: 2.7764713764190674 / Valid loss: 6.449720046633765
Training loss: 2.08288836479187 / Valid loss: 6.802956994374593
Training loss: 2.139772415161133 / Valid loss: 6.3264600345066615
Training loss: 1.405703067779541 / Valid loss: 6.28228584471203

Epoch: 20
Training loss: 1.3874616622924805 / Valid loss: 6.347855944860549
Training loss: 1.5261539220809937 / Valid loss: 6.460552524384998
Training loss: 1.8891382217407227 / Valid loss: 6.369381300608317
Training loss: 2.465214967727661 / Valid loss: 6.532475907461984
Training loss: 1.8322231769561768 / Valid loss: 6.343320297059559

Epoch: 21
Training loss: 1.7291579246520996 / Valid loss: 6.312827398663475
Training loss: 1.4778952598571777 / Valid loss: 6.329848716372536
Training loss: 2.075546979904175 / Valid loss: 6.3541476476760135
Training loss: 1.9983172416687012 / Valid loss: 6.529545068740845
Training loss: 2.0440828800201416 / Valid loss: 6.398956712086996

Epoch: 22
Training loss: 1.2081115245819092 / Valid loss: 6.475120424088978
Training loss: 1.6865599155426025 / Valid loss: 6.5642771175929475
Training loss: 2.368455410003662 / Valid loss: 6.998839037758963
Training loss: 1.5687150955200195 / Valid loss: 6.422386062712897
Training loss: 1.3970706462860107 / Valid loss: 6.480036812736874

Epoch: 23
Training loss: 1.6374409198760986 / Valid loss: 6.527257115500314
Training loss: 1.8463696241378784 / Valid loss: 6.472088831946963
Training loss: 2.17460560798645 / Valid loss: 6.716503422600883
Training loss: 2.0419583320617676 / Valid loss: 6.553533367883592
Training loss: 1.3195946216583252 / Valid loss: 6.509785320645287

Epoch: 24
Training loss: 0.9632573127746582 / Valid loss: 6.786494414011638
Training loss: 2.161050796508789 / Valid loss: 6.576518758138021
Training loss: 0.9973966479301453 / Valid loss: 6.6802029745919365
Training loss: 1.7548105716705322 / Valid loss: 6.5076683884575255
Training loss: 1.4880642890930176 / Valid loss: 6.5951555025009885

Epoch: 25
Training loss: 1.164557933807373 / Valid loss: 6.613419237590971
Training loss: 1.8219079971313477 / Valid loss: 6.587387743450346
Training loss: 1.480905294418335 / Valid loss: 6.683131626674107
Training loss: 1.542391300201416 / Valid loss: 6.7504057203020364
Training loss: 1.3713080883026123 / Valid loss: 6.637079559053693

Epoch: 26
Training loss: 1.5010294914245605 / Valid loss: 6.838565304165795
Training loss: 1.6990028619766235 / Valid loss: 6.833563389096941
Training loss: 1.647304892539978 / Valid loss: 6.986454813820975
Training loss: 1.8020609617233276 / Valid loss: 6.636682051704043
Training loss: 1.589633584022522 / Valid loss: 6.654491392771403

Epoch: 27
Training loss: 1.3943285942077637 / Valid loss: 6.6356805892217725
Training loss: 0.995750904083252 / Valid loss: 6.731440448760987
Training loss: 1.5247769355773926 / Valid loss: 6.692764822642008
Training loss: 1.3581902980804443 / Valid loss: 7.26837861651466
Training loss: 1.0418033599853516 / Valid loss: 6.676517332167853

Epoch: 28
Training loss: 0.6629917621612549 / Valid loss: 6.693877565293085
Training loss: 1.379703402519226 / Valid loss: 6.8592827547164195
Training loss: 1.0912469625473022 / Valid loss: 6.863615176791236
Training loss: 1.1063610315322876 / Valid loss: 6.844954290844146
Training loss: 1.3837019205093384 / Valid loss: 6.963094420660109

Epoch: 29
Training loss: 1.0201003551483154 / Valid loss: 6.746186515263148
Training loss: 0.9874293804168701 / Valid loss: 6.866666525886172
Training loss: 0.9811508655548096 / Valid loss: 6.77959026382083
Training loss: 1.6192638874053955 / Valid loss: 7.102864097413563

Epoch: 30
Training loss: 0.9306873083114624 / Valid loss: 7.120335669744582
Training loss: 1.5959800481796265 / Valid loss: 7.055652282351539
Training loss: 0.8025593757629395 / Valid loss: 6.8105706737155005
Training loss: 1.075927972793579 / Valid loss: 7.172974023364839
Training loss: 1.733196496963501 / Valid loss: 7.14609203338623

Epoch: 31
Training loss: 0.8117022514343262 / Valid loss: 8.074024095989408
Training loss: 1.7545924186706543 / Valid loss: 6.967454705919538
Training loss: 1.2076795101165771 / Valid loss: 6.857500764301845
Training loss: 1.1912091970443726 / Valid loss: 6.8442380837031775
Training loss: 1.2517788410186768 / Valid loss: 7.073267600649879

Epoch: 32
Training loss: 0.6158621311187744 / Valid loss: 6.9288228988647464
Training loss: 1.4258272647857666 / Valid loss: 7.293109544118246
Training loss: 0.9333984851837158 / Valid loss: 6.938070814950126
Training loss: 0.6497617363929749 / Valid loss: 7.0537382489158995
Training loss: 0.8921734094619751 / Valid loss: 6.9075604393368675

Epoch: 33
Training loss: 0.9078625440597534 / Valid loss: 7.163627174922398
Training loss: 0.9280069470405579 / Valid loss: 7.11833811260405
Training loss: 1.1662657260894775 / Valid loss: 6.917031921659198
Training loss: 1.370581865310669 / Valid loss: 7.290657220567976
Training loss: 0.7273364067077637 / Valid loss: 7.170710808890206

Epoch: 34
Training loss: 1.3582934141159058 / Valid loss: 7.0295683293115525
Training loss: 0.9722646474838257 / Valid loss: 6.873892861320859
Training loss: 0.7352050542831421 / Valid loss: 6.954585983639672
Training loss: 0.9213889241218567 / Valid loss: 7.026775373731341
Training loss: 1.1259057521820068 / Valid loss: 7.190350319090344

Epoch: 35
Training loss: 0.9235047101974487 / Valid loss: 7.870264371236165
Training loss: 0.9232757091522217 / Valid loss: 7.0828199976966495
Training loss: 1.0799190998077393 / Valid loss: 7.096007596878779
Training loss: 0.5745688080787659 / Valid loss: 7.138947055453346
Training loss: 0.8419153094291687 / Valid loss: 7.365202526819139

Epoch: 36
Training loss: 0.6563662886619568 / Valid loss: 6.938310913812547
Training loss: 0.9207398891448975 / Valid loss: 7.059687805175781
Training loss: 0.9757116436958313 / Valid loss: 7.512586266653878
Training loss: 1.231321096420288 / Valid loss: 6.981192225501651
Training loss: 0.7989597320556641 / Valid loss: 7.086079057057699

Epoch: 37
Training loss: 0.544928789138794 / Valid loss: 7.206286364509946
Training loss: 0.5447613596916199 / Valid loss: 7.212695671263195
Training loss: 0.7931307554244995 / Valid loss: 6.993155974433535
Training loss: 0.7320933938026428 / Valid loss: 7.181392247336252
Training loss: 1.1800637245178223 / Valid loss: 7.499863029661633

Epoch: 38
Training loss: 0.707287073135376 / Valid loss: 7.313116881960914
Training loss: 0.5276812314987183 / Valid loss: 7.112008816855294
Training loss: 0.7552834749221802 / Valid loss: 7.541641480582101
Training loss: 0.7265993356704712 / Valid loss: 7.0216155733381
Training loss: 1.1091328859329224 / Valid loss: 7.112737728300549

Epoch: 39
Training loss: 0.835628092288971 / Valid loss: 7.197056298028855
Training loss: 0.638336718082428 / Valid loss: 7.311685353233701
Training loss: 0.8097957968711853 / Valid loss: 7.097062101818266
Training loss: 0.6675061583518982 / Valid loss: 7.103133969079881
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 3000): 5.487365711302984
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 13.485506057739258 / Valid loss: 16.65686529250372
Model is saved in epoch 0, overall batch: 0
Training loss: 8.353206634521484 / Valid loss: 13.521799278259277
Model is saved in epoch 0, overall batch: 100
Training loss: 9.878503799438477 / Valid loss: 11.759729703267416
Model is saved in epoch 0, overall batch: 200
Training loss: 8.94137191772461 / Valid loss: 10.662537161509196
Model is saved in epoch 0, overall batch: 300
Training loss: 7.284815788269043 / Valid loss: 9.390944031306676
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 10.039335250854492 / Valid loss: 8.696337148121424
Model is saved in epoch 1, overall batch: 500
Training loss: 7.005838394165039 / Valid loss: 8.029387324196952
Model is saved in epoch 1, overall batch: 600
Training loss: 5.503600120544434 / Valid loss: 7.7247564951578775
Model is saved in epoch 1, overall batch: 700
Training loss: 4.808224678039551 / Valid loss: 7.312022408984956
Model is saved in epoch 1, overall batch: 800
Training loss: 7.261209487915039 / Valid loss: 6.5132375376565115
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.985238552093506 / Valid loss: 6.447634903589885
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.526694297790527 / Valid loss: 6.254937900815691
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.164834976196289 / Valid loss: 6.039568033672515
Model is saved in epoch 2, overall batch: 1200
Training loss: 6.574172019958496 / Valid loss: 6.229684359686715
Training loss: 5.600335121154785 / Valid loss: 5.881400735037667
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.95905876159668 / Valid loss: 5.844621519815354
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.330679416656494 / Valid loss: 5.776907546179635
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.640631198883057 / Valid loss: 5.808078479766846
Training loss: 5.0541486740112305 / Valid loss: 5.724885020937238
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.931326866149902 / Valid loss: 5.870811798459008

Epoch: 4
Training loss: 3.0871315002441406 / Valid loss: 5.709361264819191
Model is saved in epoch 4, overall batch: 2000
Training loss: 4.433083534240723 / Valid loss: 5.730548393158686
Training loss: 4.452796936035156 / Valid loss: 5.696143711180914
Model is saved in epoch 4, overall batch: 2200
Training loss: 5.721818923950195 / Valid loss: 5.648150330498105
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.366955757141113 / Valid loss: 5.647107378641764
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 5.745756149291992 / Valid loss: 5.6408832663581485
Model is saved in epoch 5, overall batch: 2500
Training loss: 5.553313255310059 / Valid loss: 5.6616314433869865
Training loss: 4.732484817504883 / Valid loss: 5.6739409719194684
Training loss: 3.131633996963501 / Valid loss: 5.6669052192143035
Training loss: 5.890240669250488 / Valid loss: 5.66292613801502

Epoch: 6
Training loss: 4.598144054412842 / Valid loss: 5.616068295070103
Model is saved in epoch 6, overall batch: 3000
Training loss: 2.662383794784546 / Valid loss: 5.648987913131714
Training loss: 4.985711574554443 / Valid loss: 5.671016164053054
Training loss: 3.953214168548584 / Valid loss: 5.638598957515899
Training loss: 4.566308975219727 / Valid loss: 5.640433627083188

Epoch: 7
Training loss: 4.156808853149414 / Valid loss: 5.689528122402373
Training loss: 3.4089722633361816 / Valid loss: 5.644060443696522
Training loss: 3.246748447418213 / Valid loss: 5.690044534774054
Training loss: 5.015870094299316 / Valid loss: 5.668030132566179
Training loss: 3.3967485427856445 / Valid loss: 5.687952327728271

Epoch: 8
Training loss: 3.4443063735961914 / Valid loss: 5.687449761799404
Training loss: 3.8567824363708496 / Valid loss: 5.7315665562947595
Training loss: 3.371671199798584 / Valid loss: 5.728163660140265
Training loss: 4.314582347869873 / Valid loss: 5.716966506413051
Training loss: 5.038008689880371 / Valid loss: 5.701957194010417

Epoch: 9
Training loss: 2.5830044746398926 / Valid loss: 5.708862697510492
Training loss: 4.443760395050049 / Valid loss: 5.760594090961274
Training loss: 4.531096458435059 / Valid loss: 5.713971337817964
Training loss: 3.9602108001708984 / Valid loss: 5.7519662561870755

Epoch: 10
Training loss: 3.5154776573181152 / Valid loss: 5.737186840602329
Training loss: 2.520353317260742 / Valid loss: 5.8736334255763465
Training loss: 3.983414649963379 / Valid loss: 5.814599643434797
Training loss: 3.3685693740844727 / Valid loss: 5.788043726058233
Training loss: 4.5594482421875 / Valid loss: 5.818863880066645

Epoch: 11
Training loss: 3.87076735496521 / Valid loss: 5.783256959915161
Training loss: 4.052943706512451 / Valid loss: 5.8612574781690325
Training loss: 3.9032142162323 / Valid loss: 5.865874801363264
Training loss: 3.6001839637756348 / Valid loss: 5.809257100877308
Training loss: 3.462160348892212 / Valid loss: 5.867175331569853

Epoch: 12
Training loss: 2.964606761932373 / Valid loss: 5.82909452120463
Training loss: 4.083935260772705 / Valid loss: 5.887970987955729
Training loss: 3.479766845703125 / Valid loss: 5.982326893579392
Training loss: 3.0734119415283203 / Valid loss: 5.922819148926508
Training loss: 4.220417499542236 / Valid loss: 5.849537774494716

Epoch: 13
Training loss: 2.746410846710205 / Valid loss: 5.862987164088658
Training loss: 3.640397310256958 / Valid loss: 5.847666343053182
Training loss: 2.2294650077819824 / Valid loss: 5.899101320902506
Training loss: 2.802825927734375 / Valid loss: 5.9219545546032135
Training loss: 3.8831982612609863 / Valid loss: 5.886543382917131

Epoch: 14
Training loss: 2.9754090309143066 / Valid loss: 6.011862434659686
Training loss: 2.6414871215820312 / Valid loss: 5.95482854389009
Training loss: 2.263568878173828 / Valid loss: 5.931399250030518
Training loss: 2.90621018409729 / Valid loss: 5.9436772959572926
Training loss: 3.536487340927124 / Valid loss: 6.115878050667899

Epoch: 15
Training loss: 2.9359636306762695 / Valid loss: 6.004129984265282
Training loss: 3.1707684993743896 / Valid loss: 6.067693424224854
Training loss: 2.401552677154541 / Valid loss: 5.937879991531372
Training loss: 3.032639980316162 / Valid loss: 6.058730002811977
Training loss: 2.7447853088378906 / Valid loss: 6.265884067898705

Epoch: 16
Training loss: 3.6856701374053955 / Valid loss: 6.04999658039638
Training loss: 2.451396942138672 / Valid loss: 6.124616186959403
Training loss: 3.2596335411071777 / Valid loss: 6.096704592023577
Training loss: 2.748605966567993 / Valid loss: 6.107167595908756
Training loss: 2.65793514251709 / Valid loss: 6.136062769662766

Epoch: 17
Training loss: 2.22114896774292 / Valid loss: 6.120861761910575
Training loss: 2.6353988647460938 / Valid loss: 6.120443870907738
Training loss: 2.4817914962768555 / Valid loss: 6.112530751455398
Training loss: 2.601989269256592 / Valid loss: 6.197050226302374
Training loss: 2.61820650100708 / Valid loss: 6.131532376153128

Epoch: 18
Training loss: 1.897641897201538 / Valid loss: 6.12552950949896
Training loss: 3.0300724506378174 / Valid loss: 6.40071781703404
Training loss: 2.2169699668884277 / Valid loss: 6.259964341209049
Training loss: 2.5125937461853027 / Valid loss: 6.229538102377028
Training loss: 2.699610710144043 / Valid loss: 6.136258195695423

Epoch: 19
Training loss: 2.7344160079956055 / Valid loss: 6.399385052635556
Training loss: 2.0468637943267822 / Valid loss: 6.698107347034273
Training loss: 2.131164789199829 / Valid loss: 6.313529414222354
Training loss: 1.4408583641052246 / Valid loss: 6.268746575855073

Epoch: 20
Training loss: 1.3555372953414917 / Valid loss: 6.329960318974086
Training loss: 1.5082488059997559 / Valid loss: 6.466554369245257
Training loss: 1.8306336402893066 / Valid loss: 6.3386065414973665
Training loss: 2.420936107635498 / Valid loss: 6.476195180983771
Training loss: 1.856691837310791 / Valid loss: 6.3449314480736145

Epoch: 21
Training loss: 1.697894811630249 / Valid loss: 6.296647053673154
Training loss: 1.4380688667297363 / Valid loss: 6.305201550892421
Training loss: 2.0616707801818848 / Valid loss: 6.337183793385823
Training loss: 1.9648354053497314 / Valid loss: 6.527297174362909
Training loss: 1.9874435663223267 / Valid loss: 6.383680786405291

Epoch: 22
Training loss: 1.1621345281600952 / Valid loss: 6.428021217527844
Training loss: 1.7197518348693848 / Valid loss: 6.544288753327869
Training loss: 2.312610149383545 / Valid loss: 6.979579861958822
Training loss: 1.539841651916504 / Valid loss: 6.428294715427217
Training loss: 1.4026331901550293 / Valid loss: 6.489497974940709

Epoch: 23
Training loss: 1.6402705907821655 / Valid loss: 6.563517456962948
Training loss: 1.840158462524414 / Valid loss: 6.4640163194565545
Training loss: 2.069133758544922 / Valid loss: 6.690505420593988
Training loss: 2.0702567100524902 / Valid loss: 6.560672351292202
Training loss: 1.2703077793121338 / Valid loss: 6.49818279629662

Epoch: 24
Training loss: 0.9721953272819519 / Valid loss: 6.7891516140529085
Training loss: 2.1654844284057617 / Valid loss: 6.5885450953529
Training loss: 1.0084071159362793 / Valid loss: 6.616366027650379
Training loss: 1.7734315395355225 / Valid loss: 6.509913789658319
Training loss: 1.5384106636047363 / Valid loss: 6.598796344938732

Epoch: 25
Training loss: 1.2197117805480957 / Valid loss: 6.5840700830732075
Training loss: 1.90034818649292 / Valid loss: 6.557688193094163
Training loss: 1.5462193489074707 / Valid loss: 6.691208635057722
Training loss: 1.4884769916534424 / Valid loss: 6.638636938730875
Training loss: 1.4037668704986572 / Valid loss: 6.615759470349267

Epoch: 26
Training loss: 1.4579012393951416 / Valid loss: 6.745490110488165
Training loss: 1.5244169235229492 / Valid loss: 6.801304317656017
Training loss: 1.662968397140503 / Valid loss: 6.948247718811035
Training loss: 1.8315834999084473 / Valid loss: 6.612623210180374
Training loss: 1.687178134918213 / Valid loss: 6.645747682026454

Epoch: 27
Training loss: 1.429898977279663 / Valid loss: 6.6318483125595815
Training loss: 1.0081281661987305 / Valid loss: 6.688324328831264
Training loss: 1.532057523727417 / Valid loss: 6.698321505955287
Training loss: 1.3524127006530762 / Valid loss: 7.212002817789713
Training loss: 1.0320549011230469 / Valid loss: 6.6481355258396695

Epoch: 28
Training loss: 0.6422528624534607 / Valid loss: 6.683400978360857
Training loss: 1.4121860265731812 / Valid loss: 6.880040252776373
Training loss: 1.13655424118042 / Valid loss: 6.897121011643183
Training loss: 1.1121156215667725 / Valid loss: 6.830797009241014
Training loss: 1.3821744918823242 / Valid loss: 6.911225841158912

Epoch: 29
Training loss: 1.0381155014038086 / Valid loss: 6.736249038151333
Training loss: 0.9994634389877319 / Valid loss: 6.8240773973010835
Training loss: 1.022413969039917 / Valid loss: 6.75021227882022
Training loss: 1.753894567489624 / Valid loss: 7.1532640230088

Epoch: 30
Training loss: 0.9273747205734253 / Valid loss: 7.0560093743460515
Training loss: 1.5944349765777588 / Valid loss: 6.955992108299618
Training loss: 0.7782996892929077 / Valid loss: 6.788073868978591
Training loss: 1.0462950468063354 / Valid loss: 7.087753829501924
Training loss: 1.750807762145996 / Valid loss: 7.086756302061535

Epoch: 31
Training loss: 0.805302619934082 / Valid loss: 7.917025979359945
Training loss: 1.8008174896240234 / Valid loss: 6.97725613457816
Training loss: 1.255727767944336 / Valid loss: 6.843931336629958
Training loss: 1.142902135848999 / Valid loss: 6.797447958446685
Training loss: 1.1237173080444336 / Valid loss: 7.017990811665853

Epoch: 32
Training loss: 0.6604433655738831 / Valid loss: 6.890942628043039
Training loss: 1.4021739959716797 / Valid loss: 7.646020099094936
Training loss: 0.9027867317199707 / Valid loss: 6.899314308166504
Training loss: 0.6986550092697144 / Valid loss: 7.216286786397299
Training loss: 0.9145023226737976 / Valid loss: 6.884086177462623

Epoch: 33
Training loss: 0.886635959148407 / Valid loss: 7.184925279163179
Training loss: 0.9270902872085571 / Valid loss: 7.074525928497314
Training loss: 1.1084861755371094 / Valid loss: 6.891116980143956
Training loss: 1.4421281814575195 / Valid loss: 7.257675634111677
Training loss: 0.7376590967178345 / Valid loss: 7.126362337384905

Epoch: 34
Training loss: 1.4094706773757935 / Valid loss: 6.977462105523973
Training loss: 0.9770162105560303 / Valid loss: 6.864550199962798
Training loss: 0.7687748074531555 / Valid loss: 6.9599832307724725
Training loss: 0.7825204133987427 / Valid loss: 7.093968250637963
Training loss: 1.1407135725021362 / Valid loss: 7.3669549760364355

Epoch: 35
Training loss: 0.9097706079483032 / Valid loss: 7.951324135916574
Training loss: 0.9057708382606506 / Valid loss: 7.0059714657919745
Training loss: 1.2001674175262451 / Valid loss: 7.129163687569754
Training loss: 0.5651934146881104 / Valid loss: 7.099989663986933
Training loss: 0.8360775709152222 / Valid loss: 7.3382340431213375

Epoch: 36
Training loss: 0.6672576665878296 / Valid loss: 6.92677884328933
Training loss: 0.8876467943191528 / Valid loss: 7.035166790371849
Training loss: 0.9372352957725525 / Valid loss: 7.592065901983352
Training loss: 1.229445219039917 / Valid loss: 6.977384708041236
Training loss: 0.7575091123580933 / Valid loss: 7.096185234614781

Epoch: 37
Training loss: 0.5469309091567993 / Valid loss: 7.201233193987892
Training loss: 0.5681086778640747 / Valid loss: 7.146391759599958
Training loss: 0.7425472736358643 / Valid loss: 6.978943979172479
Training loss: 0.7240840196609497 / Valid loss: 7.214114388965425
Training loss: 1.0955049991607666 / Valid loss: 7.366892224266415

Epoch: 38
Training loss: 0.7164493799209595 / Valid loss: 7.331668567657471
Training loss: 0.5134152173995972 / Valid loss: 7.107007998511905
Training loss: 0.7605041861534119 / Valid loss: 7.744343108222598
Training loss: 0.6761759519577026 / Valid loss: 7.00246273449489
Training loss: 1.1313230991363525 / Valid loss: 7.257789166768392

Epoch: 39
Training loss: 0.7168183922767639 / Valid loss: 7.2774656204950245
Training loss: 0.6122442483901978 / Valid loss: 7.344598420461019
Training loss: 0.7731577157974243 / Valid loss: 7.104126576014927
Training loss: 0.6650978326797485 / Valid loss: 7.062222349076044
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 3000): 5.4913954825628375
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.62260627746582 / Valid loss: 16.374367114475795
Model is saved in epoch 0, overall batch: 0
Training loss: 9.723286628723145 / Valid loss: 8.03689811797369
Model is saved in epoch 0, overall batch: 100
Training loss: 7.480327129364014 / Valid loss: 6.270145116533552
Model is saved in epoch 0, overall batch: 200
Training loss: 6.113794803619385 / Valid loss: 5.767492276146299
Model is saved in epoch 0, overall batch: 300
Training loss: 8.645269393920898 / Valid loss: 5.683840640385946
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.7855329513549805 / Valid loss: 5.636974350611369
Model is saved in epoch 1, overall batch: 500
Training loss: 5.952703475952148 / Valid loss: 5.620285365695045
Model is saved in epoch 1, overall batch: 600
Training loss: 7.779379844665527 / Valid loss: 5.607268519628615
Model is saved in epoch 1, overall batch: 700
Training loss: 5.61826229095459 / Valid loss: 5.591315775825864
Model is saved in epoch 1, overall batch: 800
Training loss: 5.868439674377441 / Valid loss: 5.5809512751443044
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.585034370422363 / Valid loss: 5.568961988176619
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.186855792999268 / Valid loss: 5.56346074058896
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.649401664733887 / Valid loss: 5.552265010561261
Model is saved in epoch 2, overall batch: 1200
Training loss: 6.644792556762695 / Valid loss: 5.551360021318708
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.73404598236084 / Valid loss: 5.551116521017892
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.002068519592285 / Valid loss: 5.543919526963007
Model is saved in epoch 3, overall batch: 1500
Training loss: 5.211565971374512 / Valid loss: 5.530445948101225
Model is saved in epoch 3, overall batch: 1600
Training loss: 4.413474082946777 / Valid loss: 5.5389294419969834
Training loss: 5.650099277496338 / Valid loss: 5.534274639402117
Training loss: 4.929032325744629 / Valid loss: 5.520228640238444
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.663520097732544 / Valid loss: 5.525265702747164
Training loss: 4.548025131225586 / Valid loss: 5.503819140933809
Model is saved in epoch 4, overall batch: 2100
Training loss: 4.473043441772461 / Valid loss: 5.504852301733834
Training loss: 5.676331043243408 / Valid loss: 5.504366359256562
Training loss: 7.401066303253174 / Valid loss: 5.5190348693302695

Epoch: 5
Training loss: 4.693532466888428 / Valid loss: 5.516100965227399
Training loss: 4.425579071044922 / Valid loss: 5.505906166349138
Training loss: 6.140329360961914 / Valid loss: 5.507202395938692
Training loss: 6.074995994567871 / Valid loss: 5.505961988085792
Training loss: 8.119731903076172 / Valid loss: 5.552161357516334

Epoch: 6
Training loss: 4.920608043670654 / Valid loss: 5.4966453120822
Model is saved in epoch 6, overall batch: 3000
Training loss: 3.5722315311431885 / Valid loss: 5.499852854864938
Training loss: 5.343437671661377 / Valid loss: 5.503778316861108
Training loss: 4.2412567138671875 / Valid loss: 5.506594853174119
Training loss: 3.6292004585266113 / Valid loss: 5.508016241164435

Epoch: 7
Training loss: 3.034107208251953 / Valid loss: 5.492367213112967
Model is saved in epoch 7, overall batch: 3500
Training loss: 4.586381912231445 / Valid loss: 5.499123493830363
Training loss: 4.650707721710205 / Valid loss: 5.510363678705125
Training loss: 5.466540336608887 / Valid loss: 5.497427647454398
Training loss: 5.5551300048828125 / Valid loss: 5.492298257918585
Model is saved in epoch 7, overall batch: 3900

Epoch: 8
Training loss: 4.886886119842529 / Valid loss: 5.474659220377604
Model is saved in epoch 8, overall batch: 4000
Training loss: 4.101755619049072 / Valid loss: 5.49233805565607
Training loss: 6.47113037109375 / Valid loss: 5.503897562481108
Training loss: 5.396266937255859 / Valid loss: 5.508086136409214
Training loss: 4.64796257019043 / Valid loss: 5.505141226450602

Epoch: 9
Training loss: 4.131557941436768 / Valid loss: 5.504770914713542
Training loss: 4.4432172775268555 / Valid loss: 5.503911220459711
Training loss: 4.806093215942383 / Valid loss: 5.502504818780082
Training loss: 3.09077787399292 / Valid loss: 5.526000349862235

Epoch: 10
Training loss: 3.6631410121917725 / Valid loss: 5.479949353990101
Training loss: 4.112114906311035 / Valid loss: 5.509141531444731
Training loss: 4.880876064300537 / Valid loss: 5.4891913050696965
Training loss: 4.935422897338867 / Valid loss: 5.4816707951681956
Training loss: 4.868746280670166 / Valid loss: 5.496526232219877

Epoch: 11
Training loss: 5.796143531799316 / Valid loss: 5.490044389452253
Training loss: 4.455735683441162 / Valid loss: 5.489797442299979
Training loss: 4.99518346786499 / Valid loss: 5.50384598232451
Training loss: 4.147449493408203 / Valid loss: 5.487559879393805
Training loss: 4.281708240509033 / Valid loss: 5.502151564189366

Epoch: 12
Training loss: 6.217983245849609 / Valid loss: 5.480363432566325
Training loss: 4.277544975280762 / Valid loss: 5.503284899393718
Training loss: 5.311287879943848 / Valid loss: 5.5023855845133465
Training loss: 4.662668228149414 / Valid loss: 5.4882111140659875
Training loss: 4.268229007720947 / Valid loss: 5.495553059805007

Epoch: 13
Training loss: 4.589259147644043 / Valid loss: 5.500099588575817
Training loss: 4.708662986755371 / Valid loss: 5.517121866771153
Training loss: 5.091606616973877 / Valid loss: 5.527061525980631
Training loss: 3.2598397731781006 / Valid loss: 5.498584615616571
Training loss: 4.63021183013916 / Valid loss: 5.505291664032709

Epoch: 14
Training loss: 4.8848066329956055 / Valid loss: 5.512971237727574
Training loss: 3.3257455825805664 / Valid loss: 5.500072043282645
Training loss: 5.672566890716553 / Valid loss: 5.4990129538944785
Training loss: 4.858784198760986 / Valid loss: 5.500127120245071
Training loss: 4.002681255340576 / Valid loss: 5.510383708136422

Epoch: 15
Training loss: 3.3913888931274414 / Valid loss: 5.49686845824832
Training loss: 5.701915740966797 / Valid loss: 5.509979325249081
Training loss: 4.671329498291016 / Valid loss: 5.500605694452921
Training loss: 4.647863864898682 / Valid loss: 5.508078250430879
Training loss: 3.8420639038085938 / Valid loss: 5.495794729959397

Epoch: 16
Training loss: 5.851274490356445 / Valid loss: 5.518983300526937
Training loss: 4.4100141525268555 / Valid loss: 5.518141233353388
Training loss: 3.2053446769714355 / Valid loss: 5.5425974573407855
Training loss: 5.162504196166992 / Valid loss: 5.508488487062
Training loss: 4.365017414093018 / Valid loss: 5.526884128933861

Epoch: 17
Training loss: 4.261116981506348 / Valid loss: 5.52179615838187
Training loss: 4.306163787841797 / Valid loss: 5.510702339808146
Training loss: 2.8198914527893066 / Valid loss: 5.506832947049822
Training loss: 6.722352504730225 / Valid loss: 5.542065100442795
Training loss: 3.9405417442321777 / Valid loss: 5.5194371223449705

Epoch: 18
Training loss: 4.212460041046143 / Valid loss: 5.535814968744914
Training loss: 3.9081106185913086 / Valid loss: 5.5212704386029925
Training loss: 4.497907638549805 / Valid loss: 5.5538211459205264
Training loss: 4.670535087585449 / Valid loss: 5.562773643221174
Training loss: 6.115987777709961 / Valid loss: 5.540162744976225

Epoch: 19
Training loss: 4.058779716491699 / Valid loss: 5.573500356220063
Training loss: 3.6568846702575684 / Valid loss: 5.561927629652478
Training loss: 5.323386192321777 / Valid loss: 5.577203487214588
Training loss: 4.756336212158203 / Valid loss: 5.543887578873408

Epoch: 20
Training loss: 3.6379096508026123 / Valid loss: 5.566910766419911
Training loss: 3.4808106422424316 / Valid loss: 5.551528624125889
Training loss: 3.5602786540985107 / Valid loss: 5.571799677894229
Training loss: 4.524139404296875 / Valid loss: 5.535100693929763
Training loss: 2.596696376800537 / Valid loss: 5.555288137708391

Epoch: 21
Training loss: 4.588597297668457 / Valid loss: 5.570718812942505
Training loss: 4.202706336975098 / Valid loss: 5.56803754397801
Training loss: 5.053424835205078 / Valid loss: 5.556990984507969
Training loss: 5.559844970703125 / Valid loss: 5.615033858163017
Training loss: 3.5589447021484375 / Valid loss: 5.570238447189331

Epoch: 22
Training loss: 2.8279476165771484 / Valid loss: 5.552148457935878
Training loss: 3.9962644577026367 / Valid loss: 5.5627135685511995
Training loss: 4.602024078369141 / Valid loss: 5.578122120811826
Training loss: 5.275311470031738 / Valid loss: 5.574670871098836
Training loss: 3.4007039070129395 / Valid loss: 5.611990236100697

Epoch: 23
Training loss: 3.506819248199463 / Valid loss: 5.608497685477847
Training loss: 4.4308295249938965 / Valid loss: 5.6097136474791025
Training loss: 3.846752643585205 / Valid loss: 5.592912217548915
Training loss: 3.874382495880127 / Valid loss: 5.588585467565627
Training loss: 3.9579169750213623 / Valid loss: 5.612251442954654

Epoch: 24
Training loss: 3.2407426834106445 / Valid loss: 5.611904269173031
Training loss: 5.147474765777588 / Valid loss: 5.600155026572091
Training loss: 4.280293941497803 / Valid loss: 5.636954307556152
Training loss: 3.4166159629821777 / Valid loss: 5.621891839163644
Training loss: 4.060394287109375 / Valid loss: 5.605735340572539

Epoch: 25
Training loss: 3.2495851516723633 / Valid loss: 5.611431936990647
Training loss: 3.4407732486724854 / Valid loss: 5.644020244053432
Training loss: 4.946911811828613 / Valid loss: 5.6506095908937
Training loss: 3.0040249824523926 / Valid loss: 5.635368921643212
Training loss: 3.889343738555908 / Valid loss: 5.62805655343192

Epoch: 26
Training loss: 2.996662139892578 / Valid loss: 5.611626445679438
Training loss: 2.925067901611328 / Valid loss: 5.62699419430324
Training loss: 3.187985420227051 / Valid loss: 5.639360772995722
Training loss: 3.893946886062622 / Valid loss: 5.621298131488619
Training loss: 2.843684673309326 / Valid loss: 5.620655965805054

Epoch: 27
Training loss: 3.7672834396362305 / Valid loss: 5.64639957064674
Training loss: 2.7289276123046875 / Valid loss: 5.633789216904413
Training loss: 3.2504992485046387 / Valid loss: 5.641236021405175
Training loss: 4.243644714355469 / Valid loss: 5.655892219997588
Training loss: 5.524693489074707 / Valid loss: 5.6541340510050455

Epoch: 28
Training loss: 3.8135952949523926 / Valid loss: 5.6530906654539566
Training loss: 3.6007578372955322 / Valid loss: 5.6477264858427505
Training loss: 4.173424243927002 / Valid loss: 5.6668715658641995
Training loss: 5.246459007263184 / Valid loss: 5.665747717448643
Training loss: 3.0439608097076416 / Valid loss: 5.642090011778332

Epoch: 29
Training loss: 3.58587646484375 / Valid loss: 5.662632715134394
Training loss: 2.7467689514160156 / Valid loss: 5.680554662431989
Training loss: 4.383121490478516 / Valid loss: 5.698691611062912
Training loss: 3.34489107131958 / Valid loss: 5.758611615498861

Epoch: 30
Training loss: 4.151186943054199 / Valid loss: 5.671727868488857
Training loss: 3.674302339553833 / Valid loss: 5.703754268373761
Training loss: 3.884908676147461 / Valid loss: 5.725432897749402
Training loss: 3.5936551094055176 / Valid loss: 5.684290361404419
Training loss: 2.7679266929626465 / Valid loss: 5.715865341822306

Epoch: 31
Training loss: 4.056114196777344 / Valid loss: 5.683624188105266
Training loss: 3.664895534515381 / Valid loss: 5.692878041948591
Training loss: 3.0966057777404785 / Valid loss: 5.70778560865493
Training loss: 3.4828014373779297 / Valid loss: 5.718252057120914
Training loss: 3.1745595932006836 / Valid loss: 5.747333760488601

Epoch: 32
Training loss: 2.7213573455810547 / Valid loss: 5.714215083349319
Training loss: 3.069550037384033 / Valid loss: 5.7990729468209405
Training loss: 2.665076732635498 / Valid loss: 5.713065317698887
Training loss: 3.8052682876586914 / Valid loss: 5.692632895424253
Training loss: 2.8512539863586426 / Valid loss: 5.745132795969645

Epoch: 33
Training loss: 2.8758857250213623 / Valid loss: 5.7084731737772625
Training loss: 3.3629555702209473 / Valid loss: 5.727412873222715
Training loss: 2.928328037261963 / Valid loss: 5.724246585936774
Training loss: 2.756891965866089 / Valid loss: 5.745341103417533
Training loss: 3.2078771591186523 / Valid loss: 5.771967158998762

Epoch: 34
Training loss: 2.970460891723633 / Valid loss: 5.740310643968128
Training loss: 4.128597259521484 / Valid loss: 5.750034207389469
Training loss: 3.399350643157959 / Valid loss: 5.76904753957476
Training loss: 4.350603103637695 / Valid loss: 5.795042848587036
Training loss: 1.7697536945343018 / Valid loss: 5.760461968467349

Epoch: 35
Training loss: 3.291290521621704 / Valid loss: 5.798365856352307
Training loss: 2.5077061653137207 / Valid loss: 5.7504554385230655
Training loss: 2.8008763790130615 / Valid loss: 5.76286772546314
Training loss: 3.295492172241211 / Valid loss: 5.82649644442967
Training loss: 2.7681703567504883 / Valid loss: 5.787088768822806

Epoch: 36
Training loss: 3.7842483520507812 / Valid loss: 5.762243874867758
Training loss: 2.9187448024749756 / Valid loss: 5.824392820539929
Training loss: 3.0790843963623047 / Valid loss: 5.817818934576852
Training loss: 2.727980136871338 / Valid loss: 5.8526769047691705
Training loss: 3.242953300476074 / Valid loss: 5.801236824762253

Epoch: 37
Training loss: 3.147149085998535 / Valid loss: 5.819170138949439
Training loss: 2.2690486907958984 / Valid loss: 5.788810834430513
Training loss: 3.48783540725708 / Valid loss: 5.796838149570283
Training loss: 3.169800281524658 / Valid loss: 5.834577371960594
Training loss: 2.5627899169921875 / Valid loss: 5.785411105837141

Epoch: 38
Training loss: 2.7082982063293457 / Valid loss: 5.808466570717948
Training loss: 2.8753890991210938 / Valid loss: 5.796747266678583
Training loss: 3.0182604789733887 / Valid loss: 5.8696159022195
Training loss: 2.8858819007873535 / Valid loss: 5.844158967336019
Training loss: 3.5476016998291016 / Valid loss: 5.907640389033726

Epoch: 39
Training loss: 2.4712467193603516 / Valid loss: 5.832042759940737
Training loss: 3.704918384552002 / Valid loss: 5.857532598858788
Training loss: 2.9705753326416016 / Valid loss: 5.89889053617205
Training loss: 2.5232200622558594 / Valid loss: 5.889031610034761
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 4000): 5.347537930806478
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.62260627746582 / Valid loss: 16.374367069062732
Model is saved in epoch 0, overall batch: 0
Training loss: 9.723288536071777 / Valid loss: 8.036899130684988
Model is saved in epoch 0, overall batch: 100
Training loss: 7.480328559875488 / Valid loss: 6.270146097455706
Model is saved in epoch 0, overall batch: 200
Training loss: 6.113795757293701 / Valid loss: 5.767492705299741
Model is saved in epoch 0, overall batch: 300
Training loss: 8.645271301269531 / Valid loss: 5.683840822038197
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.785533905029297 / Valid loss: 5.636974459602719
Model is saved in epoch 1, overall batch: 500
Training loss: 5.952703475952148 / Valid loss: 5.620285324823289
Model is saved in epoch 1, overall batch: 600
Training loss: 7.7793803215026855 / Valid loss: 5.607268383389427
Model is saved in epoch 1, overall batch: 700
Training loss: 5.6182637214660645 / Valid loss: 5.591315344401768
Model is saved in epoch 1, overall batch: 800
Training loss: 5.868444442749023 / Valid loss: 5.5809510162898475
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.585034370422363 / Valid loss: 5.568961549940563
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.186882972717285 / Valid loss: 5.563456505820865
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.649396896362305 / Valid loss: 5.552265065056937
Model is saved in epoch 2, overall batch: 1200
Training loss: 6.644820690155029 / Valid loss: 5.551359042667207
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.73406982421875 / Valid loss: 5.551118437449137
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.002080917358398 / Valid loss: 5.54392496971857
Model is saved in epoch 3, overall batch: 1500
Training loss: 5.211609840393066 / Valid loss: 5.530446481704712
Model is saved in epoch 3, overall batch: 1600
Training loss: 4.413424968719482 / Valid loss: 5.53893966220674
Training loss: 5.649910926818848 / Valid loss: 5.534280388695853
Training loss: 4.928735733032227 / Valid loss: 5.520259437106905
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 3.6637821197509766 / Valid loss: 5.525248706908453
Training loss: 4.548374176025391 / Valid loss: 5.5037627606164845
Model is saved in epoch 4, overall batch: 2100
Training loss: 4.473306655883789 / Valid loss: 5.504871674946377
Training loss: 5.676448822021484 / Valid loss: 5.504417635145641
Training loss: 7.401418685913086 / Valid loss: 5.519118574687413

Epoch: 5
Training loss: 4.693766117095947 / Valid loss: 5.5161796524411155
Training loss: 4.425787925720215 / Valid loss: 5.5060372647785005
Training loss: 6.140506744384766 / Valid loss: 5.507337004797799
Training loss: 6.074079513549805 / Valid loss: 5.506215143203735
Training loss: 8.116631507873535 / Valid loss: 5.5523426146734325

Epoch: 6
Training loss: 4.918657302856445 / Valid loss: 5.49670557975769
Model is saved in epoch 6, overall batch: 3000
Training loss: 3.572598934173584 / Valid loss: 5.499859056018647
Training loss: 5.342824935913086 / Valid loss: 5.503897421700614
Training loss: 4.241785049438477 / Valid loss: 5.506638651802426
Training loss: 3.6312294006347656 / Valid loss: 5.507924595333281

Epoch: 7
Training loss: 3.0334644317626953 / Valid loss: 5.492257717677525
Model is saved in epoch 7, overall batch: 3500
Training loss: 4.586280822753906 / Valid loss: 5.498974861417498
Training loss: 4.651912212371826 / Valid loss: 5.510370858510336
Training loss: 5.4689178466796875 / Valid loss: 5.497255911145891
Training loss: 5.552107810974121 / Valid loss: 5.4920355456216
Model is saved in epoch 7, overall batch: 3900

Epoch: 8
Training loss: 4.887378692626953 / Valid loss: 5.474436853045509
Model is saved in epoch 8, overall batch: 4000
Training loss: 4.09902811050415 / Valid loss: 5.492501835595994
Training loss: 6.473148345947266 / Valid loss: 5.5038538160778225
Training loss: 5.394598960876465 / Valid loss: 5.507829214277722
Training loss: 4.645275592803955 / Valid loss: 5.504847755886259

Epoch: 9
Training loss: 4.130380630493164 / Valid loss: 5.5047260284423825
Training loss: 4.44229793548584 / Valid loss: 5.503754538581485
Training loss: 4.80728816986084 / Valid loss: 5.501977189381917
Training loss: 3.0919346809387207 / Valid loss: 5.525579050609044

Epoch: 10
Training loss: 3.6636745929718018 / Valid loss: 5.48000789596921
Training loss: 4.1081085205078125 / Valid loss: 5.50879343804859
Training loss: 4.881070137023926 / Valid loss: 5.488865981783186
Training loss: 4.931209564208984 / Valid loss: 5.481945859818231
Training loss: 4.870820999145508 / Valid loss: 5.496198449816022

Epoch: 11
Training loss: 5.798501968383789 / Valid loss: 5.4900022279648555
Training loss: 4.457118511199951 / Valid loss: 5.489596387318202
Training loss: 4.992867946624756 / Valid loss: 5.503861288797288
Training loss: 4.149594783782959 / Valid loss: 5.487805162157331
Training loss: 4.281369209289551 / Valid loss: 5.501844896589007

Epoch: 12
Training loss: 6.214230060577393 / Valid loss: 5.480278766722906
Training loss: 4.279483795166016 / Valid loss: 5.503466715131487
Training loss: 5.3076372146606445 / Valid loss: 5.50272361664545
Training loss: 4.665176868438721 / Valid loss: 5.488132712954567
Training loss: 4.263731002807617 / Valid loss: 5.4953823657262895

Epoch: 13
Training loss: 4.586020469665527 / Valid loss: 5.500082785742624
Training loss: 4.711299896240234 / Valid loss: 5.517089986801148
Training loss: 5.08984375 / Valid loss: 5.527070376986549
Training loss: 3.2612791061401367 / Valid loss: 5.498793138776507
Training loss: 4.635196685791016 / Valid loss: 5.505122815994989

Epoch: 14
Training loss: 4.882696151733398 / Valid loss: 5.5130198887416295
Training loss: 3.326587677001953 / Valid loss: 5.500034645625523
Training loss: 5.667545795440674 / Valid loss: 5.498861271994454
Training loss: 4.85887336730957 / Valid loss: 5.500060017903646
Training loss: 4.000823020935059 / Valid loss: 5.510410758427211

Epoch: 15
Training loss: 3.392103672027588 / Valid loss: 5.49733343351455
Training loss: 5.707502365112305 / Valid loss: 5.509734651020595
Training loss: 4.669041633605957 / Valid loss: 5.500930011840094
Training loss: 4.644891738891602 / Valid loss: 5.508569651558286
Training loss: 3.8455424308776855 / Valid loss: 5.496074151992798

Epoch: 16
Training loss: 5.846846103668213 / Valid loss: 5.518978148414975
Training loss: 4.404280662536621 / Valid loss: 5.518353366851807
Training loss: 3.203958511352539 / Valid loss: 5.54269662357512
Training loss: 5.1612420082092285 / Valid loss: 5.508572966711862
Training loss: 4.3693389892578125 / Valid loss: 5.527254061471848

Epoch: 17
Training loss: 4.256529331207275 / Valid loss: 5.521834389368693
Training loss: 4.305562496185303 / Valid loss: 5.511171745118641
Training loss: 2.8240883350372314 / Valid loss: 5.507188887823196
Training loss: 6.718035697937012 / Valid loss: 5.543031006767636
Training loss: 3.942157745361328 / Valid loss: 5.519395971298218

Epoch: 18
Training loss: 4.213829040527344 / Valid loss: 5.536194256373814
Training loss: 3.9074597358703613 / Valid loss: 5.520773467563448
Training loss: 4.503572940826416 / Valid loss: 5.553511113212222
Training loss: 4.6747002601623535 / Valid loss: 5.562828570320493
Training loss: 6.113270282745361 / Valid loss: 5.540248244149344

Epoch: 19
Training loss: 4.061033725738525 / Valid loss: 5.573297848020281
Training loss: 3.658078908920288 / Valid loss: 5.5624868778955365
Training loss: 5.31833553314209 / Valid loss: 5.577847453526088
Training loss: 4.755132675170898 / Valid loss: 5.544396672930036

Epoch: 20
Training loss: 3.6343235969543457 / Valid loss: 5.567087527683803
Training loss: 3.4778671264648438 / Valid loss: 5.551196091515678
Training loss: 3.559666872024536 / Valid loss: 5.5723910172780355
Training loss: 4.5180277824401855 / Valid loss: 5.535435683386666
Training loss: 2.596496105194092 / Valid loss: 5.5550783906664165

Epoch: 21
Training loss: 4.585768222808838 / Valid loss: 5.571046545391991
Training loss: 4.205103874206543 / Valid loss: 5.567989188148862
Training loss: 5.05683708190918 / Valid loss: 5.5564980189005535
Training loss: 5.564887046813965 / Valid loss: 5.615860151109241
Training loss: 3.5518887042999268 / Valid loss: 5.569772772561937

Epoch: 22
Training loss: 2.8231382369995117 / Valid loss: 5.552004439490182
Training loss: 3.999560832977295 / Valid loss: 5.561977479571388
Training loss: 4.605886459350586 / Valid loss: 5.577895411990938
Training loss: 5.269722938537598 / Valid loss: 5.574755455198742
Training loss: 3.4074487686157227 / Valid loss: 5.61271154085795

Epoch: 23
Training loss: 3.50044846534729 / Valid loss: 5.608724369321551
Training loss: 4.42423152923584 / Valid loss: 5.6104021049681165
Training loss: 3.857783794403076 / Valid loss: 5.5929040999639605
Training loss: 3.8760228157043457 / Valid loss: 5.589006955283029
Training loss: 3.9602839946746826 / Valid loss: 5.6122960908072335

Epoch: 24
Training loss: 3.243488311767578 / Valid loss: 5.611888529005505
Training loss: 5.143440246582031 / Valid loss: 5.6004069214775445
Training loss: 4.278865814208984 / Valid loss: 5.637746938069662
Training loss: 3.4115703105926514 / Valid loss: 5.622137680507842
Training loss: 4.056952476501465 / Valid loss: 5.6055088792528425

Epoch: 25
Training loss: 3.2521443367004395 / Valid loss: 5.611518680481684
Training loss: 3.436547040939331 / Valid loss: 5.645196817034766
Training loss: 4.957773208618164 / Valid loss: 5.650494736716861
Training loss: 2.9985718727111816 / Valid loss: 5.634252941040765
Training loss: 3.8763015270233154 / Valid loss: 5.627186309723627

Epoch: 26
Training loss: 3.0038132667541504 / Valid loss: 5.611103607359387
Training loss: 2.9202394485473633 / Valid loss: 5.626494859513782
Training loss: 3.184746265411377 / Valid loss: 5.638265403111776
Training loss: 3.8921730518341064 / Valid loss: 5.619991756620861
Training loss: 2.834413528442383 / Valid loss: 5.619405056181408

Epoch: 27
Training loss: 3.765237331390381 / Valid loss: 5.6457273914700465
Training loss: 2.736143112182617 / Valid loss: 5.6327878248123895
Training loss: 3.2522430419921875 / Valid loss: 5.639432913916451
Training loss: 4.243781566619873 / Valid loss: 5.656237034570603
Training loss: 5.518776893615723 / Valid loss: 5.654074473608108

Epoch: 28
Training loss: 3.818024158477783 / Valid loss: 5.653284354436965
Training loss: 3.607302665710449 / Valid loss: 5.647713767914545
Training loss: 4.173489570617676 / Valid loss: 5.667348979768299
Training loss: 5.24009370803833 / Valid loss: 5.665690272194999
Training loss: 3.0571224689483643 / Valid loss: 5.642434042976016

Epoch: 29
Training loss: 3.5868301391601562 / Valid loss: 5.662393120356969
Training loss: 2.7315616607666016 / Valid loss: 5.680823019572666
Training loss: 4.389266014099121 / Valid loss: 5.6992232118334085
Training loss: 3.347064733505249 / Valid loss: 5.7580490339370005

Epoch: 30
Training loss: 4.158736705780029 / Valid loss: 5.671867252531506
Training loss: 3.6776657104492188 / Valid loss: 5.703842830657959
Training loss: 3.8741354942321777 / Valid loss: 5.724953349431356
Training loss: 3.591240644454956 / Valid loss: 5.68401924996149
Training loss: 2.7636148929595947 / Valid loss: 5.715060704095023

Epoch: 31
Training loss: 4.077682018280029 / Valid loss: 5.68218392871675
Training loss: 3.6701948642730713 / Valid loss: 5.6917330878121515
Training loss: 3.0798521041870117 / Valid loss: 5.707406618481591
Training loss: 3.482316732406616 / Valid loss: 5.718590929394677
Training loss: 3.156865119934082 / Valid loss: 5.748066641035534

Epoch: 32
Training loss: 2.7124955654144287 / Valid loss: 5.71425260362171
Training loss: 3.0766119956970215 / Valid loss: 5.80007396652585
Training loss: 2.663081645965576 / Valid loss: 5.712989214488438
Training loss: 3.798677921295166 / Valid loss: 5.694481831505185
Training loss: 2.8487086296081543 / Valid loss: 5.7449558689480735

Epoch: 33
Training loss: 2.8693737983703613 / Valid loss: 5.7086273919968376
Training loss: 3.3576180934906006 / Valid loss: 5.727203873225621
Training loss: 2.9315943717956543 / Valid loss: 5.725746018545968
Training loss: 2.7588160037994385 / Valid loss: 5.743486068362281
Training loss: 3.2157323360443115 / Valid loss: 5.772179297038487

Epoch: 34
Training loss: 2.9776384830474854 / Valid loss: 5.740868484406244
Training loss: 4.1263017654418945 / Valid loss: 5.748862788790748
Training loss: 3.4109034538269043 / Valid loss: 5.768495523361933
Training loss: 4.3480682373046875 / Valid loss: 5.792703356061663
Training loss: 1.765679955482483 / Valid loss: 5.760578541528611

Epoch: 35
Training loss: 3.2896625995635986 / Valid loss: 5.796803440366473
Training loss: 2.512876272201538 / Valid loss: 5.751514452979678
Training loss: 2.813260078430176 / Valid loss: 5.762008424032302
Training loss: 3.2870430946350098 / Valid loss: 5.827727281479609
Training loss: 2.757317066192627 / Valid loss: 5.786882259732201

Epoch: 36
Training loss: 3.7821316719055176 / Valid loss: 5.762293306986491
Training loss: 2.936312675476074 / Valid loss: 5.824738257271903
Training loss: 3.0810415744781494 / Valid loss: 5.817609789257958
Training loss: 2.7263834476470947 / Valid loss: 5.85210379645938
Training loss: 3.2343170642852783 / Valid loss: 5.8006615343548

Epoch: 37
Training loss: 3.1482715606689453 / Valid loss: 5.818430805206299
Training loss: 2.27854323387146 / Valid loss: 5.7896670114426385
Training loss: 3.4916045665740967 / Valid loss: 5.797757246380761
Training loss: 3.166198253631592 / Valid loss: 5.834591388702393
Training loss: 2.585472583770752 / Valid loss: 5.786121674946377

Epoch: 38
Training loss: 2.686634063720703 / Valid loss: 5.808879663830711
Training loss: 2.8723983764648438 / Valid loss: 5.796317808968681
Training loss: 3.013582706451416 / Valid loss: 5.869918562117077
Training loss: 2.8888444900512695 / Valid loss: 5.842481488273258
Training loss: 3.5383005142211914 / Valid loss: 5.9082797867911205

Epoch: 39
Training loss: 2.469965934753418 / Valid loss: 5.833531970069522
Training loss: 3.7018380165100098 / Valid loss: 5.857479356584095
Training loss: 2.9695982933044434 / Valid loss: 5.898307629993984
Training loss: 2.5115745067596436 / Valid loss: 5.8887995311192105
ModuleList(
  (0): Linear(in_features=5376, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 4000): 5.347630900428409
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)

Epoch: 0
Training loss: 19.044994354248047 / Valid loss: 11.349013292221796
Model is saved in epoch 0, overall batch: 0
Training loss: 6.31700325012207 / Valid loss: 5.905089914231073
Model is saved in epoch 0, overall batch: 100
Training loss: 6.593419075012207 / Valid loss: 5.793816791261945
Model is saved in epoch 0, overall batch: 200
Training loss: 6.715387344360352 / Valid loss: 5.764516687393188
Model is saved in epoch 0, overall batch: 300
Training loss: 3.6688740253448486 / Valid loss: 5.729876740773519
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.5627760887146 / Valid loss: 5.704308709644136
Model is saved in epoch 1, overall batch: 500
Training loss: 4.95412540435791 / Valid loss: 5.68383511588687
Model is saved in epoch 1, overall batch: 600
Training loss: 7.187725067138672 / Valid loss: 5.662028414862497
Model is saved in epoch 1, overall batch: 700
Training loss: 4.050303936004639 / Valid loss: 5.710240899948847
Training loss: 4.780117988586426 / Valid loss: 5.64637401898702
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.331770896911621 / Valid loss: 5.626282887231736
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.333743572235107 / Valid loss: 5.622991518747239
Model is saved in epoch 2, overall batch: 1100
Training loss: 6.338711738586426 / Valid loss: 5.603515872501192
Model is saved in epoch 2, overall batch: 1200
Training loss: 3.971149444580078 / Valid loss: 5.621077514830089
Training loss: 4.3727850914001465 / Valid loss: 5.595108715693156
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.965295791625977 / Valid loss: 5.59591551281157
Training loss: 3.79403018951416 / Valid loss: 5.608600754964919
Training loss: 7.483267307281494 / Valid loss: 5.5810742173876084
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.969719409942627 / Valid loss: 5.572096129826137
Model is saved in epoch 3, overall batch: 1800
Training loss: 6.026633262634277 / Valid loss: 5.579239931560698

Epoch: 4
Training loss: 6.12208366394043 / Valid loss: 5.567405548549834
Model is saved in epoch 4, overall batch: 2000
Training loss: 4.679856300354004 / Valid loss: 5.571770406904674
Training loss: 4.540025234222412 / Valid loss: 5.625917841139294
Training loss: 4.039777755737305 / Valid loss: 5.559551327569144
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.791963577270508 / Valid loss: 5.552749061584473
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 7.602558135986328 / Valid loss: 5.561055862335932
Training loss: 4.626372337341309 / Valid loss: 5.547312102999006
Model is saved in epoch 5, overall batch: 2600
Training loss: 5.16206693649292 / Valid loss: 5.553382106054396
Training loss: 4.927486896514893 / Valid loss: 5.544127788997832
Model is saved in epoch 5, overall batch: 2800
Training loss: 6.704800605773926 / Valid loss: 5.543352762858073
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 5.106719970703125 / Valid loss: 5.531643209003267
Model is saved in epoch 6, overall batch: 3000
Training loss: 6.313854694366455 / Valid loss: 5.543575286865234
Training loss: 5.482738494873047 / Valid loss: 5.536455724352882
Training loss: 4.352067947387695 / Valid loss: 5.541580599830264
Training loss: 6.432716369628906 / Valid loss: 5.541386985778809

Epoch: 7
Training loss: 4.820429801940918 / Valid loss: 5.529477678026471
Model is saved in epoch 7, overall batch: 3500
Training loss: 4.448275566101074 / Valid loss: 5.537089259283883
Training loss: 5.638060569763184 / Valid loss: 5.5239050002325145
Model is saved in epoch 7, overall batch: 3700
Training loss: 6.014341354370117 / Valid loss: 5.526779236112322
Training loss: 5.269235610961914 / Valid loss: 5.5226217451549715
Model is saved in epoch 7, overall batch: 3900

Epoch: 8
Training loss: 4.808967590332031 / Valid loss: 5.525526039940971
Training loss: 4.049810886383057 / Valid loss: 5.528590202331543
Training loss: 4.1143388748168945 / Valid loss: 5.510333054406303
Model is saved in epoch 8, overall batch: 4200
Training loss: 5.021595001220703 / Valid loss: 5.513820098695301
Training loss: 2.7308027744293213 / Valid loss: 5.52213910647801

Epoch: 9
Training loss: 3.946047306060791 / Valid loss: 5.530519451413836
Training loss: 5.276594161987305 / Valid loss: 5.502488211223057
Model is saved in epoch 9, overall batch: 4600
Training loss: 5.750149726867676 / Valid loss: 5.534052090417771
Training loss: 6.312538146972656 / Valid loss: 5.509636336281186

Epoch: 10
Training loss: 4.237011909484863 / Valid loss: 5.519681794302804
Training loss: 4.898504257202148 / Valid loss: 5.555679891222999
Training loss: 5.249238967895508 / Valid loss: 5.509013382593791
Training loss: 5.578888416290283 / Valid loss: 5.504198844092233
Training loss: 4.453372001647949 / Valid loss: 5.5083538373311365

Epoch: 11
Training loss: 5.554969310760498 / Valid loss: 5.498457177480062
Model is saved in epoch 11, overall batch: 5400
Training loss: 5.553383827209473 / Valid loss: 5.510567140579224
Training loss: 5.373669147491455 / Valid loss: 5.503158010755267
Training loss: 7.921215057373047 / Valid loss: 5.505223256065732
Training loss: 4.971521854400635 / Valid loss: 5.5033982004438124

Epoch: 12
Training loss: 5.788403034210205 / Valid loss: 5.504535032453991
Training loss: 5.972039699554443 / Valid loss: 5.508869541259039
Training loss: 3.0506229400634766 / Valid loss: 5.511122347059704
Training loss: 4.655502796173096 / Valid loss: 5.49941128776187
Training loss: 4.3259992599487305 / Valid loss: 5.51968119712103

Epoch: 13
Training loss: 5.089466094970703 / Valid loss: 5.499350543249221
Training loss: 4.569154262542725 / Valid loss: 5.49602207229251
Model is saved in epoch 13, overall batch: 6500
Training loss: 6.18456506729126 / Valid loss: 5.504382449104672
Training loss: 5.97960090637207 / Valid loss: 5.499639688219343
Training loss: 5.067204475402832 / Valid loss: 5.495998766308739
Model is saved in epoch 13, overall batch: 6800

Epoch: 14
Training loss: 5.982233047485352 / Valid loss: 5.501403588340396
Training loss: 5.610561847686768 / Valid loss: 5.504923336846488
Training loss: 6.13571834564209 / Valid loss: 5.501434346607753
Training loss: 3.9645848274230957 / Valid loss: 5.499323206856137
Training loss: 6.767385482788086 / Valid loss: 5.492342338107881
Model is saved in epoch 14, overall batch: 7300

Epoch: 15
Training loss: 6.0597662925720215 / Valid loss: 5.4893773101624985
Model is saved in epoch 15, overall batch: 7400
Training loss: 5.276383399963379 / Valid loss: 5.495992676417033
Training loss: 4.7205047607421875 / Valid loss: 5.496350585846674
Training loss: 6.450070381164551 / Valid loss: 5.496711767287481
Training loss: 5.178670406341553 / Valid loss: 5.5094423021589005

Epoch: 16
Training loss: 4.506061553955078 / Valid loss: 5.520187525522141
Training loss: 4.055694580078125 / Valid loss: 5.557353119623094
Training loss: 5.324777126312256 / Valid loss: 5.492258709952945
Training loss: 5.822493076324463 / Valid loss: 5.4979752245403475
Training loss: 4.790768146514893 / Valid loss: 5.493253835042318

Epoch: 17
Training loss: 6.292952537536621 / Valid loss: 5.48845979145595
Model is saved in epoch 17, overall batch: 8400
Training loss: 4.928757667541504 / Valid loss: 5.489072197959537
Training loss: 4.912337303161621 / Valid loss: 5.497484643118722
Training loss: 4.328956127166748 / Valid loss: 5.488738291604179
Training loss: 7.2870564460754395 / Valid loss: 5.4914187204270135

Epoch: 18
Training loss: 5.80822229385376 / Valid loss: 5.547522692453294
Training loss: 5.147208213806152 / Valid loss: 5.521215600059146
Training loss: 5.566795825958252 / Valid loss: 5.49931644258045
Training loss: 4.34276008605957 / Valid loss: 5.499067660740444
Training loss: 4.98811149597168 / Valid loss: 5.488948817480178

Epoch: 19
Training loss: 4.171673774719238 / Valid loss: 5.489723993483044
Training loss: 4.714555740356445 / Valid loss: 5.490117747443063
Training loss: 4.793225288391113 / Valid loss: 5.508531870160784
Training loss: 4.156498908996582 / Valid loss: 5.480194850195022
Model is saved in epoch 19, overall batch: 9700

Epoch: 20
Training loss: 4.910816669464111 / Valid loss: 5.494253921508789
Training loss: 5.096790313720703 / Valid loss: 5.495265288580032
Training loss: 3.3895273208618164 / Valid loss: 5.4833473886762345
Training loss: 3.437947988510132 / Valid loss: 5.484725759142921
Training loss: 5.0664896965026855 / Valid loss: 5.481539297103882

Epoch: 21
Training loss: 4.811033725738525 / Valid loss: 5.50449990999131
Training loss: 6.319272518157959 / Valid loss: 5.481419595082601
Training loss: 6.112035751342773 / Valid loss: 5.477619913646153
Model is saved in epoch 21, overall batch: 10500
Training loss: 7.5436320304870605 / Valid loss: 5.501037225269136
Training loss: 6.399226665496826 / Valid loss: 5.494377949124291

Epoch: 22
Training loss: 5.843699932098389 / Valid loss: 5.480241262345087
Training loss: 4.525263786315918 / Valid loss: 5.494183102108184
Training loss: 6.729159832000732 / Valid loss: 5.49206379935855
Training loss: 5.906726360321045 / Valid loss: 5.490429283323742
Training loss: 4.255196571350098 / Valid loss: 5.488908379418509

Epoch: 23
Training loss: 5.01151180267334 / Valid loss: 5.484398092542376
Training loss: 6.18739128112793 / Valid loss: 5.486584454491025
Training loss: 5.0955586433410645 / Valid loss: 5.491677520388649
Training loss: 5.543523788452148 / Valid loss: 5.486950822103591
Training loss: 7.15694522857666 / Valid loss: 5.5044311387198315

Epoch: 24
Training loss: 7.150004863739014 / Valid loss: 5.498113316581363
Training loss: 5.420799255371094 / Valid loss: 5.485689921606155
Training loss: 5.240779399871826 / Valid loss: 5.486647008714222
Training loss: 7.460471153259277 / Valid loss: 5.480557938984462
Training loss: 3.609050750732422 / Valid loss: 5.516377208346412

Epoch: 25
Training loss: 4.251532554626465 / Valid loss: 5.482911028180804
Training loss: 3.0913028717041016 / Valid loss: 5.521411421185448
Training loss: 3.9029440879821777 / Valid loss: 5.488104002816336
Training loss: 7.109899520874023 / Valid loss: 5.482312788282122
Training loss: 3.4698028564453125 / Valid loss: 5.495575203214373

Epoch: 26
Training loss: 5.193143367767334 / Valid loss: 5.497448814482916
Training loss: 6.061352729797363 / Valid loss: 5.491607195990426
Training loss: 6.145755767822266 / Valid loss: 5.505419168018159
Training loss: 5.721583366394043 / Valid loss: 5.479939331327166
Training loss: 5.359509468078613 / Valid loss: 5.485637646629697

Epoch: 27
Training loss: 7.404482841491699 / Valid loss: 5.499993194852556
Training loss: 4.219019889831543 / Valid loss: 5.48054221698216
Training loss: 5.110233783721924 / Valid loss: 5.473795718238467
Model is saved in epoch 27, overall batch: 13500
Training loss: 6.23029899597168 / Valid loss: 5.478098292577834
Training loss: 5.6072306632995605 / Valid loss: 5.4844209307716

Epoch: 28
Training loss: 4.329490661621094 / Valid loss: 5.4802176384698775
Training loss: 4.251819610595703 / Valid loss: 5.506153009051368
Training loss: 5.255067825317383 / Valid loss: 5.484463435127622
Training loss: 4.710307598114014 / Valid loss: 5.48685021627517
Training loss: 5.102937698364258 / Valid loss: 5.470061143239339
Model is saved in epoch 28, overall batch: 14200

Epoch: 29
Training loss: 6.412256717681885 / Valid loss: 5.487200966335478
Training loss: 5.032387733459473 / Valid loss: 5.489397069386073
Training loss: 6.419496536254883 / Valid loss: 5.494018295833043
Training loss: 6.481260299682617 / Valid loss: 5.482462274460565

Epoch: 30
Training loss: 5.8938188552856445 / Valid loss: 5.4826468694777715
Training loss: 5.669979572296143 / Valid loss: 5.480869023005168
Training loss: 5.075404167175293 / Valid loss: 5.519598654338291
Training loss: 4.429727077484131 / Valid loss: 5.486859564554123
Training loss: 5.020639419555664 / Valid loss: 5.478564189729236

Epoch: 31
Training loss: 5.210454940795898 / Valid loss: 5.479577561787196
Training loss: 5.510164260864258 / Valid loss: 5.490337878181821
Training loss: 4.326659202575684 / Valid loss: 5.481498543421427
Training loss: 5.283143043518066 / Valid loss: 5.481548207146781
Training loss: 6.486907005310059 / Valid loss: 5.483907547451201

Epoch: 32
Training loss: 6.168606281280518 / Valid loss: 5.480494142714001
Training loss: 6.515811443328857 / Valid loss: 5.474907870519729
Training loss: 4.865724086761475 / Valid loss: 5.489056825637817
Training loss: 4.477866172790527 / Valid loss: 5.54154832249596
Training loss: 7.398874282836914 / Valid loss: 5.483900642395019

Epoch: 33
Training loss: 4.180497169494629 / Valid loss: 5.473323635827928
Training loss: 6.794442176818848 / Valid loss: 5.483029694784255
Training loss: 5.458721160888672 / Valid loss: 5.484720956711542
Training loss: 5.885800361633301 / Valid loss: 5.487234814961751
Training loss: 6.703483581542969 / Valid loss: 5.483438548587618

Epoch: 34
Training loss: 5.3401079177856445 / Valid loss: 5.483200684047881
Training loss: 6.267440319061279 / Valid loss: 5.500939675739834
Training loss: 6.1842169761657715 / Valid loss: 5.5115320682525635
Training loss: 6.426022529602051 / Valid loss: 5.480967596599034
Training loss: 4.01905632019043 / Valid loss: 5.50996458871024

Epoch: 35
Training loss: 5.459199905395508 / Valid loss: 5.478091635022845
Training loss: 4.765631675720215 / Valid loss: 5.485227995827085
Training loss: 5.977102756500244 / Valid loss: 5.481779940923055
Training loss: 5.2541375160217285 / Valid loss: 5.4808813594636465
Training loss: 4.252647876739502 / Valid loss: 5.484035151345389

Epoch: 36
Training loss: 3.701453447341919 / Valid loss: 5.521883510407948
Training loss: 6.359225749969482 / Valid loss: 5.503930282592774
Training loss: 3.539958953857422 / Valid loss: 5.487509196145194
Training loss: 6.484989643096924 / Valid loss: 5.489508104324341
Training loss: 3.859426975250244 / Valid loss: 5.522289868763515

Epoch: 37
Training loss: 5.326664924621582 / Valid loss: 5.491401218232655
Training loss: 4.161127090454102 / Valid loss: 5.488222505932763
Training loss: 4.989400386810303 / Valid loss: 5.4926572527204245
Training loss: 5.756670951843262 / Valid loss: 5.4904121467045375
Training loss: 5.478690147399902 / Valid loss: 5.490102120808193

Epoch: 38
Training loss: 5.3428263664245605 / Valid loss: 5.486334852945237
Training loss: 5.796513557434082 / Valid loss: 5.482932006745111
Training loss: 5.841974258422852 / Valid loss: 5.4856246539524625
Training loss: 4.924668312072754 / Valid loss: 5.484494783764794
Training loss: 4.375683307647705 / Valid loss: 5.481277322769165

Epoch: 39
Training loss: 4.437516212463379 / Valid loss: 5.48297914550418
Training loss: 5.576230049133301 / Valid loss: 5.526049495878674
Training loss: 5.6154913902282715 / Valid loss: 5.486648916062855
Training loss: 5.331355094909668 / Valid loss: 5.484620675586519
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 14200): 5.308370290483747
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : Bert
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)

Epoch: 0
Training loss: 19.044994354248047 / Valid loss: 11.34901328768049
Model is saved in epoch 0, overall batch: 0
Training loss: 6.31700325012207 / Valid loss: 5.90508999824524
Model is saved in epoch 0, overall batch: 100
Training loss: 6.593419075012207 / Valid loss: 5.793816764014108
Model is saved in epoch 0, overall batch: 200
Training loss: 6.715387344360352 / Valid loss: 5.76451671009972
Model is saved in epoch 0, overall batch: 300
Training loss: 3.6688742637634277 / Valid loss: 5.729876756668091
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.5627760887146 / Valid loss: 5.704308727809361
Model is saved in epoch 1, overall batch: 500
Training loss: 4.95412540435791 / Valid loss: 5.683835134052095
Model is saved in epoch 1, overall batch: 600
Training loss: 7.187725067138672 / Valid loss: 5.662028439839681
Model is saved in epoch 1, overall batch: 700
Training loss: 4.0503034591674805 / Valid loss: 5.710240947632562
Training loss: 4.780118465423584 / Valid loss: 5.646374112083798
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.331770896911621 / Valid loss: 5.626282896314349
Model is saved in epoch 2, overall batch: 1000
Training loss: 5.333744049072266 / Valid loss: 5.62299157778422
Model is saved in epoch 2, overall batch: 1100
Training loss: 6.338712692260742 / Valid loss: 5.603515904290336
Model is saved in epoch 2, overall batch: 1200
Training loss: 3.971149444580078 / Valid loss: 5.621077539807274
Training loss: 4.372785568237305 / Valid loss: 5.595108731587728
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.965295791625977 / Valid loss: 5.595915576389857
Training loss: 3.79403018951416 / Valid loss: 5.608600802648635
Training loss: 7.483266830444336 / Valid loss: 5.58107426961263
Model is saved in epoch 3, overall batch: 1700
Training loss: 4.969719409942627 / Valid loss: 5.572096202487037
Model is saved in epoch 3, overall batch: 1800
Training loss: 6.026633262634277 / Valid loss: 5.579239999680292

Epoch: 4
Training loss: 6.1220831871032715 / Valid loss: 5.5674056053161625
Model is saved in epoch 4, overall batch: 2000
Training loss: 4.67985725402832 / Valid loss: 5.571770459129697
Training loss: 4.540025234222412 / Valid loss: 5.625917900176275
Training loss: 4.039778232574463 / Valid loss: 5.5595514115833105
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.791963577270508 / Valid loss: 5.5527491297040665
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 7.602559566497803 / Valid loss: 5.56105587595985
Training loss: 4.626372337341309 / Valid loss: 5.547312159765334
Model is saved in epoch 5, overall batch: 2600
Training loss: 5.162067413330078 / Valid loss: 5.5533820765359065
Training loss: 4.927486419677734 / Valid loss: 5.544127829869589
Model is saved in epoch 5, overall batch: 2800
Training loss: 6.704800605773926 / Valid loss: 5.543352837789627
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 5.106719970703125 / Valid loss: 5.531643317994617
Model is saved in epoch 6, overall batch: 3000
Training loss: 6.313855171203613 / Valid loss: 5.543575320925031
Training loss: 5.482738494873047 / Valid loss: 5.536455774307251
Training loss: 4.352067947387695 / Valid loss: 5.5415807065509615
Training loss: 6.432715892791748 / Valid loss: 5.5413870493570965

Epoch: 7
Training loss: 4.820430755615234 / Valid loss: 5.5294777347928
Model is saved in epoch 7, overall batch: 3500
Training loss: 4.448276042938232 / Valid loss: 5.537089363733927
Training loss: 5.6380615234375 / Valid loss: 5.523905109223866
Model is saved in epoch 7, overall batch: 3700
Training loss: 6.014341354370117 / Valid loss: 5.526779306502569
Training loss: 5.2692365646362305 / Valid loss: 5.5226217565082365
Model is saved in epoch 7, overall batch: 3900

Epoch: 8
Training loss: 4.8089680671691895 / Valid loss: 5.525526051294236
Training loss: 4.049810886383057 / Valid loss: 5.528590172813052
Training loss: 4.1143388748168945 / Valid loss: 5.510333031699771
Model is saved in epoch 8, overall batch: 4200
Training loss: 5.021595001220703 / Valid loss: 5.513820080530076
Training loss: 2.7308030128479004 / Valid loss: 5.522139154161725

Epoch: 9
Training loss: 3.946046829223633 / Valid loss: 5.530519585382371
Training loss: 5.276594638824463 / Valid loss: 5.50248821349371
Model is saved in epoch 9, overall batch: 4600
Training loss: 5.750150203704834 / Valid loss: 5.534052122206915
Training loss: 6.312538146972656 / Valid loss: 5.509636438460577

Epoch: 10
Training loss: 4.2370123863220215 / Valid loss: 5.519681832903907
Training loss: 4.898505687713623 / Valid loss: 5.555679873057774
Training loss: 5.249239444732666 / Valid loss: 5.5090134302775065
Training loss: 5.578887939453125 / Valid loss: 5.504198948542277
Training loss: 4.453372001647949 / Valid loss: 5.508353855496361

Epoch: 11
Training loss: 5.5549702644348145 / Valid loss: 5.498457170668102
Model is saved in epoch 11, overall batch: 5400
Training loss: 5.553383827209473 / Valid loss: 5.510567161015102
Training loss: 5.37367057800293 / Valid loss: 5.503158028920492
Training loss: 7.921215534210205 / Valid loss: 5.505223253795079
Training loss: 4.971521854400635 / Valid loss: 5.503398184549241

Epoch: 12
Training loss: 5.788402557373047 / Valid loss: 5.504534989311582
Training loss: 5.972040176391602 / Valid loss: 5.508869588942755
Training loss: 3.0506229400634766 / Valid loss: 5.511122360683623
Training loss: 4.65550422668457 / Valid loss: 5.499411246890113
Training loss: 4.3259992599487305 / Valid loss: 5.519681194850377

Epoch: 13
Training loss: 5.089466094970703 / Valid loss: 5.499350565955752
Training loss: 4.569154739379883 / Valid loss: 5.496021938323975
Model is saved in epoch 13, overall batch: 6500
Training loss: 6.184564590454102 / Valid loss: 5.504382492247082
Training loss: 5.979602813720703 / Valid loss: 5.499639706384568
Training loss: 5.067203521728516 / Valid loss: 5.495998770850045
Model is saved in epoch 13, overall batch: 6800

Epoch: 14
Training loss: 5.982232570648193 / Valid loss: 5.5014036110469275
Training loss: 5.610562801361084 / Valid loss: 5.504923341387794
Training loss: 6.13571834564209 / Valid loss: 5.50143435114906
Training loss: 3.9645848274230957 / Valid loss: 5.499323263622466
Training loss: 6.767387390136719 / Valid loss: 5.492342426663353
Model is saved in epoch 14, overall batch: 7300

Epoch: 15
Training loss: 6.059767723083496 / Valid loss: 5.489377316974458
Model is saved in epoch 15, overall batch: 7400
Training loss: 5.276382923126221 / Valid loss: 5.495992626462664
Training loss: 4.720505714416504 / Valid loss: 5.496350594929287
Training loss: 6.450072288513184 / Valid loss: 5.496711785452707
Training loss: 5.1786699295043945 / Valid loss: 5.50944227264041

Epoch: 16
Training loss: 4.506061553955078 / Valid loss: 5.520187482379732
Training loss: 4.055694580078125 / Valid loss: 5.557353051503499
Training loss: 5.3247785568237305 / Valid loss: 5.492258694058373
Training loss: 5.822493553161621 / Valid loss: 5.497975174585978
Training loss: 4.790768623352051 / Valid loss: 5.493253832771664

Epoch: 17
Training loss: 6.292952537536621 / Valid loss: 5.488459739230928
Model is saved in epoch 17, overall batch: 8400
Training loss: 4.928757667541504 / Valid loss: 5.489072172982352
Training loss: 4.912339210510254 / Valid loss: 5.497484597705659
Training loss: 4.3289570808410645 / Valid loss: 5.488738230296543
Training loss: 7.287056922912598 / Valid loss: 5.491418661390032

Epoch: 18
Training loss: 5.808223724365234 / Valid loss: 5.5475227128891715
Training loss: 5.147208213806152 / Valid loss: 5.5212155523754305
Training loss: 5.56679630279541 / Valid loss: 5.499316283634731
Training loss: 4.342759609222412 / Valid loss: 5.499067540395828
Training loss: 4.988112449645996 / Valid loss: 5.488948772067116

Epoch: 19
Training loss: 4.1716742515563965 / Valid loss: 5.489723904927572
Training loss: 4.71455717086792 / Valid loss: 5.490117672511509
Training loss: 4.793225288391113 / Valid loss: 5.50853180204119
Training loss: 4.156498908996582 / Valid loss: 5.480194795699346
Model is saved in epoch 19, overall batch: 9700

Epoch: 20
Training loss: 4.910816192626953 / Valid loss: 5.494253782998948
Training loss: 5.096790790557861 / Valid loss: 5.495265252249581
Training loss: 3.3895277976989746 / Valid loss: 5.483347288767496
Training loss: 3.4379470348358154 / Valid loss: 5.484725661504836
Training loss: 5.066490173339844 / Valid loss: 5.481539199465797

Epoch: 21
Training loss: 4.81103515625 / Valid loss: 5.504499821435838
Training loss: 6.319272041320801 / Valid loss: 5.481419479279291
Training loss: 6.112038612365723 / Valid loss: 5.477619818278721
Model is saved in epoch 21, overall batch: 10500
Training loss: 7.543632984161377 / Valid loss: 5.501037129901705
Training loss: 6.399227142333984 / Valid loss: 5.494377867380778

Epoch: 22
Training loss: 5.843701362609863 / Valid loss: 5.4802411238352455
Training loss: 4.525265216827393 / Valid loss: 5.494182977222261
Training loss: 6.729162216186523 / Valid loss: 5.49206363360087
Training loss: 5.906727313995361 / Valid loss: 5.4904292152041485
Training loss: 4.255195617675781 / Valid loss: 5.488908236367362

Epoch: 23
Training loss: 5.011513710021973 / Valid loss: 5.484397997174944
Training loss: 6.187391757965088 / Valid loss: 5.48658439318339
Training loss: 5.095558166503906 / Valid loss: 5.491677350089663
Training loss: 5.543525695800781 / Valid loss: 5.486950676781791
Training loss: 7.15694522857666 / Valid loss: 5.504431088765462

Epoch: 24
Training loss: 7.150007247924805 / Valid loss: 5.4981131213051935
Training loss: 5.42080020904541 / Valid loss: 5.485689683187576
Training loss: 5.240781307220459 / Valid loss: 5.4866468066260925
Training loss: 7.46047306060791 / Valid loss: 5.4805577210017615
Training loss: 3.6090497970581055 / Valid loss: 5.516376942679996

Epoch: 25
Training loss: 4.251533508300781 / Valid loss: 5.482910871505737
Training loss: 3.091303825378418 / Valid loss: 5.5214112259092785
Training loss: 3.902947425842285 / Valid loss: 5.488103909719558
Training loss: 7.109899520874023 / Valid loss: 5.482312622524443
Training loss: 3.469803810119629 / Valid loss: 5.495574989772979

Epoch: 26
Training loss: 5.193141937255859 / Valid loss: 5.4974485147567025
Training loss: 6.06135368347168 / Valid loss: 5.4916069870903375
Training loss: 6.145756721496582 / Valid loss: 5.505419020425705
Training loss: 5.721582412719727 / Valid loss: 5.479939138321649
Training loss: 5.359508514404297 / Valid loss: 5.4856374627067925

Epoch: 27
Training loss: 7.404483318328857 / Valid loss: 5.499993013200306
Training loss: 4.219019889831543 / Valid loss: 5.480542019435338
Training loss: 5.110235214233398 / Valid loss: 5.473795420782906
Model is saved in epoch 27, overall batch: 13500
Training loss: 6.230298042297363 / Valid loss: 5.4780981086549305
Training loss: 5.607233047485352 / Valid loss: 5.484420758201963

Epoch: 28
Training loss: 4.329489707946777 / Valid loss: 5.480217493148077
Training loss: 4.251819610595703 / Valid loss: 5.506152791068668
Training loss: 5.255070209503174 / Valid loss: 5.484463210332962
Training loss: 4.7103095054626465 / Valid loss: 5.48685001418704
Training loss: 5.102938652038574 / Valid loss: 5.470060927527292
Model is saved in epoch 28, overall batch: 14200

Epoch: 29
Training loss: 6.412261962890625 / Valid loss: 5.487200748352778
Training loss: 5.0323896408081055 / Valid loss: 5.489396862756639
Training loss: 6.419497966766357 / Valid loss: 5.494018059685117
Training loss: 6.481260776519775 / Valid loss: 5.482462017876761

Epoch: 30
Training loss: 5.8938188552856445 / Valid loss: 5.482646635600498
Training loss: 5.669981002807617 / Valid loss: 5.480868716466994
Training loss: 5.075405120849609 / Valid loss: 5.519598415919712
Training loss: 4.4297285079956055 / Valid loss: 5.486859317052932
Training loss: 5.0206403732299805 / Valid loss: 5.478563905897595

Epoch: 31
Training loss: 5.210454940795898 / Valid loss: 5.479577191670736
Training loss: 5.510165214538574 / Valid loss: 5.49033762386867
Training loss: 4.326659202575684 / Valid loss: 5.481498257319132
Training loss: 5.283144950866699 / Valid loss: 5.481547891525995
Training loss: 6.486910820007324 / Valid loss: 5.483907202311925

Epoch: 32
Training loss: 6.16860818862915 / Valid loss: 5.48049380892799
Training loss: 6.515811920166016 / Valid loss: 5.474907534463065
Training loss: 4.865725994110107 / Valid loss: 5.489056466874622
Training loss: 4.477869033813477 / Valid loss: 5.541547859282721
Training loss: 7.398873805999756 / Valid loss: 5.483900317691622

Epoch: 33
Training loss: 4.1804962158203125 / Valid loss: 5.473323320207141
Training loss: 6.794445037841797 / Valid loss: 5.483029376892817
Training loss: 5.458723068237305 / Valid loss: 5.4847205616178965
Training loss: 5.885801315307617 / Valid loss: 5.487234419868106
Training loss: 6.703484058380127 / Valid loss: 5.483438212530953

Epoch: 34
Training loss: 5.340108871459961 / Valid loss: 5.4832003207433795
Training loss: 6.267439842224121 / Valid loss: 5.5009393283299035
Training loss: 6.184216022491455 / Valid loss: 5.511531673158918
Training loss: 6.426023483276367 / Valid loss: 5.480967221941267
Training loss: 4.0190582275390625 / Valid loss: 5.509964116414388

Epoch: 35
Training loss: 5.459201335906982 / Valid loss: 5.47809116045634
Training loss: 4.765635967254639 / Valid loss: 5.485227646146502
Training loss: 5.977105140686035 / Valid loss: 5.481779582159859
Training loss: 5.254140377044678 / Valid loss: 5.480880975723267
Training loss: 4.252647876739502 / Valid loss: 5.484034785770235

Epoch: 36
Training loss: 3.7014541625976562 / Valid loss: 5.521883022217524
Training loss: 6.35922908782959 / Valid loss: 5.503929896581741
Training loss: 3.539961814880371 / Valid loss: 5.487508735202607
Training loss: 6.484991073608398 / Valid loss: 5.48950761840457
Training loss: 3.8594260215759277 / Valid loss: 5.522289480481829

Epoch: 37
Training loss: 5.326666355133057 / Valid loss: 5.491400791349865
Training loss: 4.161128044128418 / Valid loss: 5.48822200411842
Training loss: 4.9894022941589355 / Valid loss: 5.49265676453
Training loss: 5.7566704750061035 / Valid loss: 5.490411622183664
Training loss: 5.478691101074219 / Valid loss: 5.490101725714547

Epoch: 38
Training loss: 5.342828273773193 / Valid loss: 5.486334371566772
Training loss: 5.796510696411133 / Valid loss: 5.482931552614485
Training loss: 5.841977119445801 / Valid loss: 5.485624206633795
Training loss: 4.9246697425842285 / Valid loss: 5.484494177500407
Training loss: 4.37568473815918 / Valid loss: 5.481276800518944

Epoch: 39
Training loss: 4.437519073486328 / Valid loss: 5.482978600547427
Training loss: 5.576231956481934 / Valid loss: 5.5260490167708625
Training loss: 5.6154913902282715 / Valid loss: 5.486648291633243
Training loss: 5.331357002258301 / Valid loss: 5.48462013290042
ModuleList(
  (0): Linear(in_features=5376, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 14200): 5.308370052065168
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.8549747467041 / Valid loss: 16.9357366107759
Model is saved in epoch 0, overall batch: 0
Training loss: 11.266608238220215 / Valid loss: 9.523037792387463
Model is saved in epoch 0, overall batch: 100
Training loss: 6.149203300476074 / Valid loss: 6.396393517085484
Model is saved in epoch 0, overall batch: 200
Training loss: 5.120197772979736 / Valid loss: 5.9006664798373265
Model is saved in epoch 0, overall batch: 300
Training loss: 5.95668363571167 / Valid loss: 5.706494136083694
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 2.7963478565216064 / Valid loss: 5.552826677049909
Model is saved in epoch 1, overall batch: 500
Training loss: 2.8173084259033203 / Valid loss: 5.7068634055909655
Training loss: 3.520646572113037 / Valid loss: 5.89227967943464
Training loss: 2.2036571502685547 / Valid loss: 5.850766981215704
Training loss: 3.2528553009033203 / Valid loss: 6.043879145667667

Epoch: 2
Training loss: 1.6515642404556274 / Valid loss: 5.969488116673061
Training loss: 0.9178176522254944 / Valid loss: 6.0846241655803865
Training loss: 1.227339506149292 / Valid loss: 6.080244865871611
Training loss: 1.506656289100647 / Valid loss: 6.158902985709054
Training loss: 1.0559616088867188 / Valid loss: 6.115778707322621

Epoch: 3
Training loss: 0.7872043251991272 / Valid loss: 6.035268175034296
Training loss: 0.786126434803009 / Valid loss: 6.112765189579555
Training loss: 0.5500298738479614 / Valid loss: 6.120031111580985
Training loss: 0.5695881843566895 / Valid loss: 6.085886553355626
Training loss: 0.9227229952812195 / Valid loss: 6.108186326708112

Epoch: 4
Training loss: 0.46121251583099365 / Valid loss: 6.154210474377587
Training loss: 0.5750457048416138 / Valid loss: 6.195507249378022
Training loss: 0.39887627959251404 / Valid loss: 6.210601779392787
Training loss: 0.5247560739517212 / Valid loss: 6.190851958592733
Training loss: 1.0380500555038452 / Valid loss: 6.188308611370268

Epoch: 5
Training loss: 0.2904283106327057 / Valid loss: 6.180140613374256
Training loss: 0.4295879304409027 / Valid loss: 6.175466292245048
Training loss: 0.8931962251663208 / Valid loss: 6.181893219266619
Training loss: 0.7811853885650635 / Valid loss: 6.229309304555257
Training loss: 0.43158119916915894 / Valid loss: 6.218490741366431

Epoch: 6
Training loss: 0.4444792866706848 / Valid loss: 6.202837065287999
Training loss: 0.3046914339065552 / Valid loss: 6.188191709064302
Training loss: 0.33894574642181396 / Valid loss: 6.212352950232369
Training loss: 0.5585474371910095 / Valid loss: 6.212226061593919
Training loss: 0.3251414895057678 / Valid loss: 6.203022693452381

Epoch: 7
Training loss: 0.3377915620803833 / Valid loss: 6.176128671282814
Training loss: 0.5679395198822021 / Valid loss: 6.23431366057623
Training loss: 0.37839555740356445 / Valid loss: 6.177515127545312
Training loss: 0.19989770650863647 / Valid loss: 6.234965508324759
Training loss: 0.45943403244018555 / Valid loss: 6.206577185222081

Epoch: 8
Training loss: 0.5378811955451965 / Valid loss: 6.172047985167731
Training loss: 0.29323872923851013 / Valid loss: 6.20896205447969
Training loss: 0.4565853774547577 / Valid loss: 6.14265863327753
Training loss: 0.42448174953460693 / Valid loss: 6.185994675045921
Training loss: 0.18387985229492188 / Valid loss: 6.206834107353574

Epoch: 9
Training loss: 0.12144435942173004 / Valid loss: 6.1595263594672796
Training loss: 0.8092493414878845 / Valid loss: 6.213527856554304
Training loss: 0.447419136762619 / Valid loss: 6.21893720626831
Training loss: 0.22194750607013702 / Valid loss: 6.204195385887509

Epoch: 10
Training loss: 0.2604650557041168 / Valid loss: 6.232494635809036
Training loss: 0.26918262243270874 / Valid loss: 6.212143616449265
Training loss: 0.20589199662208557 / Valid loss: 6.17953238714309
Training loss: 0.27420011162757874 / Valid loss: 6.195103649866013
Training loss: 0.2180672585964203 / Valid loss: 6.208147950399489

Epoch: 11
Training loss: 0.21569587290287018 / Valid loss: 6.216910198756627
Training loss: 0.30855172872543335 / Valid loss: 6.173682476225354
Training loss: 0.15654835104942322 / Valid loss: 6.190852462677729
Training loss: 0.5984473824501038 / Valid loss: 6.242023835863386
Training loss: 0.2900468707084656 / Valid loss: 6.190125022615705

Epoch: 12
Training loss: 0.15029942989349365 / Valid loss: 6.2172260783967515
Training loss: 0.14557161927223206 / Valid loss: 6.21975371497018
Training loss: 0.1429368257522583 / Valid loss: 6.211455045427595
Training loss: 0.321735680103302 / Valid loss: 6.205333237420945
Training loss: 0.3569595217704773 / Valid loss: 6.2033148266020275

Epoch: 13
Training loss: 0.3477235734462738 / Valid loss: 6.216657495498657
Training loss: 0.15796750783920288 / Valid loss: 6.21848882720584
Training loss: 0.15093085169792175 / Valid loss: 6.209054565429687
Training loss: 0.3092525899410248 / Valid loss: 6.2249824228740875
Training loss: 0.7142677307128906 / Valid loss: 6.214859040578206

Epoch: 14
Training loss: 0.2284826934337616 / Valid loss: 6.209638021105811
Training loss: 0.34421098232269287 / Valid loss: 6.203762769699097
Training loss: 0.12664544582366943 / Valid loss: 6.182803076789493
Training loss: 0.12341361492872238 / Valid loss: 6.20778706414359
Training loss: 0.2802501916885376 / Valid loss: 6.190065629141671

Epoch: 15
Training loss: 0.17474551498889923 / Valid loss: 6.1927930423191615
Training loss: 0.15642721951007843 / Valid loss: 6.177108446756999
Training loss: 0.2086191475391388 / Valid loss: 6.182577205839611
Training loss: 0.16999304294586182 / Valid loss: 6.190401376996721
Training loss: 0.20696862041950226 / Valid loss: 6.155451120649065

Epoch: 16
Training loss: 0.12964963912963867 / Valid loss: 6.178640370141892
Training loss: 0.6673232316970825 / Valid loss: 6.190405688967023
Training loss: 0.17895367741584778 / Valid loss: 6.269633218220302
Training loss: 0.27044346928596497 / Valid loss: 6.22947276433309
Training loss: 0.4006708562374115 / Valid loss: 6.17575120698838

Epoch: 17
Training loss: 0.35005447268486023 / Valid loss: 6.202607415971302
Training loss: 0.09980528056621552 / Valid loss: 6.205633747010004
Training loss: 0.2831971049308777 / Valid loss: 6.199316310882568
Training loss: 0.1760769486427307 / Valid loss: 6.222300602140881
Training loss: 0.13179361820220947 / Valid loss: 6.211033548627581

Epoch: 18
Training loss: 0.22980549931526184 / Valid loss: 6.189044085003081
Training loss: 0.25061148405075073 / Valid loss: 6.216439467384702
Training loss: 0.22299523651599884 / Valid loss: 6.237522881371635
Training loss: 0.15645331144332886 / Valid loss: 6.207676630928403
Training loss: 0.10219702124595642 / Valid loss: 6.180511295227777

Epoch: 19
Training loss: 0.1987057775259018 / Valid loss: 6.2016715276808965
Training loss: 0.12267002463340759 / Valid loss: 6.23605207261585
Training loss: 0.5227261781692505 / Valid loss: 6.211519082387288
Training loss: 0.1607000231742859 / Valid loss: 6.178372623806908

Epoch: 20
Training loss: 0.08375345915555954 / Valid loss: 6.184640471140543
Training loss: 0.5064467191696167 / Valid loss: 6.189357712155297
Training loss: 0.27951598167419434 / Valid loss: 6.184775525047666
Training loss: 0.3504669964313507 / Valid loss: 6.1750514007750015
Training loss: 0.09191440045833588 / Valid loss: 6.176305237270537

Epoch: 21
Training loss: 0.49572503566741943 / Valid loss: 6.2058111145382835
Training loss: 0.3724380135536194 / Valid loss: 6.186918824059623
Training loss: 0.15515568852424622 / Valid loss: 6.17070323399135
Training loss: 0.0890810638666153 / Valid loss: 6.172920599437895
Training loss: 0.1286560297012329 / Valid loss: 6.18083549681164

Epoch: 22
Training loss: 0.12809978425502777 / Valid loss: 6.236441825685047
Training loss: 0.4780343174934387 / Valid loss: 6.1713118053617935
Training loss: 0.609171450138092 / Valid loss: 6.198770432245164
Training loss: 0.11067506670951843 / Valid loss: 6.215669529778617
Training loss: 0.34211698174476624 / Valid loss: 6.211965726670765

Epoch: 23
Training loss: 0.4548148512840271 / Valid loss: 6.1693613256726945
Training loss: 0.278087854385376 / Valid loss: 6.1970593429747085
Training loss: 0.2770261764526367 / Valid loss: 6.175291093190511
Training loss: 0.14068885147571564 / Valid loss: 6.168465979894003
Training loss: 0.227820485830307 / Valid loss: 6.176496968950544

Epoch: 24
Training loss: 0.11105221509933472 / Valid loss: 6.184954407101586
Training loss: 0.18908604979515076 / Valid loss: 6.169698233831497
Training loss: 0.1811853051185608 / Valid loss: 6.175984586988177
Training loss: 0.12412957847118378 / Valid loss: 6.197892259416126
Training loss: 0.45211538672447205 / Valid loss: 6.206819804509481

Epoch: 25
Training loss: 0.09245023876428604 / Valid loss: 6.203082466125489
Training loss: 0.11188855767250061 / Valid loss: 6.164387133007958
Training loss: 0.1712624430656433 / Valid loss: 6.203320621308826
Training loss: 0.09410358220338821 / Valid loss: 6.188354464939662
Training loss: 0.27762487530708313 / Valid loss: 6.176909567060925

Epoch: 26
Training loss: 0.11729630827903748 / Valid loss: 6.169882029578799
Training loss: 0.6148108243942261 / Valid loss: 6.191447030930292
Training loss: 0.10055960714817047 / Valid loss: 6.191082155136835
Training loss: 0.11149563640356064 / Valid loss: 6.194039596830096
Training loss: 0.21255166828632355 / Valid loss: 6.207614589872814

Epoch: 27
Training loss: 0.0739080160856247 / Valid loss: 6.176359349205381
Training loss: 0.13900092244148254 / Valid loss: 6.243327685764858
Training loss: 0.3815408945083618 / Valid loss: 6.198147642044794
Training loss: 0.08257903158664703 / Valid loss: 6.201475788298107
Training loss: 0.07550384849309921 / Valid loss: 6.189716448102678

Epoch: 28
Training loss: 0.21582290530204773 / Valid loss: 6.190124693371001
Training loss: 0.15363940596580505 / Valid loss: 6.199678738911946
Training loss: 0.23376424610614777 / Valid loss: 6.176391188303629
Training loss: 0.15724411606788635 / Valid loss: 6.176252705710275
Training loss: 0.08521375060081482 / Valid loss: 6.211078362237839

Epoch: 29
Training loss: 0.16138559579849243 / Valid loss: 6.19079681805202
Training loss: 0.10967972874641418 / Valid loss: 6.1766690844581245
Training loss: 0.2428106665611267 / Valid loss: 6.2096471695672895
Training loss: 0.23722518980503082 / Valid loss: 6.162337911696661

Epoch: 30
Training loss: 0.11921732127666473 / Valid loss: 6.1821780772436234
Training loss: 0.5915604829788208 / Valid loss: 6.186112367539179
Training loss: 0.241560697555542 / Valid loss: 6.191456803821382
Training loss: 0.3199322521686554 / Valid loss: 6.194605355035691
Training loss: 0.47826844453811646 / Valid loss: 6.1689517793201265

Epoch: 31
Training loss: 0.1314447969198227 / Valid loss: 6.209898435501826
Training loss: 0.13315436244010925 / Valid loss: 6.205817411059425
Training loss: 0.19155637919902802 / Valid loss: 6.151007899783907
Training loss: 0.1083885133266449 / Valid loss: 6.225577647345407
Training loss: 0.16295288503170013 / Valid loss: 6.200004216602871

Epoch: 32
Training loss: 0.15370473265647888 / Valid loss: 6.194208944411505
Training loss: 0.2264830470085144 / Valid loss: 6.211492683773949
Training loss: 0.1635819673538208 / Valid loss: 6.190025113877796
Training loss: 0.23285356163978577 / Valid loss: 6.181747375215803
Training loss: 0.09561176598072052 / Valid loss: 6.161502563385737

Epoch: 33
Training loss: 0.09425005316734314 / Valid loss: 6.216283241907756
Training loss: 0.13881030678749084 / Valid loss: 6.173772607530866
Training loss: 0.0999162346124649 / Valid loss: 6.163437216622489
Training loss: 0.09019620716571808 / Valid loss: 6.158094110943022
Training loss: 0.06042468175292015 / Valid loss: 6.186356869198027

Epoch: 34
Training loss: 0.11325554549694061 / Valid loss: 6.190722020467122
Training loss: 0.09125010669231415 / Valid loss: 6.171721299489339
Training loss: 0.08920504152774811 / Valid loss: 6.144030675433931
Training loss: 0.20980022847652435 / Valid loss: 6.159716242835636
Training loss: 0.5171259641647339 / Valid loss: 6.168434706188384

Epoch: 35
Training loss: 0.16195902228355408 / Valid loss: 6.168486567905971
Training loss: 0.09841390699148178 / Valid loss: 6.169085305077689
Training loss: 0.1088998094201088 / Valid loss: 6.180281605039324
Training loss: 0.24946299195289612 / Valid loss: 6.190810051418486
Training loss: 0.1630064845085144 / Valid loss: 6.162713093984695

Epoch: 36
Training loss: 0.2346387803554535 / Valid loss: 6.17828277179173
Training loss: 0.2278759628534317 / Valid loss: 6.223278395334879
Training loss: 0.5999125242233276 / Valid loss: 6.152900400615874
Training loss: 0.39420652389526367 / Valid loss: 6.176826799483527
Training loss: 0.11340292543172836 / Valid loss: 6.175700378417969

Epoch: 37
Training loss: 0.1008567214012146 / Valid loss: 6.151941971551804
Training loss: 0.1414588987827301 / Valid loss: 6.169651948838007
Training loss: 0.17863836884498596 / Valid loss: 6.150829310644241
Training loss: 0.8564020395278931 / Valid loss: 6.186394938968477
Training loss: 0.40568655729293823 / Valid loss: 6.146649621781849

Epoch: 38
Training loss: 0.1784229725599289 / Valid loss: 6.191709425335839
Training loss: 0.08228680491447449 / Valid loss: 6.176133139928182
Training loss: 0.1956903636455536 / Valid loss: 6.194205654235113
Training loss: 0.21173900365829468 / Valid loss: 6.16919595173427
Training loss: 0.3868247866630554 / Valid loss: 6.181982851028442

Epoch: 39
Training loss: 0.09939633309841156 / Valid loss: 6.2068204652695425
Training loss: 0.2221776843070984 / Valid loss: 6.181054594403221
Training loss: 0.12811005115509033 / Valid loss: 6.147280079977853
Training loss: 0.1389683187007904 / Valid loss: 6.144784809294201
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.503542666208176
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.8549747467041 / Valid loss: 16.9357366107759
Model is saved in epoch 0, overall batch: 0
Training loss: 11.247732162475586 / Valid loss: 9.521352486383348
Model is saved in epoch 0, overall batch: 100
Training loss: 6.054732322692871 / Valid loss: 6.392146492004395
Model is saved in epoch 0, overall batch: 200
Training loss: 4.955467224121094 / Valid loss: 5.88338106473287
Model is saved in epoch 0, overall batch: 300
Training loss: 6.1123456954956055 / Valid loss: 5.678419390178862
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 2.6838510036468506 / Valid loss: 5.580679412115188
Model is saved in epoch 1, overall batch: 500
Training loss: 2.4503366947174072 / Valid loss: 5.731778403690883
Training loss: 3.348144054412842 / Valid loss: 5.933210724876041
Training loss: 2.2029480934143066 / Valid loss: 5.870650239217849
Training loss: 2.810688018798828 / Valid loss: 6.026853000550043

Epoch: 2
Training loss: 1.7469104528427124 / Valid loss: 5.9502841699691045
Training loss: 0.7144379019737244 / Valid loss: 6.080973761422293
Training loss: 1.499056100845337 / Valid loss: 6.083347261519659
Training loss: 1.6833775043487549 / Valid loss: 6.212684549604144
Training loss: 0.9024189710617065 / Valid loss: 6.12751149677095

Epoch: 3
Training loss: 0.7126747369766235 / Valid loss: 6.125129976726714
Training loss: 1.0344750881195068 / Valid loss: 6.136645555496216
Training loss: 0.6473336219787598 / Valid loss: 6.186532154537383
Training loss: 0.5266923904418945 / Valid loss: 6.18366262345087
Training loss: 0.6776912212371826 / Valid loss: 6.16247919400533

Epoch: 4
Training loss: 0.6021193861961365 / Valid loss: 6.230539244697208
Training loss: 0.7194924354553223 / Valid loss: 6.222459055128551
Training loss: 0.3835063576698303 / Valid loss: 6.220370953423636
Training loss: 0.5437254309654236 / Valid loss: 6.203007938748314
Training loss: 1.041001558303833 / Valid loss: 6.213936864762079

Epoch: 5
Training loss: 0.30050307512283325 / Valid loss: 6.182308564867292
Training loss: 0.3288611173629761 / Valid loss: 6.192198698861258
Training loss: 1.0752031803131104 / Valid loss: 6.174419750486101
Training loss: 0.9393135905265808 / Valid loss: 6.243907799039568
Training loss: 0.3644601106643677 / Valid loss: 6.222333042962211

Epoch: 6
Training loss: 0.4301179349422455 / Valid loss: 6.209005680538359
Training loss: 0.4530653655529022 / Valid loss: 6.201139073144822
Training loss: 0.38168275356292725 / Valid loss: 6.214615824109032
Training loss: 0.46992361545562744 / Valid loss: 6.20733764285133
Training loss: 0.23296022415161133 / Valid loss: 6.208723174958002

Epoch: 7
Training loss: 0.44887012243270874 / Valid loss: 6.1626691818237305
Training loss: 0.5639652013778687 / Valid loss: 6.210383973802839
Training loss: 0.3809382915496826 / Valid loss: 6.168381041572207
Training loss: 0.2994673252105713 / Valid loss: 6.200307843798683
Training loss: 0.46127423644065857 / Valid loss: 6.198924852552868

Epoch: 8
Training loss: 0.4737512469291687 / Valid loss: 6.167570538747878
Training loss: 0.30484098196029663 / Valid loss: 6.210565294538226
Training loss: 0.5541300773620605 / Valid loss: 6.137626647949219
Training loss: 0.5123100876808167 / Valid loss: 6.179030357088362
Training loss: 0.2604491114616394 / Valid loss: 6.214282201585315

Epoch: 9
Training loss: 0.15352094173431396 / Valid loss: 6.16996054649353
Training loss: 0.8873046636581421 / Valid loss: 6.241441924231393
Training loss: 0.41080087423324585 / Valid loss: 6.163867335092454
Training loss: 0.16754017770290375 / Valid loss: 6.167633061181931

Epoch: 10
Training loss: 0.2807498872280121 / Valid loss: 6.187973808106922
Training loss: 0.2957007884979248 / Valid loss: 6.204161875588554
Training loss: 0.21006813645362854 / Valid loss: 6.157477022352673
Training loss: 0.3405899107456207 / Valid loss: 6.179490003131685
Training loss: 0.3632138669490814 / Valid loss: 6.203440909158616

Epoch: 11
Training loss: 0.2922247052192688 / Valid loss: 6.239735535212925
Training loss: 0.3235633969306946 / Valid loss: 6.192229050681704
Training loss: 0.15734225511550903 / Valid loss: 6.1986680212475
Training loss: 0.5458800196647644 / Valid loss: 6.224522495269776
Training loss: 0.27090001106262207 / Valid loss: 6.20362582887922

Epoch: 12
Training loss: 0.1394079625606537 / Valid loss: 6.218436842872983
Training loss: 0.15267206728458405 / Valid loss: 6.222361514681862
Training loss: 0.2085350602865219 / Valid loss: 6.200389294397263
Training loss: 0.28398388624191284 / Valid loss: 6.2170823914664135
Training loss: 0.35118818283081055 / Valid loss: 6.204065486363002

Epoch: 13
Training loss: 0.286962628364563 / Valid loss: 6.230254066558111
Training loss: 0.17979252338409424 / Valid loss: 6.205006392796834
Training loss: 0.23200546205043793 / Valid loss: 6.203673580714635
Training loss: 0.3026382625102997 / Valid loss: 6.230164359864735
Training loss: 0.6990079879760742 / Valid loss: 6.1963624659038725

Epoch: 14
Training loss: 0.2433614432811737 / Valid loss: 6.184242057800293
Training loss: 0.31550535559654236 / Valid loss: 6.194104557945614
Training loss: 0.11972963064908981 / Valid loss: 6.207668422517322
Training loss: 0.14999932050704956 / Valid loss: 6.200459067026774
Training loss: 0.21422025561332703 / Valid loss: 6.215623669397264

Epoch: 15
Training loss: 0.12404309958219528 / Valid loss: 6.219933371316819
Training loss: 0.2118341624736786 / Valid loss: 6.218441141219366
Training loss: 0.22642064094543457 / Valid loss: 6.209057594480969
Training loss: 0.09481434524059296 / Valid loss: 6.220694224039714
Training loss: 0.20790749788284302 / Valid loss: 6.173027765183222

Epoch: 16
Training loss: 0.08639994263648987 / Valid loss: 6.196213606425694
Training loss: 0.5371291637420654 / Valid loss: 6.195135325477237
Training loss: 0.238973468542099 / Valid loss: 6.230330755597069
Training loss: 0.21855448186397552 / Valid loss: 6.216544210343134
Training loss: 0.3583762049674988 / Valid loss: 6.194821425846645

Epoch: 17
Training loss: 0.2962363064289093 / Valid loss: 6.1872084367842906
Training loss: 0.07413240522146225 / Valid loss: 6.2046700227828255
Training loss: 0.22813227772712708 / Valid loss: 6.211738831656319
Training loss: 0.1974148154258728 / Valid loss: 6.202784513291858
Training loss: 0.09816203266382217 / Valid loss: 6.183567346845354

Epoch: 18
Training loss: 0.20096221566200256 / Valid loss: 6.185086379732405
Training loss: 0.2574182152748108 / Valid loss: 6.182824053083148
Training loss: 0.1878441572189331 / Valid loss: 6.229755453836351
Training loss: 0.12667371332645416 / Valid loss: 6.199477595374698
Training loss: 0.14588871598243713 / Valid loss: 6.185078323455084

Epoch: 19
Training loss: 0.23033472895622253 / Valid loss: 6.191850426083519
Training loss: 0.12827911972999573 / Valid loss: 6.2398223945072715
Training loss: 0.48677757382392883 / Valid loss: 6.2066809222811745
Training loss: 0.20867154002189636 / Valid loss: 6.188653217043195

Epoch: 20
Training loss: 0.08835510909557343 / Valid loss: 6.169152377900623
Training loss: 0.4938054382801056 / Valid loss: 6.160564361299787
Training loss: 0.28889232873916626 / Valid loss: 6.196634496961321
Training loss: 0.33803269267082214 / Valid loss: 6.162196858723958
Training loss: 0.15372446179389954 / Valid loss: 6.176246227536883

Epoch: 21
Training loss: 0.5251806974411011 / Valid loss: 6.207161399296352
Training loss: 0.3098805248737335 / Valid loss: 6.185379677727109
Training loss: 0.14793044328689575 / Valid loss: 6.193930675869896
Training loss: 0.11619611084461212 / Valid loss: 6.202252342587426
Training loss: 0.12440228462219238 / Valid loss: 6.199078548522222

Epoch: 22
Training loss: 0.12971913814544678 / Valid loss: 6.220184067317418
Training loss: 0.33158108592033386 / Valid loss: 6.158862901869274
Training loss: 0.6318716406822205 / Valid loss: 6.169933423541841
Training loss: 0.10735262185335159 / Valid loss: 6.207619040352958
Training loss: 0.2953369915485382 / Valid loss: 6.195360744567145

Epoch: 23
Training loss: 0.5761594176292419 / Valid loss: 6.166971846989223
Training loss: 0.3265545964241028 / Valid loss: 6.1944428648267476
Training loss: 0.2014850378036499 / Valid loss: 6.164150951022194
Training loss: 0.12118951231241226 / Valid loss: 6.162658255440848
Training loss: 0.25863656401634216 / Valid loss: 6.173405034201486

Epoch: 24
Training loss: 0.08837377279996872 / Valid loss: 6.193221101306734
Training loss: 0.17541997134685516 / Valid loss: 6.167576092765445
Training loss: 0.20173147320747375 / Valid loss: 6.204788737069993
Training loss: 0.0979883223772049 / Valid loss: 6.224052517754691
Training loss: 0.3902744650840759 / Valid loss: 6.21241542725336

Epoch: 25
Training loss: 0.09202530980110168 / Valid loss: 6.205329291025797
Training loss: 0.10551400482654572 / Valid loss: 6.181945948373704
Training loss: 0.15161819756031036 / Valid loss: 6.208929584139869
Training loss: 0.14243841171264648 / Valid loss: 6.190195390156338
Training loss: 0.262045681476593 / Valid loss: 6.181363464537121

Epoch: 26
Training loss: 0.1134800910949707 / Valid loss: 6.162843038922264
Training loss: 0.64703369140625 / Valid loss: 6.1947913692111065
Training loss: 0.0942574217915535 / Valid loss: 6.216188353583926
Training loss: 0.1263359785079956 / Valid loss: 6.186015424274263
Training loss: 0.3255085349082947 / Valid loss: 6.226255410058158

Epoch: 27
Training loss: 0.0790952518582344 / Valid loss: 6.180622200738816
Training loss: 0.16633468866348267 / Valid loss: 6.254300435384114
Training loss: 0.418076753616333 / Valid loss: 6.2169169925508045
Training loss: 0.07454633712768555 / Valid loss: 6.210244362694876
Training loss: 0.07383733987808228 / Valid loss: 6.2138688882191975

Epoch: 28
Training loss: 0.2022048383951187 / Valid loss: 6.205051708221435
Training loss: 0.17078718543052673 / Valid loss: 6.205931443259829
Training loss: 0.24535994231700897 / Valid loss: 6.204930169241769
Training loss: 0.1438925862312317 / Valid loss: 6.1638839880625405
Training loss: 0.10332944989204407 / Valid loss: 6.205415080842518

Epoch: 29
Training loss: 0.09683011472225189 / Valid loss: 6.187597451891218
Training loss: 0.1305246204137802 / Valid loss: 6.185398867016747
Training loss: 0.25717592239379883 / Valid loss: 6.211606802259173
Training loss: 0.25932300090789795 / Valid loss: 6.17248759042649

Epoch: 30
Training loss: 0.11300504207611084 / Valid loss: 6.215397989182245
Training loss: 0.5545114874839783 / Valid loss: 6.203387748627436
Training loss: 0.18704070150852203 / Valid loss: 6.202723407745362
Training loss: 0.29921087622642517 / Valid loss: 6.179388954525902
Training loss: 0.5665305852890015 / Valid loss: 6.188432057698567

Epoch: 31
Training loss: 0.15392497181892395 / Valid loss: 6.211655074074155
Training loss: 0.15922734141349792 / Valid loss: 6.21270736058553
Training loss: 0.21352741122245789 / Valid loss: 6.165621391932169
Training loss: 0.09381236135959625 / Valid loss: 6.2027682644980295
Training loss: 0.151943176984787 / Valid loss: 6.1996924037025085

Epoch: 32
Training loss: 0.19814646244049072 / Valid loss: 6.196359820592971
Training loss: 0.2506045401096344 / Valid loss: 6.222453471592495
Training loss: 0.13150063157081604 / Valid loss: 6.196546974636259
Training loss: 0.20372363924980164 / Valid loss: 6.201944401150658
Training loss: 0.1439688503742218 / Valid loss: 6.188135864621117

Epoch: 33
Training loss: 0.1509559154510498 / Valid loss: 6.237697440101987
Training loss: 0.12150486558675766 / Valid loss: 6.191201625551496
Training loss: 0.119204580783844 / Valid loss: 6.167061029161726
Training loss: 0.12385156005620956 / Valid loss: 6.171203965232486
Training loss: 0.07949694991111755 / Valid loss: 6.204034264882406

Epoch: 34
Training loss: 0.1634928286075592 / Valid loss: 6.198082932971773
Training loss: 0.10595746338367462 / Valid loss: 6.186195532480876
Training loss: 0.09954461455345154 / Valid loss: 6.13436712537493
Training loss: 0.2602192759513855 / Valid loss: 6.1651871408735
Training loss: 0.6275748014450073 / Valid loss: 6.173972520374116

Epoch: 35
Training loss: 0.14846184849739075 / Valid loss: 6.168538211640858
Training loss: 0.12076452374458313 / Valid loss: 6.174266002291724
Training loss: 0.10891592502593994 / Valid loss: 6.169545105525425
Training loss: 0.25998902320861816 / Valid loss: 6.187179292951312
Training loss: 0.11730644106864929 / Valid loss: 6.148718847547259

Epoch: 36
Training loss: 0.23681050539016724 / Valid loss: 6.155094332922072
Training loss: 0.18953144550323486 / Valid loss: 6.193688801356724
Training loss: 0.5566242933273315 / Valid loss: 6.140300403322493
Training loss: 0.32057273387908936 / Valid loss: 6.161515630994525
Training loss: 0.08942092955112457 / Valid loss: 6.15473579225086

Epoch: 37
Training loss: 0.09478134661912918 / Valid loss: 6.134618863605318
Training loss: 0.1934906393289566 / Valid loss: 6.181275424503145
Training loss: 0.18434911966323853 / Valid loss: 6.15878267288208
Training loss: 0.7654696702957153 / Valid loss: 6.177561980202085
Training loss: 0.37362682819366455 / Valid loss: 6.142395401000977

Epoch: 38
Training loss: 0.17092347145080566 / Valid loss: 6.174234980628604
Training loss: 0.11457540839910507 / Valid loss: 6.165759222848075
Training loss: 0.3101544678211212 / Valid loss: 6.194050695782616
Training loss: 0.23595818877220154 / Valid loss: 6.178992534819104
Training loss: 0.3581318259239197 / Valid loss: 6.182569957914806

Epoch: 39
Training loss: 0.07270406931638718 / Valid loss: 6.199436323983329
Training loss: 0.2636544704437256 / Valid loss: 6.195595704941523
Training loss: 0.15511614084243774 / Valid loss: 6.145508434658959
Training loss: 0.1463928371667862 / Valid loss: 6.143433339255196
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.5114956492469425
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.313697814941406 / Valid loss: 15.710995428902763
Model is saved in epoch 0, overall batch: 0
Training loss: 6.693487167358398 / Valid loss: 7.94097721463158
Model is saved in epoch 0, overall batch: 100
Training loss: 4.713494777679443 / Valid loss: 6.06045857157026
Model is saved in epoch 0, overall batch: 200
Training loss: 3.6556923389434814 / Valid loss: 5.729180526733399
Model is saved in epoch 0, overall batch: 300
Training loss: 4.41060209274292 / Valid loss: 5.634358926046462
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 2.174398899078369 / Valid loss: 5.57524341628665
Model is saved in epoch 1, overall batch: 500
Training loss: 3.6296706199645996 / Valid loss: 5.696300411224366
Training loss: 3.6789073944091797 / Valid loss: 5.81929517246428
Training loss: 3.2285637855529785 / Valid loss: 5.896200507027762
Training loss: 2.4243991374969482 / Valid loss: 5.970458968480428

Epoch: 2
Training loss: 1.1159389019012451 / Valid loss: 5.9013681230090915
Training loss: 1.0134650468826294 / Valid loss: 5.978096585046678
Training loss: 1.5080945491790771 / Valid loss: 6.005766936710903
Training loss: 1.6234768629074097 / Valid loss: 6.098042756035214
Training loss: 2.2251100540161133 / Valid loss: 6.088698391687302

Epoch: 3
Training loss: 1.3796395063400269 / Valid loss: 6.070711878367833
Training loss: 0.9390164613723755 / Valid loss: 6.117112425395421
Training loss: 1.5787723064422607 / Valid loss: 6.153964283352806
Training loss: 1.2675819396972656 / Valid loss: 6.233835526875088
Training loss: 1.5633068084716797 / Valid loss: 6.233606788090297

Epoch: 4
Training loss: 0.9100593328475952 / Valid loss: 6.186894855045137
Training loss: 0.659858226776123 / Valid loss: 6.226443742570423
Training loss: 0.8217017650604248 / Valid loss: 6.281921030226208
Training loss: 0.5659178495407104 / Valid loss: 6.257337052481515
Training loss: 0.6509620547294617 / Valid loss: 6.327810289746239

Epoch: 5
Training loss: 0.4321506917476654 / Valid loss: 6.279960014706567
Training loss: 0.7610546350479126 / Valid loss: 6.29423200743539
Training loss: 0.5839325785636902 / Valid loss: 6.318549035844349
Training loss: 0.6163688898086548 / Valid loss: 6.339775212605795
Training loss: 0.6345599889755249 / Valid loss: 6.376816622416178

Epoch: 6
Training loss: 0.34426283836364746 / Valid loss: 6.361897180193947
Training loss: 0.4302700161933899 / Valid loss: 6.371565312430972
Training loss: 0.3585285246372223 / Valid loss: 6.409477860586984
Training loss: 0.38469451665878296 / Valid loss: 6.336991167068481
Training loss: 0.2802124619483948 / Valid loss: 6.3328330584934776

Epoch: 7
Training loss: 0.32584407925605774 / Valid loss: 6.374144858405703
Training loss: 0.17186623811721802 / Valid loss: 6.371310306730725
Training loss: 0.42771655321121216 / Valid loss: 6.353809697287423
Training loss: 0.41399577260017395 / Valid loss: 6.35535185223534
Training loss: 0.20840537548065186 / Valid loss: 6.368763206118629

Epoch: 8
Training loss: 0.3360731303691864 / Valid loss: 6.384148395629156
Training loss: 0.2387213408946991 / Valid loss: 6.434075675691877
Training loss: 0.3432292342185974 / Valid loss: 6.358226623989287
Training loss: 0.24294115602970123 / Valid loss: 6.416694920403617
Training loss: 0.5685476660728455 / Valid loss: 6.389681577682495

Epoch: 9
Training loss: 0.2566887438297272 / Valid loss: 6.398699333554222
Training loss: 0.2359044849872589 / Valid loss: 6.381323421569098
Training loss: 0.46938928961753845 / Valid loss: 6.392754802249727
Training loss: 0.9757698774337769 / Valid loss: 6.407532533009847

Epoch: 10
Training loss: 0.20331430435180664 / Valid loss: 6.441188417162214
Training loss: 0.16380774974822998 / Valid loss: 6.390819381532215
Training loss: 0.29721441864967346 / Valid loss: 6.437963953472319
Training loss: 0.29748669266700745 / Valid loss: 6.427624454952421
Training loss: 0.16724148392677307 / Valid loss: 6.4528216361999515

Epoch: 11
Training loss: 0.3550906777381897 / Valid loss: 6.4270206019991925
Training loss: 0.23889034986495972 / Valid loss: 6.416763725734892
Training loss: 0.15599419176578522 / Valid loss: 6.403796818142846
Training loss: 0.22497838735580444 / Valid loss: 6.419845344906761
Training loss: 0.6803861260414124 / Valid loss: 6.375630396888369

Epoch: 12
Training loss: 0.2076413780450821 / Valid loss: 6.372348254067557
Training loss: 0.2367829978466034 / Valid loss: 6.4409469150361565
Training loss: 0.1000632792711258 / Valid loss: 6.497088931855702
Training loss: 0.4958052933216095 / Valid loss: 6.418691778182984
Training loss: 0.23160548508167267 / Valid loss: 6.427995177677699

Epoch: 13
Training loss: 0.42226070165634155 / Valid loss: 6.396045173917498
Training loss: 0.1732461154460907 / Valid loss: 6.407709643954322
Training loss: 0.23142847418785095 / Valid loss: 6.389214633759998
Training loss: 0.11442537605762482 / Valid loss: 6.40034617242359
Training loss: 0.1899152398109436 / Valid loss: 6.417223871321905

Epoch: 14
Training loss: 0.15138466656208038 / Valid loss: 6.412236583800543
Training loss: 0.5034239292144775 / Valid loss: 6.458081667763846
Training loss: 0.16216754913330078 / Valid loss: 6.4078673657916845
Training loss: 0.3805120885372162 / Valid loss: 6.432611819675991
Training loss: 0.17468854784965515 / Valid loss: 6.437816445032755

Epoch: 15
Training loss: 0.5516554713249207 / Valid loss: 6.377750006176177
Training loss: 0.12635573744773865 / Valid loss: 6.424671218508766
Training loss: 0.7529295682907104 / Valid loss: 6.410544706526257
Training loss: 0.41396597027778625 / Valid loss: 6.404819145656767
Training loss: 1.1647602319717407 / Valid loss: 6.382866614205497

Epoch: 16
Training loss: 0.20450067520141602 / Valid loss: 6.40255293619065
Training loss: 0.36846932768821716 / Valid loss: 6.380177041462489
Training loss: 0.4016403555870056 / Valid loss: 6.409206560679844
Training loss: 0.41343408823013306 / Valid loss: 6.404156319300333
Training loss: 0.19668669998645782 / Valid loss: 6.420651156561715

Epoch: 17
Training loss: 0.08061890304088593 / Valid loss: 6.3783062571570985
Training loss: 0.12587332725524902 / Valid loss: 6.413145474025181
Training loss: 0.16460376977920532 / Valid loss: 6.388549436841692
Training loss: 0.2623389959335327 / Valid loss: 6.439386186145601
Training loss: 0.07493194937705994 / Valid loss: 6.434070927756173

Epoch: 18
Training loss: 0.2147849202156067 / Valid loss: 6.404447085516793
Training loss: 0.1816127896308899 / Valid loss: 6.383923612322126
Training loss: 0.1704881489276886 / Valid loss: 6.3946556409200035
Training loss: 0.12022557854652405 / Valid loss: 6.396899200621105
Training loss: 0.24054473638534546 / Valid loss: 6.371029340653193

Epoch: 19
Training loss: 0.3546257019042969 / Valid loss: 6.3720236619313555
Training loss: 0.21519437432289124 / Valid loss: 6.384689542225429
Training loss: 0.14952969551086426 / Valid loss: 6.429422893978301
Training loss: 0.17285913228988647 / Valid loss: 6.416379047575451

Epoch: 20
Training loss: 0.22926272451877594 / Valid loss: 6.427398686181931
Training loss: 0.1855291724205017 / Valid loss: 6.402102418172927
Training loss: 0.2274470031261444 / Valid loss: 6.41881772904169
Training loss: 0.0792718231678009 / Valid loss: 6.4009586039043604
Training loss: 0.1418582648038864 / Valid loss: 6.385804516928537

Epoch: 21
Training loss: 0.08202238380908966 / Valid loss: 6.416809549785795
Training loss: 0.2611373960971832 / Valid loss: 6.389248936516898
Training loss: 0.13348054885864258 / Valid loss: 6.4197997320266
Training loss: 0.15777519345283508 / Valid loss: 6.414675558181036
Training loss: 0.11795774847269058 / Valid loss: 6.3980605420612155

Epoch: 22
Training loss: 0.29350295662879944 / Valid loss: 6.415463288625081
Training loss: 0.05233814939856529 / Valid loss: 6.392173185802641
Training loss: 0.24553711712360382 / Valid loss: 6.419549133664086
Training loss: 0.4721547067165375 / Valid loss: 6.47679005804516
Training loss: 0.1435103416442871 / Valid loss: 6.41757444427127

Epoch: 23
Training loss: 0.2271484136581421 / Valid loss: 6.404998758860997
Training loss: 0.25057166814804077 / Valid loss: 6.403441601707822
Training loss: 0.9112774729728699 / Valid loss: 6.357498103096372
Training loss: 0.09259437024593353 / Valid loss: 6.4185223965417775
Training loss: 0.16969501972198486 / Valid loss: 6.39496629805792

Epoch: 24
Training loss: 0.16973868012428284 / Valid loss: 6.4011090324038555
Training loss: 0.1309262067079544 / Valid loss: 6.377680026917231
Training loss: 0.449266254901886 / Valid loss: 6.402200510388329
Training loss: 0.13381201028823853 / Valid loss: 6.378987407684326
Training loss: 0.3165583610534668 / Valid loss: 6.389180521737962

Epoch: 25
Training loss: 0.3495137691497803 / Valid loss: 6.375565376735869
Training loss: 0.14425653219223022 / Valid loss: 6.3718473252795995
Training loss: 0.0862378478050232 / Valid loss: 6.397254008338565
Training loss: 0.23660631477832794 / Valid loss: 6.399982506888254
Training loss: 0.3484743535518646 / Valid loss: 6.407733726501465

Epoch: 26
Training loss: 0.08829178661108017 / Valid loss: 6.396904423123314
Training loss: 0.1138201355934143 / Valid loss: 6.382459093275524
Training loss: 0.13225576281547546 / Valid loss: 6.3908842858814054
Training loss: 0.14515888690948486 / Valid loss: 6.398280852181571
Training loss: 0.12957793474197388 / Valid loss: 6.394602080753871

Epoch: 27
Training loss: 0.1861947625875473 / Valid loss: 6.385333992186046
Training loss: 0.2746526896953583 / Valid loss: 6.382232441220965
Training loss: 0.2544490396976471 / Valid loss: 6.384291283289591
Training loss: 0.07639587670564651 / Valid loss: 6.389006301334926
Training loss: 0.09647639840841293 / Valid loss: 6.372446200961158

Epoch: 28
Training loss: 0.13556596636772156 / Valid loss: 6.3843527657645085
Training loss: 0.10524403303861618 / Valid loss: 6.342844145638602
Training loss: 0.42748165130615234 / Valid loss: 6.356502510252453
Training loss: 0.1386190950870514 / Valid loss: 6.37835256485712
Training loss: 0.10855071246623993 / Valid loss: 6.367849844977969

Epoch: 29
Training loss: 0.07779675722122192 / Valid loss: 6.371734996069045
Training loss: 0.07084205746650696 / Valid loss: 6.360093761625744
Training loss: 0.21046435832977295 / Valid loss: 6.367005684262231
Training loss: 0.13471724092960358 / Valid loss: 6.374379421415783

Epoch: 30
Training loss: 0.26199400424957275 / Valid loss: 6.424960617792038
Training loss: 0.0950772687792778 / Valid loss: 6.364937823159354
Training loss: 0.1115197017788887 / Valid loss: 6.395916380201068
Training loss: 0.19507640600204468 / Valid loss: 6.4031848544166206
Training loss: 0.2785162329673767 / Valid loss: 6.382240161441621

Epoch: 31
Training loss: 0.24120444059371948 / Valid loss: 6.3677611237480525
Training loss: 0.09216558188199997 / Valid loss: 6.378975595746722
Training loss: 0.1940295398235321 / Valid loss: 6.4027522200629825
Training loss: 0.1334836781024933 / Valid loss: 6.412582190831502
Training loss: 0.05778255686163902 / Valid loss: 6.411199912570772

Epoch: 32
Training loss: 0.17262986302375793 / Valid loss: 6.394750996998378
Training loss: 0.09537789225578308 / Valid loss: 6.391217872074672
Training loss: 0.16474243998527527 / Valid loss: 6.367023154667446
Training loss: 0.27199381589889526 / Valid loss: 6.368146900903611
Training loss: 0.05765031650662422 / Valid loss: 6.382350017910912

Epoch: 33
Training loss: 0.15895172953605652 / Valid loss: 6.381916384469895
Training loss: 0.09962973743677139 / Valid loss: 6.346818551563081
Training loss: 0.16259679198265076 / Valid loss: 6.389968231746129
Training loss: 0.06625325977802277 / Valid loss: 6.357768787656512
Training loss: 0.08386208117008209 / Valid loss: 6.38534912835984

Epoch: 34
Training loss: 0.4113806486129761 / Valid loss: 6.354113274528867
Training loss: 0.10379534959793091 / Valid loss: 6.363376151947748
Training loss: 0.11693699657917023 / Valid loss: 6.378735694431123
Training loss: 0.14404082298278809 / Valid loss: 6.36807651973906
Training loss: 0.10219311714172363 / Valid loss: 6.386925220489502

Epoch: 35
Training loss: 0.0800231322646141 / Valid loss: 6.351863250278291
Training loss: 0.18779070675373077 / Valid loss: 6.3819535527910505
Training loss: 0.11220481991767883 / Valid loss: 6.362518072128296
Training loss: 0.07533351331949234 / Valid loss: 6.368343380519322
Training loss: 0.5151684880256653 / Valid loss: 6.367831355049496

Epoch: 36
Training loss: 0.05849061906337738 / Valid loss: 6.370656860442389
Training loss: 0.19003014266490936 / Valid loss: 6.359356521424793
Training loss: 0.19059306383132935 / Valid loss: 6.385091677166167
Training loss: 0.11015687137842178 / Valid loss: 6.372510816937401
Training loss: 0.1283651888370514 / Valid loss: 6.370831491833641

Epoch: 37
Training loss: 0.10356411337852478 / Valid loss: 6.384374722980318
Training loss: 0.08925776183605194 / Valid loss: 6.331318269457136
Training loss: 0.5786986351013184 / Valid loss: 6.345981379917689
Training loss: 0.14986248314380646 / Valid loss: 6.373185057867141
Training loss: 0.20423755049705505 / Valid loss: 6.361886215209961

Epoch: 38
Training loss: 0.08133244514465332 / Valid loss: 6.339011228652227
Training loss: 0.06886245310306549 / Valid loss: 6.357574562799363
Training loss: 0.12340563535690308 / Valid loss: 6.372902416047596
Training loss: 0.13941875100135803 / Valid loss: 6.358955560411726
Training loss: 0.0643780454993248 / Valid loss: 6.374346533275786

Epoch: 39
Training loss: 0.13556677103042603 / Valid loss: 6.364626432600476
Training loss: 0.22109530866146088 / Valid loss: 6.357280538195655
Training loss: 0.08114738762378693 / Valid loss: 6.360996323540097
Training loss: 0.10063710063695908 / Valid loss: 6.3637498719351635
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.501304238183158
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.313697814941406 / Valid loss: 15.710995428902763
Model is saved in epoch 0, overall batch: 0
Training loss: 6.725375175476074 / Valid loss: 7.942087423233759
Model is saved in epoch 0, overall batch: 100
Training loss: 4.702406883239746 / Valid loss: 6.041560874666486
Model is saved in epoch 0, overall batch: 200
Training loss: 3.807941198348999 / Valid loss: 5.735292262122744
Model is saved in epoch 0, overall batch: 300
Training loss: 4.383848667144775 / Valid loss: 5.654091190156483
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 2.182692527770996 / Valid loss: 5.5764250074114114
Model is saved in epoch 1, overall batch: 500
Training loss: 3.4356136322021484 / Valid loss: 5.701510188693092
Training loss: 3.9540419578552246 / Valid loss: 5.816774647576469
Training loss: 3.201383113861084 / Valid loss: 5.910975958052136
Training loss: 2.269318103790283 / Valid loss: 5.974729338146392

Epoch: 2
Training loss: 1.155382513999939 / Valid loss: 5.90073976062593
Training loss: 1.1493868827819824 / Valid loss: 5.974457706723895
Training loss: 1.53009033203125 / Valid loss: 5.989865798041934
Training loss: 1.4866089820861816 / Valid loss: 6.08333022253854
Training loss: 2.1203389167785645 / Valid loss: 6.072006316412063

Epoch: 3
Training loss: 1.4465630054473877 / Valid loss: 6.089099511646089
Training loss: 0.9358624815940857 / Valid loss: 6.094346720831735
Training loss: 1.5709664821624756 / Valid loss: 6.153422696249826
Training loss: 1.4804552793502808 / Valid loss: 6.2086128325689405
Training loss: 1.4422643184661865 / Valid loss: 6.233088506971087

Epoch: 4
Training loss: 0.896241307258606 / Valid loss: 6.18218362444923
Training loss: 0.7185964584350586 / Valid loss: 6.226618033363706
Training loss: 0.8521873950958252 / Valid loss: 6.271161915007092
Training loss: 0.7277450561523438 / Valid loss: 6.2507317429497125
Training loss: 0.6156652569770813 / Valid loss: 6.315886156899588

Epoch: 5
Training loss: 0.4846537709236145 / Valid loss: 6.282777141389393
Training loss: 0.698937714099884 / Valid loss: 6.274791996819633
Training loss: 0.6898013353347778 / Valid loss: 6.312999032792591
Training loss: 0.686073899269104 / Valid loss: 6.379533424831572
Training loss: 0.6047977805137634 / Valid loss: 6.338167587916057

Epoch: 6
Training loss: 0.3312166929244995 / Valid loss: 6.351317973363967
Training loss: 0.3801138997077942 / Valid loss: 6.364869966961089
Training loss: 0.26489537954330444 / Valid loss: 6.386529070990426
Training loss: 0.2924625873565674 / Valid loss: 6.324202916735694
Training loss: 0.28909289836883545 / Valid loss: 6.31839116414388

Epoch: 7
Training loss: 0.239985853433609 / Valid loss: 6.332022825876872
Training loss: 0.23317408561706543 / Valid loss: 6.349886085873559
Training loss: 0.45383593440055847 / Valid loss: 6.345950053987049
Training loss: 0.3779183030128479 / Valid loss: 6.352935872759137
Training loss: 0.2481425702571869 / Valid loss: 6.377603782926287

Epoch: 8
Training loss: 0.32644563913345337 / Valid loss: 6.366324878874279
Training loss: 0.2504947781562805 / Valid loss: 6.404029526029315
Training loss: 0.2783892750740051 / Valid loss: 6.349887168975103
Training loss: 0.14187154173851013 / Valid loss: 6.413859174365089
Training loss: 0.5875739455223083 / Valid loss: 6.359587648936681

Epoch: 9
Training loss: 0.23763427138328552 / Valid loss: 6.351294830867222
Training loss: 0.24149107933044434 / Valid loss: 6.371309373492286
Training loss: 0.491719126701355 / Valid loss: 6.380292147681827
Training loss: 0.9894688129425049 / Valid loss: 6.387549972534179

Epoch: 10
Training loss: 0.1746543049812317 / Valid loss: 6.388116296132406
Training loss: 0.15634571015834808 / Valid loss: 6.350954291934059
Training loss: 0.35772547125816345 / Valid loss: 6.390824386051723
Training loss: 0.2359614372253418 / Valid loss: 6.380708260763259
Training loss: 0.1785287857055664 / Valid loss: 6.398962125324068

Epoch: 11
Training loss: 0.32419300079345703 / Valid loss: 6.400275630042667
Training loss: 0.25757864117622375 / Valid loss: 6.348183064233689
Training loss: 0.1490851789712906 / Valid loss: 6.378296316237677
Training loss: 0.25188153982162476 / Valid loss: 6.3859323796771825
Training loss: 0.6387617588043213 / Valid loss: 6.348535269782657

Epoch: 12
Training loss: 0.19684576988220215 / Valid loss: 6.34313637415568
Training loss: 0.18406978249549866 / Valid loss: 6.368673590251378
Training loss: 0.10638885200023651 / Valid loss: 6.426484634762718
Training loss: 0.6260542273521423 / Valid loss: 6.38868845757984
Training loss: 0.2837603688240051 / Valid loss: 6.397825749715169

Epoch: 13
Training loss: 0.427876353263855 / Valid loss: 6.349088979902722
Training loss: 0.15343095362186432 / Valid loss: 6.3920352754138765
Training loss: 0.2186891734600067 / Valid loss: 6.36644063222976
Training loss: 0.11313729733228683 / Valid loss: 6.380622879664103
Training loss: 0.17325465381145477 / Valid loss: 6.394435859861828

Epoch: 14
Training loss: 0.1147758737206459 / Valid loss: 6.392515688850766
Training loss: 0.36730489134788513 / Valid loss: 6.40704679716201
Training loss: 0.19260765612125397 / Valid loss: 6.387901026862008
Training loss: 0.35366034507751465 / Valid loss: 6.390944410505749
Training loss: 0.17656251788139343 / Valid loss: 6.404665129525321

Epoch: 15
Training loss: 0.40578675270080566 / Valid loss: 6.348808038802374
Training loss: 0.15610815584659576 / Valid loss: 6.40020888192313
Training loss: 0.7451571226119995 / Valid loss: 6.372138675053915
Training loss: 0.2611580789089203 / Valid loss: 6.37483784584772
Training loss: 1.2519934177398682 / Valid loss: 6.349670226233346

Epoch: 16
Training loss: 0.19143900275230408 / Valid loss: 6.373471843628656
Training loss: 0.3948594033718109 / Valid loss: 6.363354800996326
Training loss: 0.41986599564552307 / Valid loss: 6.38583888780503
Training loss: 0.3495466709136963 / Valid loss: 6.388717201777867
Training loss: 0.17951276898384094 / Valid loss: 6.3987262907482325

Epoch: 17
Training loss: 0.09984377026557922 / Valid loss: 6.374601641155425
Training loss: 0.13315419852733612 / Valid loss: 6.371629701341901
Training loss: 0.14321258664131165 / Valid loss: 6.365561240059989
Training loss: 0.29012295603752136 / Valid loss: 6.427581446511405
Training loss: 0.07816649973392487 / Valid loss: 6.408714351199922

Epoch: 18
Training loss: 0.1759021133184433 / Valid loss: 6.36628763562157
Training loss: 0.16404025256633759 / Valid loss: 6.353469212849935
Training loss: 0.1549806296825409 / Valid loss: 6.34997258640471
Training loss: 0.1362791359424591 / Valid loss: 6.372058291662307
Training loss: 0.27259117364883423 / Valid loss: 6.3391058558509465

Epoch: 19
Training loss: 0.3134325444698334 / Valid loss: 6.3431919552031015
Training loss: 0.19196921586990356 / Valid loss: 6.3587281726655505
Training loss: 0.17138710618019104 / Valid loss: 6.389029323487055
Training loss: 0.16210754215717316 / Valid loss: 6.358572460356212

Epoch: 20
Training loss: 0.24646013975143433 / Valid loss: 6.377629486719767
Training loss: 0.19293606281280518 / Valid loss: 6.362969616481236
Training loss: 0.26453375816345215 / Valid loss: 6.365097781590053
Training loss: 0.10771262645721436 / Valid loss: 6.371193715504238
Training loss: 0.13389208912849426 / Valid loss: 6.359991084961664

Epoch: 21
Training loss: 0.0632951483130455 / Valid loss: 6.389475241161528
Training loss: 0.2829534411430359 / Valid loss: 6.3520969254629955
Training loss: 0.1311277151107788 / Valid loss: 6.356442274366106
Training loss: 0.11922013759613037 / Valid loss: 6.351744088672456
Training loss: 0.11476090550422668 / Valid loss: 6.344046599524361

Epoch: 22
Training loss: 0.314433217048645 / Valid loss: 6.356889738355364
Training loss: 0.09955817461013794 / Valid loss: 6.332163860684349
Training loss: 0.2845553159713745 / Valid loss: 6.35846548534575
Training loss: 0.5135579109191895 / Valid loss: 6.430278264908564
Training loss: 0.12044011056423187 / Valid loss: 6.354330292202177

Epoch: 23
Training loss: 0.23284736275672913 / Valid loss: 6.3608480680556525
Training loss: 0.24441230297088623 / Valid loss: 6.387201138905117
Training loss: 0.9421707391738892 / Valid loss: 6.329638126918248
Training loss: 0.09244180470705032 / Valid loss: 6.3635794571467805
Training loss: 0.2128239870071411 / Valid loss: 6.37289034979684

Epoch: 24
Training loss: 0.1628289520740509 / Valid loss: 6.379758262634278
Training loss: 0.11558397114276886 / Valid loss: 6.340266236804781
Training loss: 0.45293766260147095 / Valid loss: 6.364972614106678
Training loss: 0.1176397055387497 / Valid loss: 6.3469495114826024
Training loss: 0.31118571758270264 / Valid loss: 6.355879531587873

Epoch: 25
Training loss: 0.2861742079257965 / Valid loss: 6.3548418930598665
Training loss: 0.11810358613729477 / Valid loss: 6.3427767640068415
Training loss: 0.10699048638343811 / Valid loss: 6.373280084700811
Training loss: 0.21103748679161072 / Valid loss: 6.374943147386824
Training loss: 0.2994774281978607 / Valid loss: 6.36057790347508

Epoch: 26
Training loss: 0.0956144705414772 / Valid loss: 6.35219265846979
Training loss: 0.12337621301412582 / Valid loss: 6.3522225175585065
Training loss: 0.17329171299934387 / Valid loss: 6.359630157834007
Training loss: 0.14460070431232452 / Valid loss: 6.360416330610003
Training loss: 0.15792521834373474 / Valid loss: 6.356482026690529

Epoch: 27
Training loss: 0.188100203871727 / Valid loss: 6.361922014327276
Training loss: 0.3075993061065674 / Valid loss: 6.346123438789731
Training loss: 0.23173390328884125 / Valid loss: 6.354307606106713
Training loss: 0.06332435458898544 / Valid loss: 6.347758901686896
Training loss: 0.07888472825288773 / Valid loss: 6.338309664953322

Epoch: 28
Training loss: 0.10007684677839279 / Valid loss: 6.34368618329366
Training loss: 0.11534956842660904 / Valid loss: 6.317665994734991
Training loss: 0.41201603412628174 / Valid loss: 6.316208875746954
Training loss: 0.13325849175453186 / Valid loss: 6.35376516977946
Training loss: 0.10960765182971954 / Valid loss: 6.343856757027762

Epoch: 29
Training loss: 0.09998475015163422 / Valid loss: 6.337999493735177
Training loss: 0.07018780708312988 / Valid loss: 6.327555197761172
Training loss: 0.21861006319522858 / Valid loss: 6.3385934693472725
Training loss: 0.12166809290647507 / Valid loss: 6.357101290566581

Epoch: 30
Training loss: 0.2651839852333069 / Valid loss: 6.376119838442121
Training loss: 0.08739358931779861 / Valid loss: 6.314281331925165
Training loss: 0.10595417022705078 / Valid loss: 6.364610699244908
Training loss: 0.16797873377799988 / Valid loss: 6.368594980239868
Training loss: 0.2882004976272583 / Valid loss: 6.345144603365943

Epoch: 31
Training loss: 0.2058052271604538 / Valid loss: 6.346668211619059
Training loss: 0.11733370274305344 / Valid loss: 6.353152170635405
Training loss: 0.2024632841348648 / Valid loss: 6.362552765437535
Training loss: 0.093886598944664 / Valid loss: 6.366285296848842
Training loss: 0.048515111207962036 / Valid loss: 6.35177971294948

Epoch: 32
Training loss: 0.15887847542762756 / Valid loss: 6.351299776349749
Training loss: 0.09056556224822998 / Valid loss: 6.354667102722894
Training loss: 0.1883920282125473 / Valid loss: 6.313526158105759
Training loss: 0.23301085829734802 / Valid loss: 6.3347747552962534
Training loss: 0.10837091505527496 / Valid loss: 6.3476919764564155

Epoch: 33
Training loss: 0.14217688143253326 / Valid loss: 6.330272486096336
Training loss: 0.08026276528835297 / Valid loss: 6.306903943561372
Training loss: 0.10877158492803574 / Valid loss: 6.346770386468797
Training loss: 0.05563948303461075 / Valid loss: 6.31724534034729
Training loss: 0.08913753926753998 / Valid loss: 6.350783125559489

Epoch: 34
Training loss: 0.3834734559059143 / Valid loss: 6.322335329509917
Training loss: 0.11476555466651917 / Valid loss: 6.34281390508016
Training loss: 0.11446040868759155 / Valid loss: 6.356400848570324
Training loss: 0.17669464647769928 / Valid loss: 6.322026929401216
Training loss: 0.08984917402267456 / Valid loss: 6.338594443457467

Epoch: 35
Training loss: 0.09176599979400635 / Valid loss: 6.320191976002285
Training loss: 0.1967650055885315 / Valid loss: 6.342931259246099
Training loss: 0.09958475083112717 / Valid loss: 6.3163760389600485
Training loss: 0.07774113118648529 / Valid loss: 6.336566277912685
Training loss: 0.4753665626049042 / Valid loss: 6.33703430720738

Epoch: 36
Training loss: 0.0855315700173378 / Valid loss: 6.35292904263451
Training loss: 0.23633906245231628 / Valid loss: 6.346286723727272
Training loss: 0.1468266099691391 / Valid loss: 6.36680569875808
Training loss: 0.10524575412273407 / Valid loss: 6.3476219086419965
Training loss: 0.15384256839752197 / Valid loss: 6.3325327827816915

Epoch: 37
Training loss: 0.09761010110378265 / Valid loss: 6.349820436750139
Training loss: 0.09277644753456116 / Valid loss: 6.322392336527506
Training loss: 0.5939313769340515 / Valid loss: 6.34310857682001
Training loss: 0.24201560020446777 / Valid loss: 6.35947132337661
Training loss: 0.1858079433441162 / Valid loss: 6.336963083630517

Epoch: 38
Training loss: 0.11439502984285355 / Valid loss: 6.324086302802677
Training loss: 0.07631140947341919 / Valid loss: 6.325651597976685
Training loss: 0.1396043598651886 / Valid loss: 6.340085006895519
Training loss: 0.1194457933306694 / Valid loss: 6.330145695095971
Training loss: 0.09066614508628845 / Valid loss: 6.3449593362354095

Epoch: 39
Training loss: 0.1308739334344864 / Valid loss: 6.3326011407943
Training loss: 0.1915464997291565 / Valid loss: 6.319933466684251
Training loss: 0.07677540183067322 / Valid loss: 6.326484839121501
Training loss: 0.12588277459144592 / Valid loss: 6.332446566082182
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.520022467204503
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.220863342285156 / Valid loss: 16.393978872753326
Model is saved in epoch 0, overall batch: 0
Training loss: 6.950390815734863 / Valid loss: 6.82338509332566
Model is saved in epoch 0, overall batch: 100
Training loss: 4.82531213760376 / Valid loss: 5.854198537554059
Model is saved in epoch 0, overall batch: 200
Training loss: 4.192346572875977 / Valid loss: 5.807580464226859
Model is saved in epoch 0, overall batch: 300
Training loss: 5.7569074630737305 / Valid loss: 5.696045373734973
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.408085823059082 / Valid loss: 5.636620062873477
Model is saved in epoch 1, overall batch: 500
Training loss: 3.645799398422241 / Valid loss: 5.780500532331921
Training loss: 3.391494035720825 / Valid loss: 5.9138802074250725
Training loss: 4.34486198425293 / Valid loss: 6.142236021586827
Training loss: 4.439403533935547 / Valid loss: 5.916895657493955

Epoch: 2
Training loss: 1.9834133386611938 / Valid loss: 5.937925488608224
Training loss: 2.8391470909118652 / Valid loss: 6.094314114252726
Training loss: 2.855729579925537 / Valid loss: 6.111214115506127
Training loss: 1.9998453855514526 / Valid loss: 6.154920430410476
Training loss: 2.434849500656128 / Valid loss: 6.177736096155076

Epoch: 3
Training loss: 2.05216646194458 / Valid loss: 6.207331700552078
Training loss: 2.096348285675049 / Valid loss: 6.382853948502314
Training loss: 1.8594611883163452 / Valid loss: 6.3469143504188175
Training loss: 1.7997956275939941 / Valid loss: 6.3521278381347654
Training loss: 2.936952590942383 / Valid loss: 6.281514494759696

Epoch: 4
Training loss: 1.4008034467697144 / Valid loss: 6.404708149319603
Training loss: 1.2083890438079834 / Valid loss: 6.40638401621864
Training loss: 1.3007365465164185 / Valid loss: 6.493873855045863
Training loss: 2.1210110187530518 / Valid loss: 6.492948775064377
Training loss: 1.1316049098968506 / Valid loss: 6.521965633119856

Epoch: 5
Training loss: 0.9016947746276855 / Valid loss: 6.470519597189767
Training loss: 1.6965577602386475 / Valid loss: 6.627134577433268
Training loss: 0.9127664566040039 / Valid loss: 6.525098600841704
Training loss: 1.661691427230835 / Valid loss: 6.589740158262707
Training loss: 1.689554214477539 / Valid loss: 6.621862761179606

Epoch: 6
Training loss: 0.8141546249389648 / Valid loss: 6.544092589332944
Training loss: 1.249220371246338 / Valid loss: 6.618073122841971
Training loss: 1.424410104751587 / Valid loss: 6.614254006885347
Training loss: 0.8493683338165283 / Valid loss: 6.645631190708706
Training loss: 1.8747713565826416 / Valid loss: 6.632725486301241

Epoch: 7
Training loss: 0.8722219467163086 / Valid loss: 6.6716000329880485
Training loss: 0.5133195519447327 / Valid loss: 6.741432868866694
Training loss: 0.9211058616638184 / Valid loss: 6.641839931124733
Training loss: 0.8676416873931885 / Valid loss: 6.699280577614194
Training loss: 1.2192366123199463 / Valid loss: 6.767608233860561

Epoch: 8
Training loss: 0.4832886755466461 / Valid loss: 6.7018872828710645
Training loss: 0.7298845648765564 / Valid loss: 6.70756992385501
Training loss: 0.6494537591934204 / Valid loss: 6.725547772362119
Training loss: 0.6193346977233887 / Valid loss: 6.737971142360142
Training loss: 0.7750704288482666 / Valid loss: 6.780278115045457

Epoch: 9
Training loss: 0.48598724603652954 / Valid loss: 6.710648661568051
Training loss: 0.4702382981777191 / Valid loss: 6.729933484395345
Training loss: 0.44677335023880005 / Valid loss: 6.735917182195754
Training loss: 0.26093924045562744 / Valid loss: 6.766480697904314

Epoch: 10
Training loss: 0.20831982791423798 / Valid loss: 6.76352825164795
Training loss: 0.40861114859580994 / Valid loss: 6.75679829461234
Training loss: 0.21374467015266418 / Valid loss: 6.736504111971174
Training loss: 0.9156556129455566 / Valid loss: 6.763773459479922
Training loss: 0.29189813137054443 / Valid loss: 6.793487694149926

Epoch: 11
Training loss: 0.21669955551624298 / Valid loss: 6.776631700424921
Training loss: 0.3701687157154083 / Valid loss: 6.751632604144868
Training loss: 0.28558075428009033 / Valid loss: 6.764737946646554
Training loss: 0.38041460514068604 / Valid loss: 6.775603239876883
Training loss: 0.2748730480670929 / Valid loss: 6.75230058034261

Epoch: 12
Training loss: 0.23764126002788544 / Valid loss: 6.790800140017555
Training loss: 0.17625558376312256 / Valid loss: 6.800352503004528
Training loss: 0.5052230954170227 / Valid loss: 6.834218724568685
Training loss: 0.3282228708267212 / Valid loss: 6.757922267913818
Training loss: 0.24219930171966553 / Valid loss: 6.82920796303522

Epoch: 13
Training loss: 0.31549516320228577 / Valid loss: 6.848487145560128
Training loss: 0.13276666402816772 / Valid loss: 6.824462940579369
Training loss: 0.23020052909851074 / Valid loss: 6.781087975274949
Training loss: 0.11618486046791077 / Valid loss: 6.825079836164202
Training loss: 0.41749078035354614 / Valid loss: 6.803944442385719

Epoch: 14
Training loss: 0.19638192653656006 / Valid loss: 6.826554148537772
Training loss: 0.1346035599708557 / Valid loss: 6.814500672476632
Training loss: 0.26664212346076965 / Valid loss: 6.802070143109276
Training loss: 0.2566507160663605 / Valid loss: 6.8199559075491765
Training loss: 0.16003315150737762 / Valid loss: 6.802277719406854

Epoch: 15
Training loss: 0.3428838551044464 / Valid loss: 6.83650623957316
Training loss: 0.2979288399219513 / Valid loss: 6.842084049043201
Training loss: 0.2518506646156311 / Valid loss: 6.864933581579299
Training loss: 0.3272779583930969 / Valid loss: 6.863692842211042
Training loss: 0.17355574667453766 / Valid loss: 6.881246943700881

Epoch: 16
Training loss: 0.23739860951900482 / Valid loss: 6.849971199035645
Training loss: 0.23054343461990356 / Valid loss: 6.826543029149374
Training loss: 0.11159612238407135 / Valid loss: 6.820881330399287
Training loss: 0.18667051196098328 / Valid loss: 6.873093714032854
Training loss: 0.24271151423454285 / Valid loss: 6.8250594865708125

Epoch: 17
Training loss: 0.11067895591259003 / Valid loss: 6.841089875357492
Training loss: 0.0813533216714859 / Valid loss: 6.849985131763277
Training loss: 0.13982553780078888 / Valid loss: 6.8243595940726145
Training loss: 0.20707112550735474 / Valid loss: 6.8212613786969865
Training loss: 0.22129148244857788 / Valid loss: 6.839079461778913

Epoch: 18
Training loss: 0.25001493096351624 / Valid loss: 6.8294623057047525
Training loss: 0.3857252597808838 / Valid loss: 6.828814774467832
Training loss: 0.36503344774246216 / Valid loss: 6.866144770667667
Training loss: 0.14550575613975525 / Valid loss: 6.852942739214216
Training loss: 0.2232048511505127 / Valid loss: 6.866510184605916

Epoch: 19
Training loss: 0.6372464895248413 / Valid loss: 6.835455617450532
Training loss: 0.21749453246593475 / Valid loss: 6.851958070482526
Training loss: 0.3034130930900574 / Valid loss: 6.895770740509033
Training loss: 0.12826129794120789 / Valid loss: 6.85516532716297

Epoch: 20
Training loss: 0.1368284970521927 / Valid loss: 6.878189513796852
Training loss: 0.199870765209198 / Valid loss: 6.812482057298933
Training loss: 0.16220107674598694 / Valid loss: 6.82926279703776
Training loss: 0.20118914544582367 / Valid loss: 6.862219070252918
Training loss: 0.14325936138629913 / Valid loss: 6.862007604326521

Epoch: 21
Training loss: 0.08248455822467804 / Valid loss: 6.89731646719433
Training loss: 0.155981183052063 / Valid loss: 6.856298809959775
Training loss: 0.24711565673351288 / Valid loss: 6.875075889769055
Training loss: 0.23854897916316986 / Valid loss: 6.862183666229248
Training loss: 0.2122732549905777 / Valid loss: 6.886985422316052

Epoch: 22
Training loss: 0.18395864963531494 / Valid loss: 6.8817937896365216
Training loss: 0.10614835470914841 / Valid loss: 6.879094214666457
Training loss: 0.3852521479129791 / Valid loss: 6.86791272844587
Training loss: 0.21029776334762573 / Valid loss: 6.900392875217256
Training loss: 0.11270299553871155 / Valid loss: 6.833392102377755

Epoch: 23
Training loss: 0.08909773081541061 / Valid loss: 6.850704311189197
Training loss: 0.13861559331417084 / Valid loss: 6.86902954464867
Training loss: 0.13606125116348267 / Valid loss: 6.85819448970613
Training loss: 0.3629767894744873 / Valid loss: 6.900230044410343
Training loss: 0.14103704690933228 / Valid loss: 6.859778549557641

Epoch: 24
Training loss: 0.08569955825805664 / Valid loss: 6.849300688789004
Training loss: 0.19008740782737732 / Valid loss: 6.868378743671236
Training loss: 0.5738720893859863 / Valid loss: 6.873212598619007
Training loss: 0.08215358853340149 / Valid loss: 6.8977472237178254
Training loss: 0.08626773953437805 / Valid loss: 6.862959489368257

Epoch: 25
Training loss: 0.23030945658683777 / Valid loss: 6.8270149957566035
Training loss: 0.11255639046430588 / Valid loss: 6.854418377649217
Training loss: 0.0952090248465538 / Valid loss: 6.87209324155535
Training loss: 0.11542341113090515 / Valid loss: 6.870381577809652
Training loss: 0.1287255585193634 / Valid loss: 6.851056403205508

Epoch: 26
Training loss: 0.2479320764541626 / Valid loss: 6.887711534045992
Training loss: 0.1491987705230713 / Valid loss: 6.860861224219913
Training loss: 0.07739858329296112 / Valid loss: 6.861242439633324
Training loss: 0.10686315596103668 / Valid loss: 6.858582510266985
Training loss: 0.40368396043777466 / Valid loss: 6.838596875326974

Epoch: 27
Training loss: 0.1953480839729309 / Valid loss: 6.870929120835804
Training loss: 0.2527312636375427 / Valid loss: 6.831080302738008
Training loss: 0.11501236259937286 / Valid loss: 6.829936331794375
Training loss: 0.07293621450662613 / Valid loss: 6.872516012191772
Training loss: 0.08122929185628891 / Valid loss: 6.867914136250814

Epoch: 28
Training loss: 0.04728984087705612 / Valid loss: 6.867459665025984
Training loss: 0.16707246005535126 / Valid loss: 6.901360593523298
Training loss: 0.14043238759040833 / Valid loss: 6.831055550348191
Training loss: 0.17438365519046783 / Valid loss: 6.882770815349761
Training loss: 0.10534228384494781 / Valid loss: 6.843616326649983

Epoch: 29
Training loss: 0.47690534591674805 / Valid loss: 6.877875464303153
Training loss: 0.08285211771726608 / Valid loss: 6.874543444315592
Training loss: 0.16169443726539612 / Valid loss: 6.830899697258359
Training loss: 0.22997316718101501 / Valid loss: 6.850728602636428

Epoch: 30
Training loss: 0.20636498928070068 / Valid loss: 6.879583631243024
Training loss: 0.23870781064033508 / Valid loss: 6.819139975593203
Training loss: 0.259239137172699 / Valid loss: 6.825209522247315
Training loss: 0.39394283294677734 / Valid loss: 6.869104383105324
Training loss: 0.09625744819641113 / Valid loss: 6.851364835103353

Epoch: 31
Training loss: 0.13379818201065063 / Valid loss: 6.8476044359661286
Training loss: 0.2040589153766632 / Valid loss: 6.847045853024437
Training loss: 0.057182855904102325 / Valid loss: 6.850955533981323
Training loss: 0.18322420120239258 / Valid loss: 6.8313463619777135
Training loss: 0.27824878692626953 / Valid loss: 6.830618717556908

Epoch: 32
Training loss: 0.04976912587881088 / Valid loss: 6.818941961015974
Training loss: 0.08113226294517517 / Valid loss: 6.845159339904785
Training loss: 0.10690615326166153 / Valid loss: 6.807340730939592
Training loss: 0.06454800814390182 / Valid loss: 6.8864845957074845
Training loss: 0.22531446814537048 / Valid loss: 6.864781238919213

Epoch: 33
Training loss: 0.3414895534515381 / Valid loss: 6.839145533243815
Training loss: 0.11443338543176651 / Valid loss: 6.838155024392265
Training loss: 0.10652416944503784 / Valid loss: 6.89386488369533
Training loss: 0.08336052298545837 / Valid loss: 6.835525340125674
Training loss: 0.1125711128115654 / Valid loss: 6.857620325542632

Epoch: 34
Training loss: 0.06971574574708939 / Valid loss: 6.846522426605224
Training loss: 0.22229725122451782 / Valid loss: 6.837329355875651
Training loss: 0.22787760198116302 / Valid loss: 6.868928255353655
Training loss: 0.1495833694934845 / Valid loss: 6.821498434884208
Training loss: 0.03812015801668167 / Valid loss: 6.821498439425514

Epoch: 35
Training loss: 0.14116984605789185 / Valid loss: 6.858575482595534
Training loss: 0.1651119738817215 / Valid loss: 6.843854972294399
Training loss: 0.12123597413301468 / Valid loss: 6.821394856770834
Training loss: 0.07945707440376282 / Valid loss: 6.8295694260370166
Training loss: 0.09358997642993927 / Valid loss: 6.851823679606119

Epoch: 36
Training loss: 0.08625078946352005 / Valid loss: 6.873786989847819
Training loss: 0.1424684375524521 / Valid loss: 6.824896985008603
Training loss: 0.06935569643974304 / Valid loss: 6.810631116231282
Training loss: 0.13521768152713776 / Valid loss: 6.843557135264079
Training loss: 0.10350160300731659 / Valid loss: 6.845188860666184

Epoch: 37
Training loss: 0.09939810633659363 / Valid loss: 6.846831126440139
Training loss: 0.9921778440475464 / Valid loss: 6.846525707699004
Training loss: 0.12309449911117554 / Valid loss: 6.834499195643834
Training loss: 0.10211436450481415 / Valid loss: 6.809490344637916
Training loss: 0.12287743389606476 / Valid loss: 6.810372881662278

Epoch: 38
Training loss: 0.11717455834150314 / Valid loss: 6.808808671860468
Training loss: 0.0923648402094841 / Valid loss: 6.819190856388637
Training loss: 0.3021281957626343 / Valid loss: 6.881321123668125
Training loss: 0.09703575819730759 / Valid loss: 6.827733625684465
Training loss: 0.1978583186864853 / Valid loss: 6.849849996112642

Epoch: 39
Training loss: 0.1060674786567688 / Valid loss: 6.807004678817022
Training loss: 0.1399092972278595 / Valid loss: 6.846773252033052
Training loss: 0.15706518292427063 / Valid loss: 6.812927159808931
Training loss: 0.14932957291603088 / Valid loss: 6.85579019728161
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.470942783355713
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.220863342285156 / Valid loss: 16.393978872753326
Model is saved in epoch 0, overall batch: 0
Training loss: 6.950389862060547 / Valid loss: 6.823386837187267
Model is saved in epoch 0, overall batch: 100
Training loss: 4.825314044952393 / Valid loss: 5.85419774055481
Model is saved in epoch 0, overall batch: 200
Training loss: 4.192330837249756 / Valid loss: 5.8075833479563395
Model is saved in epoch 0, overall batch: 300
Training loss: 5.757596015930176 / Valid loss: 5.696112294424148
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.407743453979492 / Valid loss: 5.636658293860299
Model is saved in epoch 1, overall batch: 500
Training loss: 3.6482276916503906 / Valid loss: 5.780814336595081
Training loss: 3.3800506591796875 / Valid loss: 5.912850636527652
Training loss: 4.326949119567871 / Valid loss: 6.146570991334461
Training loss: 4.446280479431152 / Valid loss: 5.915807072321574

Epoch: 2
Training loss: 1.9682738780975342 / Valid loss: 5.938239133925665
Training loss: 2.808359146118164 / Valid loss: 6.096815418061756
Training loss: 2.8295798301696777 / Valid loss: 6.113782807758876
Training loss: 2.002133846282959 / Valid loss: 6.156531533740815
Training loss: 2.454880714416504 / Valid loss: 6.174669506436302

Epoch: 3
Training loss: 2.0396203994750977 / Valid loss: 6.204541104180472
Training loss: 2.1274211406707764 / Valid loss: 6.388644468216669
Training loss: 1.870819091796875 / Valid loss: 6.347062735330491
Training loss: 1.791976809501648 / Valid loss: 6.351198516573224
Training loss: 2.9232239723205566 / Valid loss: 6.277477845691499

Epoch: 4
Training loss: 1.4149925708770752 / Valid loss: 6.401439380645752
Training loss: 1.2083492279052734 / Valid loss: 6.40446215130034
Training loss: 1.3228694200515747 / Valid loss: 6.493797361283075
Training loss: 2.1583826541900635 / Valid loss: 6.497202607563564
Training loss: 1.1012547016143799 / Valid loss: 6.526996984935942

Epoch: 5
Training loss: 0.9091033935546875 / Valid loss: 6.473464906783331
Training loss: 1.7096089124679565 / Valid loss: 6.626062247866676
Training loss: 0.8798978924751282 / Valid loss: 6.531043910980225
Training loss: 1.6770687103271484 / Valid loss: 6.591767733437674
Training loss: 1.691240906715393 / Valid loss: 6.61938974289667

Epoch: 6
Training loss: 0.807741105556488 / Valid loss: 6.544423428035918
Training loss: 1.2203484773635864 / Valid loss: 6.619204521179199
Training loss: 1.4919532537460327 / Valid loss: 6.612187803359259
Training loss: 0.8539100885391235 / Valid loss: 6.644665050506592
Training loss: 1.8492655754089355 / Valid loss: 6.634890556335449

Epoch: 7
Training loss: 0.8523644208908081 / Valid loss: 6.6783981187003
Training loss: 0.5235430598258972 / Valid loss: 6.746110416594005
Training loss: 0.9463388323783875 / Valid loss: 6.649046584538051
Training loss: 0.8369169235229492 / Valid loss: 6.696163202467419
Training loss: 1.2549309730529785 / Valid loss: 6.786598503021967

Epoch: 8
Training loss: 0.467570960521698 / Valid loss: 6.705645690645491
Training loss: 0.7313639521598816 / Valid loss: 6.712345559256417
Training loss: 0.6206578016281128 / Valid loss: 6.7289872669038315
Training loss: 0.6486619710922241 / Valid loss: 6.750807916550409
Training loss: 0.79973304271698 / Valid loss: 6.782346221378871

Epoch: 9
Training loss: 0.5099425911903381 / Valid loss: 6.721122616813297
Training loss: 0.46302029490470886 / Valid loss: 6.738413274855841
Training loss: 0.4550943374633789 / Valid loss: 6.749341823941186
Training loss: 0.2520282566547394 / Valid loss: 6.768888577960786

Epoch: 10
Training loss: 0.21973860263824463 / Valid loss: 6.765588805789039
Training loss: 0.41237014532089233 / Valid loss: 6.772572576432001
Training loss: 0.2198469191789627 / Valid loss: 6.752135067894345
Training loss: 0.9434274435043335 / Valid loss: 6.771069156555902
Training loss: 0.2923482358455658 / Valid loss: 6.803058737800235

Epoch: 11
Training loss: 0.21857169270515442 / Valid loss: 6.7810729435512
Training loss: 0.37607914209365845 / Valid loss: 6.7513659477233885
Training loss: 0.27926331758499146 / Valid loss: 6.763767410459972
Training loss: 0.3868863582611084 / Valid loss: 6.78081260408674
Training loss: 0.2781660556793213 / Valid loss: 6.758492142813546

Epoch: 12
Training loss: 0.256039559841156 / Valid loss: 6.798655164809454
Training loss: 0.164953351020813 / Valid loss: 6.802181952340263
Training loss: 0.5123470425605774 / Valid loss: 6.836387872695923
Training loss: 0.31927555799484253 / Valid loss: 6.77131005695888
Training loss: 0.23837053775787354 / Valid loss: 6.835114846910749

Epoch: 13
Training loss: 0.3634691834449768 / Valid loss: 6.866267817361014
Training loss: 0.14183494448661804 / Valid loss: 6.830464930761428
Training loss: 0.2085065245628357 / Valid loss: 6.78370137441726
Training loss: 0.1076415404677391 / Valid loss: 6.823090680440267
Training loss: 0.4173184037208557 / Valid loss: 6.800145925794329

Epoch: 14
Training loss: 0.17928539216518402 / Valid loss: 6.826139704386393
Training loss: 0.14116495847702026 / Valid loss: 6.8190976415361675
Training loss: 0.26789388060569763 / Valid loss: 6.81031386965797
Training loss: 0.25107502937316895 / Valid loss: 6.823447322845459
Training loss: 0.15624257922172546 / Valid loss: 6.811030315217518

Epoch: 15
Training loss: 0.3320198059082031 / Valid loss: 6.8460303442818775
Training loss: 0.3130181133747101 / Valid loss: 6.851102647327242
Training loss: 0.2516714036464691 / Valid loss: 6.8676203409830725
Training loss: 0.3003346920013428 / Valid loss: 6.886373179299491
Training loss: 0.17006796598434448 / Valid loss: 6.885657215118409

Epoch: 16
Training loss: 0.24567854404449463 / Valid loss: 6.854483663468134
Training loss: 0.25134244561195374 / Valid loss: 6.828474330902099
Training loss: 0.11478491127490997 / Valid loss: 6.819993831997826
Training loss: 0.18078425526618958 / Valid loss: 6.877669484274728
Training loss: 0.2403116226196289 / Valid loss: 6.8216426576886855

Epoch: 17
Training loss: 0.09745955467224121 / Valid loss: 6.8287151881626675
Training loss: 0.07810890674591064 / Valid loss: 6.847783265795027
Training loss: 0.1458316594362259 / Valid loss: 6.819705527169364
Training loss: 0.20374032855033875 / Valid loss: 6.812095478602818
Training loss: 0.20875155925750732 / Valid loss: 6.831107221330915

Epoch: 18
Training loss: 0.25016939640045166 / Valid loss: 6.822411918640137
Training loss: 0.35422831773757935 / Valid loss: 6.826651012329828
Training loss: 0.3715594708919525 / Valid loss: 6.85956541470119
Training loss: 0.13625629246234894 / Valid loss: 6.8501672835577105
Training loss: 0.20888051390647888 / Valid loss: 6.848807244073777

Epoch: 19
Training loss: 0.6043421030044556 / Valid loss: 6.8344897951398575
Training loss: 0.18034552037715912 / Valid loss: 6.8459519704182945
Training loss: 0.2889408469200134 / Valid loss: 6.891586948576427
Training loss: 0.131466805934906 / Valid loss: 6.855650642939977

Epoch: 20
Training loss: 0.1372525542974472 / Valid loss: 6.872846943991525
Training loss: 0.1918274164199829 / Valid loss: 6.811973335629418
Training loss: 0.16359901428222656 / Valid loss: 6.824751440684
Training loss: 0.2144174575805664 / Valid loss: 6.85197970526559
Training loss: 0.12851914763450623 / Valid loss: 6.852413890475319

Epoch: 21
Training loss: 0.09356936812400818 / Valid loss: 6.894689575831095
Training loss: 0.15108530223369598 / Valid loss: 6.841455750238328
Training loss: 0.2527719736099243 / Valid loss: 6.864632844924927
Training loss: 0.25448840856552124 / Valid loss: 6.852382451012021
Training loss: 0.21429768204689026 / Valid loss: 6.885511589050293

Epoch: 22
Training loss: 0.16047386825084686 / Valid loss: 6.871924859001523
Training loss: 0.11063627898693085 / Valid loss: 6.872855826786586
Training loss: 0.3942066431045532 / Valid loss: 6.864327094668433
Training loss: 0.1964876353740692 / Valid loss: 6.886187276386079
Training loss: 0.10802587866783142 / Valid loss: 6.823644435973394

Epoch: 23
Training loss: 0.0845547690987587 / Valid loss: 6.83895244144258
Training loss: 0.15161976218223572 / Valid loss: 6.8650788352603005
Training loss: 0.15459929406642914 / Valid loss: 6.85512790906997
Training loss: 0.3658031225204468 / Valid loss: 6.890100011371431
Training loss: 0.1383472979068756 / Valid loss: 6.859720432190668

Epoch: 24
Training loss: 0.0824507549405098 / Valid loss: 6.844408943539574
Training loss: 0.17867611348628998 / Valid loss: 6.8621987615312845
Training loss: 0.5252395272254944 / Valid loss: 6.859406730106898
Training loss: 0.10420887172222137 / Valid loss: 6.8954416933513825
Training loss: 0.08316303789615631 / Valid loss: 6.857410294669015

Epoch: 25
Training loss: 0.24557878077030182 / Valid loss: 6.8276024909246535
Training loss: 0.09017512947320938 / Valid loss: 6.854347996484666
Training loss: 0.09165014326572418 / Valid loss: 6.865915212177095
Training loss: 0.1305646002292633 / Valid loss: 6.866161076227824
Training loss: 0.12365283817052841 / Valid loss: 6.841561639876593

Epoch: 26
Training loss: 0.24425509572029114 / Valid loss: 6.881639852977934
Training loss: 0.14586421847343445 / Valid loss: 6.860540049416678
Training loss: 0.08034604787826538 / Valid loss: 6.852903161730085
Training loss: 0.1048135906457901 / Valid loss: 6.855480539231073
Training loss: 0.40866798162460327 / Valid loss: 6.841754186721075

Epoch: 27
Training loss: 0.19582800567150116 / Valid loss: 6.860644821893601
Training loss: 0.24356114864349365 / Valid loss: 6.825841529028756
Training loss: 0.112874336540699 / Valid loss: 6.8287713323320665
Training loss: 0.07593929767608643 / Valid loss: 6.873615746271042
Training loss: 0.0868779867887497 / Valid loss: 6.867521989913214

Epoch: 28
Training loss: 0.04342581331729889 / Valid loss: 6.857335985274542
Training loss: 0.15604519844055176 / Valid loss: 6.8970821108136855
Training loss: 0.1393381506204605 / Valid loss: 6.830530507223947
Training loss: 0.19369161128997803 / Valid loss: 6.888026653017317
Training loss: 0.10757333785295486 / Valid loss: 6.840520014081682

Epoch: 29
Training loss: 0.4702719449996948 / Valid loss: 6.871748969668434
Training loss: 0.10238046199083328 / Valid loss: 6.866490345909482
Training loss: 0.16582179069519043 / Valid loss: 6.825358059292748
Training loss: 0.21744456887245178 / Valid loss: 6.8376730464753654

Epoch: 30
Training loss: 0.20327584445476532 / Valid loss: 6.87271343866984
Training loss: 0.24326147139072418 / Valid loss: 6.820710332053048
Training loss: 0.25664299726486206 / Valid loss: 6.820058809007917
Training loss: 0.377429336309433 / Valid loss: 6.858282434372675
Training loss: 0.09283601492643356 / Valid loss: 6.841122586386544

Epoch: 31
Training loss: 0.12137959897518158 / Valid loss: 6.838675167447045
Training loss: 0.19082525372505188 / Valid loss: 6.8428982098897295
Training loss: 0.05909354239702225 / Valid loss: 6.8455548740568615
Training loss: 0.1878955066204071 / Valid loss: 6.823439779735747
Training loss: 0.2647661566734314 / Valid loss: 6.818842497326079

Epoch: 32
Training loss: 0.0505913645029068 / Valid loss: 6.8203532491411485
Training loss: 0.0843193531036377 / Valid loss: 6.843948841094971
Training loss: 0.10372567176818848 / Valid loss: 6.807317356836228
Training loss: 0.057869866490364075 / Valid loss: 6.885180105481829
Training loss: 0.21665088832378387 / Valid loss: 6.85936868304298

Epoch: 33
Training loss: 0.33497607707977295 / Valid loss: 6.834811038062686
Training loss: 0.11952337622642517 / Valid loss: 6.836437565939767
Training loss: 0.09987235069274902 / Valid loss: 6.891586744217646
Training loss: 0.08636674284934998 / Valid loss: 6.835414550417945
Training loss: 0.11926237493753433 / Valid loss: 6.859885987781343

Epoch: 34
Training loss: 0.07573606818914413 / Valid loss: 6.848186002458845
Training loss: 0.19641859829425812 / Valid loss: 6.837522538503011
Training loss: 0.22875800728797913 / Valid loss: 6.87107143629165
Training loss: 0.14782066643238068 / Valid loss: 6.825635390054612
Training loss: 0.040376678109169006 / Valid loss: 6.81749534152803

Epoch: 35
Training loss: 0.13860230147838593 / Valid loss: 6.8595517612638925
Training loss: 0.16933287680149078 / Valid loss: 6.846603502546038
Training loss: 0.11515150964260101 / Valid loss: 6.826332977839878
Training loss: 0.08145473897457123 / Valid loss: 6.828476587931315
Training loss: 0.09382928907871246 / Valid loss: 6.852891783487229

Epoch: 36
Training loss: 0.08319086581468582 / Valid loss: 6.871769460042318
Training loss: 0.14307168126106262 / Valid loss: 6.825379276275635
Training loss: 0.062125805765390396 / Valid loss: 6.810343910398937
Training loss: 0.145666241645813 / Valid loss: 6.835819058191209
Training loss: 0.1044132262468338 / Valid loss: 6.850530129387265

Epoch: 37
Training loss: 0.10232094675302505 / Valid loss: 6.845210184369768
Training loss: 0.9380553960800171 / Valid loss: 6.847133214133127
Training loss: 0.12696167826652527 / Valid loss: 6.8371652921040855
Training loss: 0.10104699432849884 / Valid loss: 6.813304154078166
Training loss: 0.12738704681396484 / Valid loss: 6.812761302221389

Epoch: 38
Training loss: 0.11309570074081421 / Valid loss: 6.807810252053397
Training loss: 0.10019385814666748 / Valid loss: 6.825658925374349
Training loss: 0.3007975220680237 / Valid loss: 6.8791591235569545
Training loss: 0.0971461683511734 / Valid loss: 6.829976204463414
Training loss: 0.19918319582939148 / Valid loss: 6.852606950487409

Epoch: 39
Training loss: 0.10756073892116547 / Valid loss: 6.808152537118821
Training loss: 0.14876770973205566 / Valid loss: 6.845035371326265
Training loss: 0.15292830765247345 / Valid loss: 6.820069376627604
Training loss: 0.13143491744995117 / Valid loss: 6.868331675302414
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 500): 5.470940530867804
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.986988067626953 / Valid loss: 16.433027476356145
Model is saved in epoch 0, overall batch: 0
Training loss: 15.960634231567383 / Valid loss: 13.008776069822765
Model is saved in epoch 0, overall batch: 100
Training loss: 6.908407688140869 / Valid loss: 10.72540210088094
Model is saved in epoch 0, overall batch: 200
Training loss: 9.450263977050781 / Valid loss: 9.172034100123815
Model is saved in epoch 0, overall batch: 300
Training loss: 6.161900043487549 / Valid loss: 8.1243883450826
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.731409549713135 / Valid loss: 7.433190756752378
Model is saved in epoch 1, overall batch: 500
Training loss: 5.558803558349609 / Valid loss: 6.982147645950318
Model is saved in epoch 1, overall batch: 600
Training loss: 6.517811298370361 / Valid loss: 6.670591526939756
Model is saved in epoch 1, overall batch: 700
Training loss: 5.857550144195557 / Valid loss: 6.459752475647699
Model is saved in epoch 1, overall batch: 800
Training loss: 9.24473762512207 / Valid loss: 6.310963934943789
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.138407230377197 / Valid loss: 6.208769160225278
Model is saved in epoch 2, overall batch: 1000
Training loss: 4.327468395233154 / Valid loss: 6.15566049076262
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.517959117889404 / Valid loss: 6.111716951642718
Model is saved in epoch 2, overall batch: 1200
Training loss: 4.740536689758301 / Valid loss: 6.0822539806365965
Model is saved in epoch 2, overall batch: 1300
Training loss: 6.678840637207031 / Valid loss: 6.055055320830572
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.042050838470459 / Valid loss: 6.038378844942366
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.203843593597412 / Valid loss: 6.02483002117702
Model is saved in epoch 3, overall batch: 1600
Training loss: 5.312495708465576 / Valid loss: 6.024246252150762
Model is saved in epoch 3, overall batch: 1700
Training loss: 6.669886589050293 / Valid loss: 6.021089319955735
Model is saved in epoch 3, overall batch: 1800
Training loss: 7.518589019775391 / Valid loss: 6.015333227884202
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 8.562137603759766 / Valid loss: 6.015072327568418
Model is saved in epoch 4, overall batch: 2000
Training loss: 6.722714424133301 / Valid loss: 6.008289014725458
Model is saved in epoch 4, overall batch: 2100
Training loss: 7.407953262329102 / Valid loss: 6.007616088503883
Model is saved in epoch 4, overall batch: 2200
Training loss: 7.1916046142578125 / Valid loss: 6.001187460763114
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.161715507507324 / Valid loss: 5.998244271959577
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 4.407268524169922 / Valid loss: 5.989955143701462
Model is saved in epoch 5, overall batch: 2500
Training loss: 7.172090530395508 / Valid loss: 6.000783061981201
Training loss: 4.792024612426758 / Valid loss: 5.999831826346261
Training loss: 6.890211582183838 / Valid loss: 5.999444625491187
Training loss: 4.259645462036133 / Valid loss: 5.991952539625622

Epoch: 6
Training loss: 4.462637901306152 / Valid loss: 5.98975312823341
Model is saved in epoch 6, overall batch: 3000
Training loss: 6.137667655944824 / Valid loss: 5.9974379653022405
Training loss: 3.912099838256836 / Valid loss: 5.991954446974255
Training loss: 5.891840934753418 / Valid loss: 5.986907261893863
Model is saved in epoch 6, overall batch: 3300
Training loss: 4.294204235076904 / Valid loss: 5.992651001612345

Epoch: 7
Training loss: 5.018281936645508 / Valid loss: 5.991269810994466
Training loss: 7.830850601196289 / Valid loss: 5.988702081498646
Training loss: 3.8882598876953125 / Valid loss: 5.983658070791336
Model is saved in epoch 7, overall batch: 3700
Training loss: 5.590728759765625 / Valid loss: 5.976753650392805
Model is saved in epoch 7, overall batch: 3800
Training loss: 5.879327774047852 / Valid loss: 5.984952908470517

Epoch: 8
Training loss: 6.452438831329346 / Valid loss: 5.97269579796564
Model is saved in epoch 8, overall batch: 4000
Training loss: 7.177490711212158 / Valid loss: 5.973862788790748
Training loss: 5.158285140991211 / Valid loss: 5.980816200801304
Training loss: 8.123091697692871 / Valid loss: 5.983102721259708
Training loss: 5.883680820465088 / Valid loss: 5.976775446392241

Epoch: 9
Training loss: 7.363086223602295 / Valid loss: 5.975892977487473
Training loss: 6.456648826599121 / Valid loss: 5.97587156749907
Training loss: 6.85090446472168 / Valid loss: 5.96822212764195
Model is saved in epoch 9, overall batch: 4700
Training loss: 5.352444171905518 / Valid loss: 5.971919734137399

Epoch: 10
Training loss: 5.529453754425049 / Valid loss: 5.973263374964396
Training loss: 6.948630332946777 / Valid loss: 5.968433750243414
Training loss: 7.88192892074585 / Valid loss: 5.964317546572004
Model is saved in epoch 10, overall batch: 5100
Training loss: 5.0806474685668945 / Valid loss: 5.9669674737112866
Training loss: 5.985946178436279 / Valid loss: 5.961588089806693
Model is saved in epoch 10, overall batch: 5300

Epoch: 11
Training loss: 6.6322808265686035 / Valid loss: 5.963266277313233
Training loss: 6.172384738922119 / Valid loss: 5.961288368134271
Model is saved in epoch 11, overall batch: 5500
Training loss: 8.624393463134766 / Valid loss: 5.965678451174782
Training loss: 5.980727195739746 / Valid loss: 5.961539733977545
Training loss: 4.666723251342773 / Valid loss: 5.947411550794329
Model is saved in epoch 11, overall batch: 5800

Epoch: 12
Training loss: 7.401723861694336 / Valid loss: 5.964691511789957
Training loss: 4.5629658699035645 / Valid loss: 5.960872532072521
Training loss: 4.004026412963867 / Valid loss: 5.95900038537525
Training loss: 7.920845985412598 / Valid loss: 5.959185938608079
Training loss: 4.627005100250244 / Valid loss: 5.949772305715651

Epoch: 13
Training loss: 4.662203788757324 / Valid loss: 5.956892027173724
Training loss: 3.6807591915130615 / Valid loss: 5.956464601698376
Training loss: 8.132196426391602 / Valid loss: 5.957332215990339
Training loss: 6.524435520172119 / Valid loss: 5.9546771980467295
Training loss: 6.485379219055176 / Valid loss: 5.954784168515887

Epoch: 14
Training loss: 6.952240467071533 / Valid loss: 5.952480815705799
Training loss: 4.654551029205322 / Valid loss: 5.944109507969448
Model is saved in epoch 14, overall batch: 7000
Training loss: 6.301856994628906 / Valid loss: 5.94946767943246
Training loss: 6.396191596984863 / Valid loss: 5.951248818352109
Training loss: 5.649456024169922 / Valid loss: 5.949746343067714

Epoch: 15
Training loss: 6.868973255157471 / Valid loss: 5.942510500408354
Model is saved in epoch 15, overall batch: 7400
Training loss: 6.727950096130371 / Valid loss: 5.9390660013471335
Model is saved in epoch 15, overall batch: 7500
Training loss: 4.512300968170166 / Valid loss: 5.9415370487031485
Training loss: 5.1274189949035645 / Valid loss: 5.942951372691563
Training loss: 7.298161506652832 / Valid loss: 5.941979185740153

Epoch: 16
Training loss: 4.548820495605469 / Valid loss: 5.937155119578043
Model is saved in epoch 16, overall batch: 7900
Training loss: 7.767178535461426 / Valid loss: 5.932112478074574
Model is saved in epoch 16, overall batch: 8000
Training loss: 6.783501625061035 / Valid loss: 5.941150059018816
Training loss: 6.346513271331787 / Valid loss: 5.93187536966233
Model is saved in epoch 16, overall batch: 8200
Training loss: 6.920520305633545 / Valid loss: 5.939194615681966

Epoch: 17
Training loss: 9.807075500488281 / Valid loss: 5.938436639876593
Training loss: 6.183329105377197 / Valid loss: 5.930161501112439
Model is saved in epoch 17, overall batch: 8500
Training loss: 5.619330883026123 / Valid loss: 5.936927018846784
Training loss: 4.664616584777832 / Valid loss: 5.934784582683018
Training loss: 6.6575117111206055 / Valid loss: 5.9270184675852455
Model is saved in epoch 17, overall batch: 8800

Epoch: 18
Training loss: 5.431646347045898 / Valid loss: 5.933090993336269
Training loss: 5.746004581451416 / Valid loss: 5.926326011476062
Model is saved in epoch 18, overall batch: 9000
Training loss: 4.791284561157227 / Valid loss: 5.929976747149513
Training loss: 5.657647132873535 / Valid loss: 5.931246621268136
Training loss: 6.001861095428467 / Valid loss: 5.930930085409255

Epoch: 19
Training loss: 5.187009334564209 / Valid loss: 5.919781855174473
Model is saved in epoch 19, overall batch: 9400
Training loss: 8.426803588867188 / Valid loss: 5.923086672737485
Training loss: 4.707317352294922 / Valid loss: 5.925490395228068
Training loss: 7.573236465454102 / Valid loss: 5.9196335429237
Model is saved in epoch 19, overall batch: 9700

Epoch: 20
Training loss: 5.379003524780273 / Valid loss: 5.918080173219953
Model is saved in epoch 20, overall batch: 9800
Training loss: 6.467043399810791 / Valid loss: 5.925782221839541
Training loss: 5.670929908752441 / Valid loss: 5.922256571905954
Training loss: 6.680147647857666 / Valid loss: 5.913141100747245
Model is saved in epoch 20, overall batch: 10100
Training loss: 5.653591156005859 / Valid loss: 5.916262292861939

Epoch: 21
Training loss: 6.824953079223633 / Valid loss: 5.917975546064831
Training loss: 5.100434303283691 / Valid loss: 5.921179108392625
Training loss: 5.709525108337402 / Valid loss: 5.9194612730117075
Training loss: 6.614285945892334 / Valid loss: 5.9185850597563245
Training loss: 4.567889213562012 / Valid loss: 5.918235910506476

Epoch: 22
Training loss: 7.410634994506836 / Valid loss: 5.912731479463123
Model is saved in epoch 22, overall batch: 10800
Training loss: 6.660888671875 / Valid loss: 5.912866585595268
Training loss: 5.225458145141602 / Valid loss: 5.9123710836683
Model is saved in epoch 22, overall batch: 11000
Training loss: 5.630494594573975 / Valid loss: 5.913310702641805
Training loss: 4.970524311065674 / Valid loss: 5.906619453430176
Model is saved in epoch 22, overall batch: 11200

Epoch: 23
Training loss: 5.4521613121032715 / Valid loss: 5.911107542401268
Training loss: 7.411023139953613 / Valid loss: 5.911992004939488
Training loss: 6.478480339050293 / Valid loss: 5.909901639393397
Training loss: 5.9072771072387695 / Valid loss: 5.91108132544018
Training loss: 5.316864490509033 / Valid loss: 5.908563420886085

Epoch: 24
Training loss: 5.141655921936035 / Valid loss: 5.907531270526705
Training loss: 6.416437149047852 / Valid loss: 5.901499932152884
Model is saved in epoch 24, overall batch: 11900
Training loss: 3.7152152061462402 / Valid loss: 5.9048634619939895
Training loss: 6.899860382080078 / Valid loss: 5.9022299039931525
Training loss: 5.8034820556640625 / Valid loss: 5.899966437476022
Model is saved in epoch 24, overall batch: 12200

Epoch: 25
Training loss: 5.266563892364502 / Valid loss: 5.895134244646345
Model is saved in epoch 25, overall batch: 12300
Training loss: 4.4269914627075195 / Valid loss: 5.901698525746664
Training loss: 7.535314559936523 / Valid loss: 5.9041909422193255
Training loss: 5.754476547241211 / Valid loss: 5.9000836372375485
Training loss: 6.190211296081543 / Valid loss: 5.893832990101406
Model is saved in epoch 25, overall batch: 12700

Epoch: 26
Training loss: 4.452482223510742 / Valid loss: 5.898263847260248
Training loss: 3.8799617290496826 / Valid loss: 5.899072699319749
Training loss: 5.108888149261475 / Valid loss: 5.897274212610154
Training loss: 5.854191303253174 / Valid loss: 5.8983590807233535
Training loss: 4.095089912414551 / Valid loss: 5.897111763272966

Epoch: 27
Training loss: 5.504939079284668 / Valid loss: 5.894587475912911
Training loss: 5.8385910987854 / Valid loss: 5.894506922222319
Training loss: 5.72952938079834 / Valid loss: 5.895987769535609
Training loss: 6.37205696105957 / Valid loss: 5.891858405158633
Model is saved in epoch 27, overall batch: 13600
Training loss: 6.557824611663818 / Valid loss: 5.888319821584792
Model is saved in epoch 27, overall batch: 13700

Epoch: 28
Training loss: 5.678814888000488 / Valid loss: 5.891935230436779
Training loss: 5.3992767333984375 / Valid loss: 5.884179880505516
Model is saved in epoch 28, overall batch: 13900
Training loss: 4.834161758422852 / Valid loss: 5.890512745721
Training loss: 6.42996883392334 / Valid loss: 5.883734085446313
Model is saved in epoch 28, overall batch: 14100
Training loss: 5.222161293029785 / Valid loss: 5.888733468736921

Epoch: 29
Training loss: 6.437051773071289 / Valid loss: 5.884501332328433
Training loss: 5.96113920211792 / Valid loss: 5.881782563527425
Model is saved in epoch 29, overall batch: 14400
Training loss: 6.635345935821533 / Valid loss: 5.884788406462897
Training loss: 6.2707061767578125 / Valid loss: 5.883510909761701

Epoch: 30
Training loss: 6.383345127105713 / Valid loss: 5.879212747301374
Model is saved in epoch 30, overall batch: 14700
Training loss: 6.1892547607421875 / Valid loss: 5.884919248308454
Training loss: 5.663574695587158 / Valid loss: 5.885546298254104
Training loss: 5.231940746307373 / Valid loss: 5.884535769053868
Training loss: 6.334755897521973 / Valid loss: 5.885033291862125

Epoch: 31
Training loss: 7.065471649169922 / Valid loss: 5.882654049282983
Training loss: 6.240786552429199 / Valid loss: 5.876942582357497
Model is saved in epoch 31, overall batch: 15300
Training loss: 6.011225700378418 / Valid loss: 5.8764953045617965
Model is saved in epoch 31, overall batch: 15400
Training loss: 7.166303634643555 / Valid loss: 5.880070836203439
Training loss: 7.373757362365723 / Valid loss: 5.873163084756761
Model is saved in epoch 31, overall batch: 15600

Epoch: 32
Training loss: 4.549452304840088 / Valid loss: 5.875726774760655
Training loss: 6.353355407714844 / Valid loss: 5.877273757117135
Training loss: 6.308059215545654 / Valid loss: 5.877142018363589
Training loss: 5.710290908813477 / Valid loss: 5.874617790040515
Training loss: 3.769893169403076 / Valid loss: 5.877451676414126

Epoch: 33
Training loss: 7.0919342041015625 / Valid loss: 5.875806971958705
Training loss: 4.067174434661865 / Valid loss: 5.872469350269863
Model is saved in epoch 33, overall batch: 16300
Training loss: 6.634932994842529 / Valid loss: 5.874169567653111
Training loss: 5.213997840881348 / Valid loss: 5.866764486403692
Model is saved in epoch 33, overall batch: 16500
Training loss: 7.601323127746582 / Valid loss: 5.87489025252206

Epoch: 34
Training loss: 5.410492897033691 / Valid loss: 5.8737227190108525
Training loss: 5.329509258270264 / Valid loss: 5.872398753393264
Training loss: 4.8996782302856445 / Valid loss: 5.864129114151001
Model is saved in epoch 34, overall batch: 16900
Training loss: 5.517840385437012 / Valid loss: 5.872045719055903
Training loss: 6.510210990905762 / Valid loss: 5.862417448134649
Model is saved in epoch 34, overall batch: 17100

Epoch: 35
Training loss: 5.689896583557129 / Valid loss: 5.866246559506371
Training loss: 5.638918876647949 / Valid loss: 5.86774351029169
Training loss: 5.863049507141113 / Valid loss: 5.86992564201355
Training loss: 8.378063201904297 / Valid loss: 5.862837289628528
Training loss: 7.89011812210083 / Valid loss: 5.853699318567911
Model is saved in epoch 35, overall batch: 17600

Epoch: 36
Training loss: 5.455414772033691 / Valid loss: 5.865546939486549
Training loss: 7.780214786529541 / Valid loss: 5.853013933272589
Model is saved in epoch 36, overall batch: 17800
Training loss: 4.770391464233398 / Valid loss: 5.86084645816258
Training loss: 5.007848739624023 / Valid loss: 5.866225022361392
Training loss: 4.28102970123291 / Valid loss: 5.865143632888794

Epoch: 37
Training loss: 4.621203422546387 / Valid loss: 5.862824367341541
Training loss: 5.598779678344727 / Valid loss: 5.863843936011905
Training loss: 5.615748405456543 / Valid loss: 5.85942785626366
Training loss: 5.7797532081604 / Valid loss: 5.853658019928705
Training loss: 5.420075416564941 / Valid loss: 5.855952857789539

Epoch: 38
Training loss: 4.519730091094971 / Valid loss: 5.857780906132289
Training loss: 6.457266807556152 / Valid loss: 5.8600417999994185
Training loss: 5.965558052062988 / Valid loss: 5.856905805496942
Training loss: 5.648101806640625 / Valid loss: 5.857809034983317
Training loss: 7.202353477478027 / Valid loss: 5.856153002239409

Epoch: 39
Training loss: 4.069289207458496 / Valid loss: 5.85680144627889
Training loss: 6.685877799987793 / Valid loss: 5.857815242948986
Training loss: 7.069761276245117 / Valid loss: 5.856225018274216
Training loss: 6.624588489532471 / Valid loss: 5.858062869026547
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 17800): 5.7011634031931555
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.986988067626953 / Valid loss: 16.433027476356145
Model is saved in epoch 0, overall batch: 0
Training loss: 15.960636138916016 / Valid loss: 13.00877769561041
Model is saved in epoch 0, overall batch: 100
Training loss: 6.908411979675293 / Valid loss: 10.725407268887475
Model is saved in epoch 0, overall batch: 200
Training loss: 9.450271606445312 / Valid loss: 9.17204049428304
Model is saved in epoch 0, overall batch: 300
Training loss: 6.16190767288208 / Valid loss: 8.124396714710054
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.73141622543335 / Valid loss: 7.433199392046247
Model is saved in epoch 1, overall batch: 500
Training loss: 5.558809280395508 / Valid loss: 6.982157975151425
Model is saved in epoch 1, overall batch: 600
Training loss: 6.517823696136475 / Valid loss: 6.670602718989055
Model is saved in epoch 1, overall batch: 700
Training loss: 5.857564926147461 / Valid loss: 6.459763324828375
Model is saved in epoch 1, overall batch: 800
Training loss: 9.24475383758545 / Valid loss: 6.3109733013879685
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 5.138413906097412 / Valid loss: 6.20877765927996
Model is saved in epoch 2, overall batch: 1000
Training loss: 4.3274664878845215 / Valid loss: 6.15566817692348
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.517971992492676 / Valid loss: 6.111723874864124
Model is saved in epoch 2, overall batch: 1200
Training loss: 4.740536689758301 / Valid loss: 6.0822600183032804
Model is saved in epoch 2, overall batch: 1300
Training loss: 6.678847789764404 / Valid loss: 6.055060765856788
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 5.042048454284668 / Valid loss: 6.038383620125907
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.203835487365723 / Valid loss: 6.024833997090657
Model is saved in epoch 3, overall batch: 1600
Training loss: 5.312488079071045 / Valid loss: 6.024249903361002
Model is saved in epoch 3, overall batch: 1700
Training loss: 6.669889450073242 / Valid loss: 6.021092433021182
Model is saved in epoch 3, overall batch: 1800
Training loss: 7.5186076164245605 / Valid loss: 6.015335830052694
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 8.562156677246094 / Valid loss: 6.015074904759725
Model is saved in epoch 4, overall batch: 2000
Training loss: 6.7227301597595215 / Valid loss: 6.008290951592581
Model is saved in epoch 4, overall batch: 2100
Training loss: 7.407962322235107 / Valid loss: 6.007618066242763
Model is saved in epoch 4, overall batch: 2200
Training loss: 7.191612720489502 / Valid loss: 6.001189272744315
Model is saved in epoch 4, overall batch: 2300
Training loss: 5.161693572998047 / Valid loss: 5.998245963596163
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 4.407247543334961 / Valid loss: 5.989956939788092
Model is saved in epoch 5, overall batch: 2500
Training loss: 7.1721086502075195 / Valid loss: 6.000784903480893
Training loss: 4.792020320892334 / Valid loss: 5.999833613350278
Training loss: 6.890216827392578 / Valid loss: 5.999446273985363
Training loss: 4.259634494781494 / Valid loss: 5.991953768048968

Epoch: 6
Training loss: 4.462630271911621 / Valid loss: 5.9897545769101095
Model is saved in epoch 6, overall batch: 3000
Training loss: 6.13768196105957 / Valid loss: 5.997439704622541
Training loss: 3.912097930908203 / Valid loss: 5.991955822990054
Training loss: 5.891840934753418 / Valid loss: 5.986908392679124
Model is saved in epoch 6, overall batch: 3300
Training loss: 4.2941813468933105 / Valid loss: 5.992652314049857

Epoch: 7
Training loss: 5.018284797668457 / Valid loss: 5.9912709009079705
Training loss: 7.830852508544922 / Valid loss: 5.9887030215490435
Training loss: 3.888258218765259 / Valid loss: 5.983658779235113
Model is saved in epoch 7, overall batch: 3700
Training loss: 5.590732574462891 / Valid loss: 5.97675465402149
Model is saved in epoch 7, overall batch: 3800
Training loss: 5.879328727722168 / Valid loss: 5.984954488845099

Epoch: 8
Training loss: 6.452441692352295 / Valid loss: 5.972697289784749
Model is saved in epoch 8, overall batch: 4000
Training loss: 7.177495002746582 / Valid loss: 5.973864251091367
Training loss: 5.158292770385742 / Valid loss: 5.980817962828136
Training loss: 8.123111724853516 / Valid loss: 5.9831044900984995
Training loss: 5.883690357208252 / Valid loss: 5.976777265185402

Epoch: 9
Training loss: 7.3630828857421875 / Valid loss: 5.97589483715239
Training loss: 6.456652641296387 / Valid loss: 5.975873438517253
Training loss: 6.850916862487793 / Valid loss: 5.968223769324166
Model is saved in epoch 9, overall batch: 4700
Training loss: 5.352452754974365 / Valid loss: 5.971921464375087

Epoch: 10
Training loss: 5.529453277587891 / Valid loss: 5.973265243711926
Training loss: 6.948638916015625 / Valid loss: 5.968435489563715
Training loss: 7.881943702697754 / Valid loss: 5.964319338117327
Model is saved in epoch 10, overall batch: 5100
Training loss: 5.0806450843811035 / Valid loss: 5.966969206219628
Training loss: 5.985931873321533 / Valid loss: 5.961589881352016
Model is saved in epoch 10, overall batch: 5300

Epoch: 11
Training loss: 6.632295608520508 / Valid loss: 5.963268300465175
Training loss: 6.172391414642334 / Valid loss: 5.961290450323196
Model is saved in epoch 11, overall batch: 5500
Training loss: 8.624414443969727 / Valid loss: 5.9656808580671035
Training loss: 5.980713367462158 / Valid loss: 5.961542115892683
Training loss: 4.666717529296875 / Valid loss: 5.947413721538726
Model is saved in epoch 11, overall batch: 5800

Epoch: 12
Training loss: 7.401731967926025 / Valid loss: 5.964693646203904
Training loss: 4.562963485717773 / Valid loss: 5.960874555224464
Training loss: 4.0040202140808105 / Valid loss: 5.95900266056969
Training loss: 7.920846939086914 / Valid loss: 5.959188172930763
Training loss: 4.627009391784668 / Valid loss: 5.949774462836129

Epoch: 13
Training loss: 4.662198066711426 / Valid loss: 5.956894652048747
Training loss: 3.680737018585205 / Valid loss: 5.95646725609189
Training loss: 8.132226943969727 / Valid loss: 5.957334488914126
Training loss: 6.524435043334961 / Valid loss: 5.954679802485875
Training loss: 6.48538064956665 / Valid loss: 5.954786891028995

Epoch: 14
Training loss: 6.952253818511963 / Valid loss: 5.9524834473927815
Training loss: 4.654549598693848 / Valid loss: 5.944111976169404
Model is saved in epoch 14, overall batch: 7000
Training loss: 6.301863193511963 / Valid loss: 5.949470515478225
Training loss: 6.396210193634033 / Valid loss: 5.9512518110729395
Training loss: 5.649460792541504 / Valid loss: 5.949749308540707

Epoch: 15
Training loss: 6.868963241577148 / Valid loss: 5.942513588496617
Model is saved in epoch 15, overall batch: 7400
Training loss: 6.7279558181762695 / Valid loss: 5.9390692756289525
Model is saved in epoch 15, overall batch: 7500
Training loss: 4.512299537658691 / Valid loss: 5.941540391104562
Training loss: 5.127416133880615 / Valid loss: 5.942954351788475
Training loss: 7.298173427581787 / Valid loss: 5.941982289722987

Epoch: 16
Training loss: 4.548822402954102 / Valid loss: 5.937158198583694
Model is saved in epoch 16, overall batch: 7900
Training loss: 7.7671990394592285 / Valid loss: 5.932115566162836
Model is saved in epoch 16, overall batch: 8000
Training loss: 6.7834978103637695 / Valid loss: 5.941153197061448
Training loss: 6.346520900726318 / Valid loss: 5.931878739311582
Model is saved in epoch 16, overall batch: 8200
Training loss: 6.920533657073975 / Valid loss: 5.9391981238410585

Epoch: 17
Training loss: 9.807098388671875 / Valid loss: 5.9384399550301685
Training loss: 6.183333396911621 / Valid loss: 5.930164752687727
Model is saved in epoch 17, overall batch: 8500
Training loss: 5.619326114654541 / Valid loss: 5.936930685951596
Training loss: 4.66461181640625 / Valid loss: 5.934788336072649
Training loss: 6.657522678375244 / Valid loss: 5.927022014345441
Model is saved in epoch 17, overall batch: 8800

Epoch: 18
Training loss: 5.4316511154174805 / Valid loss: 5.933094932919457
Training loss: 5.746007919311523 / Valid loss: 5.926329901104881
Model is saved in epoch 18, overall batch: 9000
Training loss: 4.791288375854492 / Valid loss: 5.929980614071801
Training loss: 5.657663822174072 / Valid loss: 5.931250558580671
Training loss: 6.001869201660156 / Valid loss: 5.930933981850034

Epoch: 19
Training loss: 5.187013626098633 / Valid loss: 5.9197856766836985
Model is saved in epoch 19, overall batch: 9400
Training loss: 8.426836013793945 / Valid loss: 5.92309042158581
Training loss: 4.7073235511779785 / Valid loss: 5.925494312104725
Training loss: 7.573235511779785 / Valid loss: 5.919637530190604
Model is saved in epoch 19, overall batch: 9700

Epoch: 20
Training loss: 5.3790130615234375 / Valid loss: 5.918084371657598
Model is saved in epoch 20, overall batch: 9800
Training loss: 6.467031955718994 / Valid loss: 5.925786340804327
Training loss: 5.670933246612549 / Valid loss: 5.922260897500174
Training loss: 6.680155277252197 / Valid loss: 5.913145251501174
Model is saved in epoch 20, overall batch: 10100
Training loss: 5.653593063354492 / Valid loss: 5.916266791025797

Epoch: 21
Training loss: 6.824960708618164 / Valid loss: 5.9179802395048595
Training loss: 5.100437164306641 / Valid loss: 5.921184174219767
Training loss: 5.709527492523193 / Valid loss: 5.9194659346625915
Training loss: 6.614297866821289 / Valid loss: 5.9185895897093275
Training loss: 4.567877292633057 / Valid loss: 5.918240660712833

Epoch: 22
Training loss: 7.410643577575684 / Valid loss: 5.912736277353196
Model is saved in epoch 22, overall batch: 10800
Training loss: 6.660901069641113 / Valid loss: 5.912871608280001
Training loss: 5.225473403930664 / Valid loss: 5.912376156307402
Model is saved in epoch 22, overall batch: 11000
Training loss: 5.6305060386657715 / Valid loss: 5.9133157026200065
Training loss: 4.9705328941345215 / Valid loss: 5.9066245623997276
Model is saved in epoch 22, overall batch: 11200

Epoch: 23
Training loss: 5.452153205871582 / Valid loss: 5.911113016945976
Training loss: 7.4110260009765625 / Valid loss: 5.91199746131897
Training loss: 6.478497505187988 / Valid loss: 5.909907166163126
Training loss: 5.907295227050781 / Valid loss: 5.911086854480562
Training loss: 5.316874980926514 / Valid loss: 5.908568554832822

Epoch: 24
Training loss: 5.141652584075928 / Valid loss: 5.907536554336548
Training loss: 6.416460990905762 / Valid loss: 5.901505084264846
Model is saved in epoch 24, overall batch: 11900
Training loss: 3.715217351913452 / Valid loss: 5.904868902478899
Training loss: 6.899845600128174 / Valid loss: 5.90223541486831
Training loss: 5.8034820556640625 / Valid loss: 5.899972070966448
Model is saved in epoch 24, overall batch: 12200

Epoch: 25
Training loss: 5.266572952270508 / Valid loss: 5.895140066600981
Model is saved in epoch 25, overall batch: 12300
Training loss: 4.426987648010254 / Valid loss: 5.901704429444813
Training loss: 7.5353169441223145 / Valid loss: 5.904196895871844
Training loss: 5.7544732093811035 / Valid loss: 5.9000897838955835
Training loss: 6.1902265548706055 / Valid loss: 5.893838925588699
Model is saved in epoch 25, overall batch: 12700

Epoch: 26
Training loss: 4.452467441558838 / Valid loss: 5.898269837243216
Training loss: 3.8799591064453125 / Valid loss: 5.899078789211455
Training loss: 5.108890533447266 / Valid loss: 5.897280438741048
Training loss: 5.85418701171875 / Valid loss: 5.898365331831433
Training loss: 4.095085620880127 / Valid loss: 5.897117984862555

Epoch: 27
Training loss: 5.504940986633301 / Valid loss: 5.8945939904167535
Training loss: 5.838595390319824 / Valid loss: 5.894513307298933
Training loss: 5.729518890380859 / Valid loss: 5.89599419548398
Training loss: 6.372089385986328 / Valid loss: 5.891864672161284
Model is saved in epoch 27, overall batch: 13600
Training loss: 6.55781888961792 / Valid loss: 5.888326356524513
Model is saved in epoch 27, overall batch: 13700

Epoch: 28
Training loss: 5.67881965637207 / Valid loss: 5.891942201341902
Training loss: 5.399277687072754 / Valid loss: 5.884186456316994
Model is saved in epoch 28, overall batch: 13900
Training loss: 4.834169387817383 / Valid loss: 5.890519564492362
Training loss: 6.429986000061035 / Valid loss: 5.883740990502494
Model is saved in epoch 28, overall batch: 14100
Training loss: 5.222173690795898 / Valid loss: 5.888740330650693

Epoch: 29
Training loss: 6.437051773071289 / Valid loss: 5.884508450826009
Training loss: 5.961143493652344 / Valid loss: 5.881789498102098
Model is saved in epoch 29, overall batch: 14400
Training loss: 6.635360240936279 / Valid loss: 5.884795742943173
Training loss: 6.270708084106445 / Valid loss: 5.8835180804843

Epoch: 30
Training loss: 6.3833489418029785 / Valid loss: 5.8792200292859755
Model is saved in epoch 30, overall batch: 14700
Training loss: 6.189260005950928 / Valid loss: 5.884926682426816
Training loss: 5.663590908050537 / Valid loss: 5.8855536074865435
Training loss: 5.231942176818848 / Valid loss: 5.884543178195045
Training loss: 6.334766387939453 / Valid loss: 5.885040771393549

Epoch: 31
Training loss: 7.065476417541504 / Valid loss: 5.88266179221017
Training loss: 6.240802764892578 / Valid loss: 5.876950640905471
Model is saved in epoch 31, overall batch: 15300
Training loss: 6.011232376098633 / Valid loss: 5.876502918061756
Model is saved in epoch 31, overall batch: 15400
Training loss: 7.16630744934082 / Valid loss: 5.8800785428001765
Training loss: 7.373793125152588 / Valid loss: 5.873171177364531
Model is saved in epoch 31, overall batch: 15600

Epoch: 32
Training loss: 4.549472332000732 / Valid loss: 5.875734501793271
Training loss: 6.353371620178223 / Valid loss: 5.877281777064005
Training loss: 6.308060646057129 / Valid loss: 5.877150151843116
Training loss: 5.710286617279053 / Valid loss: 5.874625898542858
Training loss: 3.769878387451172 / Valid loss: 5.877459696360996

Epoch: 33
Training loss: 7.091958999633789 / Valid loss: 5.875815037318638
Training loss: 4.067168235778809 / Valid loss: 5.872477674484253
Model is saved in epoch 33, overall batch: 16300
Training loss: 6.634948253631592 / Valid loss: 5.874177928197952
Training loss: 5.214022159576416 / Valid loss: 5.866772860572452
Model is saved in epoch 33, overall batch: 16500
Training loss: 7.6013383865356445 / Valid loss: 5.874898653938657

Epoch: 34
Training loss: 5.410495758056641 / Valid loss: 5.8737309978121806
Training loss: 5.329516410827637 / Valid loss: 5.872407263801211
Training loss: 4.899704933166504 / Valid loss: 5.864137824376424
Model is saved in epoch 34, overall batch: 16900
Training loss: 5.517834663391113 / Valid loss: 5.872054604121617
Training loss: 6.510198593139648 / Valid loss: 5.862426283245995
Model is saved in epoch 34, overall batch: 17100

Epoch: 35
Training loss: 5.689901351928711 / Valid loss: 5.866255530856905
Training loss: 5.63892936706543 / Valid loss: 5.867752720060802
Training loss: 5.863035202026367 / Valid loss: 5.8699349040076845
Training loss: 8.378087997436523 / Valid loss: 5.862846308662778
Training loss: 7.890151023864746 / Valid loss: 5.853708167303176
Model is saved in epoch 35, overall batch: 17600

Epoch: 36
Training loss: 5.455428123474121 / Valid loss: 5.865556078865414
Training loss: 7.780234336853027 / Valid loss: 5.853022904623122
Model is saved in epoch 36, overall batch: 17800
Training loss: 4.770374298095703 / Valid loss: 5.860855699720837
Training loss: 5.007828712463379 / Valid loss: 5.866234118597848
Training loss: 4.2810587882995605 / Valid loss: 5.865153121948242

Epoch: 37
Training loss: 4.621212959289551 / Valid loss: 5.862833901814052
Training loss: 5.598803520202637 / Valid loss: 5.863853520438784
Training loss: 5.615764617919922 / Valid loss: 5.8594375950949535
Training loss: 5.779764175415039 / Valid loss: 5.853667722429548
Training loss: 5.420071125030518 / Valid loss: 5.855962380908784

Epoch: 38
Training loss: 4.51974630355835 / Valid loss: 5.857790622257051
Training loss: 6.457296371459961 / Valid loss: 5.860051745460147
Training loss: 5.965568542480469 / Valid loss: 5.8569154444194975
Training loss: 5.648105621337891 / Valid loss: 5.8578187987917945
Training loss: 7.2023820877075195 / Valid loss: 5.856162820543561

Epoch: 39
Training loss: 4.069302558898926 / Valid loss: 5.85681113969712
Training loss: 6.685891151428223 / Valid loss: 5.857825156620571
Training loss: 7.069770812988281 / Valid loss: 5.856235122680664
Training loss: 6.624604225158691 / Valid loss: 5.858073157355899
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 17800): 5.70117073740278
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.8549747467041 / Valid loss: 16.98556380498977
Model is saved in epoch 0, overall batch: 0
Training loss: 18.370113372802734 / Valid loss: 15.340080697195871
Model is saved in epoch 0, overall batch: 100
Training loss: 13.185138702392578 / Valid loss: 13.80259150550479
Model is saved in epoch 0, overall batch: 200
Training loss: 12.001442909240723 / Valid loss: 12.704903652554467
Model is saved in epoch 0, overall batch: 300
Training loss: 13.384450912475586 / Valid loss: 11.910566802251907
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 10.165295600891113 / Valid loss: 11.237153026035854
Model is saved in epoch 1, overall batch: 500
Training loss: 11.679250717163086 / Valid loss: 10.280351697830927
Model is saved in epoch 1, overall batch: 600
Training loss: 7.903110504150391 / Valid loss: 9.590142649695986
Model is saved in epoch 1, overall batch: 700
Training loss: 6.882466793060303 / Valid loss: 9.19304183324178
Model is saved in epoch 1, overall batch: 800
Training loss: 10.106756210327148 / Valid loss: 8.729997948237829
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.134425163269043 / Valid loss: 8.340457053411575
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.792914390563965 / Valid loss: 8.014093707856677
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.778932094573975 / Valid loss: 7.60834302448091
Model is saved in epoch 2, overall batch: 1200
Training loss: 2.9477624893188477 / Valid loss: 7.303108578636532
Model is saved in epoch 2, overall batch: 1300
Training loss: 3.168562889099121 / Valid loss: 7.109654887517293
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 3.173264503479004 / Valid loss: 7.158758524485997
Training loss: 2.6434950828552246 / Valid loss: 7.101827730451311
Model is saved in epoch 3, overall batch: 1600
Training loss: 1.8282471895217896 / Valid loss: 6.836594904036749
Model is saved in epoch 3, overall batch: 1700
Training loss: 1.5270745754241943 / Valid loss: 6.875733757019043
Training loss: 2.0063443183898926 / Valid loss: 6.688943245297387
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 1.8433536291122437 / Valid loss: 6.61872470946539
Model is saved in epoch 4, overall batch: 2000
Training loss: 1.1391552686691284 / Valid loss: 6.626638294401623
Training loss: 1.1635023355484009 / Valid loss: 6.676707088379633
Training loss: 1.1196995973587036 / Valid loss: 6.58306854338873
Model is saved in epoch 4, overall batch: 2300
Training loss: 1.3368487358093262 / Valid loss: 6.648300098237537

Epoch: 5
Training loss: 0.4252380132675171 / Valid loss: 6.516572216578893
Model is saved in epoch 5, overall batch: 2500
Training loss: 0.7159550189971924 / Valid loss: 6.533344309670585
Training loss: 1.4703669548034668 / Valid loss: 6.526197878519694
Training loss: 1.6784827709197998 / Valid loss: 6.530790017900013
Training loss: 0.6802353858947754 / Valid loss: 6.550676214127313

Epoch: 6
Training loss: 0.6197327375411987 / Valid loss: 6.477980173201788
Model is saved in epoch 6, overall batch: 3000
Training loss: 0.8964049220085144 / Valid loss: 6.536249680746169
Training loss: 0.35372763872146606 / Valid loss: 6.5600087915148055
Training loss: 0.7946569919586182 / Valid loss: 6.563232998620896
Training loss: 0.34663209319114685 / Valid loss: 6.5458472728729244

Epoch: 7
Training loss: 0.8550946712493896 / Valid loss: 6.525083925610497
Training loss: 0.7453943490982056 / Valid loss: 6.527510615757533
Training loss: 0.39168545603752136 / Valid loss: 6.571585114796957
Training loss: 0.4267999231815338 / Valid loss: 6.549433226812454
Training loss: 1.1288659572601318 / Valid loss: 6.523978919074649

Epoch: 8
Training loss: 0.46070432662963867 / Valid loss: 6.510628820600964
Training loss: 0.6823257207870483 / Valid loss: 6.504397810073126
Training loss: 0.6307380199432373 / Valid loss: 6.569669541858492
Training loss: 0.5396203398704529 / Valid loss: 6.546485167457944
Training loss: 0.35352426767349243 / Valid loss: 6.531038279760452

Epoch: 9
Training loss: 0.2726200819015503 / Valid loss: 6.567454905737014
Training loss: 1.0483996868133545 / Valid loss: 6.507191821507045
Training loss: 0.6121673583984375 / Valid loss: 6.552616151173909
Training loss: 0.2362561821937561 / Valid loss: 6.535988349006289

Epoch: 10
Training loss: 0.4214290976524353 / Valid loss: 6.539107540675572
Training loss: 0.41955068707466125 / Valid loss: 6.557010137467158
Training loss: 0.2752546966075897 / Valid loss: 6.536228724888393
Training loss: 0.4526573419570923 / Valid loss: 6.510861891791934
Training loss: 0.2695642411708832 / Valid loss: 6.56998081661406

Epoch: 11
Training loss: 0.3753533959388733 / Valid loss: 6.52697590192159
Training loss: 0.3025309443473816 / Valid loss: 6.547321324121384
Training loss: 0.24937397241592407 / Valid loss: 6.542750494820731
Training loss: 0.5038042664527893 / Valid loss: 6.522921453203474
Training loss: 0.3936828076839447 / Valid loss: 6.612915504546392

Epoch: 12
Training loss: 0.20578885078430176 / Valid loss: 6.538573564801897
Training loss: 0.24773162603378296 / Valid loss: 6.5285976750510075
Training loss: 0.2547236979007721 / Valid loss: 6.569824454897926
Training loss: 0.2599613666534424 / Valid loss: 6.491025039127895
Training loss: 0.38906633853912354 / Valid loss: 6.532849218731835

Epoch: 13
Training loss: 0.3045133948326111 / Valid loss: 6.516332699003674
Training loss: 0.22812509536743164 / Valid loss: 6.492844813210624
Training loss: 0.27403944730758667 / Valid loss: 6.541236314319429
Training loss: 0.32638150453567505 / Valid loss: 6.546971248445057
Training loss: 0.714805006980896 / Valid loss: 6.560262137367612

Epoch: 14
Training loss: 0.29038071632385254 / Valid loss: 6.564548524220784
Training loss: 0.4656299948692322 / Valid loss: 6.5020357131958
Training loss: 0.2993861436843872 / Valid loss: 6.530068066006615
Training loss: 0.29140472412109375 / Valid loss: 6.512060106368292
Training loss: 0.2461581826210022 / Valid loss: 6.561662612642561

Epoch: 15
Training loss: 0.22266429662704468 / Valid loss: 6.555382846650623
Training loss: 0.28422874212265015 / Valid loss: 6.514706879570371
Training loss: 0.32838380336761475 / Valid loss: 6.515558401743571
Training loss: 0.1878272145986557 / Valid loss: 6.504833616529193
Training loss: 0.256658136844635 / Valid loss: 6.503288761774699

Epoch: 16
Training loss: 0.18026408553123474 / Valid loss: 6.536466816493443
Training loss: 0.6491253972053528 / Valid loss: 6.538299905686151
Training loss: 0.31405216455459595 / Valid loss: 6.488456485384987
Training loss: 0.2587153911590576 / Valid loss: 6.517049766722179
Training loss: 0.3882025480270386 / Valid loss: 6.543700388499668

Epoch: 17
Training loss: 0.4183448553085327 / Valid loss: 6.520161290395827
Training loss: 0.16195863485336304 / Valid loss: 6.508191571916853
Training loss: 0.3081105649471283 / Valid loss: 6.512487420581636
Training loss: 0.2299260050058365 / Valid loss: 6.535898063296363
Training loss: 0.16415627300739288 / Valid loss: 6.4740097727094374
Model is saved in epoch 17, overall batch: 8800

Epoch: 18
Training loss: 0.22157783806324005 / Valid loss: 6.450747042610532
Model is saved in epoch 18, overall batch: 8900
Training loss: 0.4199223816394806 / Valid loss: 6.4749487717946375
Training loss: 0.22095832228660583 / Valid loss: 6.479025988351731
Training loss: 0.1961851418018341 / Valid loss: 6.504963952019101
Training loss: 0.13684430718421936 / Valid loss: 6.519014655976068

Epoch: 19
Training loss: 0.24346724152565002 / Valid loss: 6.50847780136835
Training loss: 0.1603989601135254 / Valid loss: 6.505388146355038
Training loss: 0.6471158266067505 / Valid loss: 6.500861113412039
Training loss: 0.23537002503871918 / Valid loss: 6.4955864361354285

Epoch: 20
Training loss: 0.16500991582870483 / Valid loss: 6.518994730994815
Training loss: 0.462313175201416 / Valid loss: 6.505985537029448
Training loss: 0.27497032284736633 / Valid loss: 6.5317993799845375
Training loss: 0.41870540380477905 / Valid loss: 6.525396256219773
Training loss: 0.16613250970840454 / Valid loss: 6.477399621691022

Epoch: 21
Training loss: 0.7254958748817444 / Valid loss: 6.462769585564023
Training loss: 0.3735070824623108 / Valid loss: 6.507798458281018
Training loss: 0.18391135334968567 / Valid loss: 6.484205750056676
Training loss: 0.1630585491657257 / Valid loss: 6.476019832066127
Training loss: 0.21875840425491333 / Valid loss: 6.4791796616145545

Epoch: 22
Training loss: 0.1989385485649109 / Valid loss: 6.452060077303932
Training loss: 0.5824438333511353 / Valid loss: 6.530228614807129
Training loss: 0.5913780927658081 / Valid loss: 6.447432636079334
Model is saved in epoch 22, overall batch: 11000
Training loss: 0.1601232886314392 / Valid loss: 6.478959914616176
Training loss: 0.3509606719017029 / Valid loss: 6.495483602796282

Epoch: 23
Training loss: 0.6464347839355469 / Valid loss: 6.4941468420482815
Training loss: 0.20340296626091003 / Valid loss: 6.494972531000773
Training loss: 0.29250192642211914 / Valid loss: 6.533006472814651
Training loss: 0.1499750316143036 / Valid loss: 6.484795677094232
Training loss: 0.16667941212654114 / Valid loss: 6.493798732757568

Epoch: 24
Training loss: 0.1410674899816513 / Valid loss: 6.495054937544323
Training loss: 0.24303536117076874 / Valid loss: 6.5056122870672315
Training loss: 0.2489725947380066 / Valid loss: 6.487123260043917
Training loss: 0.1383134424686432 / Valid loss: 6.472282886505127
Training loss: 0.575411319732666 / Valid loss: 6.489405786423456

Epoch: 25
Training loss: 0.09836581349372864 / Valid loss: 6.476819873991467
Training loss: 0.1880277395248413 / Valid loss: 6.467638894489833
Training loss: 0.17321884632110596 / Valid loss: 6.5037042436145605
Training loss: 0.14805446565151215 / Valid loss: 6.5021975721631735
Training loss: 0.3762005865573883 / Valid loss: 6.5044565064566475

Epoch: 26
Training loss: 0.10876487195491791 / Valid loss: 6.5190412657601495
Training loss: 0.6067630052566528 / Valid loss: 6.477501530874343
Training loss: 0.16401365399360657 / Valid loss: 6.4758728504180905
Training loss: 0.147501140832901 / Valid loss: 6.465357569285802
Training loss: 0.318827748298645 / Valid loss: 6.493346925008865

Epoch: 27
Training loss: 0.1280927211046219 / Valid loss: 6.478040259225028
Training loss: 0.30351465940475464 / Valid loss: 6.513642815181187
Training loss: 0.41839706897735596 / Valid loss: 6.453796897615705
Training loss: 0.12894724309444427 / Valid loss: 6.4824866181328185
Training loss: 0.1717366874217987 / Valid loss: 6.5008357184273855

Epoch: 28
Training loss: 0.20611967146396637 / Valid loss: 6.475346653802054
Training loss: 0.33824294805526733 / Valid loss: 6.472626490820022
Training loss: 0.2120530903339386 / Valid loss: 6.483093520573207
Training loss: 0.18629056215286255 / Valid loss: 6.493287981124151
Training loss: 0.1206226795911789 / Valid loss: 6.5012575694492885

Epoch: 29
Training loss: 0.1448749303817749 / Valid loss: 6.481495612008231
Training loss: 0.15795111656188965 / Valid loss: 6.466691852751232
Training loss: 0.218043252825737 / Valid loss: 6.479352810269311
Training loss: 0.37101539969444275 / Valid loss: 6.462135605585008

Epoch: 30
Training loss: 0.15895915031433105 / Valid loss: 6.481715429396856
Training loss: 0.791946530342102 / Valid loss: 6.49113994098845
Training loss: 0.3178529739379883 / Valid loss: 6.457227965763637
Training loss: 0.3138923645019531 / Valid loss: 6.448910710925148
Training loss: 0.5159896612167358 / Valid loss: 6.467713717051915

Epoch: 31
Training loss: 0.15352864563465118 / Valid loss: 6.471227164495559
Training loss: 0.09960923343896866 / Valid loss: 6.466575799669538
Training loss: 0.25172117352485657 / Valid loss: 6.457184705280122
Training loss: 0.1680365949869156 / Valid loss: 6.44765000570388
Training loss: 0.15826363861560822 / Valid loss: 6.468280524299258

Epoch: 32
Training loss: 0.1493188440799713 / Valid loss: 6.44580857640221
Model is saved in epoch 32, overall batch: 15700
Training loss: 0.2801123857498169 / Valid loss: 6.46331345240275
Training loss: 0.1470702886581421 / Valid loss: 6.494486279714675
Training loss: 0.21457889676094055 / Valid loss: 6.494426631927491
Training loss: 0.13543006777763367 / Valid loss: 6.456940501076835

Epoch: 33
Training loss: 0.22468173503875732 / Valid loss: 6.468519033704485
Training loss: 0.18491041660308838 / Valid loss: 6.465248562040783
Training loss: 0.15967857837677002 / Valid loss: 6.457359613691057
Training loss: 0.11748937517404556 / Valid loss: 6.465940107618059
Training loss: 0.11631427705287933 / Valid loss: 6.486858445122128

Epoch: 34
Training loss: 0.18198204040527344 / Valid loss: 6.472251764933268
Training loss: 0.12389377504587173 / Valid loss: 6.467126451219831
Training loss: 0.16183750331401825 / Valid loss: 6.464357069560459
Training loss: 0.19244161248207092 / Valid loss: 6.515034525735038
Training loss: 0.5891260504722595 / Valid loss: 6.488967214311872

Epoch: 35
Training loss: 0.20429065823554993 / Valid loss: 6.44347299848284
Model is saved in epoch 35, overall batch: 17200
Training loss: 0.08835754543542862 / Valid loss: 6.468864867800758
Training loss: 0.11206720769405365 / Valid loss: 6.464025229499454
Training loss: 0.3783785104751587 / Valid loss: 6.459338288080125
Training loss: 0.1697022020816803 / Valid loss: 6.480156321752639

Epoch: 36
Training loss: 0.23789264261722565 / Valid loss: 6.480351211911156
Training loss: 0.2948715388774872 / Valid loss: 6.4834325336274645
Training loss: 0.5494040250778198 / Valid loss: 6.4501764229365754
Training loss: 0.43937504291534424 / Valid loss: 6.467038738159906
Training loss: 0.1416187584400177 / Valid loss: 6.463662088484991

Epoch: 37
Training loss: 0.12032616138458252 / Valid loss: 6.448568257831392
Training loss: 0.19505345821380615 / Valid loss: 6.460305590856643
Training loss: 0.2802160978317261 / Valid loss: 6.494084505807786
Training loss: 0.6136621236801147 / Valid loss: 6.474836151940482
Training loss: 0.52783203125 / Valid loss: 6.4520733878726055

Epoch: 38
Training loss: 0.20711086690425873 / Valid loss: 6.473358174732753
Training loss: 0.11879950761795044 / Valid loss: 6.458061515717279
Training loss: 0.3179648518562317 / Valid loss: 6.488639286586216
Training loss: 0.2329045981168747 / Valid loss: 6.480762654259092
Training loss: 0.4510064721107483 / Valid loss: 6.485677369435629

Epoch: 39
Training loss: 0.11106035113334656 / Valid loss: 6.468939781188965
Training loss: 0.223942831158638 / Valid loss: 6.48672962415786
Training loss: 0.1438540816307068 / Valid loss: 6.4853340375991095
Training loss: 0.18217574059963226 / Valid loss: 6.476154213859921
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 17200): 6.2821356909615655
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)

Epoch: 0
Training loss: 17.8549747467041 / Valid loss: 16.98556380498977
Model is saved in epoch 0, overall batch: 0
Training loss: 18.370115280151367 / Valid loss: 15.340080669948033
Model is saved in epoch 0, overall batch: 100
Training loss: 13.185052871704102 / Valid loss: 13.80254607427688
Model is saved in epoch 0, overall batch: 200
Training loss: 12.000785827636719 / Valid loss: 12.704061707996187
Model is saved in epoch 0, overall batch: 300
Training loss: 13.38081169128418 / Valid loss: 11.913240042186919
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 10.150217056274414 / Valid loss: 11.231497514815558
Model is saved in epoch 1, overall batch: 500
Training loss: 11.697408676147461 / Valid loss: 10.273166978926886
Model is saved in epoch 1, overall batch: 600
Training loss: 7.884609222412109 / Valid loss: 9.59205414908273
Model is saved in epoch 1, overall batch: 700
Training loss: 6.872313499450684 / Valid loss: 9.182303519476028
Model is saved in epoch 1, overall batch: 800
Training loss: 10.09239673614502 / Valid loss: 8.733348996298654
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 6.140331745147705 / Valid loss: 8.327122583843412
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.778614044189453 / Valid loss: 8.019690313793364
Model is saved in epoch 2, overall batch: 1100
Training loss: 5.735694885253906 / Valid loss: 7.598985631125314
Model is saved in epoch 2, overall batch: 1200
Training loss: 2.9793009757995605 / Valid loss: 7.306707929429554
Model is saved in epoch 2, overall batch: 1300
Training loss: 3.1684999465942383 / Valid loss: 7.103267594746181
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 3.0726144313812256 / Valid loss: 7.153033558527628
Training loss: 2.6544017791748047 / Valid loss: 7.1037711347852435
Training loss: 1.7760283946990967 / Valid loss: 6.8344192141578315
Model is saved in epoch 3, overall batch: 1700
Training loss: 1.4807682037353516 / Valid loss: 6.883834439232236
Training loss: 1.8556549549102783 / Valid loss: 6.687867700485956
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 1.9000153541564941 / Valid loss: 6.624194158826556
Model is saved in epoch 4, overall batch: 2000
Training loss: 1.1358006000518799 / Valid loss: 6.641524855295817
Training loss: 1.1761114597320557 / Valid loss: 6.693023422786168
Training loss: 1.1401073932647705 / Valid loss: 6.592628914969308
Model is saved in epoch 4, overall batch: 2300
Training loss: 1.349792718887329 / Valid loss: 6.623620392027355

Epoch: 5
Training loss: 0.4424031674861908 / Valid loss: 6.526985102608091
Model is saved in epoch 5, overall batch: 2500
Training loss: 0.7171494960784912 / Valid loss: 6.540999909809657
Training loss: 1.3667430877685547 / Valid loss: 6.541330519176665
Training loss: 1.7502527236938477 / Valid loss: 6.538329524085635
Training loss: 0.6402692794799805 / Valid loss: 6.559848042896816

Epoch: 6
Training loss: 0.6210000514984131 / Valid loss: 6.492893100920178
Model is saved in epoch 6, overall batch: 3000
Training loss: 0.8473236560821533 / Valid loss: 6.551750178564163
Training loss: 0.3898719251155853 / Valid loss: 6.568520330247425
Training loss: 0.6956138610839844 / Valid loss: 6.571507294972737
Training loss: 0.3748459815979004 / Valid loss: 6.551599221002488

Epoch: 7
Training loss: 0.7900097370147705 / Valid loss: 6.536466855094546
Training loss: 0.761581301689148 / Valid loss: 6.530427814665295
Training loss: 0.38346147537231445 / Valid loss: 6.562955561138335
Training loss: 0.40169161558151245 / Valid loss: 6.54427703221639
Training loss: 1.1244043111801147 / Valid loss: 6.522067932855515

Epoch: 8
Training loss: 0.430599570274353 / Valid loss: 6.5223234857831685
Training loss: 0.6695247888565063 / Valid loss: 6.505115104856945
Training loss: 0.5469335913658142 / Valid loss: 6.582829772858393
Training loss: 0.49713629484176636 / Valid loss: 6.559176862807501
Training loss: 0.32450777292251587 / Valid loss: 6.548849407831828

Epoch: 9
Training loss: 0.26076018810272217 / Valid loss: 6.560999293554397
Training loss: 1.0442626476287842 / Valid loss: 6.516612080165318
Training loss: 0.621944010257721 / Valid loss: 6.538671271006266
Training loss: 0.25763237476348877 / Valid loss: 6.519968425659906

Epoch: 10
Training loss: 0.419598251581192 / Valid loss: 6.545946988605317
Training loss: 0.4052727520465851 / Valid loss: 6.575460093361991
Training loss: 0.30595964193344116 / Valid loss: 6.542080804279872
Training loss: 0.5175817012786865 / Valid loss: 6.522386105855306
Training loss: 0.2794637084007263 / Valid loss: 6.569820372263591

Epoch: 11
Training loss: 0.37961697578430176 / Valid loss: 6.522216379074823
Training loss: 0.3210148811340332 / Valid loss: 6.538699676876976
Training loss: 0.2616908550262451 / Valid loss: 6.534801192510695
Training loss: 0.49625253677368164 / Valid loss: 6.5185445785522464
Training loss: 0.36541712284088135 / Valid loss: 6.599120421636672

Epoch: 12
Training loss: 0.1998438686132431 / Valid loss: 6.5296096438453315
Training loss: 0.2292940318584442 / Valid loss: 6.510459854489281
Training loss: 0.27345770597457886 / Valid loss: 6.567751416705904
Training loss: 0.30315300822257996 / Valid loss: 6.48005648567563
Model is saved in epoch 12, overall batch: 6200
Training loss: 0.3950767517089844 / Valid loss: 6.516451034091768

Epoch: 13
Training loss: 0.3221328854560852 / Valid loss: 6.501187644686017
Training loss: 0.2701305150985718 / Valid loss: 6.480532746087937
Training loss: 0.34685441851615906 / Valid loss: 6.533676329113188
Training loss: 0.32575735449790955 / Valid loss: 6.539380386897496
Training loss: 0.6795332431793213 / Valid loss: 6.549666695367723

Epoch: 14
Training loss: 0.32077550888061523 / Valid loss: 6.55283937000093
Training loss: 0.4722093641757965 / Valid loss: 6.492141864413306
Training loss: 0.3264869451522827 / Valid loss: 6.526413190932501
Training loss: 0.30258023738861084 / Valid loss: 6.516156727927072
Training loss: 0.22233173251152039 / Valid loss: 6.554229713621593

Epoch: 15
Training loss: 0.20214176177978516 / Valid loss: 6.549761642728533
Training loss: 0.28016388416290283 / Valid loss: 6.511290790921166
Training loss: 0.3203718364238739 / Valid loss: 6.513671166556222
Training loss: 0.17446908354759216 / Valid loss: 6.504365650812785
Training loss: 0.2559359669685364 / Valid loss: 6.4986741293044314

Epoch: 16
Training loss: 0.19086790084838867 / Valid loss: 6.539259574526832
Training loss: 0.6450624465942383 / Valid loss: 6.543641024544126
Training loss: 0.29727691411972046 / Valid loss: 6.480536070324126
Training loss: 0.2776089608669281 / Valid loss: 6.508881092071533
Training loss: 0.3997114300727844 / Valid loss: 6.536100201379686

Epoch: 17
Training loss: 0.4229925870895386 / Valid loss: 6.517830698830741
Training loss: 0.15272656083106995 / Valid loss: 6.516994131179083
Training loss: 0.3080134987831116 / Valid loss: 6.498475415366037
Training loss: 0.24151922762393951 / Valid loss: 6.54213995479402
Training loss: 0.19098009169101715 / Valid loss: 6.468824604579381
Model is saved in epoch 17, overall batch: 8800

Epoch: 18
Training loss: 0.22642424702644348 / Valid loss: 6.451470736094883
Model is saved in epoch 18, overall batch: 8900
Training loss: 0.3951858878135681 / Valid loss: 6.469610168820336
Training loss: 0.24369947612285614 / Valid loss: 6.479269104912167
Training loss: 0.19385650753974915 / Valid loss: 6.498509204955328
Training loss: 0.11355467140674591 / Valid loss: 6.518242200215657

Epoch: 19
Training loss: 0.22656214237213135 / Valid loss: 6.511174955822173
Training loss: 0.14884355664253235 / Valid loss: 6.4963198752630325
Training loss: 0.5907830595970154 / Valid loss: 6.503921467917306
Training loss: 0.22455793619155884 / Valid loss: 6.486333115895589

Epoch: 20
Training loss: 0.14042505621910095 / Valid loss: 6.514314933050247
Training loss: 0.47714439034461975 / Valid loss: 6.49943912142799
Training loss: 0.2690902352333069 / Valid loss: 6.524977436519804
Training loss: 0.4418184161186218 / Valid loss: 6.519772845222836
Training loss: 0.16328994929790497 / Valid loss: 6.473299871172224

Epoch: 21
Training loss: 0.6704257726669312 / Valid loss: 6.464974328449794
Training loss: 0.38324740529060364 / Valid loss: 6.509836115155902
Training loss: 0.15466584265232086 / Valid loss: 6.4894141151791525
Training loss: 0.1425674855709076 / Valid loss: 6.4759142784845265
Training loss: 0.20122432708740234 / Valid loss: 6.489606462206159

Epoch: 22
Training loss: 0.20072346925735474 / Valid loss: 6.46042457308088
Training loss: 0.5666954517364502 / Valid loss: 6.53824105035691
Training loss: 0.5928328633308411 / Valid loss: 6.448377963474819
Model is saved in epoch 22, overall batch: 11000
Training loss: 0.18585330247879028 / Valid loss: 6.4821158749716625
Training loss: 0.37168270349502563 / Valid loss: 6.49116207304455

Epoch: 23
Training loss: 0.6416181325912476 / Valid loss: 6.5011917568388435
Training loss: 0.209647074341774 / Valid loss: 6.49762043271746
Training loss: 0.2782609760761261 / Valid loss: 6.532602605365572
Training loss: 0.1347702592611313 / Valid loss: 6.491534167244321
Training loss: 0.1857002079486847 / Valid loss: 6.495049049740746

Epoch: 24
Training loss: 0.1450522392988205 / Valid loss: 6.491122545514788
Training loss: 0.2549489438533783 / Valid loss: 6.4985882168724425
Training loss: 0.24113252758979797 / Valid loss: 6.479571851094564
Training loss: 0.14434178173542023 / Valid loss: 6.4749782834734235
Training loss: 0.5852073431015015 / Valid loss: 6.491516026996431

Epoch: 25
Training loss: 0.08806274831295013 / Valid loss: 6.479587541307722
Training loss: 0.19140684604644775 / Valid loss: 6.474652281261625
Training loss: 0.16097494959831238 / Valid loss: 6.508217564083281
Training loss: 0.1730390042066574 / Valid loss: 6.4996919859023325
Training loss: 0.38218000531196594 / Valid loss: 6.504853334881011

Epoch: 26
Training loss: 0.1108122244477272 / Valid loss: 6.5227748439425515
Training loss: 0.5941884517669678 / Valid loss: 6.47980347815014
Training loss: 0.17057418823242188 / Valid loss: 6.47707873071943
Training loss: 0.14620257914066315 / Valid loss: 6.4704187393188475
Training loss: 0.29519325494766235 / Valid loss: 6.4992775576455255

Epoch: 27
Training loss: 0.14450016617774963 / Valid loss: 6.489098153795514
Training loss: 0.30773523449897766 / Valid loss: 6.522254140036447
Training loss: 0.4300895929336548 / Valid loss: 6.460992997033255
Training loss: 0.1513005495071411 / Valid loss: 6.495846721104213
Training loss: 0.15966163575649261 / Valid loss: 6.514820164725894

Epoch: 28
Training loss: 0.22219537198543549 / Valid loss: 6.476784133911133
Training loss: 0.276123046875 / Valid loss: 6.477217052096412
Training loss: 0.21294981241226196 / Valid loss: 6.492715767451695
Training loss: 0.22105741500854492 / Valid loss: 6.495124844142369
Training loss: 0.12487806379795074 / Valid loss: 6.504688807896206

Epoch: 29
Training loss: 0.16337159276008606 / Valid loss: 6.486474661600022
Training loss: 0.14726606011390686 / Valid loss: 6.476156187057495
Training loss: 0.22255024313926697 / Valid loss: 6.4905804316202795
Training loss: 0.39444005489349365 / Valid loss: 6.472585823422387

Epoch: 30
Training loss: 0.16824722290039062 / Valid loss: 6.49964109148298
Training loss: 0.832738995552063 / Valid loss: 6.505062884376162
Training loss: 0.3284529447555542 / Valid loss: 6.476691468556722
Training loss: 0.3344348669052124 / Valid loss: 6.470411473228818
Training loss: 0.5076229572296143 / Valid loss: 6.472340502057757

Epoch: 31
Training loss: 0.15128278732299805 / Valid loss: 6.490937142145066
Training loss: 0.10972711443901062 / Valid loss: 6.473081656864712
Training loss: 0.23027589917182922 / Valid loss: 6.465577752249581
Training loss: 0.17973938584327698 / Valid loss: 6.453521265302386
Training loss: 0.1665470153093338 / Valid loss: 6.477246722720918

Epoch: 32
Training loss: 0.19683894515037537 / Valid loss: 6.4547630786895756
Training loss: 0.3557014763355255 / Valid loss: 6.473780777340844
Training loss: 0.15107187628746033 / Valid loss: 6.505000852403187
Training loss: 0.24072007834911346 / Valid loss: 6.4966414951142815
Training loss: 0.17466753721237183 / Valid loss: 6.464361111323039

Epoch: 33
Training loss: 0.2004203051328659 / Valid loss: 6.493200601850237
Training loss: 0.1786346584558487 / Valid loss: 6.483630057743618
Training loss: 0.18893155455589294 / Valid loss: 6.466380455380395
Training loss: 0.11262771487236023 / Valid loss: 6.470817525046212
Training loss: 0.11494296044111252 / Valid loss: 6.500482779457456

Epoch: 34
Training loss: 0.1726539433002472 / Valid loss: 6.485545371827625
Training loss: 0.11415604501962662 / Valid loss: 6.482477562768119
Training loss: 0.1527385413646698 / Valid loss: 6.474804430916196
Training loss: 0.1721389889717102 / Valid loss: 6.523216043199811
Training loss: 0.6277830600738525 / Valid loss: 6.500605544589814

Epoch: 35
Training loss: 0.17010898888111115 / Valid loss: 6.455950646173386
Training loss: 0.07746388018131256 / Valid loss: 6.478216060002645
Training loss: 0.10966988652944565 / Valid loss: 6.4767315978095645
Training loss: 0.40863388776779175 / Valid loss: 6.4780453568413146
Training loss: 0.18721233308315277 / Valid loss: 6.48694814046224

Epoch: 36
Training loss: 0.2343296855688095 / Valid loss: 6.4896304857163205
Training loss: 0.320779025554657 / Valid loss: 6.485358805883498
Training loss: 0.5634625554084778 / Valid loss: 6.469052035467965
Training loss: 0.4584628939628601 / Valid loss: 6.473874723343622
Training loss: 0.1370188593864441 / Valid loss: 6.475678012484596

Epoch: 37
Training loss: 0.1031920313835144 / Valid loss: 6.460207871028355
Training loss: 0.1651936173439026 / Valid loss: 6.472078059968494
Training loss: 0.2780081331729889 / Valid loss: 6.505313546316964
Training loss: 0.6232282519340515 / Valid loss: 6.490709802082606
Training loss: 0.5240010619163513 / Valid loss: 6.469202775046939

Epoch: 38
Training loss: 0.20703858137130737 / Valid loss: 6.486783795129686
Training loss: 0.10999324917793274 / Valid loss: 6.46540227390471
Training loss: 0.34448719024658203 / Valid loss: 6.489470806575957
Training loss: 0.24956025183200836 / Valid loss: 6.498663745607648
Training loss: 0.464577317237854 / Valid loss: 6.4881670270647325

Epoch: 39
Training loss: 0.1020469218492508 / Valid loss: 6.475611305236816
Training loss: 0.2210303395986557 / Valid loss: 6.498993010748
Training loss: 0.14726832509040833 / Valid loss: 6.491069564365206
Training loss: 0.18326155841350555 / Valid loss: 6.481478981744676
ModuleList(
  (0): Linear(in_features=31191, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 11000): 6.303996703738258
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.313697814941406 / Valid loss: 15.750787707737514
Model is saved in epoch 0, overall batch: 0
Training loss: 13.752546310424805 / Valid loss: 14.405622600373768
Model is saved in epoch 0, overall batch: 100
Training loss: 11.14582633972168 / Valid loss: 11.973171179635184
Model is saved in epoch 0, overall batch: 200
Training loss: 5.633367538452148 / Valid loss: 10.573150157928467
Model is saved in epoch 0, overall batch: 300
Training loss: 6.242633819580078 / Valid loss: 9.49334682737078
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.383338451385498 / Valid loss: 8.690422039940243
Model is saved in epoch 1, overall batch: 500
Training loss: 6.363702774047852 / Valid loss: 8.08421214194525
Model is saved in epoch 1, overall batch: 600
Training loss: 7.9695892333984375 / Valid loss: 7.488432264328003
Model is saved in epoch 1, overall batch: 700
Training loss: 5.717494010925293 / Valid loss: 7.145680970237368
Model is saved in epoch 1, overall batch: 800
Training loss: 6.082059860229492 / Valid loss: 6.819272599901471
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 4.602254390716553 / Valid loss: 6.561888926369804
Model is saved in epoch 2, overall batch: 1000
Training loss: 2.746220588684082 / Valid loss: 6.455680561065674
Model is saved in epoch 2, overall batch: 1100
Training loss: 3.0845818519592285 / Valid loss: 6.213856106712704
Model is saved in epoch 2, overall batch: 1200
Training loss: 2.7944390773773193 / Valid loss: 6.18115431467692
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.1701812744140625 / Valid loss: 6.10920862924485
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 1.7566957473754883 / Valid loss: 6.012586098625547
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.1448163986206055 / Valid loss: 6.066099207741874
Training loss: 3.447523355484009 / Valid loss: 5.997189092636108
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.219184398651123 / Valid loss: 6.0307287579491025
Training loss: 3.5760350227355957 / Valid loss: 6.013129942757743

Epoch: 4
Training loss: 2.228712558746338 / Valid loss: 6.015640538079398
Training loss: 2.5953149795532227 / Valid loss: 6.0493231296539305
Training loss: 1.2539929151535034 / Valid loss: 6.092184168951852
Training loss: 1.6321380138397217 / Valid loss: 6.095207105364119
Training loss: 2.3774499893188477 / Valid loss: 6.159315583819435

Epoch: 5
Training loss: 1.2093205451965332 / Valid loss: 6.172594769795736
Training loss: 1.4256410598754883 / Valid loss: 6.2494941507067
Training loss: 1.5833646059036255 / Valid loss: 6.17278261638823
Training loss: 1.7834670543670654 / Valid loss: 6.241609773181733
Training loss: 1.4026157855987549 / Valid loss: 6.309804178419567

Epoch: 6
Training loss: 0.8246713876724243 / Valid loss: 6.314627179645357
Training loss: 1.2732572555541992 / Valid loss: 6.295249775477818
Training loss: 1.0804553031921387 / Valid loss: 6.411928447087606
Training loss: 1.018450140953064 / Valid loss: 6.379608762831915
Training loss: 0.9891133308410645 / Valid loss: 6.285004522686913

Epoch: 7
Training loss: 0.6184753179550171 / Valid loss: 6.3062852723257885
Training loss: 0.7885041236877441 / Valid loss: 6.450902116866339
Training loss: 0.7559316158294678 / Valid loss: 6.363743143989926
Training loss: 0.9045659303665161 / Valid loss: 6.519769791194371
Training loss: 0.8763160705566406 / Valid loss: 6.439106695992606

Epoch: 8
Training loss: 0.46226269006729126 / Valid loss: 6.394025384812128
Training loss: 0.5564277768135071 / Valid loss: 6.52365114802406
Training loss: 0.6284992694854736 / Valid loss: 6.52551531791687
Training loss: 0.3954620957374573 / Valid loss: 6.4662885643187025
Training loss: 1.1482419967651367 / Valid loss: 6.500650921322051

Epoch: 9
Training loss: 0.3778538107872009 / Valid loss: 6.499136847541446
Training loss: 0.5603505969047546 / Valid loss: 6.446508189610072
Training loss: 0.6609920263290405 / Valid loss: 6.526913227353777
Training loss: 1.2666382789611816 / Valid loss: 6.637362078257969

Epoch: 10
Training loss: 0.3804243206977844 / Valid loss: 6.65226407505217
Training loss: 0.4967244565486908 / Valid loss: 6.574392012187412
Training loss: 0.6021715402603149 / Valid loss: 6.570822461446126
Training loss: 0.3410438299179077 / Valid loss: 6.592532830011277
Training loss: 0.40094777941703796 / Valid loss: 6.651630819411505

Epoch: 11
Training loss: 0.32740145921707153 / Valid loss: 6.612642088390532
Training loss: 0.3443498909473419 / Valid loss: 6.629819929032099
Training loss: 0.38977736234664917 / Valid loss: 6.555317124866304
Training loss: 0.34772324562072754 / Valid loss: 6.706615168707711
Training loss: 1.1562151908874512 / Valid loss: 6.578774620237804

Epoch: 12
Training loss: 0.21552705764770508 / Valid loss: 6.659227323532105
Training loss: 0.2816885709762573 / Valid loss: 6.618158794584728
Training loss: 0.27889835834503174 / Valid loss: 6.795573874882289
Training loss: 0.6717449426651001 / Valid loss: 6.711517542884463
Training loss: 0.4888050854206085 / Valid loss: 6.742281870614915

Epoch: 13
Training loss: 0.6212354898452759 / Valid loss: 6.5133444127582365
Training loss: 0.2971513867378235 / Valid loss: 6.753168918972924
Training loss: 0.2808924913406372 / Valid loss: 6.669435003825597
Training loss: 0.2605229914188385 / Valid loss: 6.744390692029681
Training loss: 0.2855417728424072 / Valid loss: 6.654867612747919

Epoch: 14
Training loss: 0.09443530440330505 / Valid loss: 6.657351902553013
Training loss: 0.4231594204902649 / Valid loss: 6.7547127042497905
Training loss: 0.2658184766769409 / Valid loss: 6.609062494550432
Training loss: 0.35920578241348267 / Valid loss: 6.791040475027902
Training loss: 0.20278742909431458 / Valid loss: 6.798218531835647

Epoch: 15
Training loss: 0.432992160320282 / Valid loss: 6.692450714111328
Training loss: 0.1466618925333023 / Valid loss: 6.785474032447452
Training loss: 0.7693497538566589 / Valid loss: 6.71878080368042
Training loss: 0.42286422848701477 / Valid loss: 6.7291195142836795
Training loss: 1.319852352142334 / Valid loss: 6.663879376366025

Epoch: 16
Training loss: 0.13725125789642334 / Valid loss: 6.76636594136556
Training loss: 0.5221652984619141 / Valid loss: 6.706967796598162
Training loss: 0.48641639947891235 / Valid loss: 6.894375240235101
Training loss: 0.5058994293212891 / Valid loss: 6.840947114853632
Training loss: 0.25608178973197937 / Valid loss: 6.7390573229108535

Epoch: 17
Training loss: 0.1918836385011673 / Valid loss: 6.732087512243361
Training loss: 0.16115032136440277 / Valid loss: 6.688383633749826
Training loss: 0.24326539039611816 / Valid loss: 6.753077202751523
Training loss: 0.3776237964630127 / Valid loss: 6.782034987495059
Training loss: 0.19001097977161407 / Valid loss: 6.792935711996896

Epoch: 18
Training loss: 0.16935400664806366 / Valid loss: 6.828193771271478
Training loss: 0.2556479275226593 / Valid loss: 6.726686709267753
Training loss: 0.2290678322315216 / Valid loss: 6.742220236006237
Training loss: 0.24588845670223236 / Valid loss: 6.733175382160005
Training loss: 0.15663334727287292 / Valid loss: 6.660536066691081

Epoch: 19
Training loss: 0.32233622670173645 / Valid loss: 6.717272588184902
Training loss: 0.23451274633407593 / Valid loss: 6.769136544636318
Training loss: 0.19198091328144073 / Valid loss: 6.760413978213355
Training loss: 0.22626206278800964 / Valid loss: 6.906043645313808

Epoch: 20
Training loss: 0.2860901951789856 / Valid loss: 6.811398469834101
Training loss: 0.1845875084400177 / Valid loss: 6.805179936545236
Training loss: 0.293224036693573 / Valid loss: 6.848787425813221
Training loss: 0.0999111533164978 / Valid loss: 6.741202247710455
Training loss: 0.15745475888252258 / Valid loss: 6.745234146572295

Epoch: 21
Training loss: 0.1124194785952568 / Valid loss: 6.7853880291893365
Training loss: 0.35016244649887085 / Valid loss: 6.6931262107122516
Training loss: 0.11011022329330444 / Valid loss: 6.800431964510963
Training loss: 0.14652374386787415 / Valid loss: 6.717690140860421
Training loss: 0.14612317085266113 / Valid loss: 6.825021385011219

Epoch: 22
Training loss: 0.34011703729629517 / Valid loss: 6.728884029388428
Training loss: 0.11551016569137573 / Valid loss: 6.784151231674921
Training loss: 0.3597332239151001 / Valid loss: 6.854368232545399
Training loss: 0.6455160975456238 / Valid loss: 6.95086004166376
Training loss: 0.14091697335243225 / Valid loss: 6.843023027692523

Epoch: 23
Training loss: 0.22095730900764465 / Valid loss: 6.806899483998617
Training loss: 0.25889158248901367 / Valid loss: 6.836573877788726
Training loss: 0.9495658874511719 / Valid loss: 6.732906418754941
Training loss: 0.09093894064426422 / Valid loss: 6.945911461966379
Training loss: 0.17282521724700928 / Valid loss: 6.854235217684791

Epoch: 24
Training loss: 0.2201007604598999 / Valid loss: 6.839092817760649
Training loss: 0.1595841348171234 / Valid loss: 6.7613474527994795
Training loss: 0.33757925033569336 / Valid loss: 6.756878811972482
Training loss: 0.10715493559837341 / Valid loss: 6.734550771259126
Training loss: 0.3328031003475189 / Valid loss: 6.870655327751523

Epoch: 25
Training loss: 0.32627084851264954 / Valid loss: 6.798729362941923
Training loss: 0.18027272820472717 / Valid loss: 6.8056010791233605
Training loss: 0.13653545081615448 / Valid loss: 6.75097115834554
Training loss: 0.2827436923980713 / Valid loss: 6.768715063730876
Training loss: 0.3354284167289734 / Valid loss: 6.898414891106742

Epoch: 26
Training loss: 0.12854081392288208 / Valid loss: 6.872213002613613
Training loss: 0.13695694506168365 / Valid loss: 6.831159977685838
Training loss: 0.17641273140907288 / Valid loss: 6.834679394676572
Training loss: 0.154817134141922 / Valid loss: 6.790045863106137
Training loss: 0.15916016697883606 / Valid loss: 6.878605676832653

Epoch: 27
Training loss: 0.22840863466262817 / Valid loss: 6.847792938777379
Training loss: 0.34047818183898926 / Valid loss: 6.904186103457496
Training loss: 0.26766490936279297 / Valid loss: 6.827950286865234
Training loss: 0.09238219261169434 / Valid loss: 6.7731131689889095
Training loss: 0.11427048593759537 / Valid loss: 6.756735545112973

Epoch: 28
Training loss: 0.13436853885650635 / Valid loss: 6.888207365217663
Training loss: 0.10636557638645172 / Valid loss: 6.768720917474656
Training loss: 0.38606715202331543 / Valid loss: 6.740945166633242
Training loss: 0.2005416750907898 / Valid loss: 6.76485953558059
Training loss: 0.09123629331588745 / Valid loss: 6.820324679783412

Epoch: 29
Training loss: 0.1148851215839386 / Valid loss: 6.859215559278216
Training loss: 0.16934767365455627 / Valid loss: 6.782154955182757
Training loss: 0.24353675544261932 / Valid loss: 6.8492482503255205
Training loss: 0.11335033923387527 / Valid loss: 6.843271873110816

Epoch: 30
Training loss: 0.34657391905784607 / Valid loss: 6.946517335800897
Training loss: 0.1682620793581009 / Valid loss: 6.857814802442278
Training loss: 0.1614869236946106 / Valid loss: 6.819839577447801
Training loss: 0.22150158882141113 / Valid loss: 6.8093612807137625
Training loss: 0.3275872468948364 / Valid loss: 6.840968270528884

Epoch: 31
Training loss: 0.3049590587615967 / Valid loss: 6.896773635773432
Training loss: 0.12368173897266388 / Valid loss: 6.801655460539318
Training loss: 0.25453275442123413 / Valid loss: 6.847932783762614
Training loss: 0.15271787345409393 / Valid loss: 6.740194188980829
Training loss: 0.10161179304122925 / Valid loss: 6.844690177554176

Epoch: 32
Training loss: 0.09318472445011139 / Valid loss: 6.891838991074335
Training loss: 0.09654486179351807 / Valid loss: 6.887454150971912
Training loss: 0.17755402624607086 / Valid loss: 6.805028588431222
Training loss: 0.21484312415122986 / Valid loss: 6.804820739655268
Training loss: 0.061206042766571045 / Valid loss: 6.815587048303513

Epoch: 33
Training loss: 0.22382581233978271 / Valid loss: 6.8389763854798815
Training loss: 0.1888447403907776 / Valid loss: 6.796537830716088
Training loss: 0.15547055006027222 / Valid loss: 6.8769433884393605
Training loss: 0.09028446674346924 / Valid loss: 6.842890280768985
Training loss: 0.11095988005399704 / Valid loss: 6.792709872836158

Epoch: 34
Training loss: 0.47767624258995056 / Valid loss: 6.821413598741803
Training loss: 0.1261146366596222 / Valid loss: 6.8071133886064805
Training loss: 0.10480310022830963 / Valid loss: 6.778851658957345
Training loss: 0.20180505514144897 / Valid loss: 6.853417759849911
Training loss: 0.15632060170173645 / Valid loss: 6.996718415759859

Epoch: 35
Training loss: 0.11523815989494324 / Valid loss: 6.849757108234224
Training loss: 0.23535847663879395 / Valid loss: 6.817785222189767
Training loss: 0.09763751924037933 / Valid loss: 6.819322118305025
Training loss: 0.08330070972442627 / Valid loss: 6.799379612150647
Training loss: 0.6510921716690063 / Valid loss: 6.922387781597319

Epoch: 36
Training loss: 0.08871585875749588 / Valid loss: 6.773724449248541
Training loss: 0.2541382312774658 / Valid loss: 6.769306096576509
Training loss: 0.1895187348127365 / Valid loss: 6.889061998185658
Training loss: 0.14469704031944275 / Valid loss: 6.781619212740943
Training loss: 0.18574029207229614 / Valid loss: 6.91877574012393

Epoch: 37
Training loss: 0.18265843391418457 / Valid loss: 6.843391195933024
Training loss: 0.13349765539169312 / Valid loss: 6.766509714580717
Training loss: 0.5921308994293213 / Valid loss: 6.854700506301153
Training loss: 0.2067805677652359 / Valid loss: 6.8769333476112005
Training loss: 0.33891794085502625 / Valid loss: 6.856990677969796

Epoch: 38
Training loss: 0.15275037288665771 / Valid loss: 6.751754996890114
Training loss: 0.08266456425189972 / Valid loss: 6.8069756371634345
Training loss: 0.16482305526733398 / Valid loss: 6.785018948146275
Training loss: 0.12467484176158905 / Valid loss: 6.886503455752418
Training loss: 0.09030671417713165 / Valid loss: 6.836016132718041

Epoch: 39
Training loss: 0.17434200644493103 / Valid loss: 6.871005739484515
Training loss: 0.24496373534202576 / Valid loss: 6.801453608558291
Training loss: 0.14014434814453125 / Valid loss: 6.771207995641799
Training loss: 0.11060431599617004 / Valid loss: 6.816454458236694
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1700): 5.930276362101237
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)

Epoch: 0
Training loss: 14.313697814941406 / Valid loss: 15.750787707737514
Model is saved in epoch 0, overall batch: 0
Training loss: 13.752546310424805 / Valid loss: 14.40562307266962
Model is saved in epoch 0, overall batch: 100
Training loss: 11.145820617675781 / Valid loss: 11.973181715465728
Model is saved in epoch 0, overall batch: 200
Training loss: 5.633404731750488 / Valid loss: 10.573148459479922
Model is saved in epoch 0, overall batch: 300
Training loss: 6.23992919921875 / Valid loss: 9.493511422475178
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.3836469650268555 / Valid loss: 8.690918668111165
Model is saved in epoch 1, overall batch: 500
Training loss: 6.3640336990356445 / Valid loss: 8.083785009384155
Model is saved in epoch 1, overall batch: 600
Training loss: 7.966368675231934 / Valid loss: 7.489012068793887
Model is saved in epoch 1, overall batch: 700
Training loss: 5.723847389221191 / Valid loss: 7.146095073790777
Model is saved in epoch 1, overall batch: 800
Training loss: 6.082832336425781 / Valid loss: 6.814146109989712
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 4.620587348937988 / Valid loss: 6.565404424213228
Model is saved in epoch 2, overall batch: 1000
Training loss: 2.757269859313965 / Valid loss: 6.4527781940641855
Model is saved in epoch 2, overall batch: 1100
Training loss: 3.0714526176452637 / Valid loss: 6.213621632258097
Model is saved in epoch 2, overall batch: 1200
Training loss: 2.814481735229492 / Valid loss: 6.180941994984945
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.222474098205566 / Valid loss: 6.106489538011097
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 1.7811176776885986 / Valid loss: 6.009587044942947
Model is saved in epoch 3, overall batch: 1500
Training loss: 4.170924663543701 / Valid loss: 6.0698617095039005
Training loss: 3.454958915710449 / Valid loss: 5.997446568806966
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.248225688934326 / Valid loss: 6.032680679502941
Training loss: 3.5541720390319824 / Valid loss: 6.01062875248137

Epoch: 4
Training loss: 2.1937668323516846 / Valid loss: 6.0163528578622
Training loss: 2.619704246520996 / Valid loss: 6.051643105915614
Training loss: 1.245854139328003 / Valid loss: 6.093669743764968
Training loss: 1.624197244644165 / Valid loss: 6.090953073047456
Training loss: 2.355053186416626 / Valid loss: 6.156101317632766

Epoch: 5
Training loss: 1.2064509391784668 / Valid loss: 6.171777382351103
Training loss: 1.4476484060287476 / Valid loss: 6.251558648972284
Training loss: 1.602506399154663 / Valid loss: 6.1761901310512
Training loss: 1.8016101121902466 / Valid loss: 6.24242885907491
Training loss: 1.4152945280075073 / Valid loss: 6.308553684325445

Epoch: 6
Training loss: 0.8380393385887146 / Valid loss: 6.31109565553211
Training loss: 1.3254296779632568 / Valid loss: 6.303854731151036
Training loss: 1.0840120315551758 / Valid loss: 6.413820734478178
Training loss: 1.081537127494812 / Valid loss: 6.378261352720715
Training loss: 0.9837340712547302 / Valid loss: 6.292619603020804

Epoch: 7
Training loss: 0.6020177006721497 / Valid loss: 6.305991213662284
Training loss: 0.7879476547241211 / Valid loss: 6.4459523019336515
Training loss: 0.7515095472335815 / Valid loss: 6.3572220075698125
Training loss: 0.8935098648071289 / Valid loss: 6.51081337247576
Training loss: 0.9409911036491394 / Valid loss: 6.4405034678322926

Epoch: 8
Training loss: 0.46821463108062744 / Valid loss: 6.4060781342642645
Training loss: 0.5538281202316284 / Valid loss: 6.524917400450933
Training loss: 0.6420682072639465 / Valid loss: 6.524211770012265
Training loss: 0.3957318663597107 / Valid loss: 6.4707787990570065
Training loss: 1.1696783304214478 / Valid loss: 6.51260218393235

Epoch: 9
Training loss: 0.3443153202533722 / Valid loss: 6.513843427385603
Training loss: 0.5754510164260864 / Valid loss: 6.457761959802537
Training loss: 0.6708327531814575 / Valid loss: 6.543341273353214
Training loss: 1.2576781511306763 / Valid loss: 6.652582790738061

Epoch: 10
Training loss: 0.3906150460243225 / Valid loss: 6.654312090646653
Training loss: 0.520004391670227 / Valid loss: 6.587428726468768
Training loss: 0.5892007350921631 / Valid loss: 6.583062178747994
Training loss: 0.36845582723617554 / Valid loss: 6.602937203361875
Training loss: 0.39087027311325073 / Valid loss: 6.672244267236619

Epoch: 11
Training loss: 0.32808828353881836 / Valid loss: 6.627058987390427
Training loss: 0.37340882420539856 / Valid loss: 6.645089853377569
Training loss: 0.3794868290424347 / Valid loss: 6.576904719216483
Training loss: 0.3583923578262329 / Valid loss: 6.7316434201740085
Training loss: 1.145224928855896 / Valid loss: 6.587471389770508

Epoch: 12
Training loss: 0.22525563836097717 / Valid loss: 6.678234570366996
Training loss: 0.2457478642463684 / Valid loss: 6.625859941755023
Training loss: 0.2747289538383484 / Valid loss: 6.8099876948765345
Training loss: 0.6216222643852234 / Valid loss: 6.718619521458944
Training loss: 0.4416346549987793 / Valid loss: 6.751280811854771

Epoch: 13
Training loss: 0.5913529396057129 / Valid loss: 6.52348217737107
Training loss: 0.2851879894733429 / Valid loss: 6.767848550705683
Training loss: 0.2680412232875824 / Valid loss: 6.684511332284837
Training loss: 0.24765542149543762 / Valid loss: 6.75357920328776
Training loss: 0.3079662621021271 / Valid loss: 6.671405928475516

Epoch: 14
Training loss: 0.09849590063095093 / Valid loss: 6.669287088939122
Training loss: 0.4484216570854187 / Valid loss: 6.768650281996954
Training loss: 0.28983837366104126 / Valid loss: 6.618660463605608
Training loss: 0.3273373246192932 / Valid loss: 6.79736304964338
Training loss: 0.20072439312934875 / Valid loss: 6.804953974769229

Epoch: 15
Training loss: 0.4451659321784973 / Valid loss: 6.714101164681571
Training loss: 0.13981445133686066 / Valid loss: 6.816569791521345
Training loss: 0.7753744125366211 / Valid loss: 6.7326265085311165
Training loss: 0.45354098081588745 / Valid loss: 6.7436401730492
Training loss: 1.3245418071746826 / Valid loss: 6.670444956279936

Epoch: 16
Training loss: 0.1532687544822693 / Valid loss: 6.779260562715076
Training loss: 0.4533751606941223 / Valid loss: 6.709134201776414
Training loss: 0.4762685298919678 / Valid loss: 6.911589545295352
Training loss: 0.5136748552322388 / Valid loss: 6.854491356440953
Training loss: 0.2437209188938141 / Valid loss: 6.75377581914266

Epoch: 17
Training loss: 0.19150182604789734 / Valid loss: 6.7455934660775325
Training loss: 0.1576792448759079 / Valid loss: 6.709031102770851
Training loss: 0.241112619638443 / Valid loss: 6.779690851484026
Training loss: 0.3668445348739624 / Valid loss: 6.79410453978039
Training loss: 0.18193088471889496 / Valid loss: 6.80237503960019

Epoch: 18
Training loss: 0.14859934151172638 / Valid loss: 6.842456903911772
Training loss: 0.2558972239494324 / Valid loss: 6.739676511855352
Training loss: 0.2348826676607132 / Valid loss: 6.760552197410947
Training loss: 0.2656441330909729 / Valid loss: 6.737562647319975
Training loss: 0.16807541251182556 / Valid loss: 6.669100102924165

Epoch: 19
Training loss: 0.30892762541770935 / Valid loss: 6.724939925330026
Training loss: 0.21643832325935364 / Valid loss: 6.777771091461181
Training loss: 0.19023147225379944 / Valid loss: 6.769130457015264
Training loss: 0.25614693760871887 / Valid loss: 6.9147726286025275

Epoch: 20
Training loss: 0.2779431939125061 / Valid loss: 6.822689614977155
Training loss: 0.19593080878257751 / Valid loss: 6.81121666772025
Training loss: 0.28863051533699036 / Valid loss: 6.85971371786935
Training loss: 0.10072044283151627 / Valid loss: 6.758940324329195
Training loss: 0.16747424006462097 / Valid loss: 6.764230276289441

Epoch: 21
Training loss: 0.12630939483642578 / Valid loss: 6.802275121779669
Training loss: 0.3619160056114197 / Valid loss: 6.704265971410842
Training loss: 0.10763024538755417 / Valid loss: 6.817908405122303
Training loss: 0.1612715721130371 / Valid loss: 6.732329150608607
Training loss: 0.15552473068237305 / Valid loss: 6.837859135582334

Epoch: 22
Training loss: 0.3582988381385803 / Valid loss: 6.739368225279309
Training loss: 0.11768389493227005 / Valid loss: 6.799007933480399
Training loss: 0.35973745584487915 / Valid loss: 6.863946322032383
Training loss: 0.6685742139816284 / Valid loss: 6.967016174679711
Training loss: 0.1526961475610733 / Valid loss: 6.857158177239555

Epoch: 23
Training loss: 0.21749010682106018 / Valid loss: 6.82629378636678
Training loss: 0.2840449810028076 / Valid loss: 6.858238542647589
Training loss: 0.9595494270324707 / Valid loss: 6.750472302663894
Training loss: 0.09104348719120026 / Valid loss: 6.9741004467010494
Training loss: 0.1701798141002655 / Valid loss: 6.870006366003127

Epoch: 24
Training loss: 0.21922479569911957 / Valid loss: 6.856596372241065
Training loss: 0.15272292494773865 / Valid loss: 6.7802741232372465
Training loss: 0.33828333020210266 / Valid loss: 6.773258354550316
Training loss: 0.10252046585083008 / Valid loss: 6.754401665642148
Training loss: 0.2875695824623108 / Valid loss: 6.884847681862968

Epoch: 25
Training loss: 0.31707900762557983 / Valid loss: 6.809381051290603
Training loss: 0.1669425368309021 / Valid loss: 6.828378643308367
Training loss: 0.1575974076986313 / Valid loss: 6.763255973089309
Training loss: 0.2590085566043854 / Valid loss: 6.777704388754708
Training loss: 0.32790234684944153 / Valid loss: 6.90066716330392

Epoch: 26
Training loss: 0.12252919375896454 / Valid loss: 6.8906411443437845
Training loss: 0.13217787444591522 / Valid loss: 6.852225426265171
Training loss: 0.17439845204353333 / Valid loss: 6.847899720782325
Training loss: 0.1651528775691986 / Valid loss: 6.803316606794085
Training loss: 0.17362606525421143 / Valid loss: 6.893251807349069

Epoch: 27
Training loss: 0.23935472965240479 / Valid loss: 6.860097630818685
Training loss: 0.3322162926197052 / Valid loss: 6.925717122214181
Training loss: 0.2756088972091675 / Valid loss: 6.852096525828044
Training loss: 0.08667037636041641 / Valid loss: 6.791034707568941
Training loss: 0.12167812138795853 / Valid loss: 6.772105198814756

Epoch: 28
Training loss: 0.1307116001844406 / Valid loss: 6.899097835449945
Training loss: 0.109642893075943 / Valid loss: 6.777077597663516
Training loss: 0.3833046555519104 / Valid loss: 6.75321873710269
Training loss: 0.1854345053434372 / Valid loss: 6.778588667370024
Training loss: 0.09605780243873596 / Valid loss: 6.832923643929618

Epoch: 29
Training loss: 0.11303591728210449 / Valid loss: 6.869600754692441
Training loss: 0.1596488654613495 / Valid loss: 6.801270911807106
Training loss: 0.2761837840080261 / Valid loss: 6.875388156800043
Training loss: 0.09927999973297119 / Valid loss: 6.846427090962728

Epoch: 30
Training loss: 0.33593204617500305 / Valid loss: 6.96240968250093
Training loss: 0.1598406881093979 / Valid loss: 6.874108609699068
Training loss: 0.16887003183364868 / Valid loss: 6.8358447551727295
Training loss: 0.2165570855140686 / Valid loss: 6.822523362295968
Training loss: 0.3057672083377838 / Valid loss: 6.85150184177217

Epoch: 31
Training loss: 0.31108009815216064 / Valid loss: 6.919019297191075
Training loss: 0.12699046730995178 / Valid loss: 6.807961931682768
Training loss: 0.2682112753391266 / Valid loss: 6.858257784162249
Training loss: 0.12949036061763763 / Valid loss: 6.757519049871536
Training loss: 0.11751258373260498 / Valid loss: 6.855261857169015

Epoch: 32
Training loss: 0.09363396465778351 / Valid loss: 6.902794252123152
Training loss: 0.09945257753133774 / Valid loss: 6.901560088566371
Training loss: 0.1965567171573639 / Valid loss: 6.819451463790167
Training loss: 0.21929141879081726 / Valid loss: 6.824599733806791
Training loss: 0.07603336125612259 / Valid loss: 6.84108294078282

Epoch: 33
Training loss: 0.2278420776128769 / Valid loss: 6.850893774486724
Training loss: 0.1760903000831604 / Valid loss: 6.803597818102156
Training loss: 0.16436603665351868 / Valid loss: 6.885241971697126
Training loss: 0.08300463110208511 / Valid loss: 6.854697118486677
Training loss: 0.11866477876901627 / Valid loss: 6.8105942953200564

Epoch: 34
Training loss: 0.3974984884262085 / Valid loss: 6.836650058201381
Training loss: 0.12334141880273819 / Valid loss: 6.817736203329904
Training loss: 0.10166658461093903 / Valid loss: 6.795645105271112
Training loss: 0.1985720992088318 / Valid loss: 6.874444616408575
Training loss: 0.14796502888202667 / Valid loss: 7.011865461440314

Epoch: 35
Training loss: 0.12544210255146027 / Valid loss: 6.86831989969526
Training loss: 0.278059184551239 / Valid loss: 6.8302699588593985
Training loss: 0.11641350388526917 / Valid loss: 6.835746438162667
Training loss: 0.08836185932159424 / Valid loss: 6.813913433892386
Training loss: 0.6740261316299438 / Valid loss: 6.937180609930129

Epoch: 36
Training loss: 0.10009407997131348 / Valid loss: 6.789240832555862
Training loss: 0.25799065828323364 / Valid loss: 6.781307815370106
Training loss: 0.1864175796508789 / Valid loss: 6.902232381275722
Training loss: 0.14395317435264587 / Valid loss: 6.7957427728743784
Training loss: 0.18308474123477936 / Valid loss: 6.941749132247198

Epoch: 37
Training loss: 0.16366931796073914 / Valid loss: 6.853504046939668
Training loss: 0.14898714423179626 / Valid loss: 6.778587754567464
Training loss: 0.5910118818283081 / Valid loss: 6.869727940786452
Training loss: 0.21039843559265137 / Valid loss: 6.891956431525094
Training loss: 0.3578853905200958 / Valid loss: 6.86616949126834

Epoch: 38
Training loss: 0.1345343291759491 / Valid loss: 6.7618142582121346
Training loss: 0.0780668705701828 / Valid loss: 6.819538302648635
Training loss: 0.1560579091310501 / Valid loss: 6.799155292056856
Training loss: 0.13093845546245575 / Valid loss: 6.902370825267973
Training loss: 0.0877426490187645 / Valid loss: 6.854388672964913

Epoch: 39
Training loss: 0.16309547424316406 / Valid loss: 6.896144240243094
Training loss: 0.24754607677459717 / Valid loss: 6.8195316609882175
Training loss: 0.1499541699886322 / Valid loss: 6.786369160243443
Training loss: 0.12315154075622559 / Valid loss: 6.836273547581264
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1700): 5.923645064944313
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.220863342285156 / Valid loss: 16.437623568943568
Model is saved in epoch 0, overall batch: 0
Training loss: 10.162872314453125 / Valid loss: 11.46192240033831
Model is saved in epoch 0, overall batch: 100
Training loss: 7.728257656097412 / Valid loss: 7.493644923255557
Model is saved in epoch 0, overall batch: 200
Training loss: 5.181697845458984 / Valid loss: 6.415898593266805
Model is saved in epoch 0, overall batch: 300
Training loss: 6.132676601409912 / Valid loss: 5.961236447379703
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.049834728240967 / Valid loss: 5.754955423445929
Model is saved in epoch 1, overall batch: 500
Training loss: 5.399333477020264 / Valid loss: 5.646049796967279
Model is saved in epoch 1, overall batch: 600
Training loss: 4.390186309814453 / Valid loss: 5.619242511476789
Model is saved in epoch 1, overall batch: 700
Training loss: 6.24585485458374 / Valid loss: 5.597316866829281
Model is saved in epoch 1, overall batch: 800
Training loss: 5.75360107421875 / Valid loss: 5.566003392991566
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 4.249693870544434 / Valid loss: 5.551222803479149
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.9242234230041504 / Valid loss: 5.5558816410246346
Training loss: 5.117839336395264 / Valid loss: 5.554155052275885
Training loss: 3.5011258125305176 / Valid loss: 5.541973002751669
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.092771053314209 / Valid loss: 5.54207055909293

Epoch: 3
Training loss: 3.8763718605041504 / Valid loss: 5.549076657068162
Training loss: 4.535436153411865 / Valid loss: 5.568911711374919
Training loss: 4.296402931213379 / Valid loss: 5.567015586580549
Training loss: 4.037261009216309 / Valid loss: 5.570396373385475
Training loss: 5.671407222747803 / Valid loss: 5.548119183949062

Epoch: 4
Training loss: 2.987182140350342 / Valid loss: 5.574567138581049
Training loss: 3.0258584022521973 / Valid loss: 5.573652113051642
Training loss: 3.4540903568267822 / Valid loss: 5.591258907318116
Training loss: 4.158498764038086 / Valid loss: 5.580865476244972
Training loss: 3.1214723587036133 / Valid loss: 5.5777089868273055

Epoch: 5
Training loss: 3.377483367919922 / Valid loss: 5.593229064487276
Training loss: 3.6698338985443115 / Valid loss: 5.609514349982852
Training loss: 2.484265089035034 / Valid loss: 5.617801109949748
Training loss: 5.286789894104004 / Valid loss: 5.625318697520664
Training loss: 4.387582778930664 / Valid loss: 5.616320932479132

Epoch: 6
Training loss: 2.9809556007385254 / Valid loss: 5.614488987695603
Training loss: 3.939122200012207 / Valid loss: 5.645402399698893
Training loss: 3.6853437423706055 / Valid loss: 5.647940179279872
Training loss: 3.1611857414245605 / Valid loss: 5.659352768035162
Training loss: 4.829153537750244 / Valid loss: 5.651199388504028

Epoch: 7
Training loss: 3.1940383911132812 / Valid loss: 5.680399690355573
Training loss: 2.681680202484131 / Valid loss: 5.706537276222592
Training loss: 2.787493944168091 / Valid loss: 5.68465508052281
Training loss: 2.1222434043884277 / Valid loss: 5.696131815229143
Training loss: 2.5555384159088135 / Valid loss: 5.697423112960089

Epoch: 8
Training loss: 3.3916478157043457 / Valid loss: 5.716054017203195
Training loss: 3.463899850845337 / Valid loss: 5.737263913381668
Training loss: 1.8325470685958862 / Valid loss: 5.7320849940890355
Training loss: 3.1108808517456055 / Valid loss: 5.756640223094395
Training loss: 2.4824702739715576 / Valid loss: 5.755083999179658

Epoch: 9
Training loss: 1.8233802318572998 / Valid loss: 5.791031955537342
Training loss: 2.8732948303222656 / Valid loss: 5.780138385863531
Training loss: 2.505666732788086 / Valid loss: 5.789604786464146
Training loss: 3.1490368843078613 / Valid loss: 5.800741184325445

Epoch: 10
Training loss: 2.5804905891418457 / Valid loss: 5.791926349912371
Training loss: 2.3280630111694336 / Valid loss: 5.803745010920934
Training loss: 2.408802032470703 / Valid loss: 5.865981758208502
Training loss: 3.050537347793579 / Valid loss: 5.845713878813244
Training loss: 2.704247236251831 / Valid loss: 5.853253884542556

Epoch: 11
Training loss: 1.2720695734024048 / Valid loss: 5.857543899899437
Training loss: 2.110213279724121 / Valid loss: 5.865421633493333
Training loss: 1.9614864587783813 / Valid loss: 5.894107859475272
Training loss: 2.192850112915039 / Valid loss: 5.9362919217064265
Training loss: 2.966635227203369 / Valid loss: 5.903920316696167

Epoch: 12
Training loss: 2.1772656440734863 / Valid loss: 5.924064082191104
Training loss: 1.829028606414795 / Valid loss: 5.925978138333275
Training loss: 2.509511947631836 / Valid loss: 5.9562057177225745
Training loss: 2.3263590335845947 / Valid loss: 5.942862694604056
Training loss: 1.6640974283218384 / Valid loss: 5.958974061693464

Epoch: 13
Training loss: 2.673379421234131 / Valid loss: 5.951364124388922
Training loss: 1.715466022491455 / Valid loss: 5.960956416811261
Training loss: 1.9925017356872559 / Valid loss: 5.971720366250901
Training loss: 1.1694178581237793 / Valid loss: 6.001750312532697
Training loss: 2.044257879257202 / Valid loss: 6.016969971429734

Epoch: 14
Training loss: 1.814725399017334 / Valid loss: 6.021838628678095
Training loss: 1.5923326015472412 / Valid loss: 6.01718213898795
Training loss: 1.6444530487060547 / Valid loss: 6.036759465081351
Training loss: 1.627508282661438 / Valid loss: 6.039342180887858
Training loss: 1.4677393436431885 / Valid loss: 6.027566276277814

Epoch: 15
Training loss: 1.5148544311523438 / Valid loss: 6.07143216360183
Training loss: 1.6079987287521362 / Valid loss: 6.082122416723342
Training loss: 1.7907921075820923 / Valid loss: 6.090827469598679
Training loss: 2.103541135787964 / Valid loss: 6.075677231379918
Training loss: 1.1175602674484253 / Valid loss: 6.092885762169248

Epoch: 16
Training loss: 0.9142489433288574 / Valid loss: 6.1152974878038675
Training loss: 0.9642788171768188 / Valid loss: 6.127498703911191
Training loss: 0.9941952228546143 / Valid loss: 6.092090020860945
Training loss: 1.162906527519226 / Valid loss: 6.154076596668788
Training loss: 2.2604219913482666 / Valid loss: 6.1178322042737685

Epoch: 17
Training loss: 1.578902244567871 / Valid loss: 6.123935897009713
Training loss: 0.8625670671463013 / Valid loss: 6.183170949845087
Training loss: 1.5772546529769897 / Valid loss: 6.160262655076527
Training loss: 1.2776424884796143 / Valid loss: 6.1429955187298
Training loss: 1.1442631483078003 / Valid loss: 6.180877635592506

Epoch: 18
Training loss: 1.5832839012145996 / Valid loss: 6.162023805436634
Training loss: 1.4609445333480835 / Valid loss: 6.201406574249267
Training loss: 1.1281979084014893 / Valid loss: 6.176636768522717
Training loss: 1.5142030715942383 / Valid loss: 6.22344506354559
Training loss: 0.9025390148162842 / Valid loss: 6.2209892681666785

Epoch: 19
Training loss: 1.7103925943374634 / Valid loss: 6.225837142126901
Training loss: 0.8597835302352905 / Valid loss: 6.23916232245309
Training loss: 1.2152116298675537 / Valid loss: 6.268201051439558
Training loss: 1.2711611986160278 / Valid loss: 6.278043624332973

Epoch: 20
Training loss: 1.245775818824768 / Valid loss: 6.248163173312233
Training loss: 1.5654058456420898 / Valid loss: 6.252476832980201
Training loss: 0.7719752192497253 / Valid loss: 6.252116652897426
Training loss: 1.180747628211975 / Valid loss: 6.282963289533343
Training loss: 1.4022204875946045 / Valid loss: 6.277082511356899

Epoch: 21
Training loss: 0.7163229584693909 / Valid loss: 6.2824334712255565
Training loss: 1.0206637382507324 / Valid loss: 6.278896740504673
Training loss: 1.2305009365081787 / Valid loss: 6.2946239312489825
Training loss: 0.9848871827125549 / Valid loss: 6.335762509845552
Training loss: 1.068257212638855 / Valid loss: 6.32286274092538

Epoch: 22
Training loss: 0.8062034845352173 / Valid loss: 6.327463195437477
Training loss: 0.7351147532463074 / Valid loss: 6.357813058580671
Training loss: 0.8431893587112427 / Valid loss: 6.318876198359898
Training loss: 0.7057235240936279 / Valid loss: 6.366657393319266
Training loss: 0.8991497755050659 / Valid loss: 6.307925542195638

Epoch: 23
Training loss: 0.9580889940261841 / Valid loss: 6.320507680802118
Training loss: 1.3173904418945312 / Valid loss: 6.3724186261494955
Training loss: 0.9258132576942444 / Valid loss: 6.3521960235777355
Training loss: 1.2295680046081543 / Valid loss: 6.355821125847953
Training loss: 0.9861281514167786 / Valid loss: 6.39561189469837

Epoch: 24
Training loss: 0.47350648045539856 / Valid loss: 6.3722269830249605
Training loss: 0.9673754572868347 / Valid loss: 6.378224536350795
Training loss: 1.4690622091293335 / Valid loss: 6.348882818222046
Training loss: 0.7722814679145813 / Valid loss: 6.379662043707711
Training loss: 0.588304877281189 / Valid loss: 6.382402474539621

Epoch: 25
Training loss: 1.1080659627914429 / Valid loss: 6.385511316571917
Training loss: 0.8928732872009277 / Valid loss: 6.387529454912458
Training loss: 0.9630057215690613 / Valid loss: 6.407305953616188
Training loss: 0.7051867246627808 / Valid loss: 6.394479252043224
Training loss: 0.7377746105194092 / Valid loss: 6.39371428489685

Epoch: 26
Training loss: 0.5697533488273621 / Valid loss: 6.415495531899588
Training loss: 1.115095853805542 / Valid loss: 6.404920248758225
Training loss: 0.49410927295684814 / Valid loss: 6.459633436657134
Training loss: 0.7412936687469482 / Valid loss: 6.446431282588414
Training loss: 0.7663286328315735 / Valid loss: 6.439355893362136

Epoch: 27
Training loss: 1.3169740438461304 / Valid loss: 6.420620541345506
Training loss: 1.2322139739990234 / Valid loss: 6.433124537695022
Training loss: 0.826142430305481 / Valid loss: 6.47243960244315
Training loss: 0.26259732246398926 / Valid loss: 6.456501611073812
Training loss: 0.5215908288955688 / Valid loss: 6.4642588297526045

Epoch: 28
Training loss: 0.4561576247215271 / Valid loss: 6.458223558607555
Training loss: 0.8643577694892883 / Valid loss: 6.501667542684646
Training loss: 1.2927229404449463 / Valid loss: 6.468331055414109
Training loss: 0.6662229299545288 / Valid loss: 6.518440791538784
Training loss: 0.5941729545593262 / Valid loss: 6.484454768044608

Epoch: 29
Training loss: 0.9922342300415039 / Valid loss: 6.516873722984677
Training loss: 0.5629592537879944 / Valid loss: 6.520027773720877
Training loss: 0.9970008134841919 / Valid loss: 6.4995635327838714
Training loss: 0.7675613164901733 / Valid loss: 6.460218742915562

Epoch: 30
Training loss: 0.6332650184631348 / Valid loss: 6.487074429648263
Training loss: 0.5447990894317627 / Valid loss: 6.4987138112386065
Training loss: 1.0786263942718506 / Valid loss: 6.472700936453683
Training loss: 1.021272897720337 / Valid loss: 6.539091564360119
Training loss: 0.49837473034858704 / Valid loss: 6.485489182245164

Epoch: 31
Training loss: 0.6343330144882202 / Valid loss: 6.531958925156366
Training loss: 0.8077802658081055 / Valid loss: 6.51948032833281
Training loss: 0.3139021396636963 / Valid loss: 6.537646273204259
Training loss: 0.5442012548446655 / Valid loss: 6.541659977322533
Training loss: 1.1807680130004883 / Valid loss: 6.514182242893037

Epoch: 32
Training loss: 0.42236408591270447 / Valid loss: 6.5272754896254765
Training loss: 0.6197347640991211 / Valid loss: 6.537208402724493
Training loss: 0.5534011125564575 / Valid loss: 6.518199441546486
Training loss: 0.37397271394729614 / Valid loss: 6.565399283454532
Training loss: 0.8353492021560669 / Valid loss: 6.544051976430984

Epoch: 33
Training loss: 0.7232453227043152 / Valid loss: 6.526251243409656
Training loss: 0.24648690223693848 / Valid loss: 6.541423273086548
Training loss: 0.38539764285087585 / Valid loss: 6.564643031074887
Training loss: 0.39139389991760254 / Valid loss: 6.591884744734991
Training loss: 0.261605829000473 / Valid loss: 6.599912634350004

Epoch: 34
Training loss: 0.329013466835022 / Valid loss: 6.558113134474981
Training loss: 0.47069036960601807 / Valid loss: 6.548368808201381
Training loss: 0.2681701183319092 / Valid loss: 6.599685503187634
Training loss: 0.4389653503894806 / Valid loss: 6.555638844626291
Training loss: 0.5350726842880249 / Valid loss: 6.551994189761934

Epoch: 35
Training loss: 0.3467988073825836 / Valid loss: 6.59526157833281
Training loss: 0.3524527847766876 / Valid loss: 6.558172934395927
Training loss: 0.44316065311431885 / Valid loss: 6.614370575405302
Training loss: 0.4310043752193451 / Valid loss: 6.606240413302467
Training loss: 0.4590073823928833 / Valid loss: 6.595829227992467

Epoch: 36
Training loss: 0.18390178680419922 / Valid loss: 6.614191747847057
Training loss: 0.1943863034248352 / Valid loss: 6.615863831837972
Training loss: 0.39017996191978455 / Valid loss: 6.599273413703555
Training loss: 0.5189017653465271 / Valid loss: 6.597305356888544
Training loss: 0.3972909152507782 / Valid loss: 6.6388344424111505

Epoch: 37
Training loss: 0.2308070957660675 / Valid loss: 6.595498393830799
Training loss: 0.7710242867469788 / Valid loss: 6.665807297116234
Training loss: 0.23602896928787231 / Valid loss: 6.621296619233631
Training loss: 0.4588449001312256 / Valid loss: 6.615047613779704
Training loss: 0.3527889847755432 / Valid loss: 6.6141458511352536

Epoch: 38
Training loss: 0.27197086811065674 / Valid loss: 6.612652347201393
Training loss: 0.24467623233795166 / Valid loss: 6.602702036358061
Training loss: 0.37366044521331787 / Valid loss: 6.629530706859771
Training loss: 0.21999399363994598 / Valid loss: 6.614095846811931
Training loss: 0.4285861849784851 / Valid loss: 6.633166453951881

Epoch: 39
Training loss: 0.3454541862010956 / Valid loss: 6.628871667952764
Training loss: 0.28963807225227356 / Valid loss: 6.65157243183681
Training loss: 0.4004227817058563 / Valid loss: 6.630261314482916
Training loss: 0.1869071125984192 / Valid loss: 6.634673288890293
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1300): 5.3820764950343545
Training regression with following parameters:
dnn_hidden_units : 248
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)

Epoch: 0
Training loss: 15.220863342285156 / Valid loss: 16.437623568943568
Model is saved in epoch 0, overall batch: 0
Training loss: 10.162873268127441 / Valid loss: 11.461923013414655
Model is saved in epoch 0, overall batch: 100
Training loss: 7.728259086608887 / Valid loss: 7.493646344684419
Model is saved in epoch 0, overall batch: 200
Training loss: 5.181699752807617 / Valid loss: 6.415900119145712
Model is saved in epoch 0, overall batch: 300
Training loss: 6.13267707824707 / Valid loss: 5.9612377166748045
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 6.049836158752441 / Valid loss: 5.754956390744164
Model is saved in epoch 1, overall batch: 500
Training loss: 5.399328231811523 / Valid loss: 5.646059122539702
Model is saved in epoch 1, overall batch: 600
Training loss: 4.3900885581970215 / Valid loss: 5.6192512126195995
Model is saved in epoch 1, overall batch: 700
Training loss: 6.245784282684326 / Valid loss: 5.5973368167877195
Model is saved in epoch 1, overall batch: 800
Training loss: 5.753483772277832 / Valid loss: 5.566020313898722
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 4.2496113777160645 / Valid loss: 5.551245812007359
Model is saved in epoch 2, overall batch: 1000
Training loss: 3.9243431091308594 / Valid loss: 5.555892167772566
Training loss: 5.1178741455078125 / Valid loss: 5.554182384127662
Training loss: 3.5011649131774902 / Valid loss: 5.541993654341924
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.093190670013428 / Valid loss: 5.542032966159639

Epoch: 3
Training loss: 3.8765463829040527 / Valid loss: 5.549110880352202
Training loss: 4.535608768463135 / Valid loss: 5.568892846788679
Training loss: 4.297713279724121 / Valid loss: 5.567043897083828
Training loss: 4.036784648895264 / Valid loss: 5.57025678952535
Training loss: 5.671566009521484 / Valid loss: 5.548036125728062

Epoch: 4
Training loss: 2.9860939979553223 / Valid loss: 5.574525919414702
Training loss: 3.024660587310791 / Valid loss: 5.573473510288057
Training loss: 3.4548096656799316 / Valid loss: 5.591189859026954
Training loss: 4.158021926879883 / Valid loss: 5.5808030786968414
Training loss: 3.12213397026062 / Valid loss: 5.577620497204008

Epoch: 5
Training loss: 3.3778388500213623 / Valid loss: 5.592922437758673
Training loss: 3.6676573753356934 / Valid loss: 5.609402374994187
Training loss: 2.483675003051758 / Valid loss: 5.617459889820644
Training loss: 5.286402702331543 / Valid loss: 5.625150065194993
Training loss: 4.387977600097656 / Valid loss: 5.615891838073731

Epoch: 6
Training loss: 2.978485584259033 / Valid loss: 5.614286904107956
Training loss: 3.938934803009033 / Valid loss: 5.645233376820882
Training loss: 3.6828150749206543 / Valid loss: 5.647906015032814
Training loss: 3.161806106567383 / Valid loss: 5.659220547903152
Training loss: 4.830723762512207 / Valid loss: 5.650946308317638

Epoch: 7
Training loss: 3.198176145553589 / Valid loss: 5.680192652202788
Training loss: 2.6800661087036133 / Valid loss: 5.7063108285268145
Training loss: 2.789604663848877 / Valid loss: 5.684361015047346
Training loss: 2.121626377105713 / Valid loss: 5.695906380244664
Training loss: 2.5565671920776367 / Valid loss: 5.696959540957496

Epoch: 8
Training loss: 3.392026424407959 / Valid loss: 5.715607168560936
Training loss: 3.4694714546203613 / Valid loss: 5.736548280715942
Training loss: 1.8305703401565552 / Valid loss: 5.731660729362851
Training loss: 3.1078946590423584 / Valid loss: 5.7562094484056745
Training loss: 2.4809420108795166 / Valid loss: 5.754674421037946

Epoch: 9
Training loss: 1.8251574039459229 / Valid loss: 5.790660036177862
Training loss: 2.8726930618286133 / Valid loss: 5.779813925425212
Training loss: 2.5043039321899414 / Valid loss: 5.789269924163818
Training loss: 3.149143695831299 / Valid loss: 5.800293334325155

Epoch: 10
Training loss: 2.5768089294433594 / Valid loss: 5.7913214910598025
Training loss: 2.3282904624938965 / Valid loss: 5.803213977813721
Training loss: 2.4041547775268555 / Valid loss: 5.865281234468733
Training loss: 3.0563275814056396 / Valid loss: 5.844594083513532
Training loss: 2.699585437774658 / Valid loss: 5.852340396245321

Epoch: 11
Training loss: 1.272557020187378 / Valid loss: 5.8566220714932395
Training loss: 2.1103506088256836 / Valid loss: 5.864671618597848
Training loss: 1.9602439403533936 / Valid loss: 5.893105929238455
Training loss: 2.187812328338623 / Valid loss: 5.934987442834037
Training loss: 2.963822841644287 / Valid loss: 5.9029154754820325

Epoch: 12
Training loss: 2.17775297164917 / Valid loss: 5.92295043582008
Training loss: 1.8289077281951904 / Valid loss: 5.925527438663301
Training loss: 2.509678363800049 / Valid loss: 5.9553358827318466
Training loss: 2.326040506362915 / Valid loss: 5.942293809709095
Training loss: 1.6672286987304688 / Valid loss: 5.958610743568057

Epoch: 13
Training loss: 2.672255277633667 / Valid loss: 5.95093777520316
Training loss: 1.714514970779419 / Valid loss: 5.960371094658261
Training loss: 1.9917669296264648 / Valid loss: 5.970983169192359
Training loss: 1.1703284978866577 / Valid loss: 6.001695925848825
Training loss: 2.0449376106262207 / Valid loss: 6.016330448786418

Epoch: 14
Training loss: 1.8099143505096436 / Valid loss: 6.021007360730852
Training loss: 1.588735580444336 / Valid loss: 6.016606471652076
Training loss: 1.644413709640503 / Valid loss: 6.035885433923631
Training loss: 1.6314353942871094 / Valid loss: 6.038340489069621
Training loss: 1.4690899848937988 / Valid loss: 6.027055747168404

Epoch: 15
Training loss: 1.5127416849136353 / Valid loss: 6.070497867039272
Training loss: 1.6044634580612183 / Valid loss: 6.08116165342785
Training loss: 1.7960789203643799 / Valid loss: 6.089721865881057
Training loss: 2.1066462993621826 / Valid loss: 6.075113532656715
Training loss: 1.1147091388702393 / Valid loss: 6.0925258363996235

Epoch: 16
Training loss: 0.9127948880195618 / Valid loss: 6.114755998338972
Training loss: 0.965216338634491 / Valid loss: 6.126875777471633
Training loss: 0.9968759417533875 / Valid loss: 6.091260081245785
Training loss: 1.1609750986099243 / Valid loss: 6.153518549601237
Training loss: 2.2623841762542725 / Valid loss: 6.116705642427717

Epoch: 17
Training loss: 1.5766048431396484 / Valid loss: 6.122692539578392
Training loss: 0.8615378141403198 / Valid loss: 6.182141839890253
Training loss: 1.5802875757217407 / Valid loss: 6.159394316446214
Training loss: 1.272554874420166 / Valid loss: 6.141305596487863
Training loss: 1.1412055492401123 / Valid loss: 6.179976733525594

Epoch: 18
Training loss: 1.582308292388916 / Valid loss: 6.160323086239043
Training loss: 1.4641190767288208 / Valid loss: 6.2007225854056225
Training loss: 1.1274421215057373 / Valid loss: 6.175479198637463
Training loss: 1.504333257675171 / Valid loss: 6.223029216130574
Training loss: 0.9012169241905212 / Valid loss: 6.21997770127796

Epoch: 19
Training loss: 1.7114675045013428 / Valid loss: 6.225066232681274
Training loss: 0.857996940612793 / Valid loss: 6.237843181973412
Training loss: 1.2194067239761353 / Valid loss: 6.267077750251406
Training loss: 1.2664812803268433 / Valid loss: 6.276083410353888

Epoch: 20
Training loss: 1.2509760856628418 / Valid loss: 6.2463507652282715
Training loss: 1.552217960357666 / Valid loss: 6.251105996540614
Training loss: 0.770622730255127 / Valid loss: 6.25056932540167
Training loss: 1.1724815368652344 / Valid loss: 6.2821432318006245
Training loss: 1.3976631164550781 / Valid loss: 6.275702812558129

Epoch: 21
Training loss: 0.7162001132965088 / Valid loss: 6.2809153852008635
Training loss: 1.0175950527191162 / Valid loss: 6.277634938557942
Training loss: 1.2257411479949951 / Valid loss: 6.293194130488804
Training loss: 0.989205002784729 / Valid loss: 6.334260050455729
Training loss: 1.069258451461792 / Valid loss: 6.321141833350772

Epoch: 22
Training loss: 0.8089228868484497 / Valid loss: 6.326416154134841
Training loss: 0.7390959858894348 / Valid loss: 6.3563255696069625
Training loss: 0.8457450270652771 / Valid loss: 6.3170883746374225
Training loss: 0.7006126642227173 / Valid loss: 6.366050779251825
Training loss: 0.9033032655715942 / Valid loss: 6.306489951269967

Epoch: 23
Training loss: 0.966403603553772 / Valid loss: 6.318944181714739
Training loss: 1.3177956342697144 / Valid loss: 6.371384656996954
Training loss: 0.9249559044837952 / Valid loss: 6.35073413848877
Training loss: 1.2170677185058594 / Valid loss: 6.354317864917573
Training loss: 0.984456479549408 / Valid loss: 6.394049521854946

Epoch: 24
Training loss: 0.47258907556533813 / Valid loss: 6.370372735886347
Training loss: 0.9670988321304321 / Valid loss: 6.377138973417736
Training loss: 1.4672095775604248 / Valid loss: 6.347415590286255
Training loss: 0.7714106440544128 / Valid loss: 6.378341715676444
Training loss: 0.592970073223114 / Valid loss: 6.380648517608643

Epoch: 25
Training loss: 1.108354091644287 / Valid loss: 6.383794689178467
Training loss: 0.8874143958091736 / Valid loss: 6.384764278502692
Training loss: 0.9665346741676331 / Valid loss: 6.405253221875145
Training loss: 0.7102394700050354 / Valid loss: 6.391985466366722
Training loss: 0.7362215518951416 / Valid loss: 6.39168518611363

Epoch: 26
Training loss: 0.5664475560188293 / Valid loss: 6.413247494470506
Training loss: 1.1193420886993408 / Valid loss: 6.402540252322242
Training loss: 0.49550700187683105 / Valid loss: 6.456839307149251
Training loss: 0.7396376132965088 / Valid loss: 6.443589194615682
Training loss: 0.7674517631530762 / Valid loss: 6.436794360478719

Epoch: 27
Training loss: 1.3134715557098389 / Valid loss: 6.418485898063296
Training loss: 1.220839500427246 / Valid loss: 6.4310479232243125
Training loss: 0.8270434141159058 / Valid loss: 6.47019030707223
Training loss: 0.2644020617008209 / Valid loss: 6.454899744760422
Training loss: 0.5198768377304077 / Valid loss: 6.4619958832150415

Epoch: 28
Training loss: 0.45431017875671387 / Valid loss: 6.4554729052952355
Training loss: 0.8564454317092896 / Valid loss: 6.499153906958444
Training loss: 1.2904177904129028 / Valid loss: 6.46607913062686
Training loss: 0.6554880738258362 / Valid loss: 6.5165792714981805
Training loss: 0.5980106592178345 / Valid loss: 6.482405063084194

Epoch: 29
Training loss: 1.0028412342071533 / Valid loss: 6.514881937844413
Training loss: 0.5612534284591675 / Valid loss: 6.518194198608398
Training loss: 0.98646080493927 / Valid loss: 6.497687748500279
Training loss: 0.764761209487915 / Valid loss: 6.457868076506115

Epoch: 30
Training loss: 0.6276448369026184 / Valid loss: 6.484727298645746
Training loss: 0.5415441393852234 / Valid loss: 6.495795708610898
Training loss: 1.0753895044326782 / Valid loss: 6.469746930258615
Training loss: 1.0277366638183594 / Valid loss: 6.536564733868554
Training loss: 0.49992406368255615 / Valid loss: 6.482225245521182

Epoch: 31
Training loss: 0.629051923751831 / Valid loss: 6.528934982844762
Training loss: 0.7949792742729187 / Valid loss: 6.516686450867426
Training loss: 0.31347572803497314 / Valid loss: 6.5347968850816995
Training loss: 0.5451860427856445 / Valid loss: 6.539612440835862
Training loss: 1.1766440868377686 / Valid loss: 6.512470960617065

Epoch: 32
Training loss: 0.4193476140499115 / Valid loss: 6.526028247106643
Training loss: 0.6161296367645264 / Valid loss: 6.534949824923561
Training loss: 0.5450776815414429 / Valid loss: 6.51588424500965
Training loss: 0.37699729204177856 / Valid loss: 6.563296849387033
Training loss: 0.8325871229171753 / Valid loss: 6.541746695836385

Epoch: 33
Training loss: 0.725821316242218 / Valid loss: 6.524879918779646
Training loss: 0.25073760747909546 / Valid loss: 6.539357285272508
Training loss: 0.38710522651672363 / Valid loss: 6.562025065649124
Training loss: 0.39140555262565613 / Valid loss: 6.589246727171398
Training loss: 0.26015999913215637 / Valid loss: 6.597318790072486

Epoch: 34
Training loss: 0.3364987373352051 / Valid loss: 6.556257470448812
Training loss: 0.46697789430618286 / Valid loss: 6.546359430040632
Training loss: 0.26726222038269043 / Valid loss: 6.5973423412867955
Training loss: 0.43828558921813965 / Valid loss: 6.553933733985538
Training loss: 0.5344254374504089 / Valid loss: 6.550720802942911

Epoch: 35
Training loss: 0.3410457670688629 / Valid loss: 6.593110048203242
Training loss: 0.35323968529701233 / Valid loss: 6.555710542769659
Training loss: 0.44375309348106384 / Valid loss: 6.612501453218006
Training loss: 0.4325410723686218 / Valid loss: 6.6046223685854955
Training loss: 0.4666469693183899 / Valid loss: 6.593872619810559

Epoch: 36
Training loss: 0.18374580144882202 / Valid loss: 6.611550717126756
Training loss: 0.195890411734581 / Valid loss: 6.612399746122814
Training loss: 0.39075326919555664 / Valid loss: 6.596742561885288
Training loss: 0.516693115234375 / Valid loss: 6.594473243895031
Training loss: 0.39704054594039917 / Valid loss: 6.637535992122832

Epoch: 37
Training loss: 0.22647303342819214 / Valid loss: 6.593509678613572
Training loss: 0.7649720907211304 / Valid loss: 6.664076979955038
Training loss: 0.23789317905902863 / Valid loss: 6.61992278780256
Training loss: 0.4573094844818115 / Valid loss: 6.612817192077637
Training loss: 0.3551427125930786 / Valid loss: 6.612684581393287

Epoch: 38
Training loss: 0.27184736728668213 / Valid loss: 6.610507084074475
Training loss: 0.24477607011795044 / Valid loss: 6.600998823983328
Training loss: 0.3768617510795593 / Valid loss: 6.626941167740595
Training loss: 0.21788325905799866 / Valid loss: 6.611195523398263
Training loss: 0.4300714135169983 / Valid loss: 6.630119028545561

Epoch: 39
Training loss: 0.3507128059864044 / Valid loss: 6.625592036474319
Training loss: 0.28492170572280884 / Valid loss: 6.648538818813505
Training loss: 0.40043556690216064 / Valid loss: 6.627505806514195
Training loss: 0.18312153220176697 / Valid loss: 6.632551118305751
ModuleList(
  (0): Linear(in_features=31191, out_features=248, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=248, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 1300): 5.38208296866644
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0.0
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.986988067626953 / Valid loss: 16.476813561575753
Model is saved in epoch 0, overall batch: 0
Training loss: 19.34998321533203 / Valid loss: 16.06934731801351
Model is saved in epoch 0, overall batch: 100
Training loss: 10.80003547668457 / Valid loss: 15.685470281328474
Model is saved in epoch 0, overall batch: 200
Training loss: 16.251977920532227 / Valid loss: 15.302157338460287
Model is saved in epoch 0, overall batch: 300
Training loss: 12.382561683654785 / Valid loss: 14.930970473516554
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 12.209218978881836 / Valid loss: 14.565777052016486
Model is saved in epoch 1, overall batch: 500
Training loss: 11.344770431518555 / Valid loss: 14.24381088983445
Model is saved in epoch 1, overall batch: 600
Training loss: 13.974422454833984 / Valid loss: 13.929114759536017
Model is saved in epoch 1, overall batch: 700
Training loss: 14.13243293762207 / Valid loss: 13.619262872423445
Model is saved in epoch 1, overall batch: 800
Training loss: 17.79330825805664 / Valid loss: 13.302658203669957
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 11.528554916381836 / Valid loss: 13.01042663029262
Model is saved in epoch 2, overall batch: 1000
Training loss: 8.892904281616211 / Valid loss: 12.758256862277076
Model is saved in epoch 2, overall batch: 1100
Training loss: 12.997526168823242 / Valid loss: 12.491826629638672
Model is saved in epoch 2, overall batch: 1200
Training loss: 9.812393188476562 / Valid loss: 12.240631880078997
Model is saved in epoch 2, overall batch: 1300
Training loss: 12.961278915405273 / Valid loss: 11.989168403262184
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 9.544395446777344 / Valid loss: 11.744550727662586
Model is saved in epoch 3, overall batch: 1500
Training loss: 7.705786228179932 / Valid loss: 11.51660354705084
Model is saved in epoch 3, overall batch: 1600
Training loss: 8.84141731262207 / Valid loss: 11.31271125702631
Model is saved in epoch 3, overall batch: 1700
Training loss: 11.76883316040039 / Valid loss: 11.11713497071039
Model is saved in epoch 3, overall batch: 1800
Training loss: 14.732194900512695 / Valid loss: 10.914794558570499
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 15.636719703674316 / Valid loss: 10.726499975295294
Model is saved in epoch 4, overall batch: 2000
Training loss: 13.22612190246582 / Valid loss: 10.53011759349278
Model is saved in epoch 4, overall batch: 2100
Training loss: 12.822949409484863 / Valid loss: 10.36131280263265
Model is saved in epoch 4, overall batch: 2200
Training loss: 12.320323944091797 / Valid loss: 10.18590676898048
Model is saved in epoch 4, overall batch: 2300
Training loss: 6.173120021820068 / Valid loss: 10.015404428754534
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 5.468993663787842 / Valid loss: 9.851913969857351
Model is saved in epoch 5, overall batch: 2500
Training loss: 12.905267715454102 / Valid loss: 9.72491060892741
Model is saved in epoch 5, overall batch: 2600
Training loss: 7.6637749671936035 / Valid loss: 9.585472470238095
Model is saved in epoch 5, overall batch: 2700
Training loss: 10.727974891662598 / Valid loss: 9.446137223924909
Model is saved in epoch 5, overall batch: 2800
Training loss: 6.168590068817139 / Valid loss: 9.297330107007708
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 6.677627086639404 / Valid loss: 9.172478975568499
Model is saved in epoch 6, overall batch: 3000
Training loss: 10.576658248901367 / Valid loss: 9.063246858687627
Model is saved in epoch 6, overall batch: 3100
Training loss: 6.516392707824707 / Valid loss: 8.936544763474238
Model is saved in epoch 6, overall batch: 3200
Training loss: 8.662240982055664 / Valid loss: 8.816163866860526
Model is saved in epoch 6, overall batch: 3300
Training loss: 4.506814002990723 / Valid loss: 8.718346341451008
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 7.882602691650391 / Valid loss: 8.608402293069021
Model is saved in epoch 7, overall batch: 3500
Training loss: 10.497773170471191 / Valid loss: 8.506845383417039
Model is saved in epoch 7, overall batch: 3600
Training loss: 6.063195705413818 / Valid loss: 8.401728044237409
Model is saved in epoch 7, overall batch: 3700
Training loss: 8.186863899230957 / Valid loss: 8.299782893771217
Model is saved in epoch 7, overall batch: 3800
Training loss: 7.982937812805176 / Valid loss: 8.22919889177595
Model is saved in epoch 7, overall batch: 3900

Epoch: 8
Training loss: 8.706058502197266 / Valid loss: 8.127307982671828
Model is saved in epoch 8, overall batch: 4000
Training loss: 9.494544982910156 / Valid loss: 8.044699961798532
Model is saved in epoch 8, overall batch: 4100
Training loss: 7.6849589347839355 / Valid loss: 7.981806491670154
Model is saved in epoch 8, overall batch: 4200
Training loss: 11.597820281982422 / Valid loss: 7.907823135739281
Model is saved in epoch 8, overall batch: 4300
Training loss: 8.414676666259766 / Valid loss: 7.834542195002238
Model is saved in epoch 8, overall batch: 4400

Epoch: 9
Training loss: 8.724796295166016 / Valid loss: 7.757895038241432
Model is saved in epoch 9, overall batch: 4500
Training loss: 8.246739387512207 / Valid loss: 7.6940325827825635
Model is saved in epoch 9, overall batch: 4600
Training loss: 9.369951248168945 / Valid loss: 7.616812637874058
Model is saved in epoch 9, overall batch: 4700
Training loss: 7.502538681030273 / Valid loss: 7.564053017752511
Model is saved in epoch 9, overall batch: 4800

Epoch: 10
Training loss: 6.948744297027588 / Valid loss: 7.508863689785912
Model is saved in epoch 10, overall batch: 4900
Training loss: 8.99344253540039 / Valid loss: 7.444452862512498
Model is saved in epoch 10, overall batch: 5000
Training loss: 10.190987586975098 / Valid loss: 7.381536949248541
Model is saved in epoch 10, overall batch: 5100
Training loss: 6.220463275909424 / Valid loss: 7.336686790557135
Model is saved in epoch 10, overall batch: 5200
Training loss: 6.261064529418945 / Valid loss: 7.279040145874023
Model is saved in epoch 10, overall batch: 5300

Epoch: 11
Training loss: 8.717171669006348 / Valid loss: 7.234562360672724
Model is saved in epoch 11, overall batch: 5400
Training loss: 7.642792224884033 / Valid loss: 7.18451494943528
Model is saved in epoch 11, overall batch: 5500
Training loss: 10.926119804382324 / Valid loss: 7.152126745950608
Model is saved in epoch 11, overall batch: 5600
Training loss: 6.130292892456055 / Valid loss: 7.103217068172636
Model is saved in epoch 11, overall batch: 5700
Training loss: 5.269585132598877 / Valid loss: 7.042777631396339
Model is saved in epoch 11, overall batch: 5800

Epoch: 12
Training loss: 8.753782272338867 / Valid loss: 7.024901019959223
Model is saved in epoch 12, overall batch: 5900
Training loss: 5.308651924133301 / Valid loss: 6.98361515771775
Model is saved in epoch 12, overall batch: 6000
Training loss: 4.498931884765625 / Valid loss: 6.947967704137167
Model is saved in epoch 12, overall batch: 6100
Training loss: 8.780046463012695 / Valid loss: 6.910114399592081
Model is saved in epoch 12, overall batch: 6200
Training loss: 5.650957107543945 / Valid loss: 6.866040767942156
Model is saved in epoch 12, overall batch: 6300

Epoch: 13
Training loss: 5.082635879516602 / Valid loss: 6.844355810256231
Model is saved in epoch 13, overall batch: 6400
Training loss: 3.195251703262329 / Valid loss: 6.813400468372163
Model is saved in epoch 13, overall batch: 6500
Training loss: 10.482081413269043 / Valid loss: 6.782515823273432
Model is saved in epoch 13, overall batch: 6600
Training loss: 7.166502952575684 / Valid loss: 6.753756950015114
Model is saved in epoch 13, overall batch: 6700
Training loss: 7.201663017272949 / Valid loss: 6.726737733114334
Model is saved in epoch 13, overall batch: 6800

Epoch: 14
Training loss: 8.198846817016602 / Valid loss: 6.697986130487351
Model is saved in epoch 14, overall batch: 6900
Training loss: 5.18203592300415 / Valid loss: 6.662895025525774
Model is saved in epoch 14, overall batch: 7000
Training loss: 7.164454460144043 / Valid loss: 6.647327100662958
Model is saved in epoch 14, overall batch: 7100
Training loss: 7.807003498077393 / Valid loss: 6.625204540434337
Model is saved in epoch 14, overall batch: 7200
Training loss: 6.355016708374023 / Valid loss: 6.601478204273042
Model is saved in epoch 14, overall batch: 7300

Epoch: 15
Training loss: 6.88370418548584 / Valid loss: 6.570729548590524
Model is saved in epoch 15, overall batch: 7400
Training loss: 7.474422454833984 / Valid loss: 6.549579245703561
Model is saved in epoch 15, overall batch: 7500
Training loss: 4.888495445251465 / Valid loss: 6.532724855059669
Model is saved in epoch 15, overall batch: 7600
Training loss: 5.456485748291016 / Valid loss: 6.516284947168259
Model is saved in epoch 15, overall batch: 7700
Training loss: 8.253273963928223 / Valid loss: 6.496840445200602
Model is saved in epoch 15, overall batch: 7800

Epoch: 16
Training loss: 5.063161849975586 / Valid loss: 6.471500819069998
Model is saved in epoch 16, overall batch: 7900
Training loss: 8.951501846313477 / Valid loss: 6.450497747602917
Model is saved in epoch 16, overall batch: 8000
Training loss: 7.076356410980225 / Valid loss: 6.445485914321173
Model is saved in epoch 16, overall batch: 8100
Training loss: 6.965296745300293 / Valid loss: 6.419389216105143
Model is saved in epoch 16, overall batch: 8200
Training loss: 7.753829002380371 / Valid loss: 6.414013478869483
Model is saved in epoch 16, overall batch: 8300

Epoch: 17
Training loss: 10.968244552612305 / Valid loss: 6.398470497131347
Model is saved in epoch 17, overall batch: 8400
Training loss: 6.708856582641602 / Valid loss: 6.375894632793608
Model is saved in epoch 17, overall batch: 8500
Training loss: 5.771501541137695 / Valid loss: 6.371690897714524
Model is saved in epoch 17, overall batch: 8600
Training loss: 4.811310768127441 / Valid loss: 6.357729979923794
Model is saved in epoch 17, overall batch: 8700
Training loss: 7.276472091674805 / Valid loss: 6.334237348465693
Model is saved in epoch 17, overall batch: 8800

Epoch: 18
Training loss: 5.817819118499756 / Valid loss: 6.332693706239973
Model is saved in epoch 18, overall batch: 8900
Training loss: 6.118721961975098 / Valid loss: 6.314763682229178
Model is saved in epoch 18, overall batch: 9000
Training loss: 5.194003105163574 / Valid loss: 6.307945562544323
Model is saved in epoch 18, overall batch: 9100
Training loss: 6.413865089416504 / Valid loss: 6.298468399047851
Model is saved in epoch 18, overall batch: 9200
Training loss: 6.49893856048584 / Valid loss: 6.288455992653256
Model is saved in epoch 18, overall batch: 9300

Epoch: 19
Training loss: 5.580748558044434 / Valid loss: 6.267063613164993
Model is saved in epoch 19, overall batch: 9400
Training loss: 9.5861234664917 / Valid loss: 6.2613053730555945
Model is saved in epoch 19, overall batch: 9500
Training loss: 5.106604099273682 / Valid loss: 6.255815796625047
Model is saved in epoch 19, overall batch: 9600
Training loss: 7.750758647918701 / Valid loss: 6.240603274390811
Model is saved in epoch 19, overall batch: 9700

Epoch: 20
Training loss: 5.821217060089111 / Valid loss: 6.231962653568813
Model is saved in epoch 20, overall batch: 9800
Training loss: 6.33835506439209 / Valid loss: 6.231915128798712
Model is saved in epoch 20, overall batch: 9900
Training loss: 5.940007209777832 / Valid loss: 6.221218788056147
Model is saved in epoch 20, overall batch: 10000
Training loss: 7.044270038604736 / Valid loss: 6.20472362836202
Model is saved in epoch 20, overall batch: 10100
Training loss: 5.893443584442139 / Valid loss: 6.202588885171073
Model is saved in epoch 20, overall batch: 10200

Epoch: 21
Training loss: 7.1944708824157715 / Valid loss: 6.197809398741949
Model is saved in epoch 21, overall batch: 10300
Training loss: 5.305282115936279 / Valid loss: 6.195106506347656
Model is saved in epoch 21, overall batch: 10400
Training loss: 5.901378154754639 / Valid loss: 6.187827494030907
Model is saved in epoch 21, overall batch: 10500
Training loss: 7.0728583335876465 / Valid loss: 6.1809446857089085
Model is saved in epoch 21, overall batch: 10600
Training loss: 4.460885047912598 / Valid loss: 6.176180880410331
Model is saved in epoch 21, overall batch: 10700

Epoch: 22
Training loss: 7.74196195602417 / Valid loss: 6.164234481539045
Model is saved in epoch 22, overall batch: 10800
Training loss: 7.105523109436035 / Valid loss: 6.162012286413283
Model is saved in epoch 22, overall batch: 10900
Training loss: 5.673976898193359 / Valid loss: 6.1552336987994964
Model is saved in epoch 22, overall batch: 11000
Training loss: 6.026602268218994 / Valid loss: 6.152835800534203
Model is saved in epoch 22, overall batch: 11100
Training loss: 5.257007122039795 / Valid loss: 6.140394576390585
Model is saved in epoch 22, overall batch: 11200

Epoch: 23
Training loss: 5.425826072692871 / Valid loss: 6.142774288994925
Training loss: 7.564735412597656 / Valid loss: 6.139010445276896
Model is saved in epoch 23, overall batch: 11400
Training loss: 6.992520809173584 / Valid loss: 6.132829900014968
Model is saved in epoch 23, overall batch: 11500
Training loss: 6.375755310058594 / Valid loss: 6.130791137332008
Model is saved in epoch 23, overall batch: 11600
Training loss: 5.6465654373168945 / Valid loss: 6.124622213272821
Model is saved in epoch 23, overall batch: 11700

Epoch: 24
Training loss: 5.176276206970215 / Valid loss: 6.120362479346139
Model is saved in epoch 24, overall batch: 11800
Training loss: 6.997013092041016 / Valid loss: 6.110430136181059
Model is saved in epoch 24, overall batch: 11900
Training loss: 3.8620007038116455 / Valid loss: 6.112010871796381
Training loss: 6.758889198303223 / Valid loss: 6.10588390486581
Model is saved in epoch 24, overall batch: 12100
Training loss: 5.927193641662598 / Valid loss: 6.0998675300961445
Model is saved in epoch 24, overall batch: 12200

Epoch: 25
Training loss: 5.555928707122803 / Valid loss: 6.093221398762294
Model is saved in epoch 25, overall batch: 12300
Training loss: 4.464442253112793 / Valid loss: 6.098486044293359
Training loss: 7.681815147399902 / Valid loss: 6.098260861351377
Training loss: 5.8048601150512695 / Valid loss: 6.092652098337809
Model is saved in epoch 25, overall batch: 12600
Training loss: 6.5430803298950195 / Valid loss: 6.083666015806652
Model is saved in epoch 25, overall batch: 12700

Epoch: 26
Training loss: 4.301372051239014 / Valid loss: 6.085673924854824
Training loss: 3.922286033630371 / Valid loss: 6.08549477940514
Training loss: 5.263583183288574 / Valid loss: 6.082155318487258
Model is saved in epoch 26, overall batch: 13000
Training loss: 5.877579689025879 / Valid loss: 6.080836645762125
Model is saved in epoch 26, overall batch: 13100
Training loss: 4.1223320960998535 / Valid loss: 6.077714431853521
Model is saved in epoch 26, overall batch: 13200

Epoch: 27
Training loss: 5.612262725830078 / Valid loss: 6.074283073061989
Model is saved in epoch 27, overall batch: 13300
Training loss: 5.978845596313477 / Valid loss: 6.07274287995838
Model is saved in epoch 27, overall batch: 13400
Training loss: 5.682406902313232 / Valid loss: 6.072894427889869
Training loss: 6.9055938720703125 / Valid loss: 6.067263230823335
Model is saved in epoch 27, overall batch: 13600
Training loss: 6.538854598999023 / Valid loss: 6.061771585827782
Model is saved in epoch 27, overall batch: 13700

Epoch: 28
Training loss: 5.831943511962891 / Valid loss: 6.064290287381127
Training loss: 5.545108795166016 / Valid loss: 6.055346502576556
Model is saved in epoch 28, overall batch: 13900
Training loss: 5.024960994720459 / Valid loss: 6.059802609398251
Training loss: 6.7812066078186035 / Valid loss: 6.052473685854957
Model is saved in epoch 28, overall batch: 14100
Training loss: 5.48179292678833 / Valid loss: 6.057292261577788

Epoch: 29
Training loss: 6.489348411560059 / Valid loss: 6.05198837007795
Model is saved in epoch 29, overall batch: 14300
Training loss: 6.086952209472656 / Valid loss: 6.0481865701221285
Model is saved in epoch 29, overall batch: 14400
Training loss: 6.884318828582764 / Valid loss: 6.0515639645712715
Training loss: 6.370561122894287 / Valid loss: 6.048568541663034

Epoch: 30
Training loss: 6.481175899505615 / Valid loss: 6.043581399463472
Model is saved in epoch 30, overall batch: 14700
Training loss: 6.313818454742432 / Valid loss: 6.048681738263085
Training loss: 5.972087860107422 / Valid loss: 6.048769015357608
Training loss: 5.346998691558838 / Valid loss: 6.046830749511718
Training loss: 6.577076435089111 / Valid loss: 6.047090537207467

Epoch: 31
Training loss: 7.225584030151367 / Valid loss: 6.0441770803360715
Training loss: 6.518143177032471 / Valid loss: 6.037676620483398
Model is saved in epoch 31, overall batch: 15300
Training loss: 6.171324729919434 / Valid loss: 6.036115952900478
Model is saved in epoch 31, overall batch: 15400
Training loss: 7.216606616973877 / Valid loss: 6.040469778151739
Training loss: 7.862129211425781 / Valid loss: 6.0327990441095265
Model is saved in epoch 31, overall batch: 15600

Epoch: 32
Training loss: 4.881524085998535 / Valid loss: 6.034504681541806
Training loss: 6.603017807006836 / Valid loss: 6.036637099583944
Training loss: 6.406352996826172 / Valid loss: 6.036001228150868
Training loss: 5.73724365234375 / Valid loss: 6.033187066941034
Training loss: 3.7516679763793945 / Valid loss: 6.036286812736875

Epoch: 33
Training loss: 7.364428520202637 / Valid loss: 6.034087764649164
Training loss: 4.132911682128906 / Valid loss: 6.030408834275745
Model is saved in epoch 33, overall batch: 16300
Training loss: 6.785848617553711 / Valid loss: 6.03228869211106
Training loss: 5.56220817565918 / Valid loss: 6.023902216411773
Model is saved in epoch 33, overall batch: 16500
Training loss: 7.850351810455322 / Valid loss: 6.032742541176932

Epoch: 34
Training loss: 5.503496170043945 / Valid loss: 6.031330953325544
Training loss: 5.516753673553467 / Valid loss: 6.03019453003293
Training loss: 5.201821327209473 / Valid loss: 6.021613645553589
Model is saved in epoch 34, overall batch: 16900
Training loss: 5.568030834197998 / Valid loss: 6.029466424669538
Training loss: 6.518052101135254 / Valid loss: 6.020131724221366
Model is saved in epoch 34, overall batch: 17100

Epoch: 35
Training loss: 5.805660724639893 / Valid loss: 6.023340166182745
Training loss: 5.796850204467773 / Valid loss: 6.025071634565081
Training loss: 5.837444305419922 / Valid loss: 6.027455030168806
Training loss: 8.682124137878418 / Valid loss: 6.020368301300776
Training loss: 8.183069229125977 / Valid loss: 6.010874852680025
Model is saved in epoch 35, overall batch: 17600

Epoch: 36
Training loss: 5.716147422790527 / Valid loss: 6.0233515308016825
Training loss: 7.999633312225342 / Valid loss: 6.009641878945487
Model is saved in epoch 36, overall batch: 17800
Training loss: 4.748312950134277 / Valid loss: 6.018738085883005
Training loss: 4.912639141082764 / Valid loss: 6.0242931865510485
Training loss: 4.607892036437988 / Valid loss: 6.02408275604248

Epoch: 37
Training loss: 4.816366672515869 / Valid loss: 6.021961321149553
Training loss: 5.884838104248047 / Valid loss: 6.0222000326429095
Training loss: 5.932965278625488 / Valid loss: 6.017793335233416
Training loss: 5.9444074630737305 / Valid loss: 6.012081559499105
Training loss: 5.430987358093262 / Valid loss: 6.014450890677316

Epoch: 38
Training loss: 4.82620906829834 / Valid loss: 6.016681400934855
Training loss: 6.814694881439209 / Valid loss: 6.019310851324172
Training loss: 6.050845146179199 / Valid loss: 6.016461538133167
Training loss: 5.829789638519287 / Valid loss: 6.017507305599394
Training loss: 7.529239654541016 / Valid loss: 6.016030638558524

Epoch: 39
Training loss: 4.258238315582275 / Valid loss: 6.01683672723316
Training loss: 6.963862419128418 / Valid loss: 6.018061306363061
Training loss: 7.167043685913086 / Valid loss: 6.016359762918381
Training loss: 6.909680366516113 / Valid loss: 6.0185951482682
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 17800): 5.843063717796689
Training regression with following parameters:
dnn_hidden_units : 
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 40
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 1e-05
momentum : 0
embedder : TFIDF
verbose : False
reduced_classes : False
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)

Epoch: 0
Training loss: 20.986988067626953 / Valid loss: 16.476813561575753
Model is saved in epoch 0, overall batch: 0
Training loss: 19.34998321533203 / Valid loss: 16.06934731801351
Model is saved in epoch 0, overall batch: 100
Training loss: 10.80003547668457 / Valid loss: 15.685470353989373
Model is saved in epoch 0, overall batch: 200
Training loss: 16.251977920532227 / Valid loss: 15.3021576381865
Model is saved in epoch 0, overall batch: 300
Training loss: 12.382561683654785 / Valid loss: 14.93097084590367
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 12.209218978881836 / Valid loss: 14.5657775697254
Model is saved in epoch 1, overall batch: 500
Training loss: 11.344770431518555 / Valid loss: 14.243811816260928
Model is saved in epoch 1, overall batch: 600
Training loss: 13.974422454833984 / Valid loss: 13.929115749540783
Model is saved in epoch 1, overall batch: 700
Training loss: 14.132433891296387 / Valid loss: 13.619264103117443
Model is saved in epoch 1, overall batch: 800
Training loss: 17.79330825805664 / Valid loss: 13.302659570603144
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 11.528556823730469 / Valid loss: 13.010428001767114
Model is saved in epoch 2, overall batch: 1000
Training loss: 8.892904281616211 / Valid loss: 12.758258102053688
Model is saved in epoch 2, overall batch: 1100
Training loss: 12.997528076171875 / Valid loss: 12.491828423454647
Model is saved in epoch 2, overall batch: 1200
Training loss: 9.812395095825195 / Valid loss: 12.240633682977586
Model is saved in epoch 2, overall batch: 1300
Training loss: 12.961280822753906 / Valid loss: 11.989170424143474
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 9.544397354125977 / Valid loss: 11.744553075517928
Model is saved in epoch 3, overall batch: 1500
Training loss: 7.7057881355285645 / Valid loss: 11.516606099264962
Model is saved in epoch 3, overall batch: 1600
Training loss: 8.841418266296387 / Valid loss: 11.312714040847052
Model is saved in epoch 3, overall batch: 1700
Training loss: 11.76883602142334 / Valid loss: 11.117137618291945
Model is saved in epoch 3, overall batch: 1800
Training loss: 14.732199668884277 / Valid loss: 10.914797855558849
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 15.636724472045898 / Valid loss: 10.726503562927245
Model is saved in epoch 4, overall batch: 2000
Training loss: 13.226126670837402 / Valid loss: 10.530121162959508
Model is saved in epoch 4, overall batch: 2100
Training loss: 12.822952270507812 / Valid loss: 10.361316789899554
Model is saved in epoch 4, overall batch: 2200
Training loss: 12.320329666137695 / Valid loss: 10.185912159511021
Model is saved in epoch 4, overall batch: 2300
Training loss: 6.173124313354492 / Valid loss: 10.015411131722587
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 5.468997955322266 / Valid loss: 9.851920550210135
Model is saved in epoch 5, overall batch: 2500
Training loss: 12.905279159545898 / Valid loss: 9.72491877419608
Model is saved in epoch 5, overall batch: 2600
Training loss: 7.663781642913818 / Valid loss: 9.585480585552396
Model is saved in epoch 5, overall batch: 2700
Training loss: 10.727983474731445 / Valid loss: 9.4461456208002
Model is saved in epoch 5, overall batch: 2800
Training loss: 6.1685967445373535 / Valid loss: 9.297338376726422
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 6.677635192871094 / Valid loss: 9.172487853822254
Model is saved in epoch 6, overall batch: 3000
Training loss: 10.576672554016113 / Valid loss: 9.063258148375011
Model is saved in epoch 6, overall batch: 3100
Training loss: 6.5164031982421875 / Valid loss: 8.93655629839216
Model is saved in epoch 6, overall batch: 3200
Training loss: 8.662252426147461 / Valid loss: 8.816175483521961
Model is saved in epoch 6, overall batch: 3300
Training loss: 4.506819725036621 / Valid loss: 8.718357635679698
Model is saved in epoch 6, overall batch: 3400

Epoch: 7
Training loss: 7.882614612579346 / Valid loss: 8.608413473765056
Model is saved in epoch 7, overall batch: 3500
Training loss: 10.497783660888672 / Valid loss: 8.506856318882534
Model is saved in epoch 7, overall batch: 3600
Training loss: 6.063205718994141 / Valid loss: 8.401738734472366
Model is saved in epoch 7, overall batch: 3700
Training loss: 8.186874389648438 / Valid loss: 8.299793375106084
Model is saved in epoch 7, overall batch: 3800
Training loss: 7.98294734954834 / Valid loss: 8.229209663754418
Model is saved in epoch 7, overall batch: 3900

Epoch: 8
Training loss: 8.706069946289062 / Valid loss: 8.127318768274217
Model is saved in epoch 8, overall batch: 4000
Training loss: 9.494556427001953 / Valid loss: 8.04471084958031
Model is saved in epoch 8, overall batch: 4100
Training loss: 7.684971332550049 / Valid loss: 7.9818176178705125
Model is saved in epoch 8, overall batch: 4200
Training loss: 11.597835540771484 / Valid loss: 7.907833994002569
Model is saved in epoch 8, overall batch: 4300
Training loss: 8.414689064025879 / Valid loss: 7.834552951086135
Model is saved in epoch 8, overall batch: 4400

Epoch: 9
Training loss: 8.72480583190918 / Valid loss: 7.757906146276564
Model is saved in epoch 9, overall batch: 4500
Training loss: 8.246749877929688 / Valid loss: 7.694043459211077
Model is saved in epoch 9, overall batch: 4600
Training loss: 9.369965553283691 / Valid loss: 7.616823768615722
Model is saved in epoch 9, overall batch: 4700
Training loss: 7.502552509307861 / Valid loss: 7.564064084915888
Model is saved in epoch 9, overall batch: 4800

Epoch: 10
Training loss: 6.948754787445068 / Valid loss: 7.508874811444964
Model is saved in epoch 10, overall batch: 4900
Training loss: 8.99345588684082 / Valid loss: 7.444463738941011
Model is saved in epoch 10, overall batch: 5000
Training loss: 10.191001892089844 / Valid loss: 7.381547607694354
Model is saved in epoch 10, overall batch: 5100
Training loss: 6.220472812652588 / Valid loss: 7.3366972287495935
Model is saved in epoch 10, overall batch: 5200
Training loss: 6.26107120513916 / Valid loss: 7.279050454639253
Model is saved in epoch 10, overall batch: 5300

Epoch: 11
Training loss: 8.717185974121094 / Valid loss: 7.234572923751104
Model is saved in epoch 11, overall batch: 5400
Training loss: 7.64280366897583 / Valid loss: 7.184525335402716
Model is saved in epoch 11, overall batch: 5500
Training loss: 10.926135063171387 / Valid loss: 7.152136950265794
Model is saved in epoch 11, overall batch: 5600
Training loss: 6.130299091339111 / Valid loss: 7.103227113542102
Model is saved in epoch 11, overall batch: 5700
Training loss: 5.26959228515625 / Valid loss: 7.0427874587831045
Model is saved in epoch 11, overall batch: 5800

Epoch: 12
Training loss: 8.753792762756348 / Valid loss: 7.0249105748676115
Model is saved in epoch 12, overall batch: 5900
Training loss: 5.308660984039307 / Valid loss: 6.98362550281343
Model is saved in epoch 12, overall batch: 6000
Training loss: 4.498939514160156 / Valid loss: 6.947978401184082
Model is saved in epoch 12, overall batch: 6100
Training loss: 8.78005599975586 / Valid loss: 6.910125219254267
Model is saved in epoch 12, overall batch: 6200
Training loss: 5.650968551635742 / Valid loss: 6.866051785151163
Model is saved in epoch 12, overall batch: 6300

Epoch: 13
Training loss: 5.082644462585449 / Valid loss: 6.844367099943615
Model is saved in epoch 13, overall batch: 6400
Training loss: 3.195253849029541 / Valid loss: 6.813411966959635
Model is saved in epoch 13, overall batch: 6500
Training loss: 10.482104301452637 / Valid loss: 6.782527065277099
Model is saved in epoch 13, overall batch: 6600
Training loss: 7.166512489318848 / Valid loss: 6.753768446331932
Model is saved in epoch 13, overall batch: 6700
Training loss: 7.201674461364746 / Valid loss: 6.726749402000791
Model is saved in epoch 13, overall batch: 6800

Epoch: 14
Training loss: 8.19886302947998 / Valid loss: 6.697997588203067
Model is saved in epoch 14, overall batch: 6900
Training loss: 5.182045936584473 / Valid loss: 6.662906228928339
Model is saved in epoch 14, overall batch: 7000
Training loss: 7.164467811584473 / Valid loss: 6.647338120142619
Model is saved in epoch 14, overall batch: 7100
Training loss: 7.807020664215088 / Valid loss: 6.625215287435623
Model is saved in epoch 14, overall batch: 7200
Training loss: 6.355027198791504 / Valid loss: 6.601488792328608
Model is saved in epoch 14, overall batch: 7300

Epoch: 15
Training loss: 6.883708953857422 / Valid loss: 6.570740345546177
Model is saved in epoch 15, overall batch: 7400
Training loss: 7.474435806274414 / Valid loss: 6.549589740662348
Model is saved in epoch 15, overall batch: 7500
Training loss: 4.888503551483154 / Valid loss: 6.5327356338500975
Model is saved in epoch 15, overall batch: 7600
Training loss: 5.456494331359863 / Valid loss: 6.516295498893374
Model is saved in epoch 15, overall batch: 7700
Training loss: 8.253289222717285 / Valid loss: 6.496850967407227
Model is saved in epoch 15, overall batch: 7800

Epoch: 16
Training loss: 5.06317138671875 / Valid loss: 6.471511200496129
Model is saved in epoch 16, overall batch: 7900
Training loss: 8.951519966125488 / Valid loss: 6.4505085945129395
Model is saved in epoch 16, overall batch: 8000
Training loss: 7.0763654708862305 / Valid loss: 6.445496559143066
Model is saved in epoch 16, overall batch: 8100
Training loss: 6.96530818939209 / Valid loss: 6.419399924505324
Model is saved in epoch 16, overall batch: 8200
Training loss: 7.753844261169434 / Valid loss: 6.414024064654396
Model is saved in epoch 16, overall batch: 8300

Epoch: 17
Training loss: 10.96826171875 / Valid loss: 6.398481112434751
Model is saved in epoch 17, overall batch: 8400
Training loss: 6.708868026733398 / Valid loss: 6.37590495745341
Model is saved in epoch 17, overall batch: 8500
Training loss: 5.771506309509277 / Valid loss: 6.3717014017559235
Model is saved in epoch 17, overall batch: 8600
Training loss: 4.811317443847656 / Valid loss: 6.357740288689023
Model is saved in epoch 17, overall batch: 8700
Training loss: 7.276485443115234 / Valid loss: 6.33424738021124
Model is saved in epoch 17, overall batch: 8800

Epoch: 18
Training loss: 5.817826271057129 / Valid loss: 6.332703885577974
Model is saved in epoch 18, overall batch: 8900
Training loss: 6.118730545043945 / Valid loss: 6.314773614065988
Model is saved in epoch 18, overall batch: 9000
Training loss: 5.1940155029296875 / Valid loss: 6.307955671492077
Model is saved in epoch 18, overall batch: 9100
Training loss: 6.413881301879883 / Valid loss: 6.298478528431484
Model is saved in epoch 18, overall batch: 9200
Training loss: 6.4989519119262695 / Valid loss: 6.288465917678106
Model is saved in epoch 18, overall batch: 9300

Epoch: 19
Training loss: 5.580759525299072 / Valid loss: 6.267073608580089
Model is saved in epoch 19, overall batch: 9400
Training loss: 9.58614730834961 / Valid loss: 6.261315118698847
Model is saved in epoch 19, overall batch: 9500
Training loss: 5.10661506652832 / Valid loss: 6.255825381051927
Model is saved in epoch 19, overall batch: 9600
Training loss: 7.750763893127441 / Valid loss: 6.240612624940418
Model is saved in epoch 19, overall batch: 9700

Epoch: 20
Training loss: 5.821227073669434 / Valid loss: 6.231971874691191
Model is saved in epoch 20, overall batch: 9800
Training loss: 6.338356018066406 / Valid loss: 6.231924154644921
Model is saved in epoch 20, overall batch: 9900
Training loss: 5.940013408660889 / Valid loss: 6.221227652685982
Model is saved in epoch 20, overall batch: 10000
Training loss: 7.044276237487793 / Valid loss: 6.2047323067983
Model is saved in epoch 20, overall batch: 10100
Training loss: 5.893450736999512 / Valid loss: 6.20259754771278
Model is saved in epoch 20, overall batch: 10200

Epoch: 21
Training loss: 7.194483757019043 / Valid loss: 6.197817825135731
Model is saved in epoch 21, overall batch: 10300
Training loss: 5.305286407470703 / Valid loss: 6.195114776066371
Model is saved in epoch 21, overall batch: 10400
Training loss: 5.901385307312012 / Valid loss: 6.187835588909331
Model is saved in epoch 21, overall batch: 10500
Training loss: 7.072873592376709 / Valid loss: 6.180952605747041
Model is saved in epoch 21, overall batch: 10600
Training loss: 4.460886478424072 / Valid loss: 6.176189116069248
Model is saved in epoch 21, overall batch: 10700

Epoch: 22
Training loss: 7.741970539093018 / Valid loss: 6.1642425287337534
Model is saved in epoch 22, overall batch: 10800
Training loss: 7.105536937713623 / Valid loss: 6.16202020191011
Model is saved in epoch 22, overall batch: 10900
Training loss: 5.673985481262207 / Valid loss: 6.155241448538644
Model is saved in epoch 22, overall batch: 11000
Training loss: 6.026615142822266 / Valid loss: 6.152843443552653
Model is saved in epoch 22, overall batch: 11100
Training loss: 5.25701379776001 / Valid loss: 6.140402028674171
Model is saved in epoch 22, overall batch: 11200

Epoch: 23
Training loss: 5.425827980041504 / Valid loss: 6.142781657264346
Training loss: 7.564739227294922 / Valid loss: 6.139017679577782
Model is saved in epoch 23, overall batch: 11400
Training loss: 6.992535591125488 / Valid loss: 6.132836907250541
Model is saved in epoch 23, overall batch: 11500
Training loss: 6.375772476196289 / Valid loss: 6.130798462459019
Model is saved in epoch 23, overall batch: 11600
Training loss: 5.646573066711426 / Valid loss: 6.124629284086682
Model is saved in epoch 23, overall batch: 11700

Epoch: 24
Training loss: 5.176279544830322 / Valid loss: 6.120369770413354
Model is saved in epoch 24, overall batch: 11800
Training loss: 6.997026443481445 / Valid loss: 6.110437238784064
Model is saved in epoch 24, overall batch: 11900
Training loss: 3.862004280090332 / Valid loss: 6.112017881302607
Training loss: 6.758884429931641 / Valid loss: 6.105890725907825
Model is saved in epoch 24, overall batch: 12100
Training loss: 5.927199363708496 / Valid loss: 6.099874276206607
Model is saved in epoch 24, overall batch: 12200

Epoch: 25
Training loss: 5.555938720703125 / Valid loss: 6.093228035881406
Model is saved in epoch 25, overall batch: 12300
Training loss: 4.464440822601318 / Valid loss: 6.098492649623326
Training loss: 7.681820869445801 / Valid loss: 6.098267266863869
Training loss: 5.804864883422852 / Valid loss: 6.092658440272014
Model is saved in epoch 25, overall batch: 12600
Training loss: 6.543090343475342 / Valid loss: 6.083672187441871
Model is saved in epoch 25, overall batch: 12700

Epoch: 26
Training loss: 4.301365852355957 / Valid loss: 6.085680021558489
Training loss: 3.9222893714904785 / Valid loss: 6.085500876108806
Training loss: 5.263589859008789 / Valid loss: 6.082161313011532
Model is saved in epoch 26, overall batch: 13000
Training loss: 5.877579689025879 / Valid loss: 6.0808426289331345
Model is saved in epoch 26, overall batch: 13100
Training loss: 4.1223344802856445 / Valid loss: 6.077720308303833
Model is saved in epoch 26, overall batch: 13200

Epoch: 27
Training loss: 5.612268447875977 / Valid loss: 6.074288947241647
Model is saved in epoch 27, overall batch: 13300
Training loss: 5.978852272033691 / Valid loss: 6.07274861789885
Model is saved in epoch 27, overall batch: 13400
Training loss: 5.682403564453125 / Valid loss: 6.072900018237886
Training loss: 6.905617713928223 / Valid loss: 6.067268723533267
Model is saved in epoch 27, overall batch: 13600
Training loss: 6.538854598999023 / Valid loss: 6.061777067184448
Model is saved in epoch 27, overall batch: 13700

Epoch: 28
Training loss: 5.831944465637207 / Valid loss: 6.064295711971464
Training loss: 5.54510498046875 / Valid loss: 6.055351849964687
Model is saved in epoch 28, overall batch: 13900
Training loss: 5.024961471557617 / Valid loss: 6.059807872772216
Training loss: 6.781214237213135 / Valid loss: 6.052478910627819
Model is saved in epoch 28, overall batch: 14100
Training loss: 5.481802463531494 / Valid loss: 6.05729744320824

Epoch: 29
Training loss: 6.48935079574585 / Valid loss: 6.051993401845296
Model is saved in epoch 29, overall batch: 14300
Training loss: 6.086958408355713 / Valid loss: 6.048191601889474
Model is saved in epoch 29, overall batch: 14400
Training loss: 6.8843278884887695 / Valid loss: 6.0515688600994295
Training loss: 6.37056827545166 / Valid loss: 6.048573307763963

Epoch: 30
Training loss: 6.481179237365723 / Valid loss: 6.043586133775257
Model is saved in epoch 30, overall batch: 14700
Training loss: 6.313819885253906 / Valid loss: 6.0486863953726635
Training loss: 5.972093105316162 / Valid loss: 6.048773570287795
Training loss: 5.3469977378845215 / Valid loss: 6.046835238592966
Training loss: 6.57708215713501 / Valid loss: 6.047095001311529

Epoch: 31
Training loss: 7.225587368011475 / Valid loss: 6.044181535357521
Training loss: 6.51815128326416 / Valid loss: 6.03768097559611
Model is saved in epoch 31, overall batch: 15300
Training loss: 6.171327590942383 / Valid loss: 6.03612030120123
Model is saved in epoch 31, overall batch: 15400
Training loss: 7.216615676879883 / Valid loss: 6.040474012919835
Training loss: 7.862144470214844 / Valid loss: 6.032803231193906
Model is saved in epoch 31, overall batch: 15600

Epoch: 32
Training loss: 4.881529331207275 / Valid loss: 6.034508750552223
Training loss: 6.603028297424316 / Valid loss: 6.0366411186399915
Training loss: 6.4063568115234375 / Valid loss: 6.036005167734055
Training loss: 5.737242698669434 / Valid loss: 6.0331909542992
Training loss: 3.7516579627990723 / Valid loss: 6.036290625163487

Epoch: 33
Training loss: 7.364448070526123 / Valid loss: 6.034091429483323
Training loss: 4.132904052734375 / Valid loss: 6.030412503651211
Model is saved in epoch 33, overall batch: 16300
Training loss: 6.78586483001709 / Valid loss: 6.032292443230038
Training loss: 5.562211036682129 / Valid loss: 6.0239058789752775
Model is saved in epoch 33, overall batch: 16500
Training loss: 7.850358963012695 / Valid loss: 6.0327461855752125

Epoch: 34
Training loss: 5.503505706787109 / Valid loss: 6.031334486461821
Training loss: 5.516749382019043 / Valid loss: 6.030198076793126
Training loss: 5.201833724975586 / Valid loss: 6.021617119652884
Model is saved in epoch 34, overall batch: 16900
Training loss: 5.568020343780518 / Valid loss: 6.029469846543812
Training loss: 6.518040657043457 / Valid loss: 6.020135087058658
Model is saved in epoch 34, overall batch: 17100

Epoch: 35
Training loss: 5.805661201477051 / Valid loss: 6.023343538102649
Training loss: 5.796855449676514 / Valid loss: 6.025074867975144
Training loss: 5.837435722351074 / Valid loss: 6.027458274932135
Training loss: 8.682138442993164 / Valid loss: 6.020371423448835
Training loss: 8.18309211730957 / Valid loss: 6.010877918061756
Model is saved in epoch 35, overall batch: 17600

Epoch: 36
Training loss: 5.716143608093262 / Valid loss: 6.023354698362804
Training loss: 7.999642372131348 / Valid loss: 6.009644876207624
Model is saved in epoch 36, overall batch: 17800
Training loss: 4.7483062744140625 / Valid loss: 6.018741033190772
Training loss: 4.912630081176758 / Valid loss: 6.024296211061023
Training loss: 4.607905387878418 / Valid loss: 6.024085707891555

Epoch: 37
Training loss: 4.816359519958496 / Valid loss: 6.021964227585565
Training loss: 5.884843826293945 / Valid loss: 6.022202932266962
Training loss: 5.932960033416748 / Valid loss: 6.017796218962896
Training loss: 5.9444146156311035 / Valid loss: 6.012084377379645
Training loss: 5.43098783493042 / Valid loss: 6.014453647250221

Epoch: 38
Training loss: 4.826204299926758 / Valid loss: 6.0166841348012285
Training loss: 6.8147077560424805 / Valid loss: 6.019313598814465
Training loss: 6.050859451293945 / Valid loss: 6.016464174361456
Training loss: 5.82978630065918 / Valid loss: 6.017509944098336
Training loss: 7.529252052307129 / Valid loss: 6.016033240727016

Epoch: 39
Training loss: 4.25823974609375 / Valid loss: 6.016839265823364
Training loss: 6.963861465454102 / Valid loss: 6.018063894907633
Training loss: 7.167050361633301 / Valid loss: 6.016362276531401
Training loss: 6.90968132019043 / Valid loss: 6.01859766188122
ModuleList(
  (0): Linear(in_features=31191, out_features=1, bias=True)
)
Loss on test set of optimal model (batch 17800): 5.843065416245233
