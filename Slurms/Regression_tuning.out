********************************************************************************************
** WARNING: 'The 'pre2019' module environment is deprecated. Please consider switching
             to the '2019' or '2020' module environment. You can read more about our
             software policy on this page:
             https://userinfo.surfsara.nl/documentation/software-policy-lisacartesius

             If you have any question, please contact us via http://servicedesk.surfsara.nl.'
********************************************************************************************
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 80
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0
momentum : 0
embedder : Bert
verbose : False
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
neural net:
 [tensor([[ 0.0035, -0.0035,  0.0052,  ..., -0.0130,  0.0056, -0.0088],
        [ 0.0045, -0.0089, -0.0107,  ...,  0.0052,  0.0048, -0.0076],
        [ 0.0014, -0.0062,  0.0084,  ..., -0.0093,  0.0089,  0.0006],
        ...,
        [ 0.0115,  0.0001, -0.0032,  ..., -0.0018, -0.0062, -0.0085],
        [-0.0009,  0.0057,  0.0107,  ..., -0.0067, -0.0022, -0.0107],
        [ 0.0109, -0.0116, -0.0054,  ...,  0.0007, -0.0127,  0.0088]],
       device='cuda:0'), tensor([ 0.0047,  0.0122,  0.0094,  ...,  0.0037, -0.0029, -0.0056],
       device='cuda:0'), tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), tensor([[-0.0121, -0.0085,  0.0158,  ...,  0.0067, -0.0057, -0.0189],
        [ 0.0064, -0.0150,  0.0096,  ...,  0.0006,  0.0074,  0.0177],
        [ 0.0061, -0.0078,  0.0059,  ...,  0.0032, -0.0206,  0.0205],
        ...,
        [ 0.0173, -0.0032,  0.0090,  ..., -0.0031,  0.0141,  0.0130],
        [-0.0181, -0.0101,  0.0157,  ..., -0.0020, -0.0070,  0.0160],
        [ 0.0063, -0.0152, -0.0101,  ...,  0.0008,  0.0028, -0.0011]],
       device='cuda:0'), tensor([-1.1967e-02,  1.1110e-02,  1.8701e-02,  1.1023e-02, -2.0298e-02,
         5.6226e-05, -1.3542e-02,  2.7363e-03, -8.2064e-03,  1.8652e-02,
         1.4701e-02,  1.2389e-02,  2.1926e-02,  1.5760e-02, -3.8398e-03,
        -2.1899e-02,  2.2099e-02, -5.7002e-03,  2.0446e-03,  2.1733e-02,
         1.5114e-02,  2.0789e-03, -1.0899e-02, -5.6587e-03,  2.0192e-02,
         4.3223e-03,  8.8637e-03, -1.6798e-02,  1.7955e-02,  1.2753e-02,
        -5.4965e-03,  1.9481e-02,  6.9524e-03, -2.0116e-02, -5.9529e-03,
        -1.1193e-02,  1.8363e-02,  1.6020e-02, -1.6430e-02,  6.4647e-03,
         1.5684e-02,  9.1588e-03, -2.0390e-02,  9.9621e-03,  2.0163e-02,
         2.1748e-02, -1.8861e-02, -5.0238e-03,  1.9386e-02,  1.1112e-03,
        -1.3363e-02,  6.7029e-03,  1.7814e-02, -8.2254e-03,  1.3094e-02,
         1.2228e-02, -1.2875e-03,  3.9142e-03,  1.4362e-03,  5.9424e-03,
         2.2414e-03, -1.3438e-02, -7.4102e-03,  1.5021e-02, -1.8291e-02,
         1.7939e-02,  2.4626e-03, -1.6420e-03,  1.0483e-03, -6.6787e-03,
         1.9978e-02,  6.3201e-04,  1.7342e-02,  9.4639e-03,  2.2343e-02,
        -1.5528e-02,  1.0803e-02, -4.7853e-03,  1.3566e-02, -1.2095e-02,
        -5.4629e-03,  7.9233e-03, -1.5708e-02, -2.7313e-03,  8.8111e-03,
        -2.1376e-03,  2.0746e-02,  1.0951e-02, -1.4991e-02,  5.5452e-03,
        -1.4782e-02, -1.6080e-02, -5.0667e-03, -2.1050e-02, -7.7526e-03,
         1.9154e-02, -2.1401e-02, -1.0501e-02,  7.3819e-03,  2.0670e-03],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), tensor([[-0.0636, -0.0402,  0.0180,  ..., -0.0669, -0.0184, -0.0632],
        [-0.0702, -0.0480,  0.0203,  ...,  0.0432, -0.0736,  0.0113],
        [-0.0735, -0.0482,  0.0561,  ..., -0.0377,  0.0717,  0.0079],
        ...,
        [ 0.0336,  0.0795,  0.0686,  ...,  0.0617,  0.0593,  0.0145],
        [-0.0549, -0.0686,  0.0407,  ..., -0.0574,  0.0591, -0.0667],
        [-0.0106, -0.0514, -0.0024,  ...,  0.0682,  0.0980,  0.0754]],
       device='cuda:0'), tensor([-0.0598,  0.0784,  0.0041,  0.0437, -0.0314,  0.0230, -0.0737, -0.0741,
        -0.0710, -0.0826, -0.0965, -0.0397,  0.0959,  0.0370,  0.0845,  0.0369],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0'), tensor([[-0.0476,  0.1976,  0.0806, -0.1341,  0.1659,  0.0086, -0.0477,  0.0708,
          0.2368,  0.0414, -0.1084, -0.0428,  0.1062, -0.1102,  0.0168,  0.1324]],
       device='cuda:0'), tensor([0.1504], device='cuda:0')]

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 13.399635451180595
Model is saved in epoch 0, overall batch: 0
Training loss: 6.127030372619629 / Valid loss: 8.819717675163632
Model is saved in epoch 0, overall batch: 100
Training loss: 4.364099502563477 / Valid loss: 6.371737811678932
Model is saved in epoch 0, overall batch: 200
Training loss: 6.64052152633667 / Valid loss: 5.719195552099318
Model is saved in epoch 0, overall batch: 300
Training loss: 6.042747497558594 / Valid loss: 5.520333778290522
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.577243804931641 / Valid loss: 5.533714669091361
Training loss: 5.004234313964844 / Valid loss: 5.621288937614078
Training loss: 5.9308390617370605 / Valid loss: 5.55015804654076
Training loss: 4.668268203735352 / Valid loss: 5.53403328259786
Training loss: 4.401596546173096 / Valid loss: 5.578369969413394

Epoch: 2
Training loss: 6.164501190185547 / Valid loss: 5.500020435878208
Model is saved in epoch 2, overall batch: 1000
Training loss: 6.425787448883057 / Valid loss: 5.61622618720645
Training loss: 5.164256572723389 / Valid loss: 5.628493234089443
Training loss: 6.742783546447754 / Valid loss: 5.578919303984869
Training loss: 4.455643177032471 / Valid loss: 5.567807924179804

Epoch: 3
Training loss: 3.558081865310669 / Valid loss: 5.672660146440778
Training loss: 4.075849533081055 / Valid loss: 5.8182059742155525
Training loss: 5.273836135864258 / Valid loss: 5.57822737239656
Training loss: 3.2550714015960693 / Valid loss: 5.85000859215146
Training loss: 5.04761266708374 / Valid loss: 5.77729852994283

Epoch: 4
Training loss: 4.5832037925720215 / Valid loss: 5.890043653760638
Training loss: 4.638767242431641 / Valid loss: 5.875842223848616
Training loss: 4.182924270629883 / Valid loss: 5.857748565219698
Training loss: 5.140944480895996 / Valid loss: 5.778268911724999
Training loss: 4.2131147384643555 / Valid loss: 5.800540404092698

Epoch: 5
Training loss: 6.79914665222168 / Valid loss: 6.263358293260847
Training loss: 4.713563919067383 / Valid loss: 6.228647985912505
Training loss: 3.157804489135742 / Valid loss: 6.161113966078985
Training loss: 3.8821210861206055 / Valid loss: 5.930653780982608
Training loss: 3.887453556060791 / Valid loss: 5.941768146696544

Epoch: 6
Training loss: 6.122968673706055 / Valid loss: 6.3214335487002415
Training loss: 3.285597324371338 / Valid loss: 6.374911083493914
Training loss: 3.578014373779297 / Valid loss: 6.5801569416409444
Training loss: 3.965419292449951 / Valid loss: 6.214591870989119
Training loss: 3.384624719619751 / Valid loss: 6.468242118472145

Epoch: 7
Training loss: 3.21475887298584 / Valid loss: 6.497412256967454
Training loss: 2.1728620529174805 / Valid loss: 6.684479788371495
Training loss: 3.4841299057006836 / Valid loss: 6.710339098884946
Training loss: 4.321338653564453 / Valid loss: 6.417580468314035
Training loss: 2.972651481628418 / Valid loss: 6.868036967232114

Epoch: 8
Training loss: 1.7738990783691406 / Valid loss: 7.195124421800886
Training loss: 2.1441798210144043 / Valid loss: 6.642926300139655
Training loss: 3.251882553100586 / Valid loss: 6.759008693695068
Training loss: 4.096506118774414 / Valid loss: 6.78841270946321
Training loss: 3.183248519897461 / Valid loss: 6.636155123937698

Epoch: 9
Training loss: 2.029506206512451 / Valid loss: 6.818800658271426
Training loss: 2.288989543914795 / Valid loss: 6.828665347326369
Training loss: 3.3808815479278564 / Valid loss: 6.763192390260242
Training loss: 2.3981070518493652 / Valid loss: 7.739439296722412

Epoch: 10
Training loss: 1.4216480255126953 / Valid loss: 7.388351267860049
Training loss: 1.8197413682937622 / Valid loss: 7.151865641276042
Training loss: 1.5796442031860352 / Valid loss: 6.846092355818976
Training loss: 2.577202081680298 / Valid loss: 6.945623552231561
Training loss: 1.7377214431762695 / Valid loss: 7.782596015930176

Epoch: 11
Training loss: 1.0891811847686768 / Valid loss: 7.376577949523925
Training loss: 1.5657374858856201 / Valid loss: 7.172021066574823
Training loss: 1.6767587661743164 / Valid loss: 7.089032963344029
Training loss: 1.4473447799682617 / Valid loss: 7.44668386777242
Training loss: 2.353316307067871 / Valid loss: 7.667879840305873

Epoch: 12
Training loss: 0.8911303877830505 / Valid loss: 7.021634292602539
Training loss: 1.141905665397644 / Valid loss: 7.188700862157912
Training loss: 1.278510332107544 / Valid loss: 6.898002352033343
Training loss: 1.1357593536376953 / Valid loss: 7.265919263022287
Training loss: 1.4851073026657104 / Valid loss: 7.375135644276937

Epoch: 13
Training loss: 1.0675407648086548 / Valid loss: 7.076451192583357
Training loss: 1.0887376070022583 / Valid loss: 7.194370133536203
Training loss: 1.1945703029632568 / Valid loss: 7.195398943764823
Training loss: 1.4118218421936035 / Valid loss: 6.989992854708717
Training loss: 1.1957453489303589 / Valid loss: 7.465697842552548

Epoch: 14
Training loss: 0.9668686985969543 / Valid loss: 7.549950263613746
Training loss: 1.7238690853118896 / Valid loss: 7.141253562200637
Training loss: 1.3160755634307861 / Valid loss: 7.293925380706787
Training loss: 0.8440595865249634 / Valid loss: 7.21663301104591
Training loss: 1.5144141912460327 / Valid loss: 7.172609601702009

Epoch: 15
Training loss: 0.5426336526870728 / Valid loss: 7.424824369521368
Training loss: 1.441624641418457 / Valid loss: 7.28578322728475
Training loss: 0.6639300584793091 / Valid loss: 7.345029090699695
Training loss: 0.9602261185646057 / Valid loss: 7.381763621738979
Training loss: 1.199022650718689 / Valid loss: 7.348576191493443

Epoch: 16
Training loss: 0.8247591853141785 / Valid loss: 7.196861439659482
Training loss: 0.6559747457504272 / Valid loss: 7.253304481506348
Training loss: 0.5706334114074707 / Valid loss: 7.4278565043494815
Training loss: 1.1128830909729004 / Valid loss: 7.461133911496117
Training loss: 0.9497281312942505 / Valid loss: 7.330151507967995

Epoch: 17
Training loss: 1.0422170162200928 / Valid loss: 7.381999202001662
Training loss: 1.3179240226745605 / Valid loss: 7.1618699482509065
Training loss: 1.3021745681762695 / Valid loss: 7.322104590279715
Training loss: 0.7087141275405884 / Valid loss: 7.289461694444928
Training loss: 0.6131398677825928 / Valid loss: 7.1717011315482

Epoch: 18
Training loss: 0.6964162588119507 / Valid loss: 7.2683129946390785
Training loss: 0.7655309438705444 / Valid loss: 7.08795215969994
Training loss: 0.6940097808837891 / Valid loss: 7.168793280919393
Training loss: 0.9227856397628784 / Valid loss: 7.104040431976318
Training loss: 0.9503320455551147 / Valid loss: 7.439989203498477

Epoch: 19
Training loss: 1.2825490236282349 / Valid loss: 7.452070376986549
Training loss: 1.5297114849090576 / Valid loss: 7.438924239930652
Training loss: 1.0516955852508545 / Valid loss: 7.21810854048956
Training loss: 0.7481666803359985 / Valid loss: 7.474696667989095

Epoch: 20
Training loss: 0.5240218043327332 / Valid loss: 7.2247215861365905
Training loss: 1.0359026193618774 / Valid loss: 7.01406051544916
Training loss: 0.6213411688804626 / Valid loss: 7.096994400024414
Training loss: 1.0393610000610352 / Valid loss: 7.072528766450428
Training loss: 1.505245327949524 / Valid loss: 7.322477567763555

Epoch: 21
Training loss: 0.6410533785820007 / Valid loss: 7.139811697460356
Training loss: 0.5327749848365784 / Valid loss: 7.614635058811733
Training loss: 0.6896448135375977 / Valid loss: 7.432232411702474
Training loss: 0.7246840596199036 / Valid loss: 7.034297870454334
Training loss: 0.6732176542282104 / Valid loss: 7.112529209681919

Epoch: 22
Training loss: 0.7488296031951904 / Valid loss: 7.0415241922651015
Training loss: 0.43810153007507324 / Valid loss: 7.121858891986665
Training loss: 0.5923314094543457 / Valid loss: 7.196047010875883
Training loss: 1.1224586963653564 / Valid loss: 7.148241783323742
Training loss: 1.0960543155670166 / Valid loss: 7.282926409585135

Epoch: 23
Training loss: 0.6613410711288452 / Valid loss: 7.379645656404041
Training loss: 0.7701973915100098 / Valid loss: 7.032446177800496
Training loss: 0.7753510475158691 / Valid loss: 7.334156890142531
Training loss: 0.5383292436599731 / Valid loss: 7.004515000752041
Training loss: 0.7604090571403503 / Valid loss: 7.103110722133091

Epoch: 24
Training loss: 1.2114282846450806 / Valid loss: 6.867120738256546
Training loss: 0.5419753789901733 / Valid loss: 6.920538143884568
Training loss: 0.5897265672683716 / Valid loss: 7.347378356116159
Training loss: 0.8014463186264038 / Valid loss: 7.242246850331624
Training loss: 1.3461661338806152 / Valid loss: 7.075789029257638

Epoch: 25
Training loss: 0.6527553796768188 / Valid loss: 6.899279732931228
Training loss: 0.4484539330005646 / Valid loss: 6.924172608057658
Training loss: 0.7834568023681641 / Valid loss: 7.003453665687925
Training loss: 0.43774452805519104 / Valid loss: 7.3492748805454795
Training loss: 0.5079165101051331 / Valid loss: 7.132343700953892

Epoch: 26
Training loss: 1.0213607549667358 / Valid loss: 7.035318111238025
Training loss: 0.8303967118263245 / Valid loss: 6.960005424136207
Training loss: 0.6095854640007019 / Valid loss: 6.990484467006865
Training loss: 0.3739844560623169 / Valid loss: 6.997977036521548
Training loss: 0.6546720266342163 / Valid loss: 7.204307660602388

Epoch: 27
Training loss: 0.3490922451019287 / Valid loss: 6.8811381203787665
Training loss: 0.6245917081832886 / Valid loss: 6.948054860887074
Training loss: 0.6033289432525635 / Valid loss: 7.143658179328555
Training loss: 0.5362590551376343 / Valid loss: 7.027526187896728
Training loss: 0.6844990849494934 / Valid loss: 7.0928855169387095

Epoch: 28
Training loss: 0.5631611347198486 / Valid loss: 7.220712157658168
Training loss: 0.4681992828845978 / Valid loss: 6.9808939297993975
Training loss: 0.5096921920776367 / Valid loss: 7.013929980141776
Training loss: 0.3915059268474579 / Valid loss: 7.077820673443022
Training loss: 0.7864702939987183 / Valid loss: 7.266643333435058

Epoch: 29
Training loss: 0.7675877809524536 / Valid loss: 7.167349783579509
Training loss: 0.5293459892272949 / Valid loss: 6.968165688287645
Training loss: 0.42066216468811035 / Valid loss: 7.012243166423979
Training loss: 0.4675798714160919 / Valid loss: 7.008248878660656

Epoch: 30
Training loss: 0.689152717590332 / Valid loss: 7.069259302956717
Training loss: 0.4842417240142822 / Valid loss: 7.137249219985235
Training loss: 0.3676629364490509 / Valid loss: 7.02804977326166
Training loss: 0.5203980207443237 / Valid loss: 7.0413317816598076
Training loss: 0.3044419586658478 / Valid loss: 7.050244948977515

Epoch: 31
Training loss: 0.42016178369522095 / Valid loss: 6.902105474472046
Training loss: 0.4183787703514099 / Valid loss: 6.970627067202614
Training loss: 0.49750757217407227 / Valid loss: 7.0643341700236
Training loss: 0.37181830406188965 / Valid loss: 7.037158845719837
Training loss: 0.6152487993240356 / Valid loss: 7.122200321015858

Epoch: 32
Training loss: 0.3848465085029602 / Valid loss: 6.935148438953218
Training loss: 0.4010522961616516 / Valid loss: 6.868158136095319
Training loss: 0.2944841980934143 / Valid loss: 7.047273454212007
Training loss: 0.35725146532058716 / Valid loss: 6.8795719282967704
Training loss: 0.4350997805595398 / Valid loss: 7.101126832053775

Epoch: 33
Training loss: 0.3530556559562683 / Valid loss: 6.957195618039086
Training loss: 0.40896081924438477 / Valid loss: 7.0333207402910505
Training loss: 0.4382176399230957 / Valid loss: 6.992292533602034
Training loss: 0.43732786178588867 / Valid loss: 6.922511766070412
Training loss: 0.4189184010028839 / Valid loss: 6.986839857555571

Epoch: 34
Training loss: 0.8735959529876709 / Valid loss: 7.031001631418864
Training loss: 0.36404505372047424 / Valid loss: 6.8136503037952245
Training loss: 0.48089858889579773 / Valid loss: 6.983895229157947
Training loss: 0.33155110478401184 / Valid loss: 6.808161989847819
Training loss: 0.34232407808303833 / Valid loss: 6.949971784864153

Epoch: 35
Training loss: 0.9045617580413818 / Valid loss: 6.858074319930304
Training loss: 0.5713363289833069 / Valid loss: 7.007368051438105
Training loss: 0.4803009033203125 / Valid loss: 6.9159299123854865
Training loss: 0.41560590267181396 / Valid loss: 6.989744990212577
Training loss: 0.25549936294555664 / Valid loss: 7.076335602714902

Epoch: 36
Training loss: 0.43812042474746704 / Valid loss: 6.964874267578125
Training loss: 0.7390400171279907 / Valid loss: 6.944810256503877
Training loss: 0.6809818148612976 / Valid loss: 7.060747923169817
Training loss: 0.49954742193222046 / Valid loss: 7.173601718175979
Training loss: 0.5935614705085754 / Valid loss: 6.901555665334066

Epoch: 37
Training loss: 0.38267982006073 / Valid loss: 6.777287381035941
Training loss: 0.27827808260917664 / Valid loss: 7.029321152823312
Training loss: 0.5360932350158691 / Valid loss: 7.144682629903158
Training loss: 1.5006515979766846 / Valid loss: 6.902246284484863
Training loss: 0.6049326658248901 / Valid loss: 7.252998483748663

Epoch: 38
Training loss: 0.55263751745224 / Valid loss: 6.903541265215193
Training loss: 0.3912277817726135 / Valid loss: 7.067876543317523
Training loss: 0.4043325185775757 / Valid loss: 7.0610686211358935
Training loss: 0.4284650683403015 / Valid loss: 7.244679419199626
Training loss: 0.7228416204452515 / Valid loss: 7.018387458437965

Epoch: 39
Training loss: 0.4735035002231598 / Valid loss: 7.049049045926049
Training loss: 0.40887895226478577 / Valid loss: 6.9995390347072055
Training loss: 0.3522275984287262 / Valid loss: 6.915446358635312
Training loss: 0.269299179315567 / Valid loss: 7.012306485857282

Epoch: 40
Training loss: 0.46791696548461914 / Valid loss: 7.21376257623945
Training loss: 0.24236854910850525 / Valid loss: 7.250162551516579
Training loss: 0.5401368141174316 / Valid loss: 7.016613869439988
Training loss: 0.29310140013694763 / Valid loss: 6.90638062613351
Training loss: 0.28645098209381104 / Valid loss: 7.00866421744937

Epoch: 41
Training loss: 0.5362550616264343 / Valid loss: 6.995355810437884
Training loss: 0.27495113015174866 / Valid loss: 6.85082516670227
Training loss: 0.21118183434009552 / Valid loss: 6.991395550682431
Training loss: 1.0882055759429932 / Valid loss: 6.938548042660668
Training loss: 0.4951770007610321 / Valid loss: 6.982846600668771

Epoch: 42
Training loss: 0.3849049508571625 / Valid loss: 6.738570896784465
Training loss: 0.3295544683933258 / Valid loss: 6.960407647632417
Training loss: 0.403055876493454 / Valid loss: 6.893122212092082
Training loss: 0.4334084093570709 / Valid loss: 7.019912056695848
Training loss: 0.2634228467941284 / Valid loss: 6.813919103713262

Epoch: 43
Training loss: 0.3275986909866333 / Valid loss: 6.787533528464181
Training loss: 0.37901341915130615 / Valid loss: 6.7140169370742075
Training loss: 0.2204371690750122 / Valid loss: 6.905223751068116
Training loss: 0.5867722630500793 / Valid loss: 6.820777025676909
Training loss: 0.29604485630989075 / Valid loss: 6.878408391135079

Epoch: 44
Training loss: 0.4258541464805603 / Valid loss: 6.790409142630441
Training loss: 0.29014852643013 / Valid loss: 6.83801591963995
Training loss: 0.3502117991447449 / Valid loss: 6.908721901121593
Training loss: 0.3110758066177368 / Valid loss: 6.975122665223621
Training loss: 0.5069660544395447 / Valid loss: 6.998301328931536

Epoch: 45
Training loss: 0.22383712232112885 / Valid loss: 7.066485450381324
Training loss: 0.6398098468780518 / Valid loss: 6.908342568079631
Training loss: 0.5776716470718384 / Valid loss: 6.953870986756824
Training loss: 0.2700120806694031 / Valid loss: 6.669647546041579
Training loss: 0.3675152063369751 / Valid loss: 6.823161897205171

Epoch: 46
Training loss: 0.2949202060699463 / Valid loss: 6.878785646529424
Training loss: 0.245732843875885 / Valid loss: 6.792029535202753
Training loss: 0.6988308429718018 / Valid loss: 7.0100179922013055
Training loss: 0.20519594848155975 / Valid loss: 6.912310187021891
Training loss: 0.49549952149391174 / Valid loss: 6.97582597732544

Epoch: 47
Training loss: 0.27583441138267517 / Valid loss: 6.846087578364781
Training loss: 0.5250483751296997 / Valid loss: 6.914285911832537
Training loss: 0.36490076780319214 / Valid loss: 6.868118640354702
Training loss: 0.29066672921180725 / Valid loss: 6.962226640610468
Training loss: 0.4297463893890381 / Valid loss: 7.15857314609346

Epoch: 48
Training loss: 0.24943499267101288 / Valid loss: 6.807519131615049
Training loss: 0.4290311336517334 / Valid loss: 6.8909859975179035
Training loss: 0.48338553309440613 / Valid loss: 6.849553948356991
Training loss: 0.21784916520118713 / Valid loss: 6.697406864166259
Training loss: 0.395506352186203 / Valid loss: 6.838122831072126

Epoch: 49
Training loss: 0.4981532692909241 / Valid loss: 6.975083473750523
Training loss: 0.2853633165359497 / Valid loss: 6.841758741651263
Training loss: 0.2525501549243927 / Valid loss: 7.010902663639613
Training loss: 0.41843345761299133 / Valid loss: 6.8964976265316915

Epoch: 50
Training loss: 0.5022130608558655 / Valid loss: 6.791504832676479
Training loss: 0.4916205108165741 / Valid loss: 6.885678495679583
Training loss: 0.2293807715177536 / Valid loss: 6.683464994884672
Training loss: 0.321909099817276 / Valid loss: 6.89034925869533
Training loss: 0.4450467526912689 / Valid loss: 6.937083067212786

Epoch: 51
Training loss: 0.18771228194236755 / Valid loss: 7.028895478021531
Training loss: 0.31529486179351807 / Valid loss: 6.8530100686209545
Training loss: 0.2943069040775299 / Valid loss: 6.837882627759661
Training loss: 0.2967613935470581 / Valid loss: 6.755054414839972
Training loss: 0.39550405740737915 / Valid loss: 6.811524132319859

Epoch: 52
Training loss: 0.24211016297340393 / Valid loss: 6.751355534508114
Training loss: 0.24033130705356598 / Valid loss: 6.819704269227527
Training loss: 0.23116359114646912 / Valid loss: 6.808247516268776
Training loss: 0.22485482692718506 / Valid loss: 6.875391328902472
Training loss: 0.4469583034515381 / Valid loss: 6.884482138497489

Epoch: 53
Training loss: 0.24633878469467163 / Valid loss: 6.862690580458868
Training loss: 0.5703129768371582 / Valid loss: 6.873936871119908
Training loss: 0.304845929145813 / Valid loss: 6.933256898607526
Training loss: 0.5041393041610718 / Valid loss: 6.823003977820987
Training loss: 0.41058826446533203 / Valid loss: 6.761079093388148

Epoch: 54
Training loss: 0.3348295986652374 / Valid loss: 6.779568140847342
Training loss: 0.4200092554092407 / Valid loss: 6.920156251816523
Training loss: 0.2893892824649811 / Valid loss: 6.8204669112250915
Training loss: 0.23802801966667175 / Valid loss: 6.975771826789493
Training loss: 0.4505571722984314 / Valid loss: 6.8051290103367394

Epoch: 55
Training loss: 0.3780149221420288 / Valid loss: 6.833083665938604
Training loss: 0.2718570828437805 / Valid loss: 6.923740377880278
Training loss: 0.3408757150173187 / Valid loss: 6.843800490243094
Training loss: 0.3336857259273529 / Valid loss: 6.68986846833002
Training loss: 0.3254382610321045 / Valid loss: 6.798951798393613

Epoch: 56
Training loss: 0.44014471769332886 / Valid loss: 6.821119592303321
Training loss: 0.17795458436012268 / Valid loss: 6.901603821345738
Training loss: 0.24694913625717163 / Valid loss: 6.750751849583217
Training loss: 0.27183568477630615 / Valid loss: 6.82918301991054
Training loss: 0.34241318702697754 / Valid loss: 6.988560449509394

Epoch: 57
Training loss: 0.4569093585014343 / Valid loss: 6.757980333055769
Training loss: 0.45368272066116333 / Valid loss: 6.82969587416876
Training loss: 0.7224981784820557 / Valid loss: 7.009624817257836
Training loss: 0.4086109399795532 / Valid loss: 6.812248048328218
Training loss: 0.391318678855896 / Valid loss: 6.998959641229539

Epoch: 58
Training loss: 0.23931163549423218 / Valid loss: 6.773059245518276
Training loss: 0.6497039794921875 / Valid loss: 6.735242260070074
Training loss: 0.1888764351606369 / Valid loss: 6.947529449917021
Training loss: 0.4087572693824768 / Valid loss: 6.780976754143125
Training loss: 0.33315712213516235 / Valid loss: 6.876399639674595

Epoch: 59
Training loss: 0.2002645581960678 / Valid loss: 6.901693857283819
Training loss: 0.34995630383491516 / Valid loss: 6.677828107561384
Training loss: 0.22979050874710083 / Valid loss: 6.758138145719256
Training loss: 0.32330116629600525 / Valid loss: 6.634569581349691

Epoch: 60
Training loss: 0.3578627109527588 / Valid loss: 7.12086106254941
Training loss: 0.6669747829437256 / Valid loss: 6.771716644650414
Training loss: 0.19245126843452454 / Valid loss: 6.82211933590117
Training loss: 0.2516666650772095 / Valid loss: 6.69075095312936
Training loss: 0.14702095091342926 / Valid loss: 6.765072218577067

Epoch: 61
Training loss: 0.3248170018196106 / Valid loss: 6.743950496401106
Training loss: 0.27995240688323975 / Valid loss: 6.678152468090966
Training loss: 0.4588662385940552 / Valid loss: 6.967714439119612
Training loss: 0.40133923292160034 / Valid loss: 6.848690073830741
Training loss: 0.29317769408226013 / Valid loss: 6.791198185511997

Epoch: 62
Training loss: 0.19909627735614777 / Valid loss: 6.800127778734479
Training loss: 0.44061246514320374 / Valid loss: 6.722131906236921
Training loss: 0.23604106903076172 / Valid loss: 6.795142671040126
Training loss: 0.38966214656829834 / Valid loss: 6.858607907522292
Training loss: 0.3226313591003418 / Valid loss: 6.819323589688255

Epoch: 63
Training loss: 0.23920030891895294 / Valid loss: 6.770762143816267
Training loss: 0.330547034740448 / Valid loss: 6.9365644500369115
Training loss: 0.2723698019981384 / Valid loss: 6.823400374821254
Training loss: 0.3134589195251465 / Valid loss: 6.718061910356794
Training loss: 0.12761719524860382 / Valid loss: 6.753193528311593

Epoch: 64
Training loss: 0.2347167283296585 / Valid loss: 6.780518881479899
Training loss: 0.16747644543647766 / Valid loss: 6.8804623331342425
Training loss: 0.3943936228752136 / Valid loss: 6.802707978657314
Training loss: 0.2992875576019287 / Valid loss: 6.87196071715582
Training loss: 0.19548112154006958 / Valid loss: 6.8231279691060385

Epoch: 65
Training loss: 0.20990172028541565 / Valid loss: 6.825393622262137
Training loss: 0.47712793946266174 / Valid loss: 6.895734339668637
Training loss: 0.19779613614082336 / Valid loss: 6.770004054478236
Training loss: 0.20016932487487793 / Valid loss: 6.760459171022688
Training loss: 0.8677366971969604 / Valid loss: 6.886543026424589

Epoch: 66
Training loss: 0.23584794998168945 / Valid loss: 6.7338930629548575
Training loss: 0.2006128430366516 / Valid loss: 6.803289858500163
Training loss: 0.34674781560897827 / Valid loss: 6.847874723161969
Training loss: 0.26058679819107056 / Valid loss: 6.786290840875535
Training loss: 0.33887147903442383 / Valid loss: 6.937690108163016

Epoch: 67
Training loss: 0.2687024474143982 / Valid loss: 6.779158776147025
Training loss: 0.18528583645820618 / Valid loss: 6.715510313851492
Training loss: 0.28112515807151794 / Valid loss: 6.731954079582578
Training loss: 0.5666322708129883 / Valid loss: 6.993746739342099
Training loss: 0.23502834141254425 / Valid loss: 6.7815415632157094

Epoch: 68
Training loss: 0.27422475814819336 / Valid loss: 6.8047154381161645
Training loss: 0.2522346079349518 / Valid loss: 6.736034034547352
Training loss: 0.16804102063179016 / Valid loss: 6.993249725160145
Training loss: 0.27309340238571167 / Valid loss: 6.780811223529634
Training loss: 0.4907994270324707 / Valid loss: 6.722554315839495

Epoch: 69
Training loss: 0.270169734954834 / Valid loss: 6.654696362359183
Training loss: 0.2019234001636505 / Valid loss: 6.875401042756581
Training loss: 0.3915999233722687 / Valid loss: 6.744269698006766
Training loss: 0.3321993947029114 / Valid loss: 6.798256851377941

Epoch: 70
Training loss: 0.15277396142482758 / Valid loss: 6.799340075538272
Training loss: 0.35081416368484497 / Valid loss: 6.73735302289327
Training loss: 0.17039163410663605 / Valid loss: 6.786959520975748
Training loss: 0.24052681028842926 / Valid loss: 6.730819293430874
Training loss: 0.25248444080352783 / Valid loss: 6.694361055464972

Epoch: 71
Training loss: 0.3983907699584961 / Valid loss: 6.9452120917184015
Training loss: 0.33602553606033325 / Valid loss: 6.904700338272821
Training loss: 0.18474268913269043 / Valid loss: 6.745666780925933
Training loss: 0.15980398654937744 / Valid loss: 6.8464235532851445
Training loss: 0.41813355684280396 / Valid loss: 6.785341603415353

Epoch: 72
Training loss: 0.17966073751449585 / Valid loss: 6.752444514774141
Training loss: 0.4873162806034088 / Valid loss: 6.668183435712542
Training loss: 0.22473925352096558 / Valid loss: 6.703145335969471
Training loss: 0.2712843418121338 / Valid loss: 6.806968906947545
Training loss: 0.3317486047744751 / Valid loss: 6.703223964146205

Epoch: 73
Training loss: 0.30949437618255615 / Valid loss: 6.600711572737921
Training loss: 0.34906408190727234 / Valid loss: 6.796590920857021
Training loss: 0.3080303966999054 / Valid loss: 6.684067826043992
Training loss: 0.40322181582450867 / Valid loss: 6.775865743273781
Training loss: 0.1882997751235962 / Valid loss: 6.677812283379691

Epoch: 74
Training loss: 0.1359628140926361 / Valid loss: 6.681673930940174
Training loss: 0.2174985706806183 / Valid loss: 6.7226333345685685
Training loss: 0.3890039920806885 / Valid loss: 6.682108552115304
Training loss: 0.3262813985347748 / Valid loss: 6.710959348224458
Training loss: 0.3439076542854309 / Valid loss: 6.72366517384847

Epoch: 75
Training loss: 0.27852174639701843 / Valid loss: 6.707524295080276
Training loss: 0.2972131669521332 / Valid loss: 6.691528567813692
Training loss: 0.5231931209564209 / Valid loss: 6.7678989319574265
Training loss: 0.253578782081604 / Valid loss: 6.7402241343543645
Training loss: 0.33494675159454346 / Valid loss: 6.6984045028686525

Epoch: 76
Training loss: 0.29479843378067017 / Valid loss: 6.684396757398333
Training loss: 0.40875691175460815 / Valid loss: 6.647468303498767
Training loss: 0.18798518180847168 / Valid loss: 6.777016825903029
Training loss: 0.1941707581281662 / Valid loss: 6.7427805764334545
Training loss: 0.1324498951435089 / Valid loss: 6.849903701600574

Epoch: 77
Training loss: 0.1589755117893219 / Valid loss: 6.712900184449696
Training loss: 0.18605273962020874 / Valid loss: 6.854461313429333
Training loss: 0.2120855450630188 / Valid loss: 6.801223813919794
Training loss: 0.27976441383361816 / Valid loss: 6.685082281203497
Training loss: 0.39022016525268555 / Valid loss: 6.805451729184106

Epoch: 78
Training loss: 0.2694767713546753 / Valid loss: 6.734313583374023
Training loss: 0.1732800155878067 / Valid loss: 6.716629364376977
Training loss: 0.15089979767799377 / Valid loss: 6.769556488309588
Training loss: 0.18094484508037567 / Valid loss: 6.845108091263544
Training loss: 0.3911951184272766 / Valid loss: 6.6995929400126135

Epoch: 79
Training loss: 0.14435258507728577 / Valid loss: 6.638266211464291
Training loss: 0.2574714720249176 / Valid loss: 6.759087135678246
Training loss: 0.13571076095104218 / Valid loss: 6.740478238605317
Training loss: 0.26609545946121216 / Valid loss: 6.730795751299177
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model: 5.325703477859497
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 80
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0
momentum : 0
embedder : Bert
verbose : False
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
neural net:
 [tensor([[ 0.0035, -0.0035,  0.0052,  ..., -0.0130,  0.0056, -0.0088],
        [ 0.0045, -0.0089, -0.0107,  ...,  0.0052,  0.0048, -0.0076],
        [ 0.0014, -0.0062,  0.0084,  ..., -0.0093,  0.0089,  0.0006],
        ...,
        [ 0.0115,  0.0001, -0.0032,  ..., -0.0018, -0.0062, -0.0085],
        [-0.0009,  0.0057,  0.0107,  ..., -0.0067, -0.0022, -0.0107],
        [ 0.0109, -0.0116, -0.0054,  ...,  0.0007, -0.0127,  0.0088]],
       device='cuda:0'), tensor([ 0.0047,  0.0122,  0.0094,  ...,  0.0037, -0.0029, -0.0056],
       device='cuda:0'), tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), tensor([[-0.0121, -0.0085,  0.0158,  ...,  0.0067, -0.0057, -0.0189],
        [ 0.0064, -0.0150,  0.0096,  ...,  0.0006,  0.0074,  0.0177],
        [ 0.0061, -0.0078,  0.0059,  ...,  0.0032, -0.0206,  0.0205],
        ...,
        [ 0.0173, -0.0032,  0.0090,  ..., -0.0031,  0.0141,  0.0130],
        [-0.0181, -0.0101,  0.0157,  ..., -0.0020, -0.0070,  0.0160],
        [ 0.0063, -0.0152, -0.0101,  ...,  0.0008,  0.0028, -0.0011]],
       device='cuda:0'), tensor([-1.1967e-02,  1.1110e-02,  1.8701e-02,  1.1023e-02, -2.0298e-02,
         5.6226e-05, -1.3542e-02,  2.7363e-03, -8.2064e-03,  1.8652e-02,
         1.4701e-02,  1.2389e-02,  2.1926e-02,  1.5760e-02, -3.8398e-03,
        -2.1899e-02,  2.2099e-02, -5.7002e-03,  2.0446e-03,  2.1733e-02,
         1.5114e-02,  2.0789e-03, -1.0899e-02, -5.6587e-03,  2.0192e-02,
         4.3223e-03,  8.8637e-03, -1.6798e-02,  1.7955e-02,  1.2753e-02,
        -5.4965e-03,  1.9481e-02,  6.9524e-03, -2.0116e-02, -5.9529e-03,
        -1.1193e-02,  1.8363e-02,  1.6020e-02, -1.6430e-02,  6.4647e-03,
         1.5684e-02,  9.1588e-03, -2.0390e-02,  9.9621e-03,  2.0163e-02,
         2.1748e-02, -1.8861e-02, -5.0238e-03,  1.9386e-02,  1.1112e-03,
        -1.3363e-02,  6.7029e-03,  1.7814e-02, -8.2254e-03,  1.3094e-02,
         1.2228e-02, -1.2875e-03,  3.9142e-03,  1.4362e-03,  5.9424e-03,
         2.2414e-03, -1.3438e-02, -7.4102e-03,  1.5021e-02, -1.8291e-02,
         1.7939e-02,  2.4626e-03, -1.6420e-03,  1.0483e-03, -6.6787e-03,
         1.9978e-02,  6.3201e-04,  1.7342e-02,  9.4639e-03,  2.2343e-02,
        -1.5528e-02,  1.0803e-02, -4.7853e-03,  1.3566e-02, -1.2095e-02,
        -5.4629e-03,  7.9233e-03, -1.5708e-02, -2.7313e-03,  8.8111e-03,
        -2.1376e-03,  2.0746e-02,  1.0951e-02, -1.4991e-02,  5.5452e-03,
        -1.4782e-02, -1.6080e-02, -5.0667e-03, -2.1050e-02, -7.7526e-03,
         1.9154e-02, -2.1401e-02, -1.0501e-02,  7.3819e-03,  2.0670e-03],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), tensor([[-0.0636, -0.0402,  0.0180,  ..., -0.0669, -0.0184, -0.0632],
        [-0.0702, -0.0480,  0.0203,  ...,  0.0432, -0.0736,  0.0113],
        [-0.0735, -0.0482,  0.0561,  ..., -0.0377,  0.0717,  0.0079],
        ...,
        [ 0.0336,  0.0795,  0.0686,  ...,  0.0617,  0.0593,  0.0145],
        [-0.0549, -0.0686,  0.0407,  ..., -0.0574,  0.0591, -0.0667],
        [-0.0106, -0.0514, -0.0024,  ...,  0.0682,  0.0980,  0.0754]],
       device='cuda:0'), tensor([-0.0598,  0.0784,  0.0041,  0.0437, -0.0314,  0.0230, -0.0737, -0.0741,
        -0.0710, -0.0826, -0.0965, -0.0397,  0.0959,  0.0370,  0.0845,  0.0369],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0'), tensor([[-0.0476,  0.1976,  0.0806, -0.1341,  0.1659,  0.0086, -0.0477,  0.0708,
          0.2368,  0.0414, -0.1084, -0.0428,  0.1062, -0.1102,  0.0168,  0.1324]],
       device='cuda:0'), tensor([0.1504], device='cuda:0')]

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.114407076154436
Model is saved in epoch 0, overall batch: 0
Training loss: 8.912009239196777 / Valid loss: 12.768657457260858
Model is saved in epoch 0, overall batch: 100
Training loss: 9.925810813903809 / Valid loss: 12.196747702643984
Model is saved in epoch 0, overall batch: 200
Training loss: 15.783401489257812 / Valid loss: 11.78140204747518
Model is saved in epoch 0, overall batch: 300
Training loss: 11.472275733947754 / Valid loss: 11.121726526532854
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 9.930368423461914 / Valid loss: 10.86008365267799
Model is saved in epoch 1, overall batch: 500
Training loss: 9.093926429748535 / Valid loss: 10.686255200703938
Model is saved in epoch 1, overall batch: 600
Training loss: 8.314817428588867 / Valid loss: 10.258468723297119
Model is saved in epoch 1, overall batch: 700
Training loss: 8.375079154968262 / Valid loss: 9.384540948413667
Model is saved in epoch 1, overall batch: 800
Training loss: 6.540190696716309 / Valid loss: 9.523321387881325

Epoch: 2
Training loss: 9.234378814697266 / Valid loss: 8.769821966262091
Model is saved in epoch 2, overall batch: 1000
Training loss: 9.167750358581543 / Valid loss: 8.570415982745942
Model is saved in epoch 2, overall batch: 1100
Training loss: 8.85214614868164 / Valid loss: 8.411143775213333
Model is saved in epoch 2, overall batch: 1200
Training loss: 9.604043006896973 / Valid loss: 7.723625689461118
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.110571384429932 / Valid loss: 7.415789735884894
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.846023082733154 / Valid loss: 7.009148602258591
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.8052730560302734 / Valid loss: 6.786819939386277
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.784642219543457 / Valid loss: 6.671425762630644
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.459716796875 / Valid loss: 6.495018087114606
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.784595489501953 / Valid loss: 6.455052934374128
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 5.4322004318237305 / Valid loss: 7.28327697572254
Training loss: 3.8479514122009277 / Valid loss: 6.66725393931071
Training loss: 4.175478458404541 / Valid loss: 6.828827540079753
Training loss: 4.428637981414795 / Valid loss: 6.515154436656407
Training loss: 3.4555511474609375 / Valid loss: 6.198425113587152
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 5.734418869018555 / Valid loss: 6.558657866432553
Training loss: 3.515838146209717 / Valid loss: 6.385780620574951
Training loss: 2.704442024230957 / Valid loss: 6.671769423711868
Training loss: 3.083517074584961 / Valid loss: 6.238934716724214
Training loss: 3.3447868824005127 / Valid loss: 6.271911857241676

Epoch: 6
Training loss: 5.0640363693237305 / Valid loss: 7.03444573538644
Training loss: 2.181051254272461 / Valid loss: 6.528518865222023
Training loss: 3.046281337738037 / Valid loss: 7.094282913208008
Training loss: 3.0997061729431152 / Valid loss: 6.523180414381481
Training loss: 2.9891488552093506 / Valid loss: 6.588962502706618

Epoch: 7
Training loss: 2.6797165870666504 / Valid loss: 6.758096204485212
Training loss: 1.6568546295166016 / Valid loss: 6.918168528874715
Training loss: 2.4585399627685547 / Valid loss: 6.7552470752171105
Training loss: 2.754788398742676 / Valid loss: 6.856639943804059
Training loss: 2.473010540008545 / Valid loss: 6.809998221624465

Epoch: 8
Training loss: 0.8977401256561279 / Valid loss: 6.755013095764887
Training loss: 1.413731575012207 / Valid loss: 6.647775027865455
Training loss: 1.7390714883804321 / Valid loss: 6.902055279413859
Training loss: 2.391519546508789 / Valid loss: 6.718315172195434
Training loss: 2.360682487487793 / Valid loss: 6.881375512622651

Epoch: 9
Training loss: 1.6765280961990356 / Valid loss: 6.831627114613851
Training loss: 1.7128515243530273 / Valid loss: 6.944666762579055
Training loss: 2.782789707183838 / Valid loss: 8.034859157743908
Training loss: 2.0937094688415527 / Valid loss: 6.823229158492316

Epoch: 10
Training loss: 0.936830461025238 / Valid loss: 6.9010133402688165
Training loss: 1.2471961975097656 / Valid loss: 6.960337920415969
Training loss: 0.9596741199493408 / Valid loss: 7.017917206173851
Training loss: 1.8208965063095093 / Valid loss: 6.903125983192807
Training loss: 1.0655286312103271 / Valid loss: 7.356153034028553

Epoch: 11
Training loss: 0.6359700560569763 / Valid loss: 6.962087163471041
Training loss: 1.1494207382202148 / Valid loss: 6.8070917992364794
Training loss: 1.145169734954834 / Valid loss: 6.811951723552886
Training loss: 0.8636291027069092 / Valid loss: 7.136697780518305
Training loss: 1.28263521194458 / Valid loss: 7.200314989544096

Epoch: 12
Training loss: 0.808957576751709 / Valid loss: 6.973334121704101
Training loss: 0.8647042512893677 / Valid loss: 6.900782355808077
Training loss: 0.964614748954773 / Valid loss: 6.726683991295951
Training loss: 1.211578369140625 / Valid loss: 6.901320968355451
Training loss: 1.184460163116455 / Valid loss: 7.090978935786656

Epoch: 13
Training loss: 0.785484790802002 / Valid loss: 7.72143703188215
Training loss: 0.8137133717536926 / Valid loss: 6.860293036415463
Training loss: 0.9294409155845642 / Valid loss: 7.357797645387196
Training loss: 0.8816201090812683 / Valid loss: 6.893089180900937
Training loss: 0.7556352019309998 / Valid loss: 7.004174990881057

Epoch: 14
Training loss: 0.9726812243461609 / Valid loss: 7.121114428838094
Training loss: 0.6533987522125244 / Valid loss: 6.785783045632499
Training loss: 1.1122817993164062 / Valid loss: 6.872415744690668
Training loss: 0.6795768737792969 / Valid loss: 7.161128947848366
Training loss: 1.2054206132888794 / Valid loss: 7.020729609898159

Epoch: 15
Training loss: 0.6204168796539307 / Valid loss: 6.883711839857556
Training loss: 1.2871613502502441 / Valid loss: 7.012847995758056
Training loss: 0.8659645318984985 / Valid loss: 6.839447852543422
Training loss: 0.9139481782913208 / Valid loss: 6.939008117857433
Training loss: 0.8593806028366089 / Valid loss: 6.802086017245338

Epoch: 16
Training loss: 0.53940749168396 / Valid loss: 6.9785969370887395
Training loss: 0.5532370805740356 / Valid loss: 6.923516591389974
Training loss: 0.6132656335830688 / Valid loss: 6.90730920065017
Training loss: 0.6715787053108215 / Valid loss: 6.915744472685314
Training loss: 0.5064013004302979 / Valid loss: 7.258493564242408

Epoch: 17
Training loss: 0.8739537596702576 / Valid loss: 6.999483746574039
Training loss: 1.2047475576400757 / Valid loss: 6.833454740615118
Training loss: 0.814422070980072 / Valid loss: 7.017788610004243
Training loss: 0.5512487888336182 / Valid loss: 6.942955316816057
Training loss: 0.7930984497070312 / Valid loss: 6.872358394804455

Epoch: 18
Training loss: 0.6464129686355591 / Valid loss: 6.939476201647804
Training loss: 0.8608871698379517 / Valid loss: 7.0141416322617305
Training loss: 0.7545645236968994 / Valid loss: 7.13192974726359
Training loss: 0.7856454253196716 / Valid loss: 6.881633154551188
Training loss: 1.0869135856628418 / Valid loss: 7.21874339239938

Epoch: 19
Training loss: 1.059412956237793 / Valid loss: 7.130672954377674
Training loss: 1.1742831468582153 / Valid loss: 6.99260645820981
Training loss: 0.7287744879722595 / Valid loss: 6.95410057703654
Training loss: 0.663609504699707 / Valid loss: 6.842352708180745

Epoch: 20
Training loss: 0.5677882432937622 / Valid loss: 6.791662020910354
Training loss: 1.2225005626678467 / Valid loss: 6.889434378487723
Training loss: 0.47219428420066833 / Valid loss: 6.808534442810785
Training loss: 0.8761451840400696 / Valid loss: 6.8158988135201595
Training loss: 1.0279066562652588 / Valid loss: 6.7720060007912775

Epoch: 21
Training loss: 0.4475374221801758 / Valid loss: 6.937840098426456
Training loss: 0.5406501293182373 / Valid loss: 7.365072645459857
Training loss: 0.6003029346466064 / Valid loss: 7.08115739368257
Training loss: 0.7370328903198242 / Valid loss: 6.987155205862862
Training loss: 0.6210228204727173 / Valid loss: 6.9006127198537195

Epoch: 22
Training loss: 0.7910249829292297 / Valid loss: 6.7880048751831055
Training loss: 0.38785022497177124 / Valid loss: 6.869932310921805
Training loss: 0.6510220766067505 / Valid loss: 6.9390557062058225
Training loss: 1.272037386894226 / Valid loss: 6.995921611785889
Training loss: 0.9684562087059021 / Valid loss: 7.202456410725912

Epoch: 23
Training loss: 0.5385011434555054 / Valid loss: 6.851070585704985
Training loss: 1.321436882019043 / Valid loss: 6.890108635312035
Training loss: 0.9871132969856262 / Valid loss: 6.8747127146947955
Training loss: 0.6911038756370544 / Valid loss: 7.021406166894096
Training loss: 0.7842404842376709 / Valid loss: 6.97103283745902

Epoch: 24
Training loss: 0.8393596410751343 / Valid loss: 6.838232290177118
Training loss: 0.6872751712799072 / Valid loss: 6.7430394081842335
Training loss: 0.6175047159194946 / Valid loss: 6.862519613901774
Training loss: 0.6735203266143799 / Valid loss: 6.916020125434512
Training loss: 0.992667555809021 / Valid loss: 7.099601443608602

Epoch: 25
Training loss: 0.6753808856010437 / Valid loss: 6.762231259118943
Training loss: 0.43187612295150757 / Valid loss: 6.86820308140346
Training loss: 1.1698179244995117 / Valid loss: 6.860477633703322
Training loss: 0.4223865270614624 / Valid loss: 6.918870467231387
Training loss: 0.669593095779419 / Valid loss: 7.029194404965355

Epoch: 26
Training loss: 0.9046597480773926 / Valid loss: 6.797090825580415
Training loss: 1.0334348678588867 / Valid loss: 6.761004184541248
Training loss: 0.34703996777534485 / Valid loss: 6.90616911479405
Training loss: 0.3122824430465698 / Valid loss: 6.86344967796689
Training loss: 0.48323753476142883 / Valid loss: 7.03172699383327

Epoch: 27
Training loss: 0.36437496542930603 / Valid loss: 6.933468312308902
Training loss: 0.6029342412948608 / Valid loss: 6.901174036661784
Training loss: 0.614423394203186 / Valid loss: 6.947794346582322
Training loss: 0.4617449939250946 / Valid loss: 6.9538627465566
Training loss: 0.6803674697875977 / Valid loss: 7.0249468712579635

Epoch: 28
Training loss: 0.387935996055603 / Valid loss: 6.901970599946521
Training loss: 0.6165776252746582 / Valid loss: 6.909543144135248
Training loss: 0.5400832891464233 / Valid loss: 6.861772092183431
Training loss: 0.3511140048503876 / Valid loss: 7.040414101736886
Training loss: 0.539892315864563 / Valid loss: 6.983102199009487

Epoch: 29
Training loss: 0.5058392882347107 / Valid loss: 6.861928603762672
Training loss: 0.7486938238143921 / Valid loss: 7.155505398341588
Training loss: 0.3672358989715576 / Valid loss: 6.811366380964007
Training loss: 0.3118017911911011 / Valid loss: 6.879478425071353

Epoch: 30
Training loss: 0.6928936243057251 / Valid loss: 7.0182854947589695
Training loss: 0.44543325901031494 / Valid loss: 6.757925156184605
Training loss: 0.3234328627586365 / Valid loss: 7.008895224616641
Training loss: 0.45212942361831665 / Valid loss: 6.878146564392816
Training loss: 0.3734869956970215 / Valid loss: 6.909860987890334

Epoch: 31
Training loss: 0.35912203788757324 / Valid loss: 6.911509184610276
Training loss: 0.43792688846588135 / Valid loss: 7.023451936812628
Training loss: 0.5828892588615417 / Valid loss: 6.85651330947876
Training loss: 0.4035741984844208 / Valid loss: 7.197467427026658
Training loss: 0.5325425267219543 / Valid loss: 6.896336519150507

Epoch: 32
Training loss: 0.46798238158226013 / Valid loss: 6.920280415671212
Training loss: 0.5129421949386597 / Valid loss: 6.855489594595773
Training loss: 0.3181908130645752 / Valid loss: 6.914389796484084
Training loss: 0.29038310050964355 / Valid loss: 6.8406507128760925
Training loss: 0.5647710561752319 / Valid loss: 6.835189873831613

Epoch: 33
Training loss: 0.4031444489955902 / Valid loss: 6.939921156565348
Training loss: 0.49439507722854614 / Valid loss: 6.8127959501175654
Training loss: 0.36235883831977844 / Valid loss: 6.861376587549845
Training loss: 0.572578489780426 / Valid loss: 6.858007941927228
Training loss: 0.45658621191978455 / Valid loss: 6.850246170588902

Epoch: 34
Training loss: 0.6909438371658325 / Valid loss: 6.767181632632301
Training loss: 0.3100060224533081 / Valid loss: 6.902636882237026
Training loss: 0.48002326488494873 / Valid loss: 6.956700111570813
Training loss: 0.33002769947052 / Valid loss: 6.885793299902053
Training loss: 0.4485366642475128 / Valid loss: 7.008176167805989

Epoch: 35
Training loss: 0.9865694046020508 / Valid loss: 6.830015218825567
Training loss: 0.5776498913764954 / Valid loss: 6.897862325395857
Training loss: 0.5171666741371155 / Valid loss: 6.8616849853878925
Training loss: 0.3185175657272339 / Valid loss: 6.861160954974946
Training loss: 0.4183725118637085 / Valid loss: 6.805342181523641

Epoch: 36
Training loss: 0.4354248046875 / Valid loss: 6.7071679569426035
Training loss: 0.6866554021835327 / Valid loss: 6.8485821383340015
Training loss: 0.4942253530025482 / Valid loss: 6.748570701054164
Training loss: 0.30083948373794556 / Valid loss: 6.890221507208688
Training loss: 0.4545312523841858 / Valid loss: 6.851118828001477

Epoch: 37
Training loss: 0.5846376419067383 / Valid loss: 7.004612102962676
Training loss: 0.25425416231155396 / Valid loss: 6.803554030827113
Training loss: 0.46477121114730835 / Valid loss: 6.90566330864316
Training loss: 0.9308645725250244 / Valid loss: 6.774234317597889
Training loss: 0.6394990086555481 / Valid loss: 6.859247870672316

Epoch: 38
Training loss: 0.5229790806770325 / Valid loss: 6.859157198951358
Training loss: 0.4742722511291504 / Valid loss: 6.826846862974621
Training loss: 0.46572309732437134 / Valid loss: 6.895450078873408
Training loss: 0.3612024188041687 / Valid loss: 6.895119044894264
Training loss: 0.7249857187271118 / Valid loss: 6.882613790602911

Epoch: 39
Training loss: 0.46001923084259033 / Valid loss: 6.975711663564046
Training loss: 0.3459772765636444 / Valid loss: 6.810472106933593
Training loss: 0.4002076983451843 / Valid loss: 6.844748805818104
Training loss: 0.3169283866882324 / Valid loss: 6.756377181552705

Epoch: 40
Training loss: 0.3688444197177887 / Valid loss: 6.988716581889562
Training loss: 0.31692153215408325 / Valid loss: 6.891211704980759
Training loss: 0.3872334361076355 / Valid loss: 6.786558782486688
Training loss: 0.2691856026649475 / Valid loss: 6.79910143897647
Training loss: 0.2676779329776764 / Valid loss: 6.852081425984701

Epoch: 41
Training loss: 0.584932804107666 / Valid loss: 6.849190176100958
Training loss: 0.4023456275463104 / Valid loss: 6.797752480279832
Training loss: 0.30325448513031006 / Valid loss: 6.947733865465437
Training loss: 1.2078900337219238 / Valid loss: 6.976276002611432
Training loss: 0.4149470925331116 / Valid loss: 6.876149756567819

Epoch: 42
Training loss: 0.39032602310180664 / Valid loss: 6.709031223115467
Training loss: 0.3001871705055237 / Valid loss: 6.800860221045358
Training loss: 0.4784907400608063 / Valid loss: 6.735300522758847
Training loss: 0.40767455101013184 / Valid loss: 6.84482813108535
Training loss: 0.4310424327850342 / Valid loss: 6.958239301045736

Epoch: 43
Training loss: 0.4082128703594208 / Valid loss: 6.715197015943981
Training loss: 0.37803182005882263 / Valid loss: 6.753293100992838
Training loss: 0.16625966131687164 / Valid loss: 6.765981011163621
Training loss: 0.49290910363197327 / Valid loss: 7.081740915207636
Training loss: 0.5333818197250366 / Valid loss: 6.968315097263881

Epoch: 44
Training loss: 0.28737175464630127 / Valid loss: 6.7073313486008415
Training loss: 0.1722629964351654 / Valid loss: 6.769401248296102
Training loss: 0.47909608483314514 / Valid loss: 6.752234554290771
Training loss: 0.2275506556034088 / Valid loss: 6.687825638907296
Training loss: 0.5222076773643494 / Valid loss: 6.724186658859253

Epoch: 45
Training loss: 0.32304131984710693 / Valid loss: 6.887733316421508
Training loss: 0.5832504630088806 / Valid loss: 6.705071138200306
Training loss: 0.5492264628410339 / Valid loss: 6.7909575462341305
Training loss: 0.4111666977405548 / Valid loss: 6.701044732048398
Training loss: 0.36251306533813477 / Valid loss: 6.762984625498453

Epoch: 46
Training loss: 0.2727862000465393 / Valid loss: 6.678364667438325
Training loss: 0.32198768854141235 / Valid loss: 6.721526586441767
Training loss: 0.48535650968551636 / Valid loss: 6.803485983893985
Training loss: 0.3723664879798889 / Valid loss: 6.8428798766363235
Training loss: 0.6073732972145081 / Valid loss: 6.881512941632952

Epoch: 47
Training loss: 0.23989813029766083 / Valid loss: 6.78193108694894
Training loss: 0.4724132716655731 / Valid loss: 6.708305699484689
Training loss: 0.5187866687774658 / Valid loss: 6.682447340374901
Training loss: 0.3214375972747803 / Valid loss: 6.768702675047375
Training loss: 0.3901714086532593 / Valid loss: 6.913385995229086

Epoch: 48
Training loss: 0.253079354763031 / Valid loss: 6.744001343136742
Training loss: 0.5310543179512024 / Valid loss: 6.735933394659133
Training loss: 0.5105814933776855 / Valid loss: 6.80726622626895
Training loss: 0.3190954327583313 / Valid loss: 6.706569435482933
Training loss: 0.3793792128562927 / Valid loss: 6.774620124271938

Epoch: 49
Training loss: 0.4124816358089447 / Valid loss: 6.680840501331148
Training loss: 0.31002432107925415 / Valid loss: 6.703553254263742
Training loss: 0.24713607132434845 / Valid loss: 6.760096391042073
Training loss: 0.30090177059173584 / Valid loss: 6.810816387903123

Epoch: 50
Training loss: 0.3706962764263153 / Valid loss: 6.827458272661482
Training loss: 0.38320428133010864 / Valid loss: 6.750355111984979
Training loss: 0.2868631184101105 / Valid loss: 6.659401076180594
Training loss: 0.2883872389793396 / Valid loss: 6.748834330695016
Training loss: 0.562008261680603 / Valid loss: 6.719065700258527

Epoch: 51
Training loss: 0.2471512109041214 / Valid loss: 6.74639112381708
Training loss: 0.3177463412284851 / Valid loss: 6.718392630985805
Training loss: 0.289815753698349 / Valid loss: 6.743921348026821
Training loss: 0.3917962908744812 / Valid loss: 6.61454481851487
Training loss: 0.40255868434906006 / Valid loss: 6.645039985293434

Epoch: 52
Training loss: 0.20757117867469788 / Valid loss: 6.723773229689825
Training loss: 0.33904463052749634 / Valid loss: 6.709273937770298
Training loss: 0.30311405658721924 / Valid loss: 6.691586217426118
Training loss: 0.22939956188201904 / Valid loss: 6.632260150001162
Training loss: 0.4663572907447815 / Valid loss: 6.685382527396793

Epoch: 53
Training loss: 0.3068365454673767 / Valid loss: 6.78039832569304
Training loss: 0.5012734532356262 / Valid loss: 6.738596139635359
Training loss: 0.27735966444015503 / Valid loss: 6.5801393554324195
Training loss: 0.6502377986907959 / Valid loss: 6.822552317664737
Training loss: 0.35023748874664307 / Valid loss: 6.792698387872605

Epoch: 54
Training loss: 0.41845959424972534 / Valid loss: 6.687618705204555
Training loss: 0.463904470205307 / Valid loss: 6.762713443665278
Training loss: 0.22641977667808533 / Valid loss: 6.850458449409121
Training loss: 0.35759979486465454 / Valid loss: 6.697037674131847
Training loss: 0.4495754837989807 / Valid loss: 6.746020112718854

Epoch: 55
Training loss: 0.563720166683197 / Valid loss: 6.671433485121954
Training loss: 0.47859108448028564 / Valid loss: 6.6954275948660715
Training loss: 0.3678862452507019 / Valid loss: 6.809267498198009
Training loss: 0.4210607409477234 / Valid loss: 6.791607280004592
Training loss: 0.33094897866249084 / Valid loss: 6.717492662157331

Epoch: 56
Training loss: 0.6248446702957153 / Valid loss: 6.727197819664365
Training loss: 0.20745807886123657 / Valid loss: 6.7060022444952105
Training loss: 0.2868157625198364 / Valid loss: 6.61083265032087
Training loss: 0.22298631072044373 / Valid loss: 6.6888314065479095
Training loss: 0.3402419984340668 / Valid loss: 6.873394670940581

Epoch: 57
Training loss: 0.5975265502929688 / Valid loss: 6.7056925660087945
Training loss: 0.6231029629707336 / Valid loss: 6.654087171100435
Training loss: 0.39805570244789124 / Valid loss: 6.702675823938279
Training loss: 0.5699640512466431 / Valid loss: 6.68742641721453
Training loss: 0.4310111701488495 / Valid loss: 6.810134297325497

Epoch: 58
Training loss: 0.33928000926971436 / Valid loss: 6.7944656871614
Training loss: 0.5132818222045898 / Valid loss: 6.822099140712193
Training loss: 0.1570674031972885 / Valid loss: 6.769105379922049
Training loss: 0.442685067653656 / Valid loss: 6.637626884097145
Training loss: 0.2664511203765869 / Valid loss: 6.698658707028343

Epoch: 59
Training loss: 0.30701684951782227 / Valid loss: 6.684155486878895
Training loss: 0.49762654304504395 / Valid loss: 6.594595109848749
Training loss: 0.20613618195056915 / Valid loss: 6.642565118698847
Training loss: 0.31091612577438354 / Valid loss: 6.763343325115385

Epoch: 60
Training loss: 0.3153267502784729 / Valid loss: 7.0421697026207335
Training loss: 0.6598629951477051 / Valid loss: 6.864576725732713
Training loss: 0.2438582330942154 / Valid loss: 6.6290724572681246
Training loss: 0.3429989814758301 / Valid loss: 6.618421645391555
Training loss: 0.21515816450119019 / Valid loss: 6.674779733022054

Epoch: 61
Training loss: 0.3506452143192291 / Valid loss: 6.654799041293916
Training loss: 0.28080666065216064 / Valid loss: 6.598044009435744
Training loss: 0.6038864850997925 / Valid loss: 6.712476226261684
Training loss: 0.36101478338241577 / Valid loss: 6.689102495284308
Training loss: 0.19444799423217773 / Valid loss: 6.755177529652913

Epoch: 62
Training loss: 0.21449124813079834 / Valid loss: 6.597038614182245
Training loss: 0.59621661901474 / Valid loss: 6.688204840251378
Training loss: 0.2546719014644623 / Valid loss: 6.592263884771437
Training loss: 0.3417029082775116 / Valid loss: 6.633792804536365
Training loss: 0.29332274198532104 / Valid loss: 6.718352778752645

Epoch: 63
Training loss: 0.3389047384262085 / Valid loss: 6.622619578951881
Training loss: 0.3676445782184601 / Valid loss: 6.581061167944045
Training loss: 0.17669038474559784 / Valid loss: 6.6130304086776
Training loss: 0.40068429708480835 / Valid loss: 6.686364096686954
Training loss: 0.17529423534870148 / Valid loss: 6.713154234204974

Epoch: 64
Training loss: 0.2943063974380493 / Valid loss: 6.490254159200759
Training loss: 0.1906624585390091 / Valid loss: 6.567365957441784
Training loss: 0.5505138039588928 / Valid loss: 6.621095884413946
Training loss: 0.23796790838241577 / Valid loss: 6.766010693141392
Training loss: 0.24791644513607025 / Valid loss: 6.822465442475819

Epoch: 65
Training loss: 0.2073822319507599 / Valid loss: 6.5285111404600595
Training loss: 0.39523953199386597 / Valid loss: 6.591360870997111
Training loss: 0.3413603901863098 / Valid loss: 6.745652353195917
Training loss: 0.30060210824012756 / Valid loss: 6.671729065123058
Training loss: 1.2290129661560059 / Valid loss: 6.700179517836798

Epoch: 66
Training loss: 0.35805588960647583 / Valid loss: 6.5398833978743784
Training loss: 0.26997658610343933 / Valid loss: 6.533526207151867
Training loss: 0.5388424396514893 / Valid loss: 6.652726566223871
Training loss: 0.18387764692306519 / Valid loss: 6.598471357708886
Training loss: 0.4257374703884125 / Valid loss: 6.709581393287295

Epoch: 67
Training loss: 0.342794805765152 / Valid loss: 6.653990073431106
Training loss: 0.18486344814300537 / Valid loss: 6.5737508796510244
Training loss: 0.19393864274024963 / Valid loss: 6.728783398582822
Training loss: 0.741718053817749 / Valid loss: 6.671461654844738
Training loss: 0.1896497756242752 / Valid loss: 6.588567275092715

Epoch: 68
Training loss: 0.25574779510498047 / Valid loss: 6.738128162565685
Training loss: 0.32374680042266846 / Valid loss: 6.723837298438663
Training loss: 0.23225489258766174 / Valid loss: 6.698892003013974
Training loss: 0.2614475190639496 / Valid loss: 6.671468932288033
Training loss: 0.4797174334526062 / Valid loss: 6.680914987836565

Epoch: 69
Training loss: 0.37791913747787476 / Valid loss: 6.580595207214356
Training loss: 0.18169423937797546 / Valid loss: 6.662476162683396
Training loss: 0.3053697347640991 / Valid loss: 6.743009045010521
Training loss: 0.2857421040534973 / Valid loss: 6.680840984980265

Epoch: 70
Training loss: 0.15205365419387817 / Valid loss: 6.652647699628558
Training loss: 0.42214784026145935 / Valid loss: 6.69452851159232
Training loss: 0.27521079778671265 / Valid loss: 6.634945447104318
Training loss: 0.2875935435295105 / Valid loss: 6.692101669311524
Training loss: 0.34156426787376404 / Valid loss: 6.719008218674433

Epoch: 71
Training loss: 0.3640439510345459 / Valid loss: 6.77816378729684
Training loss: 0.48805707693099976 / Valid loss: 6.650673938932873
Training loss: 0.27437493205070496 / Valid loss: 6.5628108887445356
Training loss: 0.2233055830001831 / Valid loss: 6.581970973241897
Training loss: 0.47270312905311584 / Valid loss: 6.63338741120838

Epoch: 72
Training loss: 0.19438359141349792 / Valid loss: 6.579217956179664
Training loss: 0.433140367269516 / Valid loss: 6.6487732115246
Training loss: 0.2627412676811218 / Valid loss: 6.624934677850632
Training loss: 0.2024679183959961 / Valid loss: 6.592256541479202
Training loss: 0.49399858713150024 / Valid loss: 6.767768255869547

Epoch: 73
Training loss: 0.45087727904319763 / Valid loss: 6.652540788196382
Training loss: 0.3708835244178772 / Valid loss: 6.635646942683628
Training loss: 0.2231827676296234 / Valid loss: 6.662844662439255
Training loss: 0.2669631242752075 / Valid loss: 6.646315860748291
Training loss: 0.23651760816574097 / Valid loss: 6.730236961728051

Epoch: 74
Training loss: 0.19732210040092468 / Valid loss: 6.656298237755185
Training loss: 0.3130336403846741 / Valid loss: 6.5522129967099145
Training loss: 0.44681164622306824 / Valid loss: 6.492975929805211
Training loss: 0.31222066283226013 / Valid loss: 6.663017520450411
Training loss: 0.20401397347450256 / Valid loss: 6.589029700415475

Epoch: 75
Training loss: 0.4409768581390381 / Valid loss: 6.595940844217936
Training loss: 0.32814905047416687 / Valid loss: 6.648918533325196
Training loss: 0.5441973209381104 / Valid loss: 6.6851639066423685
Training loss: 0.2011454999446869 / Valid loss: 6.55321284702846
Training loss: 0.4068012833595276 / Valid loss: 6.61397309530349

Epoch: 76
Training loss: 0.3210447430610657 / Valid loss: 6.533609689985003
Training loss: 0.4163230061531067 / Valid loss: 6.5234950837634855
Training loss: 0.22019372880458832 / Valid loss: 6.769771466936384
Training loss: 0.28016138076782227 / Valid loss: 6.584591182072957
Training loss: 0.20006026327610016 / Valid loss: 6.623555912290301

Epoch: 77
Training loss: 0.19589956104755402 / Valid loss: 6.520887567883446
Training loss: 0.2464345097541809 / Valid loss: 6.551894231069656
Training loss: 0.4143565595149994 / Valid loss: 6.629134019215901
Training loss: 0.40897876024246216 / Valid loss: 6.619436840783982
Training loss: 0.5242934226989746 / Valid loss: 6.673334307897658

Epoch: 78
Training loss: 0.2526826560497284 / Valid loss: 6.635404759361631
Training loss: 0.15289264917373657 / Valid loss: 6.574841299511138
Training loss: 0.2966405153274536 / Valid loss: 6.5411285854521255
Training loss: 0.21035808324813843 / Valid loss: 6.629784123102824
Training loss: 0.4658309817314148 / Valid loss: 6.577439689636231

Epoch: 79
Training loss: 0.15384513139724731 / Valid loss: 6.599957752227783
Training loss: 0.31102845072746277 / Valid loss: 6.582543940771194
Training loss: 0.25043123960494995 / Valid loss: 6.562869358062744
Training loss: 0.2823439836502075 / Valid loss: 6.599431051526751
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model: 5.965176051003592
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 80
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0
momentum : 0
embedder : Bert
verbose : False
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
neural net:
 [tensor([[ 0.0035, -0.0035,  0.0052,  ..., -0.0130,  0.0056, -0.0088],
        [ 0.0045, -0.0089, -0.0107,  ...,  0.0052,  0.0048, -0.0076],
        [ 0.0014, -0.0062,  0.0084,  ..., -0.0093,  0.0089,  0.0006],
        ...,
        [ 0.0115,  0.0001, -0.0032,  ..., -0.0018, -0.0062, -0.0085],
        [-0.0009,  0.0057,  0.0107,  ..., -0.0067, -0.0022, -0.0107],
        [ 0.0109, -0.0116, -0.0054,  ...,  0.0007, -0.0127,  0.0088]],
       device='cuda:0'), tensor([ 0.0047,  0.0122,  0.0094,  ...,  0.0037, -0.0029, -0.0056],
       device='cuda:0'), tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), tensor([[-0.0121, -0.0085,  0.0158,  ...,  0.0067, -0.0057, -0.0189],
        [ 0.0064, -0.0150,  0.0096,  ...,  0.0006,  0.0074,  0.0177],
        [ 0.0061, -0.0078,  0.0059,  ...,  0.0032, -0.0206,  0.0205],
        ...,
        [ 0.0173, -0.0032,  0.0090,  ..., -0.0031,  0.0141,  0.0130],
        [-0.0181, -0.0101,  0.0157,  ..., -0.0020, -0.0070,  0.0160],
        [ 0.0063, -0.0152, -0.0101,  ...,  0.0008,  0.0028, -0.0011]],
       device='cuda:0'), tensor([-1.1967e-02,  1.1110e-02,  1.8701e-02,  1.1023e-02, -2.0298e-02,
         5.6226e-05, -1.3542e-02,  2.7363e-03, -8.2064e-03,  1.8652e-02,
         1.4701e-02,  1.2389e-02,  2.1926e-02,  1.5760e-02, -3.8398e-03,
        -2.1899e-02,  2.2099e-02, -5.7002e-03,  2.0446e-03,  2.1733e-02,
         1.5114e-02,  2.0789e-03, -1.0899e-02, -5.6587e-03,  2.0192e-02,
         4.3223e-03,  8.8637e-03, -1.6798e-02,  1.7955e-02,  1.2753e-02,
        -5.4965e-03,  1.9481e-02,  6.9524e-03, -2.0116e-02, -5.9529e-03,
        -1.1193e-02,  1.8363e-02,  1.6020e-02, -1.6430e-02,  6.4647e-03,
         1.5684e-02,  9.1588e-03, -2.0390e-02,  9.9621e-03,  2.0163e-02,
         2.1748e-02, -1.8861e-02, -5.0238e-03,  1.9386e-02,  1.1112e-03,
        -1.3363e-02,  6.7029e-03,  1.7814e-02, -8.2254e-03,  1.3094e-02,
         1.2228e-02, -1.2875e-03,  3.9142e-03,  1.4362e-03,  5.9424e-03,
         2.2414e-03, -1.3438e-02, -7.4102e-03,  1.5021e-02, -1.8291e-02,
         1.7939e-02,  2.4626e-03, -1.6420e-03,  1.0483e-03, -6.6787e-03,
         1.9978e-02,  6.3201e-04,  1.7342e-02,  9.4639e-03,  2.2343e-02,
        -1.5528e-02,  1.0803e-02, -4.7853e-03,  1.3566e-02, -1.2095e-02,
        -5.4629e-03,  7.9233e-03, -1.5708e-02, -2.7313e-03,  8.8111e-03,
        -2.1376e-03,  2.0746e-02,  1.0951e-02, -1.4991e-02,  5.5452e-03,
        -1.4782e-02, -1.6080e-02, -5.0667e-03, -2.1050e-02, -7.7526e-03,
         1.9154e-02, -2.1401e-02, -1.0501e-02,  7.3819e-03,  2.0670e-03],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), tensor([[-0.0636, -0.0402,  0.0180,  ..., -0.0669, -0.0184, -0.0632],
        [-0.0702, -0.0480,  0.0203,  ...,  0.0432, -0.0736,  0.0113],
        [-0.0735, -0.0482,  0.0561,  ..., -0.0377,  0.0717,  0.0079],
        ...,
        [ 0.0336,  0.0795,  0.0686,  ...,  0.0617,  0.0593,  0.0145],
        [-0.0549, -0.0686,  0.0407,  ..., -0.0574,  0.0591, -0.0667],
        [-0.0106, -0.0514, -0.0024,  ...,  0.0682,  0.0980,  0.0754]],
       device='cuda:0'), tensor([-0.0598,  0.0784,  0.0041,  0.0437, -0.0314,  0.0230, -0.0737, -0.0741,
        -0.0710, -0.0826, -0.0965, -0.0397,  0.0959,  0.0370,  0.0845,  0.0369],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0'), tensor([[-0.0476,  0.1976,  0.0806, -0.1341,  0.1659,  0.0086, -0.0477,  0.0708,
          0.2368,  0.0414, -0.1084, -0.0428,  0.1062, -0.1102,  0.0168,  0.1324]],
       device='cuda:0'), tensor([0.1504], device='cuda:0')]

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 13.399635451180595
Model is saved in epoch 0, overall batch: 0
Training loss: 6.127030372619629 / Valid loss: 8.819717675163632
Model is saved in epoch 0, overall batch: 100
Training loss: 4.364099502563477 / Valid loss: 6.371737811678932
Model is saved in epoch 0, overall batch: 200
Training loss: 6.64052152633667 / Valid loss: 5.719195552099318
Model is saved in epoch 0, overall batch: 300
Training loss: 6.042747497558594 / Valid loss: 5.520333778290522
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 5.577243804931641 / Valid loss: 5.533714669091361
Training loss: 5.004234313964844 / Valid loss: 5.621288937614078
Training loss: 5.9308390617370605 / Valid loss: 5.55015804654076
Training loss: 4.668268203735352 / Valid loss: 5.53403328259786
Training loss: 4.401596546173096 / Valid loss: 5.578369969413394

Epoch: 2
Training loss: 6.164501190185547 / Valid loss: 5.500020435878208
Model is saved in epoch 2, overall batch: 1000
Training loss: 6.425787448883057 / Valid loss: 5.61622618720645
Training loss: 5.164256572723389 / Valid loss: 5.628493234089443
Training loss: 6.742783546447754 / Valid loss: 5.578919303984869
Training loss: 4.455643177032471 / Valid loss: 5.567807924179804

Epoch: 3
Training loss: 3.558081865310669 / Valid loss: 5.672660146440778
Training loss: 4.075849533081055 / Valid loss: 5.8182059742155525
Training loss: 5.273836135864258 / Valid loss: 5.57822737239656
Training loss: 3.2550714015960693 / Valid loss: 5.85000859215146
Training loss: 5.04761266708374 / Valid loss: 5.77729852994283

Epoch: 4
Training loss: 4.5832037925720215 / Valid loss: 5.890043653760638
Training loss: 4.638767242431641 / Valid loss: 5.875842223848616
Training loss: 4.182924270629883 / Valid loss: 5.857748565219698
Training loss: 5.140944480895996 / Valid loss: 5.778268911724999
Training loss: 4.2131147384643555 / Valid loss: 5.800540404092698

Epoch: 5
Training loss: 6.79914665222168 / Valid loss: 6.263358293260847
Training loss: 4.713563919067383 / Valid loss: 6.228647985912505
Training loss: 3.157804489135742 / Valid loss: 6.161113966078985
Training loss: 3.8821210861206055 / Valid loss: 5.930653780982608
Training loss: 3.887453556060791 / Valid loss: 5.941768146696544

Epoch: 6
Training loss: 6.122968673706055 / Valid loss: 6.3214335487002415
Training loss: 3.285597324371338 / Valid loss: 6.374911083493914
Training loss: 3.578014373779297 / Valid loss: 6.5801569416409444
Training loss: 3.965419292449951 / Valid loss: 6.214591870989119
Training loss: 3.384624719619751 / Valid loss: 6.468242118472145

Epoch: 7
Training loss: 3.21475887298584 / Valid loss: 6.497412256967454
Training loss: 2.1728620529174805 / Valid loss: 6.684479788371495
Training loss: 3.4841299057006836 / Valid loss: 6.710339098884946
Training loss: 4.321338653564453 / Valid loss: 6.417580468314035
Training loss: 2.972651481628418 / Valid loss: 6.868036967232114

Epoch: 8
Training loss: 1.7738990783691406 / Valid loss: 7.195124421800886
Training loss: 2.1441798210144043 / Valid loss: 6.642926300139655
Training loss: 3.251882553100586 / Valid loss: 6.759008693695068
Training loss: 4.096506118774414 / Valid loss: 6.78841270946321
Training loss: 3.183248519897461 / Valid loss: 6.636155123937698

Epoch: 9
Training loss: 2.029506206512451 / Valid loss: 6.818800658271426
Training loss: 2.288989543914795 / Valid loss: 6.828665347326369
Training loss: 3.3808815479278564 / Valid loss: 6.763192390260242
Training loss: 2.3981070518493652 / Valid loss: 7.739439296722412

Epoch: 10
Training loss: 1.4216480255126953 / Valid loss: 7.388351267860049
Training loss: 1.8197413682937622 / Valid loss: 7.151865641276042
Training loss: 1.5796442031860352 / Valid loss: 6.846092355818976
Training loss: 2.577202081680298 / Valid loss: 6.945623552231561
Training loss: 1.7377214431762695 / Valid loss: 7.782596015930176

Epoch: 11
Training loss: 1.0891811847686768 / Valid loss: 7.376577949523925
Training loss: 1.5657374858856201 / Valid loss: 7.172021066574823
Training loss: 1.6767587661743164 / Valid loss: 7.089032963344029
Training loss: 1.4473447799682617 / Valid loss: 7.44668386777242
Training loss: 2.353316307067871 / Valid loss: 7.667879840305873

Epoch: 12
Training loss: 0.8911303877830505 / Valid loss: 7.021634292602539
Training loss: 1.141905665397644 / Valid loss: 7.188700862157912
Training loss: 1.278510332107544 / Valid loss: 6.898002352033343
Training loss: 1.1357593536376953 / Valid loss: 7.265919263022287
Training loss: 1.4851073026657104 / Valid loss: 7.375135644276937

Epoch: 13
Training loss: 1.0675407648086548 / Valid loss: 7.076451192583357
Training loss: 1.0887376070022583 / Valid loss: 7.194370133536203
Training loss: 1.1945703029632568 / Valid loss: 7.195398943764823
Training loss: 1.4118218421936035 / Valid loss: 6.989992854708717
Training loss: 1.1957453489303589 / Valid loss: 7.465697842552548

Epoch: 14
Training loss: 0.9668686985969543 / Valid loss: 7.549950263613746
Training loss: 1.7238690853118896 / Valid loss: 7.141253562200637
Training loss: 1.3160755634307861 / Valid loss: 7.293925380706787
Training loss: 0.8440595865249634 / Valid loss: 7.21663301104591
Training loss: 1.5144141912460327 / Valid loss: 7.172609601702009

Epoch: 15
Training loss: 0.5426336526870728 / Valid loss: 7.424824369521368
Training loss: 1.441624641418457 / Valid loss: 7.28578322728475
Training loss: 0.6639300584793091 / Valid loss: 7.345029090699695
Training loss: 0.9602261185646057 / Valid loss: 7.381763621738979
Training loss: 1.199022650718689 / Valid loss: 7.348576191493443

Epoch: 16
Training loss: 0.8247591853141785 / Valid loss: 7.196861439659482
Training loss: 0.6559747457504272 / Valid loss: 7.253304481506348
Training loss: 0.5706334114074707 / Valid loss: 7.4278565043494815
Training loss: 1.1128830909729004 / Valid loss: 7.461133911496117
Training loss: 0.9497281312942505 / Valid loss: 7.330151507967995

Epoch: 17
Training loss: 1.0422170162200928 / Valid loss: 7.381999202001662
Training loss: 1.3179240226745605 / Valid loss: 7.1618699482509065
Training loss: 1.3021745681762695 / Valid loss: 7.322104590279715
Training loss: 0.7087141275405884 / Valid loss: 7.289461694444928
Training loss: 0.6131398677825928 / Valid loss: 7.1717011315482

Epoch: 18
Training loss: 0.6964162588119507 / Valid loss: 7.2683129946390785
Training loss: 0.7655309438705444 / Valid loss: 7.08795215969994
Training loss: 0.6940097808837891 / Valid loss: 7.168793280919393
Training loss: 0.9227856397628784 / Valid loss: 7.104040431976318
Training loss: 0.9503320455551147 / Valid loss: 7.439989203498477

Epoch: 19
Training loss: 1.2825490236282349 / Valid loss: 7.452070376986549
Training loss: 1.5297114849090576 / Valid loss: 7.438924239930652
Training loss: 1.0516955852508545 / Valid loss: 7.21810854048956
Training loss: 0.7481666803359985 / Valid loss: 7.474696667989095

Epoch: 20
Training loss: 0.5240218043327332 / Valid loss: 7.2247215861365905
Training loss: 1.0359026193618774 / Valid loss: 7.01406051544916
Training loss: 0.6213411688804626 / Valid loss: 7.096994400024414
Training loss: 1.0393610000610352 / Valid loss: 7.072528766450428
Training loss: 1.505245327949524 / Valid loss: 7.322477567763555

Epoch: 21
Training loss: 0.6410533785820007 / Valid loss: 7.139811697460356
Training loss: 0.5327749848365784 / Valid loss: 7.614635058811733
Training loss: 0.6896448135375977 / Valid loss: 7.432232411702474
Training loss: 0.7246840596199036 / Valid loss: 7.034297870454334
Training loss: 0.6732176542282104 / Valid loss: 7.112529209681919

Epoch: 22
Training loss: 0.7488296031951904 / Valid loss: 7.0415241922651015
Training loss: 0.43810153007507324 / Valid loss: 7.121858891986665
Training loss: 0.5923314094543457 / Valid loss: 7.196047010875883
Training loss: 1.1224586963653564 / Valid loss: 7.148241783323742
Training loss: 1.0960543155670166 / Valid loss: 7.282926409585135

Epoch: 23
Training loss: 0.6613410711288452 / Valid loss: 7.379645656404041
Training loss: 0.7701973915100098 / Valid loss: 7.032446177800496
Training loss: 0.7753510475158691 / Valid loss: 7.334156890142531
Training loss: 0.5383292436599731 / Valid loss: 7.004515000752041
Training loss: 0.7604090571403503 / Valid loss: 7.103110722133091

Epoch: 24
Training loss: 1.2114282846450806 / Valid loss: 6.867120738256546
Training loss: 0.5419753789901733 / Valid loss: 6.920538143884568
Training loss: 0.5897265672683716 / Valid loss: 7.347378356116159
Training loss: 0.8014463186264038 / Valid loss: 7.242246850331624
Training loss: 1.3461661338806152 / Valid loss: 7.075789029257638

Epoch: 25
Training loss: 0.6527553796768188 / Valid loss: 6.899279732931228
Training loss: 0.4484539330005646 / Valid loss: 6.924172608057658
Training loss: 0.7834568023681641 / Valid loss: 7.003453665687925
Training loss: 0.43774452805519104 / Valid loss: 7.3492748805454795
Training loss: 0.5079165101051331 / Valid loss: 7.132343700953892

Epoch: 26
Training loss: 1.0213607549667358 / Valid loss: 7.035318111238025
Training loss: 0.8303967118263245 / Valid loss: 6.960005424136207
Training loss: 0.6095854640007019 / Valid loss: 6.990484467006865
Training loss: 0.3739844560623169 / Valid loss: 6.997977036521548
Training loss: 0.6546720266342163 / Valid loss: 7.204307660602388

Epoch: 27
Training loss: 0.3490922451019287 / Valid loss: 6.8811381203787665
Training loss: 0.6245917081832886 / Valid loss: 6.948054860887074
Training loss: 0.6033289432525635 / Valid loss: 7.143658179328555
Training loss: 0.5362590551376343 / Valid loss: 7.027526187896728
Training loss: 0.6844990849494934 / Valid loss: 7.0928855169387095

Epoch: 28
Training loss: 0.5631611347198486 / Valid loss: 7.220712157658168
Training loss: 0.4681992828845978 / Valid loss: 6.9808939297993975
Training loss: 0.5096921920776367 / Valid loss: 7.013929980141776
Training loss: 0.3915059268474579 / Valid loss: 7.077820673443022
Training loss: 0.7864702939987183 / Valid loss: 7.266643333435058

Epoch: 29
Training loss: 0.7675877809524536 / Valid loss: 7.167349783579509
Training loss: 0.5293459892272949 / Valid loss: 6.968165688287645
Training loss: 0.42066216468811035 / Valid loss: 7.012243166423979
Training loss: 0.4675798714160919 / Valid loss: 7.008248878660656

Epoch: 30
Training loss: 0.689152717590332 / Valid loss: 7.069259302956717
Training loss: 0.4842417240142822 / Valid loss: 7.137249219985235
Training loss: 0.3676629364490509 / Valid loss: 7.02804977326166
Training loss: 0.5203980207443237 / Valid loss: 7.0413317816598076
Training loss: 0.3044419586658478 / Valid loss: 7.050244948977515

Epoch: 31
Training loss: 0.42016178369522095 / Valid loss: 6.902105474472046
Training loss: 0.4183787703514099 / Valid loss: 6.970627067202614
Training loss: 0.49750757217407227 / Valid loss: 7.0643341700236
Training loss: 0.37181830406188965 / Valid loss: 7.037158845719837
Training loss: 0.6152487993240356 / Valid loss: 7.122200321015858

Epoch: 32
Training loss: 0.3848465085029602 / Valid loss: 6.935148438953218
Training loss: 0.4010522961616516 / Valid loss: 6.868158136095319
Training loss: 0.2944841980934143 / Valid loss: 7.047273454212007
Training loss: 0.35725146532058716 / Valid loss: 6.8795719282967704
Training loss: 0.4350997805595398 / Valid loss: 7.101126832053775

Epoch: 33
Training loss: 0.3530556559562683 / Valid loss: 6.957195618039086
Training loss: 0.40896081924438477 / Valid loss: 7.0333207402910505
Training loss: 0.4382176399230957 / Valid loss: 6.992292533602034
Training loss: 0.43732786178588867 / Valid loss: 6.922511766070412
Training loss: 0.4189184010028839 / Valid loss: 6.986839857555571

Epoch: 34
Training loss: 0.8735959529876709 / Valid loss: 7.031001631418864
Training loss: 0.36404505372047424 / Valid loss: 6.8136503037952245
Training loss: 0.48089858889579773 / Valid loss: 6.983895229157947
Training loss: 0.33155110478401184 / Valid loss: 6.808161989847819
Training loss: 0.34232407808303833 / Valid loss: 6.949971784864153

Epoch: 35
Training loss: 0.9045617580413818 / Valid loss: 6.858074319930304
Training loss: 0.5713363289833069 / Valid loss: 7.007368051438105
Training loss: 0.4803009033203125 / Valid loss: 6.9159299123854865
Training loss: 0.41560590267181396 / Valid loss: 6.989744990212577
Training loss: 0.25549936294555664 / Valid loss: 7.076335602714902

Epoch: 36
Training loss: 0.43812042474746704 / Valid loss: 6.964874267578125
Training loss: 0.7390400171279907 / Valid loss: 6.944810256503877
Training loss: 0.6809818148612976 / Valid loss: 7.060747923169817
Training loss: 0.49954742193222046 / Valid loss: 7.173601718175979
Training loss: 0.5935614705085754 / Valid loss: 6.901555665334066

Epoch: 37
Training loss: 0.38267982006073 / Valid loss: 6.777287381035941
Training loss: 0.27827808260917664 / Valid loss: 7.029321152823312
Training loss: 0.5360932350158691 / Valid loss: 7.144682629903158
Training loss: 1.5006515979766846 / Valid loss: 6.902246284484863
Training loss: 0.6049326658248901 / Valid loss: 7.252998483748663

Epoch: 38
Training loss: 0.55263751745224 / Valid loss: 6.903541265215193
Training loss: 0.3912277817726135 / Valid loss: 7.067876543317523
Training loss: 0.4043325185775757 / Valid loss: 7.0610686211358935
Training loss: 0.4284650683403015 / Valid loss: 7.244679419199626
Training loss: 0.7228416204452515 / Valid loss: 7.018387458437965

Epoch: 39
Training loss: 0.4735035002231598 / Valid loss: 7.049049045926049
Training loss: 0.40887895226478577 / Valid loss: 6.9995390347072055
Training loss: 0.3522275984287262 / Valid loss: 6.915446358635312
Training loss: 0.269299179315567 / Valid loss: 7.012306485857282

Epoch: 40
Training loss: 0.46791696548461914 / Valid loss: 7.21376257623945
Training loss: 0.24236854910850525 / Valid loss: 7.250162551516579
Training loss: 0.5401368141174316 / Valid loss: 7.016613869439988
Training loss: 0.29310140013694763 / Valid loss: 6.90638062613351
Training loss: 0.28645098209381104 / Valid loss: 7.00866421744937

Epoch: 41
Training loss: 0.5362550616264343 / Valid loss: 6.995355810437884
Training loss: 0.27495113015174866 / Valid loss: 6.85082516670227
Training loss: 0.21118183434009552 / Valid loss: 6.991395550682431
Training loss: 1.0882055759429932 / Valid loss: 6.938548042660668
Training loss: 0.4951770007610321 / Valid loss: 6.982846600668771

Epoch: 42
Training loss: 0.3849049508571625 / Valid loss: 6.738570896784465
Training loss: 0.3295544683933258 / Valid loss: 6.960407647632417
Training loss: 0.403055876493454 / Valid loss: 6.893122212092082
Training loss: 0.4334084093570709 / Valid loss: 7.019912056695848
Training loss: 0.2634228467941284 / Valid loss: 6.813919103713262

Epoch: 43
Training loss: 0.3275986909866333 / Valid loss: 6.787533528464181
Training loss: 0.37901341915130615 / Valid loss: 6.7140169370742075
Training loss: 0.2204371690750122 / Valid loss: 6.905223751068116
Training loss: 0.5867722630500793 / Valid loss: 6.820777025676909
Training loss: 0.29604485630989075 / Valid loss: 6.878408391135079

Epoch: 44
Training loss: 0.4258541464805603 / Valid loss: 6.790409142630441
Training loss: 0.29014852643013 / Valid loss: 6.83801591963995
Training loss: 0.3502117991447449 / Valid loss: 6.908721901121593
Training loss: 0.3110758066177368 / Valid loss: 6.975122665223621
Training loss: 0.5069660544395447 / Valid loss: 6.998301328931536

Epoch: 45
Training loss: 0.22383712232112885 / Valid loss: 7.066485450381324
Training loss: 0.6398098468780518 / Valid loss: 6.908342568079631
Training loss: 0.5776716470718384 / Valid loss: 6.953870986756824
Training loss: 0.2700120806694031 / Valid loss: 6.669647546041579
Training loss: 0.3675152063369751 / Valid loss: 6.823161897205171

Epoch: 46
Training loss: 0.2949202060699463 / Valid loss: 6.878785646529424
Training loss: 0.245732843875885 / Valid loss: 6.792029535202753
Training loss: 0.6988308429718018 / Valid loss: 7.0100179922013055
Training loss: 0.20519594848155975 / Valid loss: 6.912310187021891
Training loss: 0.49549952149391174 / Valid loss: 6.97582597732544

Epoch: 47
Training loss: 0.27583441138267517 / Valid loss: 6.846087578364781
Training loss: 0.5250483751296997 / Valid loss: 6.914285911832537
Training loss: 0.36490076780319214 / Valid loss: 6.868118640354702
Training loss: 0.29066672921180725 / Valid loss: 6.962226640610468
Training loss: 0.4297463893890381 / Valid loss: 7.15857314609346

Epoch: 48
Training loss: 0.24943499267101288 / Valid loss: 6.807519131615049
Training loss: 0.4290311336517334 / Valid loss: 6.8909859975179035
Training loss: 0.48338553309440613 / Valid loss: 6.849553948356991
Training loss: 0.21784916520118713 / Valid loss: 6.697406864166259
Training loss: 0.395506352186203 / Valid loss: 6.838122831072126

Epoch: 49
Training loss: 0.4981532692909241 / Valid loss: 6.975083473750523
Training loss: 0.2853633165359497 / Valid loss: 6.841758741651263
Training loss: 0.2525501549243927 / Valid loss: 7.010902663639613
Training loss: 0.41843345761299133 / Valid loss: 6.8964976265316915

Epoch: 50
Training loss: 0.5022130608558655 / Valid loss: 6.791504832676479
Training loss: 0.4916205108165741 / Valid loss: 6.885678495679583
Training loss: 0.2293807715177536 / Valid loss: 6.683464994884672
Training loss: 0.321909099817276 / Valid loss: 6.89034925869533
Training loss: 0.4450467526912689 / Valid loss: 6.937083067212786

Epoch: 51
Training loss: 0.18771228194236755 / Valid loss: 7.028895478021531
Training loss: 0.31529486179351807 / Valid loss: 6.8530100686209545
Training loss: 0.2943069040775299 / Valid loss: 6.837882627759661
Training loss: 0.2967613935470581 / Valid loss: 6.755054414839972
Training loss: 0.39550405740737915 / Valid loss: 6.811524132319859

Epoch: 52
Training loss: 0.24211016297340393 / Valid loss: 6.751355534508114
Training loss: 0.24033130705356598 / Valid loss: 6.819704269227527
Training loss: 0.23116359114646912 / Valid loss: 6.808247516268776
Training loss: 0.22485482692718506 / Valid loss: 6.875391328902472
Training loss: 0.4469583034515381 / Valid loss: 6.884482138497489

Epoch: 53
Training loss: 0.24633878469467163 / Valid loss: 6.862690580458868
Training loss: 0.5703129768371582 / Valid loss: 6.873936871119908
Training loss: 0.304845929145813 / Valid loss: 6.933256898607526
Training loss: 0.5041393041610718 / Valid loss: 6.823003977820987
Training loss: 0.41058826446533203 / Valid loss: 6.761079093388148

Epoch: 54
Training loss: 0.3348295986652374 / Valid loss: 6.779568140847342
Training loss: 0.4200092554092407 / Valid loss: 6.920156251816523
Training loss: 0.2893892824649811 / Valid loss: 6.8204669112250915
Training loss: 0.23802801966667175 / Valid loss: 6.975771826789493
Training loss: 0.4505571722984314 / Valid loss: 6.8051290103367394

Epoch: 55
Training loss: 0.3780149221420288 / Valid loss: 6.833083665938604
Training loss: 0.2718570828437805 / Valid loss: 6.923740377880278
Training loss: 0.3408757150173187 / Valid loss: 6.843800490243094
Training loss: 0.3336857259273529 / Valid loss: 6.68986846833002
Training loss: 0.3254382610321045 / Valid loss: 6.798951798393613

Epoch: 56
Training loss: 0.44014471769332886 / Valid loss: 6.821119592303321
Training loss: 0.17795458436012268 / Valid loss: 6.901603821345738
Training loss: 0.24694913625717163 / Valid loss: 6.750751849583217
Training loss: 0.27183568477630615 / Valid loss: 6.82918301991054
Training loss: 0.34241318702697754 / Valid loss: 6.988560449509394

Epoch: 57
Training loss: 0.4569093585014343 / Valid loss: 6.757980333055769
Training loss: 0.45368272066116333 / Valid loss: 6.82969587416876
Training loss: 0.7224981784820557 / Valid loss: 7.009624817257836
Training loss: 0.4086109399795532 / Valid loss: 6.812248048328218
Training loss: 0.391318678855896 / Valid loss: 6.998959641229539

Epoch: 58
Training loss: 0.23931163549423218 / Valid loss: 6.773059245518276
Training loss: 0.6497039794921875 / Valid loss: 6.735242260070074
Training loss: 0.1888764351606369 / Valid loss: 6.947529449917021
Training loss: 0.4087572693824768 / Valid loss: 6.780976754143125
Training loss: 0.33315712213516235 / Valid loss: 6.876399639674595

Epoch: 59
Training loss: 0.2002645581960678 / Valid loss: 6.901693857283819
Training loss: 0.34995630383491516 / Valid loss: 6.677828107561384
Training loss: 0.22979050874710083 / Valid loss: 6.758138145719256
Training loss: 0.32330116629600525 / Valid loss: 6.634569581349691

Epoch: 60
Training loss: 0.3578627109527588 / Valid loss: 7.12086106254941
Training loss: 0.6669747829437256 / Valid loss: 6.771716644650414
Training loss: 0.19245126843452454 / Valid loss: 6.82211933590117
Training loss: 0.2516666650772095 / Valid loss: 6.69075095312936
Training loss: 0.14702095091342926 / Valid loss: 6.765072218577067

Epoch: 61
Training loss: 0.3248170018196106 / Valid loss: 6.743950496401106
Training loss: 0.27995240688323975 / Valid loss: 6.678152468090966
Training loss: 0.4588662385940552 / Valid loss: 6.967714439119612
Training loss: 0.40133923292160034 / Valid loss: 6.848690073830741
Training loss: 0.29317769408226013 / Valid loss: 6.791198185511997

Epoch: 62
Training loss: 0.19909627735614777 / Valid loss: 6.800127778734479
Training loss: 0.44061246514320374 / Valid loss: 6.722131906236921
Training loss: 0.23604106903076172 / Valid loss: 6.795142671040126
Training loss: 0.38966214656829834 / Valid loss: 6.858607907522292
Training loss: 0.3226313591003418 / Valid loss: 6.819323589688255

Epoch: 63
Training loss: 0.23920030891895294 / Valid loss: 6.770762143816267
Training loss: 0.330547034740448 / Valid loss: 6.9365644500369115
Training loss: 0.2723698019981384 / Valid loss: 6.823400374821254
Training loss: 0.3134589195251465 / Valid loss: 6.718061910356794
Training loss: 0.12761719524860382 / Valid loss: 6.753193528311593

Epoch: 64
Training loss: 0.2347167283296585 / Valid loss: 6.780518881479899
Training loss: 0.16747644543647766 / Valid loss: 6.8804623331342425
Training loss: 0.3943936228752136 / Valid loss: 6.802707978657314
Training loss: 0.2992875576019287 / Valid loss: 6.87196071715582
Training loss: 0.19548112154006958 / Valid loss: 6.8231279691060385

Epoch: 65
Training loss: 0.20990172028541565 / Valid loss: 6.825393622262137
Training loss: 0.47712793946266174 / Valid loss: 6.895734339668637
Training loss: 0.19779613614082336 / Valid loss: 6.770004054478236
Training loss: 0.20016932487487793 / Valid loss: 6.760459171022688
Training loss: 0.8677366971969604 / Valid loss: 6.886543026424589

Epoch: 66
Training loss: 0.23584794998168945 / Valid loss: 6.7338930629548575
Training loss: 0.2006128430366516 / Valid loss: 6.803289858500163
Training loss: 0.34674781560897827 / Valid loss: 6.847874723161969
Training loss: 0.26058679819107056 / Valid loss: 6.786290840875535
Training loss: 0.33887147903442383 / Valid loss: 6.937690108163016

Epoch: 67
Training loss: 0.2687024474143982 / Valid loss: 6.779158776147025
Training loss: 0.18528583645820618 / Valid loss: 6.715510313851492
Training loss: 0.28112515807151794 / Valid loss: 6.731954079582578
Training loss: 0.5666322708129883 / Valid loss: 6.993746739342099
Training loss: 0.23502834141254425 / Valid loss: 6.7815415632157094

Epoch: 68
Training loss: 0.27422475814819336 / Valid loss: 6.8047154381161645
Training loss: 0.2522346079349518 / Valid loss: 6.736034034547352
Training loss: 0.16804102063179016 / Valid loss: 6.993249725160145
Training loss: 0.27309340238571167 / Valid loss: 6.780811223529634
Training loss: 0.4907994270324707 / Valid loss: 6.722554315839495

Epoch: 69
Training loss: 0.270169734954834 / Valid loss: 6.654696362359183
Training loss: 0.2019234001636505 / Valid loss: 6.875401042756581
Training loss: 0.3915999233722687 / Valid loss: 6.744269698006766
Training loss: 0.3321993947029114 / Valid loss: 6.798256851377941

Epoch: 70
Training loss: 0.15277396142482758 / Valid loss: 6.799340075538272
Training loss: 0.35081416368484497 / Valid loss: 6.73735302289327
Training loss: 0.17039163410663605 / Valid loss: 6.786959520975748
Training loss: 0.24052681028842926 / Valid loss: 6.730819293430874
Training loss: 0.25248444080352783 / Valid loss: 6.694361055464972

Epoch: 71
Training loss: 0.3983907699584961 / Valid loss: 6.9452120917184015
Training loss: 0.33602553606033325 / Valid loss: 6.904700338272821
Training loss: 0.18474268913269043 / Valid loss: 6.745666780925933
Training loss: 0.15980398654937744 / Valid loss: 6.8464235532851445
Training loss: 0.41813355684280396 / Valid loss: 6.785341603415353

Epoch: 72
Training loss: 0.17966073751449585 / Valid loss: 6.752444514774141
Training loss: 0.4873162806034088 / Valid loss: 6.668183435712542
Training loss: 0.22473925352096558 / Valid loss: 6.703145335969471
Training loss: 0.2712843418121338 / Valid loss: 6.806968906947545
Training loss: 0.3317486047744751 / Valid loss: 6.703223964146205

Epoch: 73
Training loss: 0.30949437618255615 / Valid loss: 6.600711572737921
Training loss: 0.34906408190727234 / Valid loss: 6.796590920857021
Training loss: 0.3080303966999054 / Valid loss: 6.684067826043992
Training loss: 0.40322181582450867 / Valid loss: 6.775865743273781
Training loss: 0.1882997751235962 / Valid loss: 6.677812283379691

Epoch: 74
Training loss: 0.1359628140926361 / Valid loss: 6.681673930940174
Training loss: 0.2174985706806183 / Valid loss: 6.7226333345685685
Training loss: 0.3890039920806885 / Valid loss: 6.682108552115304
Training loss: 0.3262813985347748 / Valid loss: 6.710959348224458
Training loss: 0.3439076542854309 / Valid loss: 6.72366517384847

Epoch: 75
Training loss: 0.27852174639701843 / Valid loss: 6.707524295080276
Training loss: 0.2972131669521332 / Valid loss: 6.691528567813692
Training loss: 0.5231931209564209 / Valid loss: 6.7678989319574265
Training loss: 0.253578782081604 / Valid loss: 6.7402241343543645
Training loss: 0.33494675159454346 / Valid loss: 6.6984045028686525

Epoch: 76
Training loss: 0.29479843378067017 / Valid loss: 6.684396757398333
Training loss: 0.40875691175460815 / Valid loss: 6.647468303498767
Training loss: 0.18798518180847168 / Valid loss: 6.777016825903029
Training loss: 0.1941707581281662 / Valid loss: 6.7427805764334545
Training loss: 0.1324498951435089 / Valid loss: 6.849903701600574

Epoch: 77
Training loss: 0.1589755117893219 / Valid loss: 6.712900184449696
Training loss: 0.18605273962020874 / Valid loss: 6.854461313429333
Training loss: 0.2120855450630188 / Valid loss: 6.801223813919794
Training loss: 0.27976441383361816 / Valid loss: 6.685082281203497
Training loss: 0.39022016525268555 / Valid loss: 6.805451729184106

Epoch: 78
Training loss: 0.2694767713546753 / Valid loss: 6.734313583374023
Training loss: 0.1732800155878067 / Valid loss: 6.716629364376977
Training loss: 0.15089979767799377 / Valid loss: 6.769556488309588
Training loss: 0.18094484508037567 / Valid loss: 6.845108091263544
Training loss: 0.3911951184272766 / Valid loss: 6.6995929400126135

Epoch: 79
Training loss: 0.14435258507728577 / Valid loss: 6.638266211464291
Training loss: 0.2574714720249176 / Valid loss: 6.759087135678246
Training loss: 0.13571076095104218 / Valid loss: 6.740478238605317
Training loss: 0.26609545946121216 / Valid loss: 6.730795751299177
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model: 5.325703477859497
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 80
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : Adam
amsgrad : False
batchnorm : True
weightdecay : 0
momentum : 0
embedder : Bert
verbose : False
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
neural net:
 [tensor([[ 0.0035, -0.0035,  0.0052,  ..., -0.0130,  0.0056, -0.0088],
        [ 0.0045, -0.0089, -0.0107,  ...,  0.0052,  0.0048, -0.0076],
        [ 0.0014, -0.0062,  0.0084,  ..., -0.0093,  0.0089,  0.0006],
        ...,
        [ 0.0115,  0.0001, -0.0032,  ..., -0.0018, -0.0062, -0.0085],
        [-0.0009,  0.0057,  0.0107,  ..., -0.0067, -0.0022, -0.0107],
        [ 0.0109, -0.0116, -0.0054,  ...,  0.0007, -0.0127,  0.0088]],
       device='cuda:0'), tensor([ 0.0047,  0.0122,  0.0094,  ...,  0.0037, -0.0029, -0.0056],
       device='cuda:0'), tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), tensor([[-0.0121, -0.0085,  0.0158,  ...,  0.0067, -0.0057, -0.0189],
        [ 0.0064, -0.0150,  0.0096,  ...,  0.0006,  0.0074,  0.0177],
        [ 0.0061, -0.0078,  0.0059,  ...,  0.0032, -0.0206,  0.0205],
        ...,
        [ 0.0173, -0.0032,  0.0090,  ..., -0.0031,  0.0141,  0.0130],
        [-0.0181, -0.0101,  0.0157,  ..., -0.0020, -0.0070,  0.0160],
        [ 0.0063, -0.0152, -0.0101,  ...,  0.0008,  0.0028, -0.0011]],
       device='cuda:0'), tensor([-1.1967e-02,  1.1110e-02,  1.8701e-02,  1.1023e-02, -2.0298e-02,
         5.6226e-05, -1.3542e-02,  2.7363e-03, -8.2064e-03,  1.8652e-02,
         1.4701e-02,  1.2389e-02,  2.1926e-02,  1.5760e-02, -3.8398e-03,
        -2.1899e-02,  2.2099e-02, -5.7002e-03,  2.0446e-03,  2.1733e-02,
         1.5114e-02,  2.0789e-03, -1.0899e-02, -5.6587e-03,  2.0192e-02,
         4.3223e-03,  8.8637e-03, -1.6798e-02,  1.7955e-02,  1.2753e-02,
        -5.4965e-03,  1.9481e-02,  6.9524e-03, -2.0116e-02, -5.9529e-03,
        -1.1193e-02,  1.8363e-02,  1.6020e-02, -1.6430e-02,  6.4647e-03,
         1.5684e-02,  9.1588e-03, -2.0390e-02,  9.9621e-03,  2.0163e-02,
         2.1748e-02, -1.8861e-02, -5.0238e-03,  1.9386e-02,  1.1112e-03,
        -1.3363e-02,  6.7029e-03,  1.7814e-02, -8.2254e-03,  1.3094e-02,
         1.2228e-02, -1.2875e-03,  3.9142e-03,  1.4362e-03,  5.9424e-03,
         2.2414e-03, -1.3438e-02, -7.4102e-03,  1.5021e-02, -1.8291e-02,
         1.7939e-02,  2.4626e-03, -1.6420e-03,  1.0483e-03, -6.6787e-03,
         1.9978e-02,  6.3201e-04,  1.7342e-02,  9.4639e-03,  2.2343e-02,
        -1.5528e-02,  1.0803e-02, -4.7853e-03,  1.3566e-02, -1.2095e-02,
        -5.4629e-03,  7.9233e-03, -1.5708e-02, -2.7313e-03,  8.8111e-03,
        -2.1376e-03,  2.0746e-02,  1.0951e-02, -1.4991e-02,  5.5452e-03,
        -1.4782e-02, -1.6080e-02, -5.0667e-03, -2.1050e-02, -7.7526e-03,
         1.9154e-02, -2.1401e-02, -1.0501e-02,  7.3819e-03,  2.0670e-03],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), tensor([[-0.0636, -0.0402,  0.0180,  ..., -0.0669, -0.0184, -0.0632],
        [-0.0702, -0.0480,  0.0203,  ...,  0.0432, -0.0736,  0.0113],
        [-0.0735, -0.0482,  0.0561,  ..., -0.0377,  0.0717,  0.0079],
        ...,
        [ 0.0336,  0.0795,  0.0686,  ...,  0.0617,  0.0593,  0.0145],
        [-0.0549, -0.0686,  0.0407,  ..., -0.0574,  0.0591, -0.0667],
        [-0.0106, -0.0514, -0.0024,  ...,  0.0682,  0.0980,  0.0754]],
       device='cuda:0'), tensor([-0.0598,  0.0784,  0.0041,  0.0437, -0.0314,  0.0230, -0.0737, -0.0741,
        -0.0710, -0.0826, -0.0965, -0.0397,  0.0959,  0.0370,  0.0845,  0.0369],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0'), tensor([[-0.0476,  0.1976,  0.0806, -0.1341,  0.1659,  0.0086, -0.0477,  0.0708,
          0.2368,  0.0414, -0.1084, -0.0428,  0.1062, -0.1102,  0.0168,  0.1324]],
       device='cuda:0'), tensor([0.1504], device='cuda:0')]

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.114407076154436
Model is saved in epoch 0, overall batch: 0
Training loss: 8.912009239196777 / Valid loss: 12.768657457260858
Model is saved in epoch 0, overall batch: 100
Training loss: 9.925810813903809 / Valid loss: 12.196747702643984
Model is saved in epoch 0, overall batch: 200
Training loss: 15.783401489257812 / Valid loss: 11.78140204747518
Model is saved in epoch 0, overall batch: 300
Training loss: 11.472275733947754 / Valid loss: 11.121726526532854
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 9.930368423461914 / Valid loss: 10.86008365267799
Model is saved in epoch 1, overall batch: 500
Training loss: 9.093926429748535 / Valid loss: 10.686255200703938
Model is saved in epoch 1, overall batch: 600
Training loss: 8.314817428588867 / Valid loss: 10.258468723297119
Model is saved in epoch 1, overall batch: 700
Training loss: 8.375079154968262 / Valid loss: 9.384540948413667
Model is saved in epoch 1, overall batch: 800
Training loss: 6.540190696716309 / Valid loss: 9.523321387881325

Epoch: 2
Training loss: 9.234378814697266 / Valid loss: 8.769821966262091
Model is saved in epoch 2, overall batch: 1000
Training loss: 9.167750358581543 / Valid loss: 8.570415982745942
Model is saved in epoch 2, overall batch: 1100
Training loss: 8.85214614868164 / Valid loss: 8.411143775213333
Model is saved in epoch 2, overall batch: 1200
Training loss: 9.604043006896973 / Valid loss: 7.723625689461118
Model is saved in epoch 2, overall batch: 1300
Training loss: 5.110571384429932 / Valid loss: 7.415789735884894
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.846023082733154 / Valid loss: 7.009148602258591
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.8052730560302734 / Valid loss: 6.786819939386277
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.784642219543457 / Valid loss: 6.671425762630644
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.459716796875 / Valid loss: 6.495018087114606
Model is saved in epoch 3, overall batch: 1800
Training loss: 4.784595489501953 / Valid loss: 6.455052934374128
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 5.4322004318237305 / Valid loss: 7.28327697572254
Training loss: 3.8479514122009277 / Valid loss: 6.66725393931071
Training loss: 4.175478458404541 / Valid loss: 6.828827540079753
Training loss: 4.428637981414795 / Valid loss: 6.515154436656407
Training loss: 3.4555511474609375 / Valid loss: 6.198425113587152
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 5.734418869018555 / Valid loss: 6.558657866432553
Training loss: 3.515838146209717 / Valid loss: 6.385780620574951
Training loss: 2.704442024230957 / Valid loss: 6.671769423711868
Training loss: 3.083517074584961 / Valid loss: 6.238934716724214
Training loss: 3.3447868824005127 / Valid loss: 6.271911857241676

Epoch: 6
Training loss: 5.0640363693237305 / Valid loss: 7.03444573538644
Training loss: 2.181051254272461 / Valid loss: 6.528518865222023
Training loss: 3.046281337738037 / Valid loss: 7.094282913208008
Training loss: 3.0997061729431152 / Valid loss: 6.523180414381481
Training loss: 2.9891488552093506 / Valid loss: 6.588962502706618

Epoch: 7
Training loss: 2.6797165870666504 / Valid loss: 6.758096204485212
Training loss: 1.6568546295166016 / Valid loss: 6.918168528874715
Training loss: 2.4585399627685547 / Valid loss: 6.7552470752171105
Training loss: 2.754788398742676 / Valid loss: 6.856639943804059
Training loss: 2.473010540008545 / Valid loss: 6.809998221624465

Epoch: 8
Training loss: 0.8977401256561279 / Valid loss: 6.755013095764887
Training loss: 1.413731575012207 / Valid loss: 6.647775027865455
Training loss: 1.7390714883804321 / Valid loss: 6.902055279413859
Training loss: 2.391519546508789 / Valid loss: 6.718315172195434
Training loss: 2.360682487487793 / Valid loss: 6.881375512622651

Epoch: 9
Training loss: 1.6765280961990356 / Valid loss: 6.831627114613851
Training loss: 1.7128515243530273 / Valid loss: 6.944666762579055
Training loss: 2.782789707183838 / Valid loss: 8.034859157743908
Training loss: 2.0937094688415527 / Valid loss: 6.823229158492316

Epoch: 10
Training loss: 0.936830461025238 / Valid loss: 6.9010133402688165
Training loss: 1.2471961975097656 / Valid loss: 6.960337920415969
Training loss: 0.9596741199493408 / Valid loss: 7.017917206173851
Training loss: 1.8208965063095093 / Valid loss: 6.903125983192807
Training loss: 1.0655286312103271 / Valid loss: 7.356153034028553

Epoch: 11
Training loss: 0.6359700560569763 / Valid loss: 6.962087163471041
Training loss: 1.1494207382202148 / Valid loss: 6.8070917992364794
Training loss: 1.145169734954834 / Valid loss: 6.811951723552886
Training loss: 0.8636291027069092 / Valid loss: 7.136697780518305
Training loss: 1.28263521194458 / Valid loss: 7.200314989544096

Epoch: 12
Training loss: 0.808957576751709 / Valid loss: 6.973334121704101
Training loss: 0.8647042512893677 / Valid loss: 6.900782355808077
Training loss: 0.964614748954773 / Valid loss: 6.726683991295951
Training loss: 1.211578369140625 / Valid loss: 6.901320968355451
Training loss: 1.184460163116455 / Valid loss: 7.090978935786656

Epoch: 13
Training loss: 0.785484790802002 / Valid loss: 7.72143703188215
Training loss: 0.8137133717536926 / Valid loss: 6.860293036415463
Training loss: 0.9294409155845642 / Valid loss: 7.357797645387196
Training loss: 0.8816201090812683 / Valid loss: 6.893089180900937
Training loss: 0.7556352019309998 / Valid loss: 7.004174990881057

Epoch: 14
Training loss: 0.9726812243461609 / Valid loss: 7.121114428838094
Training loss: 0.6533987522125244 / Valid loss: 6.785783045632499
Training loss: 1.1122817993164062 / Valid loss: 6.872415744690668
Training loss: 0.6795768737792969 / Valid loss: 7.161128947848366
Training loss: 1.2054206132888794 / Valid loss: 7.020729609898159

Epoch: 15
Training loss: 0.6204168796539307 / Valid loss: 6.883711839857556
Training loss: 1.2871613502502441 / Valid loss: 7.012847995758056
Training loss: 0.8659645318984985 / Valid loss: 6.839447852543422
Training loss: 0.9139481782913208 / Valid loss: 6.939008117857433
Training loss: 0.8593806028366089 / Valid loss: 6.802086017245338

Epoch: 16
Training loss: 0.53940749168396 / Valid loss: 6.9785969370887395
Training loss: 0.5532370805740356 / Valid loss: 6.923516591389974
Training loss: 0.6132656335830688 / Valid loss: 6.90730920065017
Training loss: 0.6715787053108215 / Valid loss: 6.915744472685314
Training loss: 0.5064013004302979 / Valid loss: 7.258493564242408

Epoch: 17
Training loss: 0.8739537596702576 / Valid loss: 6.999483746574039
Training loss: 1.2047475576400757 / Valid loss: 6.833454740615118
Training loss: 0.814422070980072 / Valid loss: 7.017788610004243
Training loss: 0.5512487888336182 / Valid loss: 6.942955316816057
Training loss: 0.7930984497070312 / Valid loss: 6.872358394804455

Epoch: 18
Training loss: 0.6464129686355591 / Valid loss: 6.939476201647804
Training loss: 0.8608871698379517 / Valid loss: 7.0141416322617305
Training loss: 0.7545645236968994 / Valid loss: 7.13192974726359
Training loss: 0.7856454253196716 / Valid loss: 6.881633154551188
Training loss: 1.0869135856628418 / Valid loss: 7.21874339239938

Epoch: 19
Training loss: 1.059412956237793 / Valid loss: 7.130672954377674
Training loss: 1.1742831468582153 / Valid loss: 6.99260645820981
Training loss: 0.7287744879722595 / Valid loss: 6.95410057703654
Training loss: 0.663609504699707 / Valid loss: 6.842352708180745

Epoch: 20
Training loss: 0.5677882432937622 / Valid loss: 6.791662020910354
Training loss: 1.2225005626678467 / Valid loss: 6.889434378487723
Training loss: 0.47219428420066833 / Valid loss: 6.808534442810785
Training loss: 0.8761451840400696 / Valid loss: 6.8158988135201595
Training loss: 1.0279066562652588 / Valid loss: 6.7720060007912775

Epoch: 21
Training loss: 0.4475374221801758 / Valid loss: 6.937840098426456
Training loss: 0.5406501293182373 / Valid loss: 7.365072645459857
Training loss: 0.6003029346466064 / Valid loss: 7.08115739368257
Training loss: 0.7370328903198242 / Valid loss: 6.987155205862862
Training loss: 0.6210228204727173 / Valid loss: 6.9006127198537195

Epoch: 22
Training loss: 0.7910249829292297 / Valid loss: 6.7880048751831055
Training loss: 0.38785022497177124 / Valid loss: 6.869932310921805
Training loss: 0.6510220766067505 / Valid loss: 6.9390557062058225
Training loss: 1.272037386894226 / Valid loss: 6.995921611785889
Training loss: 0.9684562087059021 / Valid loss: 7.202456410725912

Epoch: 23
Training loss: 0.5385011434555054 / Valid loss: 6.851070585704985
Training loss: 1.321436882019043 / Valid loss: 6.890108635312035
Training loss: 0.9871132969856262 / Valid loss: 6.8747127146947955
Training loss: 0.6911038756370544 / Valid loss: 7.021406166894096
Training loss: 0.7842404842376709 / Valid loss: 6.97103283745902

Epoch: 24
Training loss: 0.8393596410751343 / Valid loss: 6.838232290177118
Training loss: 0.6872751712799072 / Valid loss: 6.7430394081842335
Training loss: 0.6175047159194946 / Valid loss: 6.862519613901774
Training loss: 0.6735203266143799 / Valid loss: 6.916020125434512
Training loss: 0.992667555809021 / Valid loss: 7.099601443608602

Epoch: 25
Training loss: 0.6753808856010437 / Valid loss: 6.762231259118943
Training loss: 0.43187612295150757 / Valid loss: 6.86820308140346
Training loss: 1.1698179244995117 / Valid loss: 6.860477633703322
Training loss: 0.4223865270614624 / Valid loss: 6.918870467231387
Training loss: 0.669593095779419 / Valid loss: 7.029194404965355

Epoch: 26
Training loss: 0.9046597480773926 / Valid loss: 6.797090825580415
Training loss: 1.0334348678588867 / Valid loss: 6.761004184541248
Training loss: 0.34703996777534485 / Valid loss: 6.90616911479405
Training loss: 0.3122824430465698 / Valid loss: 6.86344967796689
Training loss: 0.48323753476142883 / Valid loss: 7.03172699383327

Epoch: 27
Training loss: 0.36437496542930603 / Valid loss: 6.933468312308902
Training loss: 0.6029342412948608 / Valid loss: 6.901174036661784
Training loss: 0.614423394203186 / Valid loss: 6.947794346582322
Training loss: 0.4617449939250946 / Valid loss: 6.9538627465566
Training loss: 0.6803674697875977 / Valid loss: 7.0249468712579635

Epoch: 28
Training loss: 0.387935996055603 / Valid loss: 6.901970599946521
Training loss: 0.6165776252746582 / Valid loss: 6.909543144135248
Training loss: 0.5400832891464233 / Valid loss: 6.861772092183431
Training loss: 0.3511140048503876 / Valid loss: 7.040414101736886
Training loss: 0.539892315864563 / Valid loss: 6.983102199009487

Epoch: 29
Training loss: 0.5058392882347107 / Valid loss: 6.861928603762672
Training loss: 0.7486938238143921 / Valid loss: 7.155505398341588
Training loss: 0.3672358989715576 / Valid loss: 6.811366380964007
Training loss: 0.3118017911911011 / Valid loss: 6.879478425071353

Epoch: 30
Training loss: 0.6928936243057251 / Valid loss: 7.0182854947589695
Training loss: 0.44543325901031494 / Valid loss: 6.757925156184605
Training loss: 0.3234328627586365 / Valid loss: 7.008895224616641
Training loss: 0.45212942361831665 / Valid loss: 6.878146564392816
Training loss: 0.3734869956970215 / Valid loss: 6.909860987890334

Epoch: 31
Training loss: 0.35912203788757324 / Valid loss: 6.911509184610276
Training loss: 0.43792688846588135 / Valid loss: 7.023451936812628
Training loss: 0.5828892588615417 / Valid loss: 6.85651330947876
Training loss: 0.4035741984844208 / Valid loss: 7.197467427026658
Training loss: 0.5325425267219543 / Valid loss: 6.896336519150507

Epoch: 32
Training loss: 0.46798238158226013 / Valid loss: 6.920280415671212
Training loss: 0.5129421949386597 / Valid loss: 6.855489594595773
Training loss: 0.3181908130645752 / Valid loss: 6.914389796484084
Training loss: 0.29038310050964355 / Valid loss: 6.8406507128760925
Training loss: 0.5647710561752319 / Valid loss: 6.835189873831613

Epoch: 33
Training loss: 0.4031444489955902 / Valid loss: 6.939921156565348
Training loss: 0.49439507722854614 / Valid loss: 6.8127959501175654
Training loss: 0.36235883831977844 / Valid loss: 6.861376587549845
Training loss: 0.572578489780426 / Valid loss: 6.858007941927228
Training loss: 0.45658621191978455 / Valid loss: 6.850246170588902

Epoch: 34
Training loss: 0.6909438371658325 / Valid loss: 6.767181632632301
Training loss: 0.3100060224533081 / Valid loss: 6.902636882237026
Training loss: 0.48002326488494873 / Valid loss: 6.956700111570813
Training loss: 0.33002769947052 / Valid loss: 6.885793299902053
Training loss: 0.4485366642475128 / Valid loss: 7.008176167805989

Epoch: 35
Training loss: 0.9865694046020508 / Valid loss: 6.830015218825567
Training loss: 0.5776498913764954 / Valid loss: 6.897862325395857
Training loss: 0.5171666741371155 / Valid loss: 6.8616849853878925
Training loss: 0.3185175657272339 / Valid loss: 6.861160954974946
Training loss: 0.4183725118637085 / Valid loss: 6.805342181523641

Epoch: 36
Training loss: 0.4354248046875 / Valid loss: 6.7071679569426035
Training loss: 0.6866554021835327 / Valid loss: 6.8485821383340015
Training loss: 0.4942253530025482 / Valid loss: 6.748570701054164
Training loss: 0.30083948373794556 / Valid loss: 6.890221507208688
Training loss: 0.4545312523841858 / Valid loss: 6.851118828001477

Epoch: 37
Training loss: 0.5846376419067383 / Valid loss: 7.004612102962676
Training loss: 0.25425416231155396 / Valid loss: 6.803554030827113
Training loss: 0.46477121114730835 / Valid loss: 6.90566330864316
Training loss: 0.9308645725250244 / Valid loss: 6.774234317597889
Training loss: 0.6394990086555481 / Valid loss: 6.859247870672316

Epoch: 38
Training loss: 0.5229790806770325 / Valid loss: 6.859157198951358
Training loss: 0.4742722511291504 / Valid loss: 6.826846862974621
Training loss: 0.46572309732437134 / Valid loss: 6.895450078873408
Training loss: 0.3612024188041687 / Valid loss: 6.895119044894264
Training loss: 0.7249857187271118 / Valid loss: 6.882613790602911

Epoch: 39
Training loss: 0.46001923084259033 / Valid loss: 6.975711663564046
Training loss: 0.3459772765636444 / Valid loss: 6.810472106933593
Training loss: 0.4002076983451843 / Valid loss: 6.844748805818104
Training loss: 0.3169283866882324 / Valid loss: 6.756377181552705

Epoch: 40
Training loss: 0.3688444197177887 / Valid loss: 6.988716581889562
Training loss: 0.31692153215408325 / Valid loss: 6.891211704980759
Training loss: 0.3872334361076355 / Valid loss: 6.786558782486688
Training loss: 0.2691856026649475 / Valid loss: 6.79910143897647
Training loss: 0.2676779329776764 / Valid loss: 6.852081425984701

Epoch: 41
Training loss: 0.584932804107666 / Valid loss: 6.849190176100958
Training loss: 0.4023456275463104 / Valid loss: 6.797752480279832
Training loss: 0.30325448513031006 / Valid loss: 6.947733865465437
Training loss: 1.2078900337219238 / Valid loss: 6.976276002611432
Training loss: 0.4149470925331116 / Valid loss: 6.876149756567819

Epoch: 42
Training loss: 0.39032602310180664 / Valid loss: 6.709031223115467
Training loss: 0.3001871705055237 / Valid loss: 6.800860221045358
Training loss: 0.4784907400608063 / Valid loss: 6.735300522758847
Training loss: 0.40767455101013184 / Valid loss: 6.84482813108535
Training loss: 0.4310424327850342 / Valid loss: 6.958239301045736

Epoch: 43
Training loss: 0.4082128703594208 / Valid loss: 6.715197015943981
Training loss: 0.37803182005882263 / Valid loss: 6.753293100992838
Training loss: 0.16625966131687164 / Valid loss: 6.765981011163621
Training loss: 0.49290910363197327 / Valid loss: 7.081740915207636
Training loss: 0.5333818197250366 / Valid loss: 6.968315097263881

Epoch: 44
Training loss: 0.28737175464630127 / Valid loss: 6.7073313486008415
Training loss: 0.1722629964351654 / Valid loss: 6.769401248296102
Training loss: 0.47909608483314514 / Valid loss: 6.752234554290771
Training loss: 0.2275506556034088 / Valid loss: 6.687825638907296
Training loss: 0.5222076773643494 / Valid loss: 6.724186658859253

Epoch: 45
Training loss: 0.32304131984710693 / Valid loss: 6.887733316421508
Training loss: 0.5832504630088806 / Valid loss: 6.705071138200306
Training loss: 0.5492264628410339 / Valid loss: 6.7909575462341305
Training loss: 0.4111666977405548 / Valid loss: 6.701044732048398
Training loss: 0.36251306533813477 / Valid loss: 6.762984625498453

Epoch: 46
Training loss: 0.2727862000465393 / Valid loss: 6.678364667438325
Training loss: 0.32198768854141235 / Valid loss: 6.721526586441767
Training loss: 0.48535650968551636 / Valid loss: 6.803485983893985
Training loss: 0.3723664879798889 / Valid loss: 6.8428798766363235
Training loss: 0.6073732972145081 / Valid loss: 6.881512941632952

Epoch: 47
Training loss: 0.23989813029766083 / Valid loss: 6.78193108694894
Training loss: 0.4724132716655731 / Valid loss: 6.708305699484689
Training loss: 0.5187866687774658 / Valid loss: 6.682447340374901
Training loss: 0.3214375972747803 / Valid loss: 6.768702675047375
Training loss: 0.3901714086532593 / Valid loss: 6.913385995229086

Epoch: 48
Training loss: 0.253079354763031 / Valid loss: 6.744001343136742
Training loss: 0.5310543179512024 / Valid loss: 6.735933394659133
Training loss: 0.5105814933776855 / Valid loss: 6.80726622626895
Training loss: 0.3190954327583313 / Valid loss: 6.706569435482933
Training loss: 0.3793792128562927 / Valid loss: 6.774620124271938

Epoch: 49
Training loss: 0.4124816358089447 / Valid loss: 6.680840501331148
Training loss: 0.31002432107925415 / Valid loss: 6.703553254263742
Training loss: 0.24713607132434845 / Valid loss: 6.760096391042073
Training loss: 0.30090177059173584 / Valid loss: 6.810816387903123

Epoch: 50
Training loss: 0.3706962764263153 / Valid loss: 6.827458272661482
Training loss: 0.38320428133010864 / Valid loss: 6.750355111984979
Training loss: 0.2868631184101105 / Valid loss: 6.659401076180594
Training loss: 0.2883872389793396 / Valid loss: 6.748834330695016
Training loss: 0.562008261680603 / Valid loss: 6.719065700258527

Epoch: 51
Training loss: 0.2471512109041214 / Valid loss: 6.74639112381708
Training loss: 0.3177463412284851 / Valid loss: 6.718392630985805
Training loss: 0.289815753698349 / Valid loss: 6.743921348026821
Training loss: 0.3917962908744812 / Valid loss: 6.61454481851487
Training loss: 0.40255868434906006 / Valid loss: 6.645039985293434

Epoch: 52
Training loss: 0.20757117867469788 / Valid loss: 6.723773229689825
Training loss: 0.33904463052749634 / Valid loss: 6.709273937770298
Training loss: 0.30311405658721924 / Valid loss: 6.691586217426118
Training loss: 0.22939956188201904 / Valid loss: 6.632260150001162
Training loss: 0.4663572907447815 / Valid loss: 6.685382527396793

Epoch: 53
Training loss: 0.3068365454673767 / Valid loss: 6.78039832569304
Training loss: 0.5012734532356262 / Valid loss: 6.738596139635359
Training loss: 0.27735966444015503 / Valid loss: 6.5801393554324195
Training loss: 0.6502377986907959 / Valid loss: 6.822552317664737
Training loss: 0.35023748874664307 / Valid loss: 6.792698387872605

Epoch: 54
Training loss: 0.41845959424972534 / Valid loss: 6.687618705204555
Training loss: 0.463904470205307 / Valid loss: 6.762713443665278
Training loss: 0.22641977667808533 / Valid loss: 6.850458449409121
Training loss: 0.35759979486465454 / Valid loss: 6.697037674131847
Training loss: 0.4495754837989807 / Valid loss: 6.746020112718854

Epoch: 55
Training loss: 0.563720166683197 / Valid loss: 6.671433485121954
Training loss: 0.47859108448028564 / Valid loss: 6.6954275948660715
Training loss: 0.3678862452507019 / Valid loss: 6.809267498198009
Training loss: 0.4210607409477234 / Valid loss: 6.791607280004592
Training loss: 0.33094897866249084 / Valid loss: 6.717492662157331

Epoch: 56
Training loss: 0.6248446702957153 / Valid loss: 6.727197819664365
Training loss: 0.20745807886123657 / Valid loss: 6.7060022444952105
Training loss: 0.2868157625198364 / Valid loss: 6.61083265032087
Training loss: 0.22298631072044373 / Valid loss: 6.6888314065479095
Training loss: 0.3402419984340668 / Valid loss: 6.873394670940581

Epoch: 57
Training loss: 0.5975265502929688 / Valid loss: 6.7056925660087945
Training loss: 0.6231029629707336 / Valid loss: 6.654087171100435
Training loss: 0.39805570244789124 / Valid loss: 6.702675823938279
Training loss: 0.5699640512466431 / Valid loss: 6.68742641721453
Training loss: 0.4310111701488495 / Valid loss: 6.810134297325497

Epoch: 58
Training loss: 0.33928000926971436 / Valid loss: 6.7944656871614
Training loss: 0.5132818222045898 / Valid loss: 6.822099140712193
Training loss: 0.1570674031972885 / Valid loss: 6.769105379922049
Training loss: 0.442685067653656 / Valid loss: 6.637626884097145
Training loss: 0.2664511203765869 / Valid loss: 6.698658707028343

Epoch: 59
Training loss: 0.30701684951782227 / Valid loss: 6.684155486878895
Training loss: 0.49762654304504395 / Valid loss: 6.594595109848749
Training loss: 0.20613618195056915 / Valid loss: 6.642565118698847
Training loss: 0.31091612577438354 / Valid loss: 6.763343325115385

Epoch: 60
Training loss: 0.3153267502784729 / Valid loss: 7.0421697026207335
Training loss: 0.6598629951477051 / Valid loss: 6.864576725732713
Training loss: 0.2438582330942154 / Valid loss: 6.6290724572681246
Training loss: 0.3429989814758301 / Valid loss: 6.618421645391555
Training loss: 0.21515816450119019 / Valid loss: 6.674779733022054

Epoch: 61
Training loss: 0.3506452143192291 / Valid loss: 6.654799041293916
Training loss: 0.28080666065216064 / Valid loss: 6.598044009435744
Training loss: 0.6038864850997925 / Valid loss: 6.712476226261684
Training loss: 0.36101478338241577 / Valid loss: 6.689102495284308
Training loss: 0.19444799423217773 / Valid loss: 6.755177529652913

Epoch: 62
Training loss: 0.21449124813079834 / Valid loss: 6.597038614182245
Training loss: 0.59621661901474 / Valid loss: 6.688204840251378
Training loss: 0.2546719014644623 / Valid loss: 6.592263884771437
Training loss: 0.3417029082775116 / Valid loss: 6.633792804536365
Training loss: 0.29332274198532104 / Valid loss: 6.718352778752645

Epoch: 63
Training loss: 0.3389047384262085 / Valid loss: 6.622619578951881
Training loss: 0.3676445782184601 / Valid loss: 6.581061167944045
Training loss: 0.17669038474559784 / Valid loss: 6.6130304086776
Training loss: 0.40068429708480835 / Valid loss: 6.686364096686954
Training loss: 0.17529423534870148 / Valid loss: 6.713154234204974

Epoch: 64
Training loss: 0.2943063974380493 / Valid loss: 6.490254159200759
Training loss: 0.1906624585390091 / Valid loss: 6.567365957441784
Training loss: 0.5505138039588928 / Valid loss: 6.621095884413946
Training loss: 0.23796790838241577 / Valid loss: 6.766010693141392
Training loss: 0.24791644513607025 / Valid loss: 6.822465442475819

Epoch: 65
Training loss: 0.2073822319507599 / Valid loss: 6.5285111404600595
Training loss: 0.39523953199386597 / Valid loss: 6.591360870997111
Training loss: 0.3413603901863098 / Valid loss: 6.745652353195917
Training loss: 0.30060210824012756 / Valid loss: 6.671729065123058
Training loss: 1.2290129661560059 / Valid loss: 6.700179517836798

Epoch: 66
Training loss: 0.35805588960647583 / Valid loss: 6.5398833978743784
Training loss: 0.26997658610343933 / Valid loss: 6.533526207151867
Training loss: 0.5388424396514893 / Valid loss: 6.652726566223871
Training loss: 0.18387764692306519 / Valid loss: 6.598471357708886
Training loss: 0.4257374703884125 / Valid loss: 6.709581393287295

Epoch: 67
Training loss: 0.342794805765152 / Valid loss: 6.653990073431106
Training loss: 0.18486344814300537 / Valid loss: 6.5737508796510244
Training loss: 0.19393864274024963 / Valid loss: 6.728783398582822
Training loss: 0.741718053817749 / Valid loss: 6.671461654844738
Training loss: 0.1896497756242752 / Valid loss: 6.588567275092715

Epoch: 68
Training loss: 0.25574779510498047 / Valid loss: 6.738128162565685
Training loss: 0.32374680042266846 / Valid loss: 6.723837298438663
Training loss: 0.23225489258766174 / Valid loss: 6.698892003013974
Training loss: 0.2614475190639496 / Valid loss: 6.671468932288033
Training loss: 0.4797174334526062 / Valid loss: 6.680914987836565

Epoch: 69
Training loss: 0.37791913747787476 / Valid loss: 6.580595207214356
Training loss: 0.18169423937797546 / Valid loss: 6.662476162683396
Training loss: 0.3053697347640991 / Valid loss: 6.743009045010521
Training loss: 0.2857421040534973 / Valid loss: 6.680840984980265

Epoch: 70
Training loss: 0.15205365419387817 / Valid loss: 6.652647699628558
Training loss: 0.42214784026145935 / Valid loss: 6.69452851159232
Training loss: 0.27521079778671265 / Valid loss: 6.634945447104318
Training loss: 0.2875935435295105 / Valid loss: 6.692101669311524
Training loss: 0.34156426787376404 / Valid loss: 6.719008218674433

Epoch: 71
Training loss: 0.3640439510345459 / Valid loss: 6.77816378729684
Training loss: 0.48805707693099976 / Valid loss: 6.650673938932873
Training loss: 0.27437493205070496 / Valid loss: 6.5628108887445356
Training loss: 0.2233055830001831 / Valid loss: 6.581970973241897
Training loss: 0.47270312905311584 / Valid loss: 6.63338741120838

Epoch: 72
Training loss: 0.19438359141349792 / Valid loss: 6.579217956179664
Training loss: 0.433140367269516 / Valid loss: 6.6487732115246
Training loss: 0.2627412676811218 / Valid loss: 6.624934677850632
Training loss: 0.2024679183959961 / Valid loss: 6.592256541479202
Training loss: 0.49399858713150024 / Valid loss: 6.767768255869547

Epoch: 73
Training loss: 0.45087727904319763 / Valid loss: 6.652540788196382
Training loss: 0.3708835244178772 / Valid loss: 6.635646942683628
Training loss: 0.2231827676296234 / Valid loss: 6.662844662439255
Training loss: 0.2669631242752075 / Valid loss: 6.646315860748291
Training loss: 0.23651760816574097 / Valid loss: 6.730236961728051

Epoch: 74
Training loss: 0.19732210040092468 / Valid loss: 6.656298237755185
Training loss: 0.3130336403846741 / Valid loss: 6.5522129967099145
Training loss: 0.44681164622306824 / Valid loss: 6.492975929805211
Training loss: 0.31222066283226013 / Valid loss: 6.663017520450411
Training loss: 0.20401397347450256 / Valid loss: 6.589029700415475

Epoch: 75
Training loss: 0.4409768581390381 / Valid loss: 6.595940844217936
Training loss: 0.32814905047416687 / Valid loss: 6.648918533325196
Training loss: 0.5441973209381104 / Valid loss: 6.6851639066423685
Training loss: 0.2011454999446869 / Valid loss: 6.55321284702846
Training loss: 0.4068012833595276 / Valid loss: 6.61397309530349

Epoch: 76
Training loss: 0.3210447430610657 / Valid loss: 6.533609689985003
Training loss: 0.4163230061531067 / Valid loss: 6.5234950837634855
Training loss: 0.22019372880458832 / Valid loss: 6.769771466936384
Training loss: 0.28016138076782227 / Valid loss: 6.584591182072957
Training loss: 0.20006026327610016 / Valid loss: 6.623555912290301

Epoch: 77
Training loss: 0.19589956104755402 / Valid loss: 6.520887567883446
Training loss: 0.2464345097541809 / Valid loss: 6.551894231069656
Training loss: 0.4143565595149994 / Valid loss: 6.629134019215901
Training loss: 0.40897876024246216 / Valid loss: 6.619436840783982
Training loss: 0.5242934226989746 / Valid loss: 6.673334307897658

Epoch: 78
Training loss: 0.2526826560497284 / Valid loss: 6.635404759361631
Training loss: 0.15289264917373657 / Valid loss: 6.574841299511138
Training loss: 0.2966405153274536 / Valid loss: 6.5411285854521255
Training loss: 0.21035808324813843 / Valid loss: 6.629784123102824
Training loss: 0.4658309817314148 / Valid loss: 6.577439689636231

Epoch: 79
Training loss: 0.15384513139724731 / Valid loss: 6.599957752227783
Training loss: 0.31102845072746277 / Valid loss: 6.582543940771194
Training loss: 0.25043123960494995 / Valid loss: 6.562869358062744
Training loss: 0.2823439836502075 / Valid loss: 6.599431051526751
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model: 5.965176051003592
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 80
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0
momentum : 0
embedder : Bert
verbose : False
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
neural net:
 [tensor([[ 0.0035, -0.0035,  0.0052,  ..., -0.0130,  0.0056, -0.0088],
        [ 0.0045, -0.0089, -0.0107,  ...,  0.0052,  0.0048, -0.0076],
        [ 0.0014, -0.0062,  0.0084,  ..., -0.0093,  0.0089,  0.0006],
        ...,
        [ 0.0115,  0.0001, -0.0032,  ..., -0.0018, -0.0062, -0.0085],
        [-0.0009,  0.0057,  0.0107,  ..., -0.0067, -0.0022, -0.0107],
        [ 0.0109, -0.0116, -0.0054,  ...,  0.0007, -0.0127,  0.0088]],
       device='cuda:0'), tensor([ 0.0047,  0.0122,  0.0094,  ...,  0.0037, -0.0029, -0.0056],
       device='cuda:0'), tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), tensor([[-0.0121, -0.0085,  0.0158,  ...,  0.0067, -0.0057, -0.0189],
        [ 0.0064, -0.0150,  0.0096,  ...,  0.0006,  0.0074,  0.0177],
        [ 0.0061, -0.0078,  0.0059,  ...,  0.0032, -0.0206,  0.0205],
        ...,
        [ 0.0173, -0.0032,  0.0090,  ..., -0.0031,  0.0141,  0.0130],
        [-0.0181, -0.0101,  0.0157,  ..., -0.0020, -0.0070,  0.0160],
        [ 0.0063, -0.0152, -0.0101,  ...,  0.0008,  0.0028, -0.0011]],
       device='cuda:0'), tensor([-1.1967e-02,  1.1110e-02,  1.8701e-02,  1.1023e-02, -2.0298e-02,
         5.6226e-05, -1.3542e-02,  2.7363e-03, -8.2064e-03,  1.8652e-02,
         1.4701e-02,  1.2389e-02,  2.1926e-02,  1.5760e-02, -3.8398e-03,
        -2.1899e-02,  2.2099e-02, -5.7002e-03,  2.0446e-03,  2.1733e-02,
         1.5114e-02,  2.0789e-03, -1.0899e-02, -5.6587e-03,  2.0192e-02,
         4.3223e-03,  8.8637e-03, -1.6798e-02,  1.7955e-02,  1.2753e-02,
        -5.4965e-03,  1.9481e-02,  6.9524e-03, -2.0116e-02, -5.9529e-03,
        -1.1193e-02,  1.8363e-02,  1.6020e-02, -1.6430e-02,  6.4647e-03,
         1.5684e-02,  9.1588e-03, -2.0390e-02,  9.9621e-03,  2.0163e-02,
         2.1748e-02, -1.8861e-02, -5.0238e-03,  1.9386e-02,  1.1112e-03,
        -1.3363e-02,  6.7029e-03,  1.7814e-02, -8.2254e-03,  1.3094e-02,
         1.2228e-02, -1.2875e-03,  3.9142e-03,  1.4362e-03,  5.9424e-03,
         2.2414e-03, -1.3438e-02, -7.4102e-03,  1.5021e-02, -1.8291e-02,
         1.7939e-02,  2.4626e-03, -1.6420e-03,  1.0483e-03, -6.6787e-03,
         1.9978e-02,  6.3201e-04,  1.7342e-02,  9.4639e-03,  2.2343e-02,
        -1.5528e-02,  1.0803e-02, -4.7853e-03,  1.3566e-02, -1.2095e-02,
        -5.4629e-03,  7.9233e-03, -1.5708e-02, -2.7313e-03,  8.8111e-03,
        -2.1376e-03,  2.0746e-02,  1.0951e-02, -1.4991e-02,  5.5452e-03,
        -1.4782e-02, -1.6080e-02, -5.0667e-03, -2.1050e-02, -7.7526e-03,
         1.9154e-02, -2.1401e-02, -1.0501e-02,  7.3819e-03,  2.0670e-03],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), tensor([[-0.0636, -0.0402,  0.0180,  ..., -0.0669, -0.0184, -0.0632],
        [-0.0702, -0.0480,  0.0203,  ...,  0.0432, -0.0736,  0.0113],
        [-0.0735, -0.0482,  0.0561,  ..., -0.0377,  0.0717,  0.0079],
        ...,
        [ 0.0336,  0.0795,  0.0686,  ...,  0.0617,  0.0593,  0.0145],
        [-0.0549, -0.0686,  0.0407,  ..., -0.0574,  0.0591, -0.0667],
        [-0.0106, -0.0514, -0.0024,  ...,  0.0682,  0.0980,  0.0754]],
       device='cuda:0'), tensor([-0.0598,  0.0784,  0.0041,  0.0437, -0.0314,  0.0230, -0.0737, -0.0741,
        -0.0710, -0.0826, -0.0965, -0.0397,  0.0959,  0.0370,  0.0845,  0.0369],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0'), tensor([[-0.0476,  0.1976,  0.0806, -0.1341,  0.1659,  0.0086, -0.0477,  0.0708,
          0.2368,  0.0414, -0.1084, -0.0428,  0.1062, -0.1102,  0.0168,  0.1324]],
       device='cuda:0'), tensor([0.1504], device='cuda:0')]

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.266402771359399
Model is saved in epoch 0, overall batch: 0
Training loss: 5.042430877685547 / Valid loss: 7.500886340368361
Model is saved in epoch 0, overall batch: 100
Training loss: 4.23737907409668 / Valid loss: 5.921776885078067
Model is saved in epoch 0, overall batch: 200
Training loss: 6.29364538192749 / Valid loss: 5.682949795041766
Model is saved in epoch 0, overall batch: 300
Training loss: 5.842200756072998 / Valid loss: 5.548471437181745
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.756719589233398 / Valid loss: 5.553807235899425
Training loss: 4.298105239868164 / Valid loss: 5.569606599353609
Training loss: 5.326928615570068 / Valid loss: 5.642630109332857
Training loss: 5.017754554748535 / Valid loss: 5.6339343093690415
Training loss: 3.9678406715393066 / Valid loss: 5.5637674763089136

Epoch: 2
Training loss: 4.209566593170166 / Valid loss: 5.612356049673898
Training loss: 4.178608417510986 / Valid loss: 5.750446242377872
Training loss: 3.3077023029327393 / Valid loss: 5.74691725685483
Training loss: 4.88202428817749 / Valid loss: 5.984495680672782
Training loss: 4.264095306396484 / Valid loss: 5.786848156792777

Epoch: 3
Training loss: 2.5928397178649902 / Valid loss: 5.875179701759702
Training loss: 2.498826503753662 / Valid loss: 6.106087437130156
Training loss: 4.6850714683532715 / Valid loss: 5.97977549235026
Training loss: 2.186169385910034 / Valid loss: 6.105248587472098
Training loss: 3.4255576133728027 / Valid loss: 6.111393374488467

Epoch: 4
Training loss: 2.261228322982788 / Valid loss: 6.212632283710298
Training loss: 2.7856407165527344 / Valid loss: 6.367695467812674
Training loss: 2.8565831184387207 / Valid loss: 6.320631045386905
Training loss: 2.643772602081299 / Valid loss: 6.169245856148856
Training loss: 2.3476099967956543 / Valid loss: 6.288671822774978

Epoch: 5
Training loss: 2.6007585525512695 / Valid loss: 6.993596685500372
Training loss: 1.937721848487854 / Valid loss: 6.489501063028971
Training loss: 2.0332107543945312 / Valid loss: 6.682902040935698
Training loss: 2.2377610206604004 / Valid loss: 6.4714922450837635
Training loss: 2.2933297157287598 / Valid loss: 6.610230096181234

Epoch: 6
Training loss: 2.3704824447631836 / Valid loss: 6.5890911692664735
Training loss: 1.2237951755523682 / Valid loss: 6.530026122501918
Training loss: 2.6668028831481934 / Valid loss: 6.618755563100179
Training loss: 1.8128156661987305 / Valid loss: 6.598240836461385
Training loss: 1.754177212715149 / Valid loss: 6.67875330334618

Epoch: 7
Training loss: 1.6314806938171387 / Valid loss: 6.549886939639137
Training loss: 1.7161669731140137 / Valid loss: 6.626883702051072
Training loss: 1.825516939163208 / Valid loss: 6.685345942633493
Training loss: 1.6144566535949707 / Valid loss: 6.730530461810884
Training loss: 1.254022240638733 / Valid loss: 6.626742994217645

Epoch: 8
Training loss: 0.8628376722335815 / Valid loss: 6.59803987003508
Training loss: 0.840111494064331 / Valid loss: 6.550634683881487
Training loss: 1.0600968599319458 / Valid loss: 6.688631010055542
Training loss: 1.804614543914795 / Valid loss: 6.621865404219855
Training loss: 1.3723456859588623 / Valid loss: 6.63860752923148

Epoch: 9
Training loss: 0.6908299922943115 / Valid loss: 6.583039256504604
Training loss: 0.7573602795600891 / Valid loss: 6.690453978947231
Training loss: 1.4202158451080322 / Valid loss: 6.676934541974749
Training loss: 1.119789958000183 / Valid loss: 6.6478013401939755

Epoch: 10
Training loss: 0.9327704906463623 / Valid loss: 6.61181899252392
Training loss: 0.7600219249725342 / Valid loss: 6.525962102980841
Training loss: 0.5637192726135254 / Valid loss: 6.57061973299299
Training loss: 1.3438125848770142 / Valid loss: 6.6073793547494075
Training loss: 0.7735837697982788 / Valid loss: 6.65690032641093

Epoch: 11
Training loss: 0.5911161303520203 / Valid loss: 6.639800630296979
Training loss: 0.9196419715881348 / Valid loss: 6.714557536443075
Training loss: 0.7024827003479004 / Valid loss: 6.790416422344389
Training loss: 0.6993299722671509 / Valid loss: 6.754666791643415
Training loss: 0.7075324058532715 / Valid loss: 6.7032001586187455

Epoch: 12
Training loss: 0.39375412464141846 / Valid loss: 6.579132511502221
Training loss: 0.5301622748374939 / Valid loss: 6.604428423018683
Training loss: 0.6285430192947388 / Valid loss: 6.513908277239119
Training loss: 0.6034932732582092 / Valid loss: 6.645499206724621
Training loss: 0.6196821331977844 / Valid loss: 6.61227723757426

Epoch: 13
Training loss: 0.37680479884147644 / Valid loss: 6.681903859547206
Training loss: 0.526980996131897 / Valid loss: 6.640049271356492
Training loss: 0.4539257884025574 / Valid loss: 6.595271096910749
Training loss: 0.5621163845062256 / Valid loss: 6.582649800890968
Training loss: 0.4460565745830536 / Valid loss: 6.5734669299352735

Epoch: 14
Training loss: 0.5914333462715149 / Valid loss: 6.621801598866781
Training loss: 0.49070507287979126 / Valid loss: 6.560744562603179
Training loss: 0.5259989500045776 / Valid loss: 6.555673088346209
Training loss: 0.25948449969291687 / Valid loss: 6.586678527650379
Training loss: 0.6754760146141052 / Valid loss: 6.649354369299752

Epoch: 15
Training loss: 0.46882298588752747 / Valid loss: 6.6811547869727725
Training loss: 0.6348530054092407 / Valid loss: 6.670148454393659
Training loss: 0.46523556113243103 / Valid loss: 6.6584804807390485
Training loss: 0.45887142419815063 / Valid loss: 6.60891052427746
Training loss: 0.48496049642562866 / Valid loss: 6.634208256857736

Epoch: 16
Training loss: 0.3049743175506592 / Valid loss: 6.565555495307559
Training loss: 0.45435047149658203 / Valid loss: 6.612872582390195
Training loss: 0.36655542254447937 / Valid loss: 6.5486997104826425
Training loss: 0.3891465663909912 / Valid loss: 6.6117689904712496
Training loss: 0.39975547790527344 / Valid loss: 6.563732653572446

Epoch: 17
Training loss: 0.7990560531616211 / Valid loss: 6.569073933646792
Training loss: 0.7692824602127075 / Valid loss: 6.557745831353324
Training loss: 0.6669824123382568 / Valid loss: 6.614656439281645
Training loss: 0.27177250385284424 / Valid loss: 6.601601418994722
Training loss: 0.3779035210609436 / Valid loss: 6.620827372868856

Epoch: 18
Training loss: 0.4896887242794037 / Valid loss: 6.604000136965797
Training loss: 0.42007529735565186 / Valid loss: 6.5563198248545325
Training loss: 0.420172780752182 / Valid loss: 6.566674936385382
Training loss: 0.45496243238449097 / Valid loss: 6.598615662256877
Training loss: 0.6350970268249512 / Valid loss: 6.5631274904523575

Epoch: 19
Training loss: 0.5448774099349976 / Valid loss: 6.654344436100551
Training loss: 0.8887042999267578 / Valid loss: 6.577063755762009
Training loss: 0.4142709970474243 / Valid loss: 6.5773750532241095
Training loss: 0.2850325107574463 / Valid loss: 6.547097835086641

Epoch: 20
Training loss: 0.32555830478668213 / Valid loss: 6.4868345578511555
Training loss: 0.6206218004226685 / Valid loss: 6.558145187014625
Training loss: 0.21500426530838013 / Valid loss: 6.539990411485944
Training loss: 0.3254860043525696 / Valid loss: 6.58965566725958
Training loss: 0.7169045209884644 / Valid loss: 6.543543238866897

Epoch: 21
Training loss: 0.3072414696216583 / Valid loss: 6.5334583850133985
Training loss: 0.4154866337776184 / Valid loss: 6.569000938960484
Training loss: 0.32305556535720825 / Valid loss: 6.564593673887707
Training loss: 0.27794021368026733 / Valid loss: 6.5852838834126795
Training loss: 0.3463420569896698 / Valid loss: 6.5662931692032585

Epoch: 22
Training loss: 0.3098863363265991 / Valid loss: 6.557641692388625
Training loss: 0.253909707069397 / Valid loss: 6.467937930425008
Training loss: 0.38885200023651123 / Valid loss: 6.539727826345534
Training loss: 1.0445916652679443 / Valid loss: 6.594146610441662
Training loss: 0.4033229351043701 / Valid loss: 6.584196276891799

Epoch: 23
Training loss: 0.24921837449073792 / Valid loss: 6.556321732203165
Training loss: 0.7468335032463074 / Valid loss: 6.482907140822638
Training loss: 0.2993065118789673 / Valid loss: 6.507833896364485
Training loss: 0.20183780789375305 / Valid loss: 6.515956374577113
Training loss: 0.436636745929718 / Valid loss: 6.586847171329317

Epoch: 24
Training loss: 0.558465301990509 / Valid loss: 6.580224627540225
Training loss: 0.396455854177475 / Valid loss: 6.5163773445856
Training loss: 0.25740212202072144 / Valid loss: 6.585533464522589
Training loss: 0.5150564908981323 / Valid loss: 6.575876276833671
Training loss: 0.8670803904533386 / Valid loss: 6.589196180161975

Epoch: 25
Training loss: 0.6435341238975525 / Valid loss: 6.579582589013236
Training loss: 0.29623180627822876 / Valid loss: 6.517912360600063
Training loss: 0.41823065280914307 / Valid loss: 6.528584094274612
Training loss: 0.2676159739494324 / Valid loss: 6.514958645048596
Training loss: 0.24382954835891724 / Valid loss: 6.54841746148609

Epoch: 26
Training loss: 0.3725477457046509 / Valid loss: 6.54251918338594
Training loss: 0.504570484161377 / Valid loss: 6.555820183526902
Training loss: 0.20895236730575562 / Valid loss: 6.497135160082863
Training loss: 0.2649601101875305 / Valid loss: 6.582287747519357
Training loss: 0.3682362735271454 / Valid loss: 6.565891969771612

Epoch: 27
Training loss: 0.26271557807922363 / Valid loss: 6.516990770612444
Training loss: 0.39834481477737427 / Valid loss: 6.5540310064951575
Training loss: 0.39193862676620483 / Valid loss: 6.584957041059222
Training loss: 0.36901986598968506 / Valid loss: 6.53929474467323
Training loss: 0.3372167944908142 / Valid loss: 6.527570056915283

Epoch: 28
Training loss: 0.2596457004547119 / Valid loss: 6.523834460122245
Training loss: 0.4018998146057129 / Valid loss: 6.519163869676136
Training loss: 0.5498902201652527 / Valid loss: 6.553589929853167
Training loss: 0.3178008794784546 / Valid loss: 6.489233925229027
Training loss: 0.29510974884033203 / Valid loss: 6.527069623129709

Epoch: 29
Training loss: 0.3009474575519562 / Valid loss: 6.478154214223226
Training loss: 0.2926444709300995 / Valid loss: 6.515493987855457
Training loss: 0.12174738943576813 / Valid loss: 6.493541413261777
Training loss: 0.20460005104541779 / Valid loss: 6.508174405779157

Epoch: 30
Training loss: 0.22724877297878265 / Valid loss: 6.498072901226226
Training loss: 0.2598232626914978 / Valid loss: 6.4871627081008185
Training loss: 0.25000786781311035 / Valid loss: 6.531401722771781
Training loss: 0.315096914768219 / Valid loss: 6.53367938768296
Training loss: 0.30825871229171753 / Valid loss: 6.511435715357463

Epoch: 31
Training loss: 0.2798534631729126 / Valid loss: 6.433579987571353
Training loss: 0.263679563999176 / Valid loss: 6.473984209696452
Training loss: 0.24449357390403748 / Valid loss: 6.47363756497701
Training loss: 0.282825767993927 / Valid loss: 6.511305145990281
Training loss: 0.18286991119384766 / Valid loss: 6.539030633653913

Epoch: 32
Training loss: 0.3272579312324524 / Valid loss: 6.530714384714762
Training loss: 0.31074586510658264 / Valid loss: 6.512446162814186
Training loss: 0.20961004495620728 / Valid loss: 6.572762714113508
Training loss: 0.1993136703968048 / Valid loss: 6.476679697490874
Training loss: 0.32529497146606445 / Valid loss: 6.516221082778204

Epoch: 33
Training loss: 0.2471737265586853 / Valid loss: 6.4616634573255265
Training loss: 0.3477966785430908 / Valid loss: 6.5341354188464935
Training loss: 0.20642465353012085 / Valid loss: 6.461787173861549
Training loss: 0.1547841727733612 / Valid loss: 6.487263852074033
Training loss: 0.28536760807037354 / Valid loss: 6.497025898524693

Epoch: 34
Training loss: 0.5804500579833984 / Valid loss: 6.44485284941537
Training loss: 0.25263845920562744 / Valid loss: 6.464764149983724
Training loss: 0.22393028438091278 / Valid loss: 6.53330568586077
Training loss: 0.21801960468292236 / Valid loss: 6.5301706541152225
Training loss: 0.19783607125282288 / Valid loss: 6.4900689147767565

Epoch: 35
Training loss: 0.7059658765792847 / Valid loss: 6.470491100492931
Training loss: 0.2884620428085327 / Valid loss: 6.443025284721738
Training loss: 0.36435389518737793 / Valid loss: 6.5528745923723495
Training loss: 0.16806453466415405 / Valid loss: 6.506603879020328
Training loss: 0.19943425059318542 / Valid loss: 6.555985153289068

Epoch: 36
Training loss: 0.24202662706375122 / Valid loss: 6.500633044469924
Training loss: 0.4951649308204651 / Valid loss: 6.4992023468017575
Training loss: 0.16354691982269287 / Valid loss: 6.4916447730291456
Training loss: 0.21823997795581818 / Valid loss: 6.5242975779942105
Training loss: 0.17191584408283234 / Valid loss: 6.5625038237798785

Epoch: 37
Training loss: 0.33796337246894836 / Valid loss: 6.470151140576317
Training loss: 0.18709143996238708 / Valid loss: 6.49595235869998
Training loss: 0.38278505206108093 / Valid loss: 6.496720563797724
Training loss: 0.6351658701896667 / Valid loss: 6.4828751791091195
Training loss: 0.36936014890670776 / Valid loss: 6.478620590482439

Epoch: 38
Training loss: 0.2900775074958801 / Valid loss: 6.45573145094372
Training loss: 0.230264350771904 / Valid loss: 6.47436508224124
Training loss: 0.24393035471439362 / Valid loss: 6.47273131779262
Training loss: 0.15568336844444275 / Valid loss: 6.479994260697138
Training loss: 0.24576780200004578 / Valid loss: 6.552403949555897

Epoch: 39
Training loss: 0.251237154006958 / Valid loss: 6.4952503749302455
Training loss: 0.17403073608875275 / Valid loss: 6.496827915736607
Training loss: 0.2253260612487793 / Valid loss: 6.455013774690174
Training loss: 0.20019690692424774 / Valid loss: 6.4839876243046355

Epoch: 40
Training loss: 0.16414153575897217 / Valid loss: 6.460242518924532
Training loss: 0.2303619533777237 / Valid loss: 6.440657617932274
Training loss: 0.20619961619377136 / Valid loss: 6.480282454263596
Training loss: 0.16448424756526947 / Valid loss: 6.504538027445475
Training loss: 0.1437200903892517 / Valid loss: 6.515617847442627

Epoch: 41
Training loss: 0.24530014395713806 / Valid loss: 6.4566205092838835
Training loss: 0.18025606870651245 / Valid loss: 6.438510840279715
Training loss: 0.21488246321678162 / Valid loss: 6.477592291150774
Training loss: 1.118802547454834 / Valid loss: 6.51607027053833
Training loss: 0.36013415455818176 / Valid loss: 6.461066686539423

Epoch: 42
Training loss: 0.1628488004207611 / Valid loss: 6.482588704427084
Training loss: 0.21322810649871826 / Valid loss: 6.496055566696894
Training loss: 0.2698012888431549 / Valid loss: 6.478043660663423
Training loss: 0.15830481052398682 / Valid loss: 6.460016657057262
Training loss: 0.16900739073753357 / Valid loss: 6.473711034229823

Epoch: 43
Training loss: 0.2869962453842163 / Valid loss: 6.469106692359561
Training loss: 0.20188933610916138 / Valid loss: 6.471167432694208
Training loss: 0.19407644867897034 / Valid loss: 6.468402635483515
Training loss: 0.4035569131374359 / Valid loss: 6.463747374216715
Training loss: 0.2676756978034973 / Valid loss: 6.464208478019351

Epoch: 44
Training loss: 0.18687205016613007 / Valid loss: 6.500656000773112
Training loss: 0.20516744256019592 / Valid loss: 6.479520874931699
Training loss: 0.2585129141807556 / Valid loss: 6.424930656523932
Training loss: 0.14302077889442444 / Valid loss: 6.410916791643415
Training loss: 0.2980179190635681 / Valid loss: 6.495898680459885

Epoch: 45
Training loss: 0.1693333089351654 / Valid loss: 6.431118411109561
Training loss: 0.33454546332359314 / Valid loss: 6.4309495835077195
Training loss: 0.4446280300617218 / Valid loss: 6.528272699174427
Training loss: 0.1860540360212326 / Valid loss: 6.471066608883086
Training loss: 0.21815894544124603 / Valid loss: 6.441833641415551

Epoch: 46
Training loss: 0.11010241508483887 / Valid loss: 6.437472425188337
Training loss: 0.2536678910255432 / Valid loss: 6.432887061436971
Training loss: 0.3705959618091583 / Valid loss: 6.459955705915179
Training loss: 0.0996260792016983 / Valid loss: 6.419195000330607
Training loss: 0.5902842283248901 / Valid loss: 6.45604582741147

Epoch: 47
Training loss: 0.10716493427753448 / Valid loss: 6.431344718024844
Training loss: 0.2454679310321808 / Valid loss: 6.490338034856887
Training loss: 0.38025957345962524 / Valid loss: 6.433895399456932
Training loss: 0.1942223310470581 / Valid loss: 6.43385599454244
Training loss: 0.18325155973434448 / Valid loss: 6.469023073287238

Epoch: 48
Training loss: 0.19283102452754974 / Valid loss: 6.492764039266677
Training loss: 0.3023659586906433 / Valid loss: 6.4602781068711055
Training loss: 0.21869555115699768 / Valid loss: 6.4754110563369025
Training loss: 0.16282393038272858 / Valid loss: 6.468641662597657
Training loss: 0.29198482632637024 / Valid loss: 6.444720488502866

Epoch: 49
Training loss: 0.3586122989654541 / Valid loss: 6.472088747932798
Training loss: 0.1698007583618164 / Valid loss: 6.4460055532909575
Training loss: 0.18436697125434875 / Valid loss: 6.462417080288842
Training loss: 0.22286131978034973 / Valid loss: 6.460023625691732

Epoch: 50
Training loss: 0.28334712982177734 / Valid loss: 6.455861414046515
Training loss: 0.22202980518341064 / Valid loss: 6.466064723332723
Training loss: 0.1678978055715561 / Valid loss: 6.423628350666591
Training loss: 0.2686591148376465 / Valid loss: 6.464798720677694
Training loss: 0.3956199586391449 / Valid loss: 6.458104767118182

Epoch: 51
Training loss: 0.1519360989332199 / Valid loss: 6.434102671486991
Training loss: 0.320894330739975 / Valid loss: 6.464236248107183
Training loss: 0.2191004902124405 / Valid loss: 6.411193995248704
Training loss: 0.19268375635147095 / Valid loss: 6.490602066403343
Training loss: 0.25765207409858704 / Valid loss: 6.3968986284165155

Epoch: 52
Training loss: 0.13751880824565887 / Valid loss: 6.4520240465799965
Training loss: 0.10310176014900208 / Valid loss: 6.431108031954084
Training loss: 0.15264007449150085 / Valid loss: 6.462610063098726
Training loss: 0.16902166604995728 / Valid loss: 6.407850683303106
Training loss: 0.3515225052833557 / Valid loss: 6.465167772202265

Epoch: 53
Training loss: 0.15263475477695465 / Valid loss: 6.409921319144113
Training loss: 0.3299385905265808 / Valid loss: 6.413794022514707
Training loss: 0.15401817858219147 / Valid loss: 6.424110623768398
Training loss: 0.4960412085056305 / Valid loss: 6.403119609469459
Training loss: 0.2546340823173523 / Valid loss: 6.444769677661714

Epoch: 54
Training loss: 0.28801462054252625 / Valid loss: 6.441448125385103
Training loss: 0.3182638883590698 / Valid loss: 6.454235426584879
Training loss: 0.1724241077899933 / Valid loss: 6.451311810811361
Training loss: 0.1500752866268158 / Valid loss: 6.4452796640850245
Training loss: 0.30608364939689636 / Valid loss: 6.425220589410691

Epoch: 55
Training loss: 0.5271943211555481 / Valid loss: 6.430661909920829
Training loss: 0.3344670534133911 / Valid loss: 6.409948033378238
Training loss: 0.4309447407722473 / Valid loss: 6.425917738959903
Training loss: 0.17575542628765106 / Valid loss: 6.459461850211734
Training loss: 0.2597511410713196 / Valid loss: 6.455588522411528

Epoch: 56
Training loss: 0.4460563659667969 / Valid loss: 6.442088445027669
Training loss: 0.15093007683753967 / Valid loss: 6.417564260391962
Training loss: 0.1513858437538147 / Valid loss: 6.392606412796747
Training loss: 0.17092105746269226 / Valid loss: 6.390770603361584
Training loss: 0.28505939245224 / Valid loss: 6.415219079880488

Epoch: 57
Training loss: 0.6073035001754761 / Valid loss: 6.4241612411680675
Training loss: 0.33692610263824463 / Valid loss: 6.426250471387591
Training loss: 0.28244641423225403 / Valid loss: 6.4169469106765025
Training loss: 0.5004138946533203 / Valid loss: 6.426950341179257
Training loss: 0.42479297518730164 / Valid loss: 6.446034488223848

Epoch: 58
Training loss: 0.17794150114059448 / Valid loss: 6.378240648905436
Training loss: 0.29364705085754395 / Valid loss: 6.422733429500035
Training loss: 0.11600545048713684 / Valid loss: 6.402946712857201
Training loss: 0.38747161626815796 / Valid loss: 6.410565873554774
Training loss: 0.14547303318977356 / Valid loss: 6.40334753763108

Epoch: 59
Training loss: 0.26980704069137573 / Valid loss: 6.415687987917946
Training loss: 0.3091619312763214 / Valid loss: 6.368788648786999
Training loss: 0.21759623289108276 / Valid loss: 6.425389405659267
Training loss: 0.2503666579723358 / Valid loss: 6.426740648632958

Epoch: 60
Training loss: 0.20996791124343872 / Valid loss: 6.448155321393695
Training loss: 0.5508055686950684 / Valid loss: 6.446132053647722
Training loss: 0.13736677169799805 / Valid loss: 6.415488663173857
Training loss: 0.24522598087787628 / Valid loss: 6.399518555686587
Training loss: 0.13587158918380737 / Valid loss: 6.404240944271996

Epoch: 61
Training loss: 0.3713666498661041 / Valid loss: 6.385814301172892
Training loss: 0.11027577519416809 / Valid loss: 6.395043316341582
Training loss: 0.4406837522983551 / Valid loss: 6.40865889957973
Training loss: 0.28984612226486206 / Valid loss: 6.417125408990042
Training loss: 0.1411258578300476 / Valid loss: 6.373506868453253

Epoch: 62
Training loss: 0.07512064278125763 / Valid loss: 6.405236148834229
Training loss: 0.12571504712104797 / Valid loss: 6.397777125948951
Training loss: 0.13357582688331604 / Valid loss: 6.406822263626825
Training loss: 0.23017603158950806 / Valid loss: 6.426078919001988
Training loss: 0.18108639121055603 / Valid loss: 6.408461993081229

Epoch: 63
Training loss: 0.2676023244857788 / Valid loss: 6.390839422316779
Training loss: 0.17496246099472046 / Valid loss: 6.384416816348121
Training loss: 0.13901154696941376 / Valid loss: 6.4285507474626815
Training loss: 0.2826370894908905 / Valid loss: 6.3926447459629605
Training loss: 0.1755586564540863 / Valid loss: 6.37895987374442

Epoch: 64
Training loss: 0.20380598306655884 / Valid loss: 6.377519598461332
Training loss: 0.13412022590637207 / Valid loss: 6.413230593999227
Training loss: 0.26380765438079834 / Valid loss: 6.400458780924479
Training loss: 0.15620093047618866 / Valid loss: 6.41102762903486
Training loss: 0.14484095573425293 / Valid loss: 6.437744172414144

Epoch: 65
Training loss: 0.18184953927993774 / Valid loss: 6.3863902773175925
Training loss: 0.307429701089859 / Valid loss: 6.3836912927173435
Training loss: 0.21065860986709595 / Valid loss: 6.3919322922116235
Training loss: 0.1430898904800415 / Valid loss: 6.38922286487761
Training loss: 0.9966495037078857 / Valid loss: 6.376174602054414

Epoch: 66
Training loss: 0.12379288673400879 / Valid loss: 6.3762088457743324
Training loss: 0.1314248889684677 / Valid loss: 6.363207939692906
Training loss: 0.49382320046424866 / Valid loss: 6.429370017278762
Training loss: 0.1896178126335144 / Valid loss: 6.44899540855771
Training loss: 0.3517659902572632 / Valid loss: 6.366899417695545

Epoch: 67
Training loss: 0.19452032446861267 / Valid loss: 6.413976224263509
Training loss: 0.08782894909381866 / Valid loss: 6.427938965388707
Training loss: 0.11512821912765503 / Valid loss: 6.401551896049863
Training loss: 0.6412409543991089 / Valid loss: 6.3999561446053645
Training loss: 0.22524496912956238 / Valid loss: 6.426959628150577

Epoch: 68
Training loss: 0.19396336376667023 / Valid loss: 6.367981415703183
Training loss: 0.24474012851715088 / Valid loss: 6.366862099511283
Training loss: 0.143762469291687 / Valid loss: 6.363574822743733
Training loss: 0.14347538352012634 / Valid loss: 6.414360616320655
Training loss: 0.2169269174337387 / Valid loss: 6.3804668471926735

Epoch: 69
Training loss: 0.2902604341506958 / Valid loss: 6.38683039574396
Training loss: 0.2057105004787445 / Valid loss: 6.380639266967774
Training loss: 0.2580888569355011 / Valid loss: 6.415684677305675
Training loss: 0.30055636167526245 / Valid loss: 6.369109069733392

Epoch: 70
Training loss: 0.15171724557876587 / Valid loss: 6.4016354969569615
Training loss: 0.48688817024230957 / Valid loss: 6.35352532523019
Training loss: 0.18163177371025085 / Valid loss: 6.377068301609584
Training loss: 0.19107021391391754 / Valid loss: 6.37387562025161
Training loss: 0.2170679122209549 / Valid loss: 6.343246403194609

Epoch: 71
Training loss: 0.27173054218292236 / Valid loss: 6.371781467256092
Training loss: 0.3603571355342865 / Valid loss: 6.351015281677246
Training loss: 0.1392894685268402 / Valid loss: 6.364069100788662
Training loss: 0.12592406570911407 / Valid loss: 6.35192776180449
Training loss: 0.1536390483379364 / Valid loss: 6.365172068277995

Epoch: 72
Training loss: 0.1500091850757599 / Valid loss: 6.365414723895845
Training loss: 0.36779162287712097 / Valid loss: 6.361125233059838
Training loss: 0.16407105326652527 / Valid loss: 6.351516230901082
Training loss: 0.14547589421272278 / Valid loss: 6.4005286693573
Training loss: 0.4628208875656128 / Valid loss: 6.349948120117188

Epoch: 73
Training loss: 0.3234761357307434 / Valid loss: 6.372968008404686
Training loss: 0.4039190709590912 / Valid loss: 6.3426520574660525
Training loss: 0.1986491084098816 / Valid loss: 6.3450053010668075
Training loss: 0.12525975704193115 / Valid loss: 6.358750141234625
Training loss: 0.1173602044582367 / Valid loss: 6.3613428797040665

Epoch: 74
Training loss: 0.13991358876228333 / Valid loss: 6.37265632947286
Training loss: 0.19992807507514954 / Valid loss: 6.366491317749023
Training loss: 0.33867907524108887 / Valid loss: 6.313078455697923
Training loss: 0.37945556640625 / Valid loss: 6.38682583173116
Training loss: 0.17365118861198425 / Valid loss: 6.391597713742938

Epoch: 75
Training loss: 0.16219261288642883 / Valid loss: 6.334480199359712
Training loss: 0.26773208379745483 / Valid loss: 6.356841763995942
Training loss: 0.4660983681678772 / Valid loss: 6.334250143596104
Training loss: 0.1330040991306305 / Valid loss: 6.308213826588222
Training loss: 0.24956099689006805 / Valid loss: 6.318492067427862

Epoch: 76
Training loss: 0.3140931725502014 / Valid loss: 6.3049195425851
Training loss: 0.48272955417633057 / Valid loss: 6.359162780216762
Training loss: 0.1641683131456375 / Valid loss: 6.3095206396920345
Training loss: 0.24776586890220642 / Valid loss: 6.360502120426723
Training loss: 0.19081729650497437 / Valid loss: 6.337754572005499

Epoch: 77
Training loss: 0.17644508183002472 / Valid loss: 6.357056837990171
Training loss: 0.23343130946159363 / Valid loss: 6.332255408877418
Training loss: 0.3262977600097656 / Valid loss: 6.363460677010672
Training loss: 0.31189480423927307 / Valid loss: 6.365561403547015
Training loss: 0.4388204514980316 / Valid loss: 6.367628008978707

Epoch: 78
Training loss: 0.20859310030937195 / Valid loss: 6.352250707717169
Training loss: 0.1349269449710846 / Valid loss: 6.363267035711379
Training loss: 0.12820421159267426 / Valid loss: 6.349946242287046
Training loss: 0.15040694177150726 / Valid loss: 6.3553952126275925
Training loss: 0.36882147192955017 / Valid loss: 6.373518496467954

Epoch: 79
Training loss: 0.0935317873954773 / Valid loss: 6.344944374901908
Training loss: 0.3093976378440857 / Valid loss: 6.366897476287115
Training loss: 0.07659834623336792 / Valid loss: 6.360671440760295
Training loss: 0.10881249606609344 / Valid loss: 6.360940138498942
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model: 5.381952912466867
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 80
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0
momentum : 0
embedder : Bert
verbose : False
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
neural net:
 [tensor([[ 0.0035, -0.0035,  0.0052,  ..., -0.0130,  0.0056, -0.0088],
        [ 0.0045, -0.0089, -0.0107,  ...,  0.0052,  0.0048, -0.0076],
        [ 0.0014, -0.0062,  0.0084,  ..., -0.0093,  0.0089,  0.0006],
        ...,
        [ 0.0115,  0.0001, -0.0032,  ..., -0.0018, -0.0062, -0.0085],
        [-0.0009,  0.0057,  0.0107,  ..., -0.0067, -0.0022, -0.0107],
        [ 0.0109, -0.0116, -0.0054,  ...,  0.0007, -0.0127,  0.0088]],
       device='cuda:0'), tensor([ 0.0047,  0.0122,  0.0094,  ...,  0.0037, -0.0029, -0.0056],
       device='cuda:0'), tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), tensor([[-0.0121, -0.0085,  0.0158,  ...,  0.0067, -0.0057, -0.0189],
        [ 0.0064, -0.0150,  0.0096,  ...,  0.0006,  0.0074,  0.0177],
        [ 0.0061, -0.0078,  0.0059,  ...,  0.0032, -0.0206,  0.0205],
        ...,
        [ 0.0173, -0.0032,  0.0090,  ..., -0.0031,  0.0141,  0.0130],
        [-0.0181, -0.0101,  0.0157,  ..., -0.0020, -0.0070,  0.0160],
        [ 0.0063, -0.0152, -0.0101,  ...,  0.0008,  0.0028, -0.0011]],
       device='cuda:0'), tensor([-1.1967e-02,  1.1110e-02,  1.8701e-02,  1.1023e-02, -2.0298e-02,
         5.6226e-05, -1.3542e-02,  2.7363e-03, -8.2064e-03,  1.8652e-02,
         1.4701e-02,  1.2389e-02,  2.1926e-02,  1.5760e-02, -3.8398e-03,
        -2.1899e-02,  2.2099e-02, -5.7002e-03,  2.0446e-03,  2.1733e-02,
         1.5114e-02,  2.0789e-03, -1.0899e-02, -5.6587e-03,  2.0192e-02,
         4.3223e-03,  8.8637e-03, -1.6798e-02,  1.7955e-02,  1.2753e-02,
        -5.4965e-03,  1.9481e-02,  6.9524e-03, -2.0116e-02, -5.9529e-03,
        -1.1193e-02,  1.8363e-02,  1.6020e-02, -1.6430e-02,  6.4647e-03,
         1.5684e-02,  9.1588e-03, -2.0390e-02,  9.9621e-03,  2.0163e-02,
         2.1748e-02, -1.8861e-02, -5.0238e-03,  1.9386e-02,  1.1112e-03,
        -1.3363e-02,  6.7029e-03,  1.7814e-02, -8.2254e-03,  1.3094e-02,
         1.2228e-02, -1.2875e-03,  3.9142e-03,  1.4362e-03,  5.9424e-03,
         2.2414e-03, -1.3438e-02, -7.4102e-03,  1.5021e-02, -1.8291e-02,
         1.7939e-02,  2.4626e-03, -1.6420e-03,  1.0483e-03, -6.6787e-03,
         1.9978e-02,  6.3201e-04,  1.7342e-02,  9.4639e-03,  2.2343e-02,
        -1.5528e-02,  1.0803e-02, -4.7853e-03,  1.3566e-02, -1.2095e-02,
        -5.4629e-03,  7.9233e-03, -1.5708e-02, -2.7313e-03,  8.8111e-03,
        -2.1376e-03,  2.0746e-02,  1.0951e-02, -1.4991e-02,  5.5452e-03,
        -1.4782e-02, -1.6080e-02, -5.0667e-03, -2.1050e-02, -7.7526e-03,
         1.9154e-02, -2.1401e-02, -1.0501e-02,  7.3819e-03,  2.0670e-03],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), tensor([[-0.0636, -0.0402,  0.0180,  ..., -0.0669, -0.0184, -0.0632],
        [-0.0702, -0.0480,  0.0203,  ...,  0.0432, -0.0736,  0.0113],
        [-0.0735, -0.0482,  0.0561,  ..., -0.0377,  0.0717,  0.0079],
        ...,
        [ 0.0336,  0.0795,  0.0686,  ...,  0.0617,  0.0593,  0.0145],
        [-0.0549, -0.0686,  0.0407,  ..., -0.0574,  0.0591, -0.0667],
        [-0.0106, -0.0514, -0.0024,  ...,  0.0682,  0.0980,  0.0754]],
       device='cuda:0'), tensor([-0.0598,  0.0784,  0.0041,  0.0437, -0.0314,  0.0230, -0.0737, -0.0741,
        -0.0710, -0.0826, -0.0965, -0.0397,  0.0959,  0.0370,  0.0845,  0.0369],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0'), tensor([[-0.0476,  0.1976,  0.0806, -0.1341,  0.1659,  0.0086, -0.0477,  0.0708,
          0.2368,  0.0414, -0.1084, -0.0428,  0.1062, -0.1102,  0.0168,  0.1324]],
       device='cuda:0'), tensor([0.1504], device='cuda:0')]

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.326356043134417
Model is saved in epoch 0, overall batch: 0
Training loss: 9.031549453735352 / Valid loss: 12.925361442565919
Model is saved in epoch 0, overall batch: 100
Training loss: 9.649495124816895 / Valid loss: 11.93502668199085
Model is saved in epoch 0, overall batch: 200
Training loss: 14.655001640319824 / Valid loss: 11.029834052494595
Model is saved in epoch 0, overall batch: 300
Training loss: 10.413329124450684 / Valid loss: 10.265079829806373
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 8.910350799560547 / Valid loss: 9.613765380496071
Model is saved in epoch 1, overall batch: 500
Training loss: 7.872617721557617 / Valid loss: 8.97424146107265
Model is saved in epoch 1, overall batch: 600
Training loss: 7.332974433898926 / Valid loss: 8.527550057002477
Model is saved in epoch 1, overall batch: 700
Training loss: 7.229619026184082 / Valid loss: 8.04958698181879
Model is saved in epoch 1, overall batch: 800
Training loss: 5.423467636108398 / Valid loss: 7.773928292592367
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 7.810892105102539 / Valid loss: 7.546292895362491
Model is saved in epoch 2, overall batch: 1000
Training loss: 7.799148082733154 / Valid loss: 7.289633210500082
Model is saved in epoch 2, overall batch: 1100
Training loss: 7.037626266479492 / Valid loss: 7.038455272856213
Model is saved in epoch 2, overall batch: 1200
Training loss: 8.314278602600098 / Valid loss: 6.658316196714129
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.37082576751709 / Valid loss: 6.479670063654582
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.279825687408447 / Valid loss: 6.456323323931013
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.787858724594116 / Valid loss: 6.2811033793858115
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.567384719848633 / Valid loss: 6.132880061013358
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.074599266052246 / Valid loss: 6.027197063536871
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.374438285827637 / Valid loss: 5.917620524905977
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 5.37590217590332 / Valid loss: 6.066183928080967
Training loss: 3.7718729972839355 / Valid loss: 6.084708586193266
Training loss: 4.1693549156188965 / Valid loss: 5.980021408626011
Training loss: 4.449491500854492 / Valid loss: 5.9436943780808225
Training loss: 3.1457338333129883 / Valid loss: 5.8985951628003805
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 6.034307479858398 / Valid loss: 5.8190965062096005
Model is saved in epoch 5, overall batch: 2500
Training loss: 4.65392541885376 / Valid loss: 5.883803662799654
Training loss: 3.245314359664917 / Valid loss: 5.882777790796189
Training loss: 3.8647522926330566 / Valid loss: 5.8596199580601285
Training loss: 3.6135709285736084 / Valid loss: 5.8016817705971855
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 5.956195831298828 / Valid loss: 5.851204438436599
Training loss: 3.4913218021392822 / Valid loss: 5.922347840808687
Training loss: 3.9809350967407227 / Valid loss: 5.927971462976365
Training loss: 3.54435396194458 / Valid loss: 5.909689396903628
Training loss: 4.117196559906006 / Valid loss: 6.0334273542676655

Epoch: 7
Training loss: 3.502049446105957 / Valid loss: 5.933380254109701
Training loss: 3.139695644378662 / Valid loss: 5.993251664297921
Training loss: 2.697209358215332 / Valid loss: 5.977835546221052
Training loss: 4.136932373046875 / Valid loss: 6.063802925745646
Training loss: 3.038693428039551 / Valid loss: 6.007130838575817

Epoch: 8
Training loss: 2.8324384689331055 / Valid loss: 6.0750229494912285
Training loss: 1.8436771631240845 / Valid loss: 6.078082086926415
Training loss: 3.234579086303711 / Valid loss: 6.108758088520595
Training loss: 4.641075134277344 / Valid loss: 6.153123925981068
Training loss: 2.8398232460021973 / Valid loss: 6.134172952742803

Epoch: 9
Training loss: 1.5098446607589722 / Valid loss: 6.158533652623494
Training loss: 2.5671896934509277 / Valid loss: 6.18920902070545
Training loss: 3.6185734272003174 / Valid loss: 6.201976576305571
Training loss: 2.732630968093872 / Valid loss: 6.195091059094383

Epoch: 10
Training loss: 1.832048773765564 / Valid loss: 6.237085744312831
Training loss: 2.661381244659424 / Valid loss: 6.234330043338594
Training loss: 1.560402512550354 / Valid loss: 6.398729760306222
Training loss: 2.305189609527588 / Valid loss: 6.391875509988694
Training loss: 2.492584705352783 / Valid loss: 6.298088019234793

Epoch: 11
Training loss: 1.2297296524047852 / Valid loss: 6.3910364877609975
Training loss: 1.4975640773773193 / Valid loss: 6.3959820860908145
Training loss: 2.548607349395752 / Valid loss: 6.465189738500686
Training loss: 1.4185914993286133 / Valid loss: 6.6187856447129025
Training loss: 2.128636360168457 / Valid loss: 6.666912732805525

Epoch: 12
Training loss: 1.1633267402648926 / Valid loss: 6.452874792189825
Training loss: 1.2297829389572144 / Valid loss: 6.56710113797869
Training loss: 1.5909547805786133 / Valid loss: 6.575278670447213
Training loss: 1.3911828994750977 / Valid loss: 6.523057824089413
Training loss: 1.3432307243347168 / Valid loss: 6.534570619038173

Epoch: 13
Training loss: 0.9551684260368347 / Valid loss: 6.706716859908331
Training loss: 0.8917296528816223 / Valid loss: 6.6589717774164106
Training loss: 1.1735572814941406 / Valid loss: 6.546581147965931
Training loss: 1.4201452732086182 / Valid loss: 6.681150949568975
Training loss: 1.3012744188308716 / Valid loss: 6.600960706529163

Epoch: 14
Training loss: 1.0165108442306519 / Valid loss: 6.741644266673497
Training loss: 1.0852172374725342 / Valid loss: 6.643154287338257
Training loss: 1.4932785034179688 / Valid loss: 6.633780184246245
Training loss: 1.1285059452056885 / Valid loss: 6.6558440480913434
Training loss: 1.6491352319717407 / Valid loss: 6.664465704418364

Epoch: 15
Training loss: 1.0540403127670288 / Valid loss: 6.7938585939861476
Training loss: 1.0658385753631592 / Valid loss: 6.817523004895165
Training loss: 1.0568095445632935 / Valid loss: 6.705037425813221
Training loss: 1.0983721017837524 / Valid loss: 6.749984929675148
Training loss: 1.1242650747299194 / Valid loss: 6.689923824582781

Epoch: 16
Training loss: 0.5892389416694641 / Valid loss: 6.810284650893438
Training loss: 0.7803249359130859 / Valid loss: 7.188371404012044
Training loss: 1.1032828092575073 / Valid loss: 6.764204202379499
Training loss: 1.0612361431121826 / Valid loss: 6.95405265490214
Training loss: 1.173661708831787 / Valid loss: 6.885861097063337

Epoch: 17
Training loss: 1.0085021257400513 / Valid loss: 6.822495294752575
Training loss: 0.9435634613037109 / Valid loss: 6.956481792813256
Training loss: 0.9202849864959717 / Valid loss: 6.930529340108236
Training loss: 0.666002094745636 / Valid loss: 6.919105189187186
Training loss: 1.0445382595062256 / Valid loss: 6.757153756277901

Epoch: 18
Training loss: 0.6763856410980225 / Valid loss: 6.79201413109189
Training loss: 0.8552510738372803 / Valid loss: 6.800828341075352
Training loss: 0.8908299207687378 / Valid loss: 6.922042628696986
Training loss: 0.9965186715126038 / Valid loss: 7.065425046284994
Training loss: 0.863675594329834 / Valid loss: 6.810019663402012

Epoch: 19
Training loss: 0.8063485026359558 / Valid loss: 6.918242499941871
Training loss: 1.2864010334014893 / Valid loss: 6.813279499326433
Training loss: 0.6465586423873901 / Valid loss: 6.807423300970168
Training loss: 0.6824859380722046 / Valid loss: 6.840832583109537

Epoch: 20
Training loss: 0.40538251399993896 / Valid loss: 6.810974334535144
Training loss: 1.036321997642517 / Valid loss: 6.8397407032194595
Training loss: 0.4680996537208557 / Valid loss: 6.826008274441674
Training loss: 0.6702053546905518 / Valid loss: 6.848900013878232
Training loss: 1.099815845489502 / Valid loss: 6.829094416754586

Epoch: 21
Training loss: 0.5849310159683228 / Valid loss: 6.897969468434652
Training loss: 0.7242369651794434 / Valid loss: 6.8522044408889045
Training loss: 0.6205954551696777 / Valid loss: 6.849351374308268
Training loss: 0.492944598197937 / Valid loss: 6.910346090225946
Training loss: 0.671619176864624 / Valid loss: 6.842398089454288

Epoch: 22
Training loss: 0.7165206670761108 / Valid loss: 6.86344078154791
Training loss: 0.554999589920044 / Valid loss: 6.95038419905163
Training loss: 0.4596119523048401 / Valid loss: 6.781331836609613
Training loss: 1.3692537546157837 / Valid loss: 6.841743144534883
Training loss: 0.7559306621551514 / Valid loss: 6.840181078229632

Epoch: 23
Training loss: 0.7283676862716675 / Valid loss: 6.892171362468175
Training loss: 0.7919235229492188 / Valid loss: 6.783128611246744
Training loss: 0.8206788897514343 / Valid loss: 6.887018896284557
Training loss: 0.5897453427314758 / Valid loss: 6.852277633122036
Training loss: 1.0101314783096313 / Valid loss: 6.9078188987005325

Epoch: 24
Training loss: 0.8423129320144653 / Valid loss: 6.823648264294579
Training loss: 0.6432785987854004 / Valid loss: 6.800510660807292
Training loss: 0.5377112030982971 / Valid loss: 6.923416818891253
Training loss: 0.9146564602851868 / Valid loss: 6.848580855414981
Training loss: 1.0273497104644775 / Valid loss: 6.892161977858771

Epoch: 25
Training loss: 1.0057035684585571 / Valid loss: 6.882301693870907
Training loss: 0.3921237587928772 / Valid loss: 6.856995741526286
Training loss: 0.8008273839950562 / Valid loss: 6.7691147622608
Training loss: 0.3524518609046936 / Valid loss: 6.795892313548497
Training loss: 0.5134373903274536 / Valid loss: 6.9135886056082585

Epoch: 26
Training loss: 0.5141182541847229 / Valid loss: 6.811796706063407
Training loss: 0.5204722881317139 / Valid loss: 6.847500975926717
Training loss: 0.3656955361366272 / Valid loss: 6.839329501560756
Training loss: 0.43640822172164917 / Valid loss: 6.982221017565046
Training loss: 0.4070833921432495 / Valid loss: 6.97004113424392

Epoch: 27
Training loss: 0.3686547875404358 / Valid loss: 6.825541260128929
Training loss: 0.47597256302833557 / Valid loss: 6.963418935594104
Training loss: 0.6055011749267578 / Valid loss: 6.956779216584705
Training loss: 0.4820709824562073 / Valid loss: 6.935784280867804
Training loss: 0.6080188751220703 / Valid loss: 6.914210024334135

Epoch: 28
Training loss: 0.4018572270870209 / Valid loss: 6.922458239964077
Training loss: 0.37014228105545044 / Valid loss: 6.837387693495978
Training loss: 0.7190433144569397 / Valid loss: 6.880425839197068
Training loss: 0.57194584608078 / Valid loss: 6.869017782665434
Training loss: 0.4527132511138916 / Valid loss: 6.875129699707031

Epoch: 29
Training loss: 0.6052722930908203 / Valid loss: 6.821631667727516
Training loss: 0.46156978607177734 / Valid loss: 6.808241335550944
Training loss: 0.31890395283699036 / Valid loss: 6.8673095703125
Training loss: 0.4430583119392395 / Valid loss: 6.832905601319813

Epoch: 30
Training loss: 0.4817635715007782 / Valid loss: 6.803235054016113
Training loss: 0.4514594078063965 / Valid loss: 6.817978398005168
Training loss: 0.3631715178489685 / Valid loss: 6.807428577968053
Training loss: 0.5134695768356323 / Valid loss: 6.876563944135394
Training loss: 0.4263817369937897 / Valid loss: 6.7823568571181525

Epoch: 31
Training loss: 0.5117906332015991 / Valid loss: 6.86391129266648
Training loss: 0.4331490993499756 / Valid loss: 6.831766886938186
Training loss: 0.4823755621910095 / Valid loss: 6.872081515902565
Training loss: 0.33831608295440674 / Valid loss: 6.8909860020592095
Training loss: 0.43682053685188293 / Valid loss: 6.999074477241153

Epoch: 32
Training loss: 0.5587068796157837 / Valid loss: 6.849571125847953
Training loss: 0.4467211365699768 / Valid loss: 6.856874209358579
Training loss: 0.4933033585548401 / Valid loss: 6.851555061340332
Training loss: 0.41836023330688477 / Valid loss: 6.806804557073684
Training loss: 0.5701733231544495 / Valid loss: 6.752812017713274

Epoch: 33
Training loss: 0.48938149213790894 / Valid loss: 6.830871200561523
Training loss: 0.5299113392829895 / Valid loss: 6.78384948912121
Training loss: 0.3791443407535553 / Valid loss: 6.854760578700474
Training loss: 0.3959607183933258 / Valid loss: 6.874238688605172
Training loss: 0.5073026418685913 / Valid loss: 6.915350973038446

Epoch: 34
Training loss: 0.84784996509552 / Valid loss: 6.810558641524542
Training loss: 0.3780098557472229 / Valid loss: 6.870923362459455
Training loss: 0.5260709524154663 / Valid loss: 6.858384286789667
Training loss: 0.3105500042438507 / Valid loss: 6.847722543988909
Training loss: 0.37165871262550354 / Valid loss: 6.790072752180554

Epoch: 35
Training loss: 0.8120191097259521 / Valid loss: 6.947482231685093
Training loss: 0.40832674503326416 / Valid loss: 6.791333491461618
Training loss: 0.2575708031654358 / Valid loss: 6.839314024788993
Training loss: 0.32510441541671753 / Valid loss: 6.917860930306571
Training loss: 0.3456174433231354 / Valid loss: 6.814979321616036

Epoch: 36
Training loss: 0.36534979939460754 / Valid loss: 6.851553079060146
Training loss: 0.7617155909538269 / Valid loss: 6.811295768192836
Training loss: 0.31516551971435547 / Valid loss: 6.866854703994024
Training loss: 0.33060356974601746 / Valid loss: 6.8456908725556875
Training loss: 0.41588684916496277 / Valid loss: 6.8495058332170755

Epoch: 37
Training loss: 0.42353859543800354 / Valid loss: 6.873068968454997
Training loss: 0.3280651867389679 / Valid loss: 6.7587628818693615
Training loss: 0.4312743842601776 / Valid loss: 6.870133681524368
Training loss: 0.7213282585144043 / Valid loss: 6.8087525526682535
Training loss: 0.48998257517814636 / Valid loss: 6.8538070451645625

Epoch: 38
Training loss: 0.3917848765850067 / Valid loss: 6.765246945335751
Training loss: 0.357817679643631 / Valid loss: 6.76726332846142
Training loss: 0.41595977544784546 / Valid loss: 6.762485940115792
Training loss: 0.3384285271167755 / Valid loss: 6.8026490415845595
Training loss: 0.32240891456604004 / Valid loss: 6.9024126370747885

Epoch: 39
Training loss: 0.5113993287086487 / Valid loss: 6.83139526049296
Training loss: 0.3025866746902466 / Valid loss: 6.79451983315604
Training loss: 0.3561829924583435 / Valid loss: 6.755090431939988
Training loss: 0.45146700739860535 / Valid loss: 6.957076218014672

Epoch: 40
Training loss: 0.36804628372192383 / Valid loss: 6.874037115914481
Training loss: 0.38985177874565125 / Valid loss: 6.743153272356306
Training loss: 0.26375043392181396 / Valid loss: 6.882572759900774
Training loss: 0.31998610496520996 / Valid loss: 6.782429118383498
Training loss: 0.3033495545387268 / Valid loss: 6.772621184303647

Epoch: 41
Training loss: 0.2733774185180664 / Valid loss: 6.866429837544759
Training loss: 0.25624948740005493 / Valid loss: 6.7811607837677
Training loss: 0.2962963581085205 / Valid loss: 6.848663548060826
Training loss: 1.2327595949172974 / Valid loss: 6.76028528213501
Training loss: 0.4783816933631897 / Valid loss: 6.814179098038446

Epoch: 42
Training loss: 0.39118099212646484 / Valid loss: 6.759415899004255
Training loss: 0.3444925844669342 / Valid loss: 6.80299927847726
Training loss: 0.3744836449623108 / Valid loss: 6.756099392118908
Training loss: 0.32826337218284607 / Valid loss: 6.747801853361584
Training loss: 0.37627869844436646 / Valid loss: 6.76649592263358

Epoch: 43
Training loss: 0.3811500370502472 / Valid loss: 6.794272372836159
Training loss: 0.32206419110298157 / Valid loss: 6.8181998207455585
Training loss: 0.2588002681732178 / Valid loss: 6.7701265902746295
Training loss: 0.5416268110275269 / Valid loss: 6.752507813771566
Training loss: 0.29299452900886536 / Valid loss: 6.75570479120527

Epoch: 44
Training loss: 0.23972025513648987 / Valid loss: 6.765600036439442
Training loss: 0.3293181359767914 / Valid loss: 6.805912362961542
Training loss: 0.3223400413990021 / Valid loss: 6.786621938432966
Training loss: 0.3097377419471741 / Valid loss: 6.794602176121303
Training loss: 0.43824344873428345 / Valid loss: 6.807029651460193

Epoch: 45
Training loss: 0.2801916003227234 / Valid loss: 6.743092972891671
Training loss: 0.4561864137649536 / Valid loss: 6.796846952892485
Training loss: 0.522200882434845 / Valid loss: 6.771978337424142
Training loss: 0.3911826014518738 / Valid loss: 6.707783487864903
Training loss: 0.38306760787963867 / Valid loss: 6.825072869800386

Epoch: 46
Training loss: 0.30472809076309204 / Valid loss: 6.757801737104144
Training loss: 0.32752498984336853 / Valid loss: 6.805779695510864
Training loss: 0.4997628927230835 / Valid loss: 6.775128409976051
Training loss: 0.3409487307071686 / Valid loss: 6.736865511394682
Training loss: 0.6677675247192383 / Valid loss: 6.694364036832537

Epoch: 47
Training loss: 0.19558504223823547 / Valid loss: 6.702166289374942
Training loss: 0.3180335760116577 / Valid loss: 6.729248537336077
Training loss: 0.48342302441596985 / Valid loss: 6.785060024261474
Training loss: 0.3862532377243042 / Valid loss: 6.806991767883301
Training loss: 0.2732636332511902 / Valid loss: 6.76103454771496

Epoch: 48
Training loss: 0.24597975611686707 / Valid loss: 6.728623362949916
Training loss: 0.4236012101173401 / Valid loss: 6.843870162963867
Training loss: 0.37488868832588196 / Valid loss: 6.704499503544398
Training loss: 0.2817808985710144 / Valid loss: 6.730080336616153
Training loss: 0.44567030668258667 / Valid loss: 6.786397407168434

Epoch: 49
Training loss: 0.4393573999404907 / Valid loss: 6.731303719111851
Training loss: 0.25647956132888794 / Valid loss: 6.797152814410982
Training loss: 0.20093435049057007 / Valid loss: 6.73744230497451
Training loss: 0.2900494635105133 / Valid loss: 6.7258472170148575

Epoch: 50
Training loss: 0.46962541341781616 / Valid loss: 6.699434284936814
Training loss: 0.3220330476760864 / Valid loss: 6.674120330810547
Training loss: 0.29386743903160095 / Valid loss: 6.711546103159587
Training loss: 0.29852235317230225 / Valid loss: 6.708015450977144
Training loss: 0.5148974657058716 / Valid loss: 6.816843416577294

Epoch: 51
Training loss: 0.23548740148544312 / Valid loss: 6.736036421003796
Training loss: 0.4178692698478699 / Valid loss: 6.767370832534064
Training loss: 0.28324565291404724 / Valid loss: 6.730896827152797
Training loss: 0.25252464413642883 / Valid loss: 6.707066404251825
Training loss: 0.35402363538742065 / Valid loss: 6.711458492279053

Epoch: 52
Training loss: 0.25875067710876465 / Valid loss: 6.724168273380824
Training loss: 0.2623487710952759 / Valid loss: 6.756116574151175
Training loss: 0.2128627449274063 / Valid loss: 6.717803519112723
Training loss: 0.1926773190498352 / Valid loss: 6.704392324175154
Training loss: 0.36973828077316284 / Valid loss: 6.713899035680861

Epoch: 53
Training loss: 0.24603763222694397 / Valid loss: 6.763773277827672
Training loss: 0.4876273274421692 / Valid loss: 6.707433228265671
Training loss: 0.2852274179458618 / Valid loss: 6.747799033210391
Training loss: 0.6705142855644226 / Valid loss: 6.7391944567362465
Training loss: 0.4465499520301819 / Valid loss: 6.76401474362328

Epoch: 54
Training loss: 0.5303177833557129 / Valid loss: 6.687629495348249
Training loss: 0.3286805748939514 / Valid loss: 6.794836509795416
Training loss: 0.3106897175312042 / Valid loss: 6.75164741334461
Training loss: 0.2318662703037262 / Valid loss: 6.7098256270090735
Training loss: 0.3557831346988678 / Valid loss: 6.758794512067523

Epoch: 55
Training loss: 0.5168576240539551 / Valid loss: 6.705766225996472
Training loss: 0.4875376224517822 / Valid loss: 6.759159515017555
Training loss: 0.5993860960006714 / Valid loss: 6.781763281141009
Training loss: 0.27772843837738037 / Valid loss: 6.810182339804513
Training loss: 0.3392465114593506 / Valid loss: 6.739926226933798

Epoch: 56
Training loss: 0.6266573071479797 / Valid loss: 6.710631915501186
Training loss: 0.1744554191827774 / Valid loss: 6.765591453370594
Training loss: 0.3134199380874634 / Valid loss: 6.69522210984003
Training loss: 0.31472039222717285 / Valid loss: 6.670461872645787
Training loss: 0.5152013897895813 / Valid loss: 6.690105004537673

Epoch: 57
Training loss: 0.7524474859237671 / Valid loss: 6.766784797395979
Training loss: 0.3706541359424591 / Valid loss: 6.710080396561396
Training loss: 0.35724395513534546 / Valid loss: 6.749285752432687
Training loss: 0.6074902415275574 / Valid loss: 6.689384891873314
Training loss: 0.44736865162849426 / Valid loss: 6.72520558493478

Epoch: 58
Training loss: 0.3819587826728821 / Valid loss: 6.689128784906297
Training loss: 0.4916685223579407 / Valid loss: 6.735916532788958
Training loss: 0.3611557185649872 / Valid loss: 6.711106672741118
Training loss: 0.534123420715332 / Valid loss: 6.67636771656218
Training loss: 0.22396443784236908 / Valid loss: 6.724246406555176

Epoch: 59
Training loss: 0.2273387461900711 / Valid loss: 6.680069655463809
Training loss: 0.42568856477737427 / Valid loss: 6.669782334282285
Training loss: 0.3666840195655823 / Valid loss: 6.721442597252982
Training loss: 0.4067525565624237 / Valid loss: 6.671936652773902

Epoch: 60
Training loss: 0.34881871938705444 / Valid loss: 6.710927477337065
Training loss: 0.6154779195785522 / Valid loss: 6.726460093543643
Training loss: 0.23126104474067688 / Valid loss: 6.669532276335216
Training loss: 0.3360059857368469 / Valid loss: 6.721975612640381
Training loss: 0.39391452074050903 / Valid loss: 6.6899144717625205

Epoch: 61
Training loss: 0.44558197259902954 / Valid loss: 6.7684749989282516
Training loss: 0.20760712027549744 / Valid loss: 6.768717820303781
Training loss: 0.550693929195404 / Valid loss: 6.6894766716730025
Training loss: 0.41413360834121704 / Valid loss: 6.741762708482288
Training loss: 0.3054914176464081 / Valid loss: 6.715096432822091

Epoch: 62
Training loss: 0.15472835302352905 / Valid loss: 6.666775571732294
Training loss: 0.27374690771102905 / Valid loss: 6.676443340664818
Training loss: 0.1859350949525833 / Valid loss: 6.6890191532316665
Training loss: 0.36541715264320374 / Valid loss: 6.7309935161045615
Training loss: 0.4565538763999939 / Valid loss: 6.716866742996943

Epoch: 63
Training loss: 0.3241569399833679 / Valid loss: 6.617739561625889
Training loss: 0.2074940949678421 / Valid loss: 6.634667342049735
Training loss: 0.2874569594860077 / Valid loss: 6.625707553681873
Training loss: 0.4158697724342346 / Valid loss: 6.709875547318232
Training loss: 0.2488686442375183 / Valid loss: 6.7120491209484285

Epoch: 64
Training loss: 0.20841088891029358 / Valid loss: 6.670325551714216
Training loss: 0.17572864890098572 / Valid loss: 6.69535178002857
Training loss: 0.5030312538146973 / Valid loss: 6.683894500278291
Training loss: 0.2802730202674866 / Valid loss: 6.668777388618106
Training loss: 0.29570499062538147 / Valid loss: 6.65590085756211

Epoch: 65
Training loss: 0.21561826765537262 / Valid loss: 6.657608425049554
Training loss: 0.4194198250770569 / Valid loss: 6.685143191473824
Training loss: 0.19582059979438782 / Valid loss: 6.695774296351842
Training loss: 0.26846858859062195 / Valid loss: 6.663293915703183
Training loss: 1.200953483581543 / Valid loss: 6.636239810216995

Epoch: 66
Training loss: 0.20386239886283875 / Valid loss: 6.635817682175409
Training loss: 0.2242625504732132 / Valid loss: 6.6748885200137185
Training loss: 0.6934322118759155 / Valid loss: 6.612638042086647
Training loss: 0.2251012921333313 / Valid loss: 6.662507272901989
Training loss: 0.48607316613197327 / Valid loss: 6.653824186325073

Epoch: 67
Training loss: 0.28059470653533936 / Valid loss: 6.696042982737223
Training loss: 0.1762937605381012 / Valid loss: 6.748717868895758
Training loss: 0.23315705358982086 / Valid loss: 6.679172847384498
Training loss: 0.9508079886436462 / Valid loss: 6.686927713666644
Training loss: 0.2847082018852234 / Valid loss: 6.6408798830849785

Epoch: 68
Training loss: 0.2023329734802246 / Valid loss: 6.638641952332996
Training loss: 0.32127800583839417 / Valid loss: 6.684152616773333
Training loss: 0.22909942269325256 / Valid loss: 6.686097304026286
Training loss: 0.25614750385284424 / Valid loss: 6.698540562675113
Training loss: 0.2671150267124176 / Valid loss: 6.679765060969761

Epoch: 69
Training loss: 0.41309303045272827 / Valid loss: 6.706050623030889
Training loss: 0.2782416343688965 / Valid loss: 6.664101877666655
Training loss: 0.3861317038536072 / Valid loss: 6.688184724535261
Training loss: 0.31186074018478394 / Valid loss: 6.675781572432745

Epoch: 70
Training loss: 0.2071351408958435 / Valid loss: 6.688096720831735
Training loss: 0.5145910978317261 / Valid loss: 6.686439979644049
Training loss: 0.35519811511039734 / Valid loss: 6.624032397497268
Training loss: 0.3331165909767151 / Valid loss: 6.6752090204329715
Training loss: 0.4087202548980713 / Valid loss: 6.661813890366327

Epoch: 71
Training loss: 0.3515656292438507 / Valid loss: 6.696151760646275
Training loss: 0.5540785789489746 / Valid loss: 6.669122055598668
Training loss: 0.2891699969768524 / Valid loss: 6.689207860401699
Training loss: 0.23078607022762299 / Valid loss: 6.65930077234904
Training loss: 0.2650478482246399 / Valid loss: 6.637486718949818

Epoch: 72
Training loss: 0.22568164765834808 / Valid loss: 6.663834410622006
Training loss: 0.5306540727615356 / Valid loss: 6.627140587852114
Training loss: 0.2193690687417984 / Valid loss: 6.640688666843233
Training loss: 0.34747231006622314 / Valid loss: 6.651382991245815
Training loss: 0.5703251361846924 / Valid loss: 6.67217553229559

Epoch: 73
Training loss: 0.44613921642303467 / Valid loss: 6.643679614294143
Training loss: 0.42118895053863525 / Valid loss: 6.644640227726527
Training loss: 0.2059280276298523 / Valid loss: 6.678271491186959
Training loss: 0.2047095000743866 / Valid loss: 6.634040482838949
Training loss: 0.26169097423553467 / Valid loss: 6.679233503341675

Epoch: 74
Training loss: 0.21150918304920197 / Valid loss: 6.645373253595261
Training loss: 0.2562561631202698 / Valid loss: 6.603808500653222
Training loss: 0.4858044683933258 / Valid loss: 6.637781792595273
Training loss: 0.43979257345199585 / Valid loss: 6.656432694480532
Training loss: 0.2181214690208435 / Valid loss: 6.632199003582909

Epoch: 75
Training loss: 0.27400001883506775 / Valid loss: 6.663445495423817
Training loss: 0.3727610111236572 / Valid loss: 6.663954471406482
Training loss: 0.5653051733970642 / Valid loss: 6.638429282960438
Training loss: 0.20947265625 / Valid loss: 6.633637993676322
Training loss: 0.40302059054374695 / Valid loss: 6.609807087126232

Epoch: 76
Training loss: 0.319435179233551 / Valid loss: 6.619022117342268
Training loss: 0.468672513961792 / Valid loss: 6.669357222602481
Training loss: 0.17770695686340332 / Valid loss: 6.673665396372477
Training loss: 0.33879488706588745 / Valid loss: 6.613214574541364
Training loss: 0.27779558300971985 / Valid loss: 6.71521782875061

Epoch: 77
Training loss: 0.2996503710746765 / Valid loss: 6.683221017746698
Training loss: 0.35909560322761536 / Valid loss: 6.705450166974749
Training loss: 0.3195039629936218 / Valid loss: 6.642677543276832
Training loss: 0.41818860173225403 / Valid loss: 6.663578564780099
Training loss: 0.44790446758270264 / Valid loss: 6.616634650457473

Epoch: 78
Training loss: 0.36747539043426514 / Valid loss: 6.666807201930455
Training loss: 0.18169009685516357 / Valid loss: 6.6603870119367325
Training loss: 0.15213361382484436 / Valid loss: 6.6445341973077685
Training loss: 0.3188200891017914 / Valid loss: 6.673798642839704
Training loss: 0.49758583307266235 / Valid loss: 6.664251005081903

Epoch: 79
Training loss: 0.2632230520248413 / Valid loss: 6.633800034295945
Training loss: 0.34705644845962524 / Valid loss: 6.6278168405805316
Training loss: 0.1976604163646698 / Valid loss: 6.662951056162516
Training loss: 0.19321796298027039 / Valid loss: 6.697877266293481
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model: 5.645615534555344
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.001
nr_epochs : 80
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0
momentum : 0
embedder : Bert
verbose : False
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
neural net:
 [tensor([[ 0.0035, -0.0035,  0.0052,  ..., -0.0130,  0.0056, -0.0088],
        [ 0.0045, -0.0089, -0.0107,  ...,  0.0052,  0.0048, -0.0076],
        [ 0.0014, -0.0062,  0.0084,  ..., -0.0093,  0.0089,  0.0006],
        ...,
        [ 0.0115,  0.0001, -0.0032,  ..., -0.0018, -0.0062, -0.0085],
        [-0.0009,  0.0057,  0.0107,  ..., -0.0067, -0.0022, -0.0107],
        [ 0.0109, -0.0116, -0.0054,  ...,  0.0007, -0.0127,  0.0088]],
       device='cuda:0'), tensor([ 0.0047,  0.0122,  0.0094,  ...,  0.0037, -0.0029, -0.0056],
       device='cuda:0'), tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), tensor([[-0.0121, -0.0085,  0.0158,  ...,  0.0067, -0.0057, -0.0189],
        [ 0.0064, -0.0150,  0.0096,  ...,  0.0006,  0.0074,  0.0177],
        [ 0.0061, -0.0078,  0.0059,  ...,  0.0032, -0.0206,  0.0205],
        ...,
        [ 0.0173, -0.0032,  0.0090,  ..., -0.0031,  0.0141,  0.0130],
        [-0.0181, -0.0101,  0.0157,  ..., -0.0020, -0.0070,  0.0160],
        [ 0.0063, -0.0152, -0.0101,  ...,  0.0008,  0.0028, -0.0011]],
       device='cuda:0'), tensor([-1.1967e-02,  1.1110e-02,  1.8701e-02,  1.1023e-02, -2.0298e-02,
         5.6226e-05, -1.3542e-02,  2.7363e-03, -8.2064e-03,  1.8652e-02,
         1.4701e-02,  1.2389e-02,  2.1926e-02,  1.5760e-02, -3.8398e-03,
        -2.1899e-02,  2.2099e-02, -5.7002e-03,  2.0446e-03,  2.1733e-02,
         1.5114e-02,  2.0789e-03, -1.0899e-02, -5.6587e-03,  2.0192e-02,
         4.3223e-03,  8.8637e-03, -1.6798e-02,  1.7955e-02,  1.2753e-02,
        -5.4965e-03,  1.9481e-02,  6.9524e-03, -2.0116e-02, -5.9529e-03,
        -1.1193e-02,  1.8363e-02,  1.6020e-02, -1.6430e-02,  6.4647e-03,
         1.5684e-02,  9.1588e-03, -2.0390e-02,  9.9621e-03,  2.0163e-02,
         2.1748e-02, -1.8861e-02, -5.0238e-03,  1.9386e-02,  1.1112e-03,
        -1.3363e-02,  6.7029e-03,  1.7814e-02, -8.2254e-03,  1.3094e-02,
         1.2228e-02, -1.2875e-03,  3.9142e-03,  1.4362e-03,  5.9424e-03,
         2.2414e-03, -1.3438e-02, -7.4102e-03,  1.5021e-02, -1.8291e-02,
         1.7939e-02,  2.4626e-03, -1.6420e-03,  1.0483e-03, -6.6787e-03,
         1.9978e-02,  6.3201e-04,  1.7342e-02,  9.4639e-03,  2.2343e-02,
        -1.5528e-02,  1.0803e-02, -4.7853e-03,  1.3566e-02, -1.2095e-02,
        -5.4629e-03,  7.9233e-03, -1.5708e-02, -2.7313e-03,  8.8111e-03,
        -2.1376e-03,  2.0746e-02,  1.0951e-02, -1.4991e-02,  5.5452e-03,
        -1.4782e-02, -1.6080e-02, -5.0667e-03, -2.1050e-02, -7.7526e-03,
         1.9154e-02, -2.1401e-02, -1.0501e-02,  7.3819e-03,  2.0670e-03],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), tensor([[-0.0636, -0.0402,  0.0180,  ..., -0.0669, -0.0184, -0.0632],
        [-0.0702, -0.0480,  0.0203,  ...,  0.0432, -0.0736,  0.0113],
        [-0.0735, -0.0482,  0.0561,  ..., -0.0377,  0.0717,  0.0079],
        ...,
        [ 0.0336,  0.0795,  0.0686,  ...,  0.0617,  0.0593,  0.0145],
        [-0.0549, -0.0686,  0.0407,  ..., -0.0574,  0.0591, -0.0667],
        [-0.0106, -0.0514, -0.0024,  ...,  0.0682,  0.0980,  0.0754]],
       device='cuda:0'), tensor([-0.0598,  0.0784,  0.0041,  0.0437, -0.0314,  0.0230, -0.0737, -0.0741,
        -0.0710, -0.0826, -0.0965, -0.0397,  0.0959,  0.0370,  0.0845,  0.0369],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0'), tensor([[-0.0476,  0.1976,  0.0806, -0.1341,  0.1659,  0.0086, -0.0477,  0.0708,
          0.2368,  0.0414, -0.1084, -0.0428,  0.1062, -0.1102,  0.0168,  0.1324]],
       device='cuda:0'), tensor([0.1504], device='cuda:0')]

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.266402771359399
Model is saved in epoch 0, overall batch: 0
Training loss: 5.042430877685547 / Valid loss: 7.500886340368361
Model is saved in epoch 0, overall batch: 100
Training loss: 4.23737907409668 / Valid loss: 5.921776885078067
Model is saved in epoch 0, overall batch: 200
Training loss: 6.29364538192749 / Valid loss: 5.682949795041766
Model is saved in epoch 0, overall batch: 300
Training loss: 5.842200756072998 / Valid loss: 5.548471437181745
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 4.756719589233398 / Valid loss: 5.553807235899425
Training loss: 4.298105239868164 / Valid loss: 5.569606599353609
Training loss: 5.326928615570068 / Valid loss: 5.642630109332857
Training loss: 5.017754554748535 / Valid loss: 5.6339343093690415
Training loss: 3.9678406715393066 / Valid loss: 5.5637674763089136

Epoch: 2
Training loss: 4.209566593170166 / Valid loss: 5.612356049673898
Training loss: 4.178608417510986 / Valid loss: 5.750446242377872
Training loss: 3.3077023029327393 / Valid loss: 5.74691725685483
Training loss: 4.88202428817749 / Valid loss: 5.984495680672782
Training loss: 4.264095306396484 / Valid loss: 5.786848156792777

Epoch: 3
Training loss: 2.5928397178649902 / Valid loss: 5.875179701759702
Training loss: 2.498826503753662 / Valid loss: 6.106087437130156
Training loss: 4.6850714683532715 / Valid loss: 5.97977549235026
Training loss: 2.186169385910034 / Valid loss: 6.105248587472098
Training loss: 3.4255576133728027 / Valid loss: 6.111393374488467

Epoch: 4
Training loss: 2.261228322982788 / Valid loss: 6.212632283710298
Training loss: 2.7856407165527344 / Valid loss: 6.367695467812674
Training loss: 2.8565831184387207 / Valid loss: 6.320631045386905
Training loss: 2.643772602081299 / Valid loss: 6.169245856148856
Training loss: 2.3476099967956543 / Valid loss: 6.288671822774978

Epoch: 5
Training loss: 2.6007585525512695 / Valid loss: 6.993596685500372
Training loss: 1.937721848487854 / Valid loss: 6.489501063028971
Training loss: 2.0332107543945312 / Valid loss: 6.682902040935698
Training loss: 2.2377610206604004 / Valid loss: 6.4714922450837635
Training loss: 2.2933297157287598 / Valid loss: 6.610230096181234

Epoch: 6
Training loss: 2.3704824447631836 / Valid loss: 6.5890911692664735
Training loss: 1.2237951755523682 / Valid loss: 6.530026122501918
Training loss: 2.6668028831481934 / Valid loss: 6.618755563100179
Training loss: 1.8128156661987305 / Valid loss: 6.598240836461385
Training loss: 1.754177212715149 / Valid loss: 6.67875330334618

Epoch: 7
Training loss: 1.6314806938171387 / Valid loss: 6.549886939639137
Training loss: 1.7161669731140137 / Valid loss: 6.626883702051072
Training loss: 1.825516939163208 / Valid loss: 6.685345942633493
Training loss: 1.6144566535949707 / Valid loss: 6.730530461810884
Training loss: 1.254022240638733 / Valid loss: 6.626742994217645

Epoch: 8
Training loss: 0.8628376722335815 / Valid loss: 6.59803987003508
Training loss: 0.840111494064331 / Valid loss: 6.550634683881487
Training loss: 1.0600968599319458 / Valid loss: 6.688631010055542
Training loss: 1.804614543914795 / Valid loss: 6.621865404219855
Training loss: 1.3723456859588623 / Valid loss: 6.63860752923148

Epoch: 9
Training loss: 0.6908299922943115 / Valid loss: 6.583039256504604
Training loss: 0.7573602795600891 / Valid loss: 6.690453978947231
Training loss: 1.4202158451080322 / Valid loss: 6.676934541974749
Training loss: 1.119789958000183 / Valid loss: 6.6478013401939755

Epoch: 10
Training loss: 0.9327704906463623 / Valid loss: 6.61181899252392
Training loss: 0.7600219249725342 / Valid loss: 6.525962102980841
Training loss: 0.5637192726135254 / Valid loss: 6.57061973299299
Training loss: 1.3438125848770142 / Valid loss: 6.6073793547494075
Training loss: 0.7735837697982788 / Valid loss: 6.65690032641093

Epoch: 11
Training loss: 0.5911161303520203 / Valid loss: 6.639800630296979
Training loss: 0.9196419715881348 / Valid loss: 6.714557536443075
Training loss: 0.7024827003479004 / Valid loss: 6.790416422344389
Training loss: 0.6993299722671509 / Valid loss: 6.754666791643415
Training loss: 0.7075324058532715 / Valid loss: 6.7032001586187455

Epoch: 12
Training loss: 0.39375412464141846 / Valid loss: 6.579132511502221
Training loss: 0.5301622748374939 / Valid loss: 6.604428423018683
Training loss: 0.6285430192947388 / Valid loss: 6.513908277239119
Training loss: 0.6034932732582092 / Valid loss: 6.645499206724621
Training loss: 0.6196821331977844 / Valid loss: 6.61227723757426

Epoch: 13
Training loss: 0.37680479884147644 / Valid loss: 6.681903859547206
Training loss: 0.526980996131897 / Valid loss: 6.640049271356492
Training loss: 0.4539257884025574 / Valid loss: 6.595271096910749
Training loss: 0.5621163845062256 / Valid loss: 6.582649800890968
Training loss: 0.4460565745830536 / Valid loss: 6.5734669299352735

Epoch: 14
Training loss: 0.5914333462715149 / Valid loss: 6.621801598866781
Training loss: 0.49070507287979126 / Valid loss: 6.560744562603179
Training loss: 0.5259989500045776 / Valid loss: 6.555673088346209
Training loss: 0.25948449969291687 / Valid loss: 6.586678527650379
Training loss: 0.6754760146141052 / Valid loss: 6.649354369299752

Epoch: 15
Training loss: 0.46882298588752747 / Valid loss: 6.6811547869727725
Training loss: 0.6348530054092407 / Valid loss: 6.670148454393659
Training loss: 0.46523556113243103 / Valid loss: 6.6584804807390485
Training loss: 0.45887142419815063 / Valid loss: 6.60891052427746
Training loss: 0.48496049642562866 / Valid loss: 6.634208256857736

Epoch: 16
Training loss: 0.3049743175506592 / Valid loss: 6.565555495307559
Training loss: 0.45435047149658203 / Valid loss: 6.612872582390195
Training loss: 0.36655542254447937 / Valid loss: 6.5486997104826425
Training loss: 0.3891465663909912 / Valid loss: 6.6117689904712496
Training loss: 0.39975547790527344 / Valid loss: 6.563732653572446

Epoch: 17
Training loss: 0.7990560531616211 / Valid loss: 6.569073933646792
Training loss: 0.7692824602127075 / Valid loss: 6.557745831353324
Training loss: 0.6669824123382568 / Valid loss: 6.614656439281645
Training loss: 0.27177250385284424 / Valid loss: 6.601601418994722
Training loss: 0.3779035210609436 / Valid loss: 6.620827372868856

Epoch: 18
Training loss: 0.4896887242794037 / Valid loss: 6.604000136965797
Training loss: 0.42007529735565186 / Valid loss: 6.5563198248545325
Training loss: 0.420172780752182 / Valid loss: 6.566674936385382
Training loss: 0.45496243238449097 / Valid loss: 6.598615662256877
Training loss: 0.6350970268249512 / Valid loss: 6.5631274904523575

Epoch: 19
Training loss: 0.5448774099349976 / Valid loss: 6.654344436100551
Training loss: 0.8887042999267578 / Valid loss: 6.577063755762009
Training loss: 0.4142709970474243 / Valid loss: 6.5773750532241095
Training loss: 0.2850325107574463 / Valid loss: 6.547097835086641

Epoch: 20
Training loss: 0.32555830478668213 / Valid loss: 6.4868345578511555
Training loss: 0.6206218004226685 / Valid loss: 6.558145187014625
Training loss: 0.21500426530838013 / Valid loss: 6.539990411485944
Training loss: 0.3254860043525696 / Valid loss: 6.58965566725958
Training loss: 0.7169045209884644 / Valid loss: 6.543543238866897

Epoch: 21
Training loss: 0.3072414696216583 / Valid loss: 6.5334583850133985
Training loss: 0.4154866337776184 / Valid loss: 6.569000938960484
Training loss: 0.32305556535720825 / Valid loss: 6.564593673887707
Training loss: 0.27794021368026733 / Valid loss: 6.5852838834126795
Training loss: 0.3463420569896698 / Valid loss: 6.5662931692032585

Epoch: 22
Training loss: 0.3098863363265991 / Valid loss: 6.557641692388625
Training loss: 0.253909707069397 / Valid loss: 6.467937930425008
Training loss: 0.38885200023651123 / Valid loss: 6.539727826345534
Training loss: 1.0445916652679443 / Valid loss: 6.594146610441662
Training loss: 0.4033229351043701 / Valid loss: 6.584196276891799

Epoch: 23
Training loss: 0.24921837449073792 / Valid loss: 6.556321732203165
Training loss: 0.7468335032463074 / Valid loss: 6.482907140822638
Training loss: 0.2993065118789673 / Valid loss: 6.507833896364485
Training loss: 0.20183780789375305 / Valid loss: 6.515956374577113
Training loss: 0.436636745929718 / Valid loss: 6.586847171329317

Epoch: 24
Training loss: 0.558465301990509 / Valid loss: 6.580224627540225
Training loss: 0.396455854177475 / Valid loss: 6.5163773445856
Training loss: 0.25740212202072144 / Valid loss: 6.585533464522589
Training loss: 0.5150564908981323 / Valid loss: 6.575876276833671
Training loss: 0.8670803904533386 / Valid loss: 6.589196180161975

Epoch: 25
Training loss: 0.6435341238975525 / Valid loss: 6.579582589013236
Training loss: 0.29623180627822876 / Valid loss: 6.517912360600063
Training loss: 0.41823065280914307 / Valid loss: 6.528584094274612
Training loss: 0.2676159739494324 / Valid loss: 6.514958645048596
Training loss: 0.24382954835891724 / Valid loss: 6.54841746148609

Epoch: 26
Training loss: 0.3725477457046509 / Valid loss: 6.54251918338594
Training loss: 0.504570484161377 / Valid loss: 6.555820183526902
Training loss: 0.20895236730575562 / Valid loss: 6.497135160082863
Training loss: 0.2649601101875305 / Valid loss: 6.582287747519357
Training loss: 0.3682362735271454 / Valid loss: 6.565891969771612

Epoch: 27
Training loss: 0.26271557807922363 / Valid loss: 6.516990770612444
Training loss: 0.39834481477737427 / Valid loss: 6.5540310064951575
Training loss: 0.39193862676620483 / Valid loss: 6.584957041059222
Training loss: 0.36901986598968506 / Valid loss: 6.53929474467323
Training loss: 0.3372167944908142 / Valid loss: 6.527570056915283

Epoch: 28
Training loss: 0.2596457004547119 / Valid loss: 6.523834460122245
Training loss: 0.4018998146057129 / Valid loss: 6.519163869676136
Training loss: 0.5498902201652527 / Valid loss: 6.553589929853167
Training loss: 0.3178008794784546 / Valid loss: 6.489233925229027
Training loss: 0.29510974884033203 / Valid loss: 6.527069623129709

Epoch: 29
Training loss: 0.3009474575519562 / Valid loss: 6.478154214223226
Training loss: 0.2926444709300995 / Valid loss: 6.515493987855457
Training loss: 0.12174738943576813 / Valid loss: 6.493541413261777
Training loss: 0.20460005104541779 / Valid loss: 6.508174405779157

Epoch: 30
Training loss: 0.22724877297878265 / Valid loss: 6.498072901226226
Training loss: 0.2598232626914978 / Valid loss: 6.4871627081008185
Training loss: 0.25000786781311035 / Valid loss: 6.531401722771781
Training loss: 0.315096914768219 / Valid loss: 6.53367938768296
Training loss: 0.30825871229171753 / Valid loss: 6.511435715357463

Epoch: 31
Training loss: 0.2798534631729126 / Valid loss: 6.433579987571353
Training loss: 0.263679563999176 / Valid loss: 6.473984209696452
Training loss: 0.24449357390403748 / Valid loss: 6.47363756497701
Training loss: 0.282825767993927 / Valid loss: 6.511305145990281
Training loss: 0.18286991119384766 / Valid loss: 6.539030633653913

Epoch: 32
Training loss: 0.3272579312324524 / Valid loss: 6.530714384714762
Training loss: 0.31074586510658264 / Valid loss: 6.512446162814186
Training loss: 0.20961004495620728 / Valid loss: 6.572762714113508
Training loss: 0.1993136703968048 / Valid loss: 6.476679697490874
Training loss: 0.32529497146606445 / Valid loss: 6.516221082778204

Epoch: 33
Training loss: 0.2471737265586853 / Valid loss: 6.4616634573255265
Training loss: 0.3477966785430908 / Valid loss: 6.5341354188464935
Training loss: 0.20642465353012085 / Valid loss: 6.461787173861549
Training loss: 0.1547841727733612 / Valid loss: 6.487263852074033
Training loss: 0.28536760807037354 / Valid loss: 6.497025898524693

Epoch: 34
Training loss: 0.5804500579833984 / Valid loss: 6.44485284941537
Training loss: 0.25263845920562744 / Valid loss: 6.464764149983724
Training loss: 0.22393028438091278 / Valid loss: 6.53330568586077
Training loss: 0.21801960468292236 / Valid loss: 6.5301706541152225
Training loss: 0.19783607125282288 / Valid loss: 6.4900689147767565

Epoch: 35
Training loss: 0.7059658765792847 / Valid loss: 6.470491100492931
Training loss: 0.2884620428085327 / Valid loss: 6.443025284721738
Training loss: 0.36435389518737793 / Valid loss: 6.5528745923723495
Training loss: 0.16806453466415405 / Valid loss: 6.506603879020328
Training loss: 0.19943425059318542 / Valid loss: 6.555985153289068

Epoch: 36
Training loss: 0.24202662706375122 / Valid loss: 6.500633044469924
Training loss: 0.4951649308204651 / Valid loss: 6.4992023468017575
Training loss: 0.16354691982269287 / Valid loss: 6.4916447730291456
Training loss: 0.21823997795581818 / Valid loss: 6.5242975779942105
Training loss: 0.17191584408283234 / Valid loss: 6.5625038237798785

Epoch: 37
Training loss: 0.33796337246894836 / Valid loss: 6.470151140576317
Training loss: 0.18709143996238708 / Valid loss: 6.49595235869998
Training loss: 0.38278505206108093 / Valid loss: 6.496720563797724
Training loss: 0.6351658701896667 / Valid loss: 6.4828751791091195
Training loss: 0.36936014890670776 / Valid loss: 6.478620590482439

Epoch: 38
Training loss: 0.2900775074958801 / Valid loss: 6.45573145094372
Training loss: 0.230264350771904 / Valid loss: 6.47436508224124
Training loss: 0.24393035471439362 / Valid loss: 6.47273131779262
Training loss: 0.15568336844444275 / Valid loss: 6.479994260697138
Training loss: 0.24576780200004578 / Valid loss: 6.552403949555897

Epoch: 39
Training loss: 0.251237154006958 / Valid loss: 6.4952503749302455
Training loss: 0.17403073608875275 / Valid loss: 6.496827915736607
Training loss: 0.2253260612487793 / Valid loss: 6.455013774690174
Training loss: 0.20019690692424774 / Valid loss: 6.4839876243046355

Epoch: 40
Training loss: 0.16414153575897217 / Valid loss: 6.460242518924532
Training loss: 0.2303619533777237 / Valid loss: 6.440657617932274
Training loss: 0.20619961619377136 / Valid loss: 6.480282454263596
Training loss: 0.16448424756526947 / Valid loss: 6.504538027445475
Training loss: 0.1437200903892517 / Valid loss: 6.515617847442627

Epoch: 41
Training loss: 0.24530014395713806 / Valid loss: 6.4566205092838835
Training loss: 0.18025606870651245 / Valid loss: 6.438510840279715
Training loss: 0.21488246321678162 / Valid loss: 6.477592291150774
Training loss: 1.118802547454834 / Valid loss: 6.51607027053833
Training loss: 0.36013415455818176 / Valid loss: 6.461066686539423

Epoch: 42
Training loss: 0.1628488004207611 / Valid loss: 6.482588704427084
Training loss: 0.21322810649871826 / Valid loss: 6.496055566696894
Training loss: 0.2698012888431549 / Valid loss: 6.478043660663423
Training loss: 0.15830481052398682 / Valid loss: 6.460016657057262
Training loss: 0.16900739073753357 / Valid loss: 6.473711034229823

Epoch: 43
Training loss: 0.2869962453842163 / Valid loss: 6.469106692359561
Training loss: 0.20188933610916138 / Valid loss: 6.471167432694208
Training loss: 0.19407644867897034 / Valid loss: 6.468402635483515
Training loss: 0.4035569131374359 / Valid loss: 6.463747374216715
Training loss: 0.2676756978034973 / Valid loss: 6.464208478019351

Epoch: 44
Training loss: 0.18687205016613007 / Valid loss: 6.500656000773112
Training loss: 0.20516744256019592 / Valid loss: 6.479520874931699
Training loss: 0.2585129141807556 / Valid loss: 6.424930656523932
Training loss: 0.14302077889442444 / Valid loss: 6.410916791643415
Training loss: 0.2980179190635681 / Valid loss: 6.495898680459885

Epoch: 45
Training loss: 0.1693333089351654 / Valid loss: 6.431118411109561
Training loss: 0.33454546332359314 / Valid loss: 6.4309495835077195
Training loss: 0.4446280300617218 / Valid loss: 6.528272699174427
Training loss: 0.1860540360212326 / Valid loss: 6.471066608883086
Training loss: 0.21815894544124603 / Valid loss: 6.441833641415551

Epoch: 46
Training loss: 0.11010241508483887 / Valid loss: 6.437472425188337
Training loss: 0.2536678910255432 / Valid loss: 6.432887061436971
Training loss: 0.3705959618091583 / Valid loss: 6.459955705915179
Training loss: 0.0996260792016983 / Valid loss: 6.419195000330607
Training loss: 0.5902842283248901 / Valid loss: 6.45604582741147

Epoch: 47
Training loss: 0.10716493427753448 / Valid loss: 6.431344718024844
Training loss: 0.2454679310321808 / Valid loss: 6.490338034856887
Training loss: 0.38025957345962524 / Valid loss: 6.433895399456932
Training loss: 0.1942223310470581 / Valid loss: 6.43385599454244
Training loss: 0.18325155973434448 / Valid loss: 6.469023073287238

Epoch: 48
Training loss: 0.19283102452754974 / Valid loss: 6.492764039266677
Training loss: 0.3023659586906433 / Valid loss: 6.4602781068711055
Training loss: 0.21869555115699768 / Valid loss: 6.4754110563369025
Training loss: 0.16282393038272858 / Valid loss: 6.468641662597657
Training loss: 0.29198482632637024 / Valid loss: 6.444720488502866

Epoch: 49
Training loss: 0.3586122989654541 / Valid loss: 6.472088747932798
Training loss: 0.1698007583618164 / Valid loss: 6.4460055532909575
Training loss: 0.18436697125434875 / Valid loss: 6.462417080288842
Training loss: 0.22286131978034973 / Valid loss: 6.460023625691732

Epoch: 50
Training loss: 0.28334712982177734 / Valid loss: 6.455861414046515
Training loss: 0.22202980518341064 / Valid loss: 6.466064723332723
Training loss: 0.1678978055715561 / Valid loss: 6.423628350666591
Training loss: 0.2686591148376465 / Valid loss: 6.464798720677694
Training loss: 0.3956199586391449 / Valid loss: 6.458104767118182

Epoch: 51
Training loss: 0.1519360989332199 / Valid loss: 6.434102671486991
Training loss: 0.320894330739975 / Valid loss: 6.464236248107183
Training loss: 0.2191004902124405 / Valid loss: 6.411193995248704
Training loss: 0.19268375635147095 / Valid loss: 6.490602066403343
Training loss: 0.25765207409858704 / Valid loss: 6.3968986284165155

Epoch: 52
Training loss: 0.13751880824565887 / Valid loss: 6.4520240465799965
Training loss: 0.10310176014900208 / Valid loss: 6.431108031954084
Training loss: 0.15264007449150085 / Valid loss: 6.462610063098726
Training loss: 0.16902166604995728 / Valid loss: 6.407850683303106
Training loss: 0.3515225052833557 / Valid loss: 6.465167772202265

Epoch: 53
Training loss: 0.15263475477695465 / Valid loss: 6.409921319144113
Training loss: 0.3299385905265808 / Valid loss: 6.413794022514707
Training loss: 0.15401817858219147 / Valid loss: 6.424110623768398
Training loss: 0.4960412085056305 / Valid loss: 6.403119609469459
Training loss: 0.2546340823173523 / Valid loss: 6.444769677661714

Epoch: 54
Training loss: 0.28801462054252625 / Valid loss: 6.441448125385103
Training loss: 0.3182638883590698 / Valid loss: 6.454235426584879
Training loss: 0.1724241077899933 / Valid loss: 6.451311810811361
Training loss: 0.1500752866268158 / Valid loss: 6.4452796640850245
Training loss: 0.30608364939689636 / Valid loss: 6.425220589410691

Epoch: 55
Training loss: 0.5271943211555481 / Valid loss: 6.430661909920829
Training loss: 0.3344670534133911 / Valid loss: 6.409948033378238
Training loss: 0.4309447407722473 / Valid loss: 6.425917738959903
Training loss: 0.17575542628765106 / Valid loss: 6.459461850211734
Training loss: 0.2597511410713196 / Valid loss: 6.455588522411528

Epoch: 56
Training loss: 0.4460563659667969 / Valid loss: 6.442088445027669
Training loss: 0.15093007683753967 / Valid loss: 6.417564260391962
Training loss: 0.1513858437538147 / Valid loss: 6.392606412796747
Training loss: 0.17092105746269226 / Valid loss: 6.390770603361584
Training loss: 0.28505939245224 / Valid loss: 6.415219079880488

Epoch: 57
Training loss: 0.6073035001754761 / Valid loss: 6.4241612411680675
Training loss: 0.33692610263824463 / Valid loss: 6.426250471387591
Training loss: 0.28244641423225403 / Valid loss: 6.4169469106765025
Training loss: 0.5004138946533203 / Valid loss: 6.426950341179257
Training loss: 0.42479297518730164 / Valid loss: 6.446034488223848

Epoch: 58
Training loss: 0.17794150114059448 / Valid loss: 6.378240648905436
Training loss: 0.29364705085754395 / Valid loss: 6.422733429500035
Training loss: 0.11600545048713684 / Valid loss: 6.402946712857201
Training loss: 0.38747161626815796 / Valid loss: 6.410565873554774
Training loss: 0.14547303318977356 / Valid loss: 6.40334753763108

Epoch: 59
Training loss: 0.26980704069137573 / Valid loss: 6.415687987917946
Training loss: 0.3091619312763214 / Valid loss: 6.368788648786999
Training loss: 0.21759623289108276 / Valid loss: 6.425389405659267
Training loss: 0.2503666579723358 / Valid loss: 6.426740648632958

Epoch: 60
Training loss: 0.20996791124343872 / Valid loss: 6.448155321393695
Training loss: 0.5508055686950684 / Valid loss: 6.446132053647722
Training loss: 0.13736677169799805 / Valid loss: 6.415488663173857
Training loss: 0.24522598087787628 / Valid loss: 6.399518555686587
Training loss: 0.13587158918380737 / Valid loss: 6.404240944271996

Epoch: 61
Training loss: 0.3713666498661041 / Valid loss: 6.385814301172892
Training loss: 0.11027577519416809 / Valid loss: 6.395043316341582
Training loss: 0.4406837522983551 / Valid loss: 6.40865889957973
Training loss: 0.28984612226486206 / Valid loss: 6.417125408990042
Training loss: 0.1411258578300476 / Valid loss: 6.373506868453253

Epoch: 62
Training loss: 0.07512064278125763 / Valid loss: 6.405236148834229
Training loss: 0.12571504712104797 / Valid loss: 6.397777125948951
Training loss: 0.13357582688331604 / Valid loss: 6.406822263626825
Training loss: 0.23017603158950806 / Valid loss: 6.426078919001988
Training loss: 0.18108639121055603 / Valid loss: 6.408461993081229

Epoch: 63
Training loss: 0.2676023244857788 / Valid loss: 6.390839422316779
Training loss: 0.17496246099472046 / Valid loss: 6.384416816348121
Training loss: 0.13901154696941376 / Valid loss: 6.4285507474626815
Training loss: 0.2826370894908905 / Valid loss: 6.3926447459629605
Training loss: 0.1755586564540863 / Valid loss: 6.37895987374442

Epoch: 64
Training loss: 0.20380598306655884 / Valid loss: 6.377519598461332
Training loss: 0.13412022590637207 / Valid loss: 6.413230593999227
Training loss: 0.26380765438079834 / Valid loss: 6.400458780924479
Training loss: 0.15620093047618866 / Valid loss: 6.41102762903486
Training loss: 0.14484095573425293 / Valid loss: 6.437744172414144

Epoch: 65
Training loss: 0.18184953927993774 / Valid loss: 6.3863902773175925
Training loss: 0.307429701089859 / Valid loss: 6.3836912927173435
Training loss: 0.21065860986709595 / Valid loss: 6.3919322922116235
Training loss: 0.1430898904800415 / Valid loss: 6.38922286487761
Training loss: 0.9966495037078857 / Valid loss: 6.376174602054414

Epoch: 66
Training loss: 0.12379288673400879 / Valid loss: 6.3762088457743324
Training loss: 0.1314248889684677 / Valid loss: 6.363207939692906
Training loss: 0.49382320046424866 / Valid loss: 6.429370017278762
Training loss: 0.1896178126335144 / Valid loss: 6.44899540855771
Training loss: 0.3517659902572632 / Valid loss: 6.366899417695545

Epoch: 67
Training loss: 0.19452032446861267 / Valid loss: 6.413976224263509
Training loss: 0.08782894909381866 / Valid loss: 6.427938965388707
Training loss: 0.11512821912765503 / Valid loss: 6.401551896049863
Training loss: 0.6412409543991089 / Valid loss: 6.3999561446053645
Training loss: 0.22524496912956238 / Valid loss: 6.426959628150577

Epoch: 68
Training loss: 0.19396336376667023 / Valid loss: 6.367981415703183
Training loss: 0.24474012851715088 / Valid loss: 6.366862099511283
Training loss: 0.143762469291687 / Valid loss: 6.363574822743733
Training loss: 0.14347538352012634 / Valid loss: 6.414360616320655
Training loss: 0.2169269174337387 / Valid loss: 6.3804668471926735

Epoch: 69
Training loss: 0.2902604341506958 / Valid loss: 6.38683039574396
Training loss: 0.2057105004787445 / Valid loss: 6.380639266967774
Training loss: 0.2580888569355011 / Valid loss: 6.415684677305675
Training loss: 0.30055636167526245 / Valid loss: 6.369109069733392

Epoch: 70
Training loss: 0.15171724557876587 / Valid loss: 6.4016354969569615
Training loss: 0.48688817024230957 / Valid loss: 6.35352532523019
Training loss: 0.18163177371025085 / Valid loss: 6.377068301609584
Training loss: 0.19107021391391754 / Valid loss: 6.37387562025161
Training loss: 0.2170679122209549 / Valid loss: 6.343246403194609

Epoch: 71
Training loss: 0.27173054218292236 / Valid loss: 6.371781467256092
Training loss: 0.3603571355342865 / Valid loss: 6.351015281677246
Training loss: 0.1392894685268402 / Valid loss: 6.364069100788662
Training loss: 0.12592406570911407 / Valid loss: 6.35192776180449
Training loss: 0.1536390483379364 / Valid loss: 6.365172068277995

Epoch: 72
Training loss: 0.1500091850757599 / Valid loss: 6.365414723895845
Training loss: 0.36779162287712097 / Valid loss: 6.361125233059838
Training loss: 0.16407105326652527 / Valid loss: 6.351516230901082
Training loss: 0.14547589421272278 / Valid loss: 6.4005286693573
Training loss: 0.4628208875656128 / Valid loss: 6.349948120117188

Epoch: 73
Training loss: 0.3234761357307434 / Valid loss: 6.372968008404686
Training loss: 0.4039190709590912 / Valid loss: 6.3426520574660525
Training loss: 0.1986491084098816 / Valid loss: 6.3450053010668075
Training loss: 0.12525975704193115 / Valid loss: 6.358750141234625
Training loss: 0.1173602044582367 / Valid loss: 6.3613428797040665

Epoch: 74
Training loss: 0.13991358876228333 / Valid loss: 6.37265632947286
Training loss: 0.19992807507514954 / Valid loss: 6.366491317749023
Training loss: 0.33867907524108887 / Valid loss: 6.313078455697923
Training loss: 0.37945556640625 / Valid loss: 6.38682583173116
Training loss: 0.17365118861198425 / Valid loss: 6.391597713742938

Epoch: 75
Training loss: 0.16219261288642883 / Valid loss: 6.334480199359712
Training loss: 0.26773208379745483 / Valid loss: 6.356841763995942
Training loss: 0.4660983681678772 / Valid loss: 6.334250143596104
Training loss: 0.1330040991306305 / Valid loss: 6.308213826588222
Training loss: 0.24956099689006805 / Valid loss: 6.318492067427862

Epoch: 76
Training loss: 0.3140931725502014 / Valid loss: 6.3049195425851
Training loss: 0.48272955417633057 / Valid loss: 6.359162780216762
Training loss: 0.1641683131456375 / Valid loss: 6.3095206396920345
Training loss: 0.24776586890220642 / Valid loss: 6.360502120426723
Training loss: 0.19081729650497437 / Valid loss: 6.337754572005499

Epoch: 77
Training loss: 0.17644508183002472 / Valid loss: 6.357056837990171
Training loss: 0.23343130946159363 / Valid loss: 6.332255408877418
Training loss: 0.3262977600097656 / Valid loss: 6.363460677010672
Training loss: 0.31189480423927307 / Valid loss: 6.365561403547015
Training loss: 0.4388204514980316 / Valid loss: 6.367628008978707

Epoch: 78
Training loss: 0.20859310030937195 / Valid loss: 6.352250707717169
Training loss: 0.1349269449710846 / Valid loss: 6.363267035711379
Training loss: 0.12820421159267426 / Valid loss: 6.349946242287046
Training loss: 0.15040694177150726 / Valid loss: 6.3553952126275925
Training loss: 0.36882147192955017 / Valid loss: 6.373518496467954

Epoch: 79
Training loss: 0.0935317873954773 / Valid loss: 6.344944374901908
Training loss: 0.3093976378440857 / Valid loss: 6.366897476287115
Training loss: 0.07659834623336792 / Valid loss: 6.360671440760295
Training loss: 0.10881249606609344 / Valid loss: 6.360940138498942
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model: 5.381952912466867
Training regression with following parameters:
dnn_hidden_units : 2000, 100, 16
dropout_probs : 0.0, 0.0
learning_rate : 0.0001
nr_epochs : 80
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : False
batchnorm : True
weightdecay : 0
momentum : 0
embedder : Bert
verbose : False
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
neural net:
 [tensor([[ 0.0035, -0.0035,  0.0052,  ..., -0.0130,  0.0056, -0.0088],
        [ 0.0045, -0.0089, -0.0107,  ...,  0.0052,  0.0048, -0.0076],
        [ 0.0014, -0.0062,  0.0084,  ..., -0.0093,  0.0089,  0.0006],
        ...,
        [ 0.0115,  0.0001, -0.0032,  ..., -0.0018, -0.0062, -0.0085],
        [-0.0009,  0.0057,  0.0107,  ..., -0.0067, -0.0022, -0.0107],
        [ 0.0109, -0.0116, -0.0054,  ...,  0.0007, -0.0127,  0.0088]],
       device='cuda:0'), tensor([ 0.0047,  0.0122,  0.0094,  ...,  0.0037, -0.0029, -0.0056],
       device='cuda:0'), tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), tensor([[-0.0121, -0.0085,  0.0158,  ...,  0.0067, -0.0057, -0.0189],
        [ 0.0064, -0.0150,  0.0096,  ...,  0.0006,  0.0074,  0.0177],
        [ 0.0061, -0.0078,  0.0059,  ...,  0.0032, -0.0206,  0.0205],
        ...,
        [ 0.0173, -0.0032,  0.0090,  ..., -0.0031,  0.0141,  0.0130],
        [-0.0181, -0.0101,  0.0157,  ..., -0.0020, -0.0070,  0.0160],
        [ 0.0063, -0.0152, -0.0101,  ...,  0.0008,  0.0028, -0.0011]],
       device='cuda:0'), tensor([-1.1967e-02,  1.1110e-02,  1.8701e-02,  1.1023e-02, -2.0298e-02,
         5.6226e-05, -1.3542e-02,  2.7363e-03, -8.2064e-03,  1.8652e-02,
         1.4701e-02,  1.2389e-02,  2.1926e-02,  1.5760e-02, -3.8398e-03,
        -2.1899e-02,  2.2099e-02, -5.7002e-03,  2.0446e-03,  2.1733e-02,
         1.5114e-02,  2.0789e-03, -1.0899e-02, -5.6587e-03,  2.0192e-02,
         4.3223e-03,  8.8637e-03, -1.6798e-02,  1.7955e-02,  1.2753e-02,
        -5.4965e-03,  1.9481e-02,  6.9524e-03, -2.0116e-02, -5.9529e-03,
        -1.1193e-02,  1.8363e-02,  1.6020e-02, -1.6430e-02,  6.4647e-03,
         1.5684e-02,  9.1588e-03, -2.0390e-02,  9.9621e-03,  2.0163e-02,
         2.1748e-02, -1.8861e-02, -5.0238e-03,  1.9386e-02,  1.1112e-03,
        -1.3363e-02,  6.7029e-03,  1.7814e-02, -8.2254e-03,  1.3094e-02,
         1.2228e-02, -1.2875e-03,  3.9142e-03,  1.4362e-03,  5.9424e-03,
         2.2414e-03, -1.3438e-02, -7.4102e-03,  1.5021e-02, -1.8291e-02,
         1.7939e-02,  2.4626e-03, -1.6420e-03,  1.0483e-03, -6.6787e-03,
         1.9978e-02,  6.3201e-04,  1.7342e-02,  9.4639e-03,  2.2343e-02,
        -1.5528e-02,  1.0803e-02, -4.7853e-03,  1.3566e-02, -1.2095e-02,
        -5.4629e-03,  7.9233e-03, -1.5708e-02, -2.7313e-03,  8.8111e-03,
        -2.1376e-03,  2.0746e-02,  1.0951e-02, -1.4991e-02,  5.5452e-03,
        -1.4782e-02, -1.6080e-02, -5.0667e-03, -2.1050e-02, -7.7526e-03,
         1.9154e-02, -2.1401e-02, -1.0501e-02,  7.3819e-03,  2.0670e-03],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.], device='cuda:0'), tensor([[-0.0636, -0.0402,  0.0180,  ..., -0.0669, -0.0184, -0.0632],
        [-0.0702, -0.0480,  0.0203,  ...,  0.0432, -0.0736,  0.0113],
        [-0.0735, -0.0482,  0.0561,  ..., -0.0377,  0.0717,  0.0079],
        ...,
        [ 0.0336,  0.0795,  0.0686,  ...,  0.0617,  0.0593,  0.0145],
        [-0.0549, -0.0686,  0.0407,  ..., -0.0574,  0.0591, -0.0667],
        [-0.0106, -0.0514, -0.0024,  ...,  0.0682,  0.0980,  0.0754]],
       device='cuda:0'), tensor([-0.0598,  0.0784,  0.0041,  0.0437, -0.0314,  0.0230, -0.0737, -0.0741,
        -0.0710, -0.0826, -0.0965, -0.0397,  0.0959,  0.0370,  0.0845,  0.0369],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0'), tensor([[-0.0476,  0.1976,  0.0806, -0.1341,  0.1659,  0.0086, -0.0477,  0.0708,
          0.2368,  0.0414, -0.1084, -0.0428,  0.1062, -0.1102,  0.0168,  0.1324]],
       device='cuda:0'), tensor([0.1504], device='cuda:0')]

Epoch: 0
Training loss: 15.765700340270996 / Valid loss: 15.326356043134417
Model is saved in epoch 0, overall batch: 0
Training loss: 9.031549453735352 / Valid loss: 12.925361442565919
Model is saved in epoch 0, overall batch: 100
Training loss: 9.649495124816895 / Valid loss: 11.93502668199085
Model is saved in epoch 0, overall batch: 200
Training loss: 14.655001640319824 / Valid loss: 11.029834052494595
Model is saved in epoch 0, overall batch: 300
Training loss: 10.413329124450684 / Valid loss: 10.265079829806373
Model is saved in epoch 0, overall batch: 400

Epoch: 1
Training loss: 8.910350799560547 / Valid loss: 9.613765380496071
Model is saved in epoch 1, overall batch: 500
Training loss: 7.872617721557617 / Valid loss: 8.97424146107265
Model is saved in epoch 1, overall batch: 600
Training loss: 7.332974433898926 / Valid loss: 8.527550057002477
Model is saved in epoch 1, overall batch: 700
Training loss: 7.229619026184082 / Valid loss: 8.04958698181879
Model is saved in epoch 1, overall batch: 800
Training loss: 5.423467636108398 / Valid loss: 7.773928292592367
Model is saved in epoch 1, overall batch: 900

Epoch: 2
Training loss: 7.810892105102539 / Valid loss: 7.546292895362491
Model is saved in epoch 2, overall batch: 1000
Training loss: 7.799148082733154 / Valid loss: 7.289633210500082
Model is saved in epoch 2, overall batch: 1100
Training loss: 7.037626266479492 / Valid loss: 7.038455272856213
Model is saved in epoch 2, overall batch: 1200
Training loss: 8.314278602600098 / Valid loss: 6.658316196714129
Model is saved in epoch 2, overall batch: 1300
Training loss: 4.37082576751709 / Valid loss: 6.479670063654582
Model is saved in epoch 2, overall batch: 1400

Epoch: 3
Training loss: 4.279825687408447 / Valid loss: 6.456323323931013
Model is saved in epoch 3, overall batch: 1500
Training loss: 3.787858724594116 / Valid loss: 6.2811033793858115
Model is saved in epoch 3, overall batch: 1600
Training loss: 6.567384719848633 / Valid loss: 6.132880061013358
Model is saved in epoch 3, overall batch: 1700
Training loss: 3.074599266052246 / Valid loss: 6.027197063536871
Model is saved in epoch 3, overall batch: 1800
Training loss: 5.374438285827637 / Valid loss: 5.917620524905977
Model is saved in epoch 3, overall batch: 1900

Epoch: 4
Training loss: 5.37590217590332 / Valid loss: 6.066183928080967
Training loss: 3.7718729972839355 / Valid loss: 6.084708586193266
Training loss: 4.1693549156188965 / Valid loss: 5.980021408626011
Training loss: 4.449491500854492 / Valid loss: 5.9436943780808225
Training loss: 3.1457338333129883 / Valid loss: 5.8985951628003805
Model is saved in epoch 4, overall batch: 2400

Epoch: 5
Training loss: 6.034307479858398 / Valid loss: 5.8190965062096005
Model is saved in epoch 5, overall batch: 2500
Training loss: 4.65392541885376 / Valid loss: 5.883803662799654
Training loss: 3.245314359664917 / Valid loss: 5.882777790796189
Training loss: 3.8647522926330566 / Valid loss: 5.8596199580601285
Training loss: 3.6135709285736084 / Valid loss: 5.8016817705971855
Model is saved in epoch 5, overall batch: 2900

Epoch: 6
Training loss: 5.956195831298828 / Valid loss: 5.851204438436599
Training loss: 3.4913218021392822 / Valid loss: 5.922347840808687
Training loss: 3.9809350967407227 / Valid loss: 5.927971462976365
Training loss: 3.54435396194458 / Valid loss: 5.909689396903628
Training loss: 4.117196559906006 / Valid loss: 6.0334273542676655

Epoch: 7
Training loss: 3.502049446105957 / Valid loss: 5.933380254109701
Training loss: 3.139695644378662 / Valid loss: 5.993251664297921
Training loss: 2.697209358215332 / Valid loss: 5.977835546221052
Training loss: 4.136932373046875 / Valid loss: 6.063802925745646
Training loss: 3.038693428039551 / Valid loss: 6.007130838575817

Epoch: 8
Training loss: 2.8324384689331055 / Valid loss: 6.0750229494912285
Training loss: 1.8436771631240845 / Valid loss: 6.078082086926415
Training loss: 3.234579086303711 / Valid loss: 6.108758088520595
Training loss: 4.641075134277344 / Valid loss: 6.153123925981068
Training loss: 2.8398232460021973 / Valid loss: 6.134172952742803

Epoch: 9
Training loss: 1.5098446607589722 / Valid loss: 6.158533652623494
Training loss: 2.5671896934509277 / Valid loss: 6.18920902070545
Training loss: 3.6185734272003174 / Valid loss: 6.201976576305571
Training loss: 2.732630968093872 / Valid loss: 6.195091059094383

Epoch: 10
Training loss: 1.832048773765564 / Valid loss: 6.237085744312831
Training loss: 2.661381244659424 / Valid loss: 6.234330043338594
Training loss: 1.560402512550354 / Valid loss: 6.398729760306222
Training loss: 2.305189609527588 / Valid loss: 6.391875509988694
Training loss: 2.492584705352783 / Valid loss: 6.298088019234793

Epoch: 11
Training loss: 1.2297296524047852 / Valid loss: 6.3910364877609975
Training loss: 1.4975640773773193 / Valid loss: 6.3959820860908145
Training loss: 2.548607349395752 / Valid loss: 6.465189738500686
Training loss: 1.4185914993286133 / Valid loss: 6.6187856447129025
Training loss: 2.128636360168457 / Valid loss: 6.666912732805525

Epoch: 12
Training loss: 1.1633267402648926 / Valid loss: 6.452874792189825
Training loss: 1.2297829389572144 / Valid loss: 6.56710113797869
Training loss: 1.5909547805786133 / Valid loss: 6.575278670447213
Training loss: 1.3911828994750977 / Valid loss: 6.523057824089413
Training loss: 1.3432307243347168 / Valid loss: 6.534570619038173

Epoch: 13
Training loss: 0.9551684260368347 / Valid loss: 6.706716859908331
Training loss: 0.8917296528816223 / Valid loss: 6.6589717774164106
Training loss: 1.1735572814941406 / Valid loss: 6.546581147965931
Training loss: 1.4201452732086182 / Valid loss: 6.681150949568975
Training loss: 1.3012744188308716 / Valid loss: 6.600960706529163

Epoch: 14
Training loss: 1.0165108442306519 / Valid loss: 6.741644266673497
Training loss: 1.0852172374725342 / Valid loss: 6.643154287338257
Training loss: 1.4932785034179688 / Valid loss: 6.633780184246245
Training loss: 1.1285059452056885 / Valid loss: 6.6558440480913434
Training loss: 1.6491352319717407 / Valid loss: 6.664465704418364

Epoch: 15
Training loss: 1.0540403127670288 / Valid loss: 6.7938585939861476
Training loss: 1.0658385753631592 / Valid loss: 6.817523004895165
Training loss: 1.0568095445632935 / Valid loss: 6.705037425813221
Training loss: 1.0983721017837524 / Valid loss: 6.749984929675148
Training loss: 1.1242650747299194 / Valid loss: 6.689923824582781

Epoch: 16
Training loss: 0.5892389416694641 / Valid loss: 6.810284650893438
Training loss: 0.7803249359130859 / Valid loss: 7.188371404012044
Training loss: 1.1032828092575073 / Valid loss: 6.764204202379499
Training loss: 1.0612361431121826 / Valid loss: 6.95405265490214
Training loss: 1.173661708831787 / Valid loss: 6.885861097063337

Epoch: 17
Training loss: 1.0085021257400513 / Valid loss: 6.822495294752575
Training loss: 0.9435634613037109 / Valid loss: 6.956481792813256
Training loss: 0.9202849864959717 / Valid loss: 6.930529340108236
Training loss: 0.666002094745636 / Valid loss: 6.919105189187186
Training loss: 1.0445382595062256 / Valid loss: 6.757153756277901

Epoch: 18
Training loss: 0.6763856410980225 / Valid loss: 6.79201413109189
Training loss: 0.8552510738372803 / Valid loss: 6.800828341075352
Training loss: 0.8908299207687378 / Valid loss: 6.922042628696986
Training loss: 0.9965186715126038 / Valid loss: 7.065425046284994
Training loss: 0.863675594329834 / Valid loss: 6.810019663402012

Epoch: 19
Training loss: 0.8063485026359558 / Valid loss: 6.918242499941871
Training loss: 1.2864010334014893 / Valid loss: 6.813279499326433
Training loss: 0.6465586423873901 / Valid loss: 6.807423300970168
Training loss: 0.6824859380722046 / Valid loss: 6.840832583109537

Epoch: 20
Training loss: 0.40538251399993896 / Valid loss: 6.810974334535144
Training loss: 1.036321997642517 / Valid loss: 6.8397407032194595
Training loss: 0.4680996537208557 / Valid loss: 6.826008274441674
Training loss: 0.6702053546905518 / Valid loss: 6.848900013878232
Training loss: 1.099815845489502 / Valid loss: 6.829094416754586

Epoch: 21
Training loss: 0.5849310159683228 / Valid loss: 6.897969468434652
Training loss: 0.7242369651794434 / Valid loss: 6.8522044408889045
Training loss: 0.6205954551696777 / Valid loss: 6.849351374308268
Training loss: 0.492944598197937 / Valid loss: 6.910346090225946
Training loss: 0.671619176864624 / Valid loss: 6.842398089454288

Epoch: 22
Training loss: 0.7165206670761108 / Valid loss: 6.86344078154791
Training loss: 0.554999589920044 / Valid loss: 6.95038419905163
Training loss: 0.4596119523048401 / Valid loss: 6.781331836609613
Training loss: 1.3692537546157837 / Valid loss: 6.841743144534883
Training loss: 0.7559306621551514 / Valid loss: 6.840181078229632

Epoch: 23
Training loss: 0.7283676862716675 / Valid loss: 6.892171362468175
Training loss: 0.7919235229492188 / Valid loss: 6.783128611246744
Training loss: 0.8206788897514343 / Valid loss: 6.887018896284557
Training loss: 0.5897453427314758 / Valid loss: 6.852277633122036
Training loss: 1.0101314783096313 / Valid loss: 6.9078188987005325

Epoch: 24
Training loss: 0.8423129320144653 / Valid loss: 6.823648264294579
Training loss: 0.6432785987854004 / Valid loss: 6.800510660807292
Training loss: 0.5377112030982971 / Valid loss: 6.923416818891253
Training loss: 0.9146564602851868 / Valid loss: 6.848580855414981
Training loss: 1.0273497104644775 / Valid loss: 6.892161977858771

Epoch: 25
Training loss: 1.0057035684585571 / Valid loss: 6.882301693870907
Training loss: 0.3921237587928772 / Valid loss: 6.856995741526286
Training loss: 0.8008273839950562 / Valid loss: 6.7691147622608
Training loss: 0.3524518609046936 / Valid loss: 6.795892313548497
Training loss: 0.5134373903274536 / Valid loss: 6.9135886056082585

Epoch: 26
Training loss: 0.5141182541847229 / Valid loss: 6.811796706063407
Training loss: 0.5204722881317139 / Valid loss: 6.847500975926717
Training loss: 0.3656955361366272 / Valid loss: 6.839329501560756
Training loss: 0.43640822172164917 / Valid loss: 6.982221017565046
Training loss: 0.4070833921432495 / Valid loss: 6.97004113424392

Epoch: 27
Training loss: 0.3686547875404358 / Valid loss: 6.825541260128929
Training loss: 0.47597256302833557 / Valid loss: 6.963418935594104
Training loss: 0.6055011749267578 / Valid loss: 6.956779216584705
Training loss: 0.4820709824562073 / Valid loss: 6.935784280867804
Training loss: 0.6080188751220703 / Valid loss: 6.914210024334135

Epoch: 28
Training loss: 0.4018572270870209 / Valid loss: 6.922458239964077
Training loss: 0.37014228105545044 / Valid loss: 6.837387693495978
Training loss: 0.7190433144569397 / Valid loss: 6.880425839197068
Training loss: 0.57194584608078 / Valid loss: 6.869017782665434
Training loss: 0.4527132511138916 / Valid loss: 6.875129699707031

Epoch: 29
Training loss: 0.6052722930908203 / Valid loss: 6.821631667727516
Training loss: 0.46156978607177734 / Valid loss: 6.808241335550944
Training loss: 0.31890395283699036 / Valid loss: 6.8673095703125
Training loss: 0.4430583119392395 / Valid loss: 6.832905601319813

Epoch: 30
Training loss: 0.4817635715007782 / Valid loss: 6.803235054016113
Training loss: 0.4514594078063965 / Valid loss: 6.817978398005168
Training loss: 0.3631715178489685 / Valid loss: 6.807428577968053
Training loss: 0.5134695768356323 / Valid loss: 6.876563944135394
Training loss: 0.4263817369937897 / Valid loss: 6.7823568571181525

Epoch: 31
Training loss: 0.5117906332015991 / Valid loss: 6.86391129266648
Training loss: 0.4331490993499756 / Valid loss: 6.831766886938186
Training loss: 0.4823755621910095 / Valid loss: 6.872081515902565
Training loss: 0.33831608295440674 / Valid loss: 6.8909860020592095
Training loss: 0.43682053685188293 / Valid loss: 6.999074477241153

Epoch: 32
Training loss: 0.5587068796157837 / Valid loss: 6.849571125847953
Training loss: 0.4467211365699768 / Valid loss: 6.856874209358579
Training loss: 0.4933033585548401 / Valid loss: 6.851555061340332
Training loss: 0.41836023330688477 / Valid loss: 6.806804557073684
Training loss: 0.5701733231544495 / Valid loss: 6.752812017713274

Epoch: 33
Training loss: 0.48938149213790894 / Valid loss: 6.830871200561523
Training loss: 0.5299113392829895 / Valid loss: 6.78384948912121
Training loss: 0.3791443407535553 / Valid loss: 6.854760578700474
Training loss: 0.3959607183933258 / Valid loss: 6.874238688605172
Training loss: 0.5073026418685913 / Valid loss: 6.915350973038446

Epoch: 34
Training loss: 0.84784996509552 / Valid loss: 6.810558641524542
Training loss: 0.3780098557472229 / Valid loss: 6.870923362459455
Training loss: 0.5260709524154663 / Valid loss: 6.858384286789667
Training loss: 0.3105500042438507 / Valid loss: 6.847722543988909
Training loss: 0.37165871262550354 / Valid loss: 6.790072752180554

Epoch: 35
Training loss: 0.8120191097259521 / Valid loss: 6.947482231685093
Training loss: 0.40832674503326416 / Valid loss: 6.791333491461618
Training loss: 0.2575708031654358 / Valid loss: 6.839314024788993
Training loss: 0.32510441541671753 / Valid loss: 6.917860930306571
Training loss: 0.3456174433231354 / Valid loss: 6.814979321616036

Epoch: 36
Training loss: 0.36534979939460754 / Valid loss: 6.851553079060146
Training loss: 0.7617155909538269 / Valid loss: 6.811295768192836
Training loss: 0.31516551971435547 / Valid loss: 6.866854703994024
Training loss: 0.33060356974601746 / Valid loss: 6.8456908725556875
Training loss: 0.41588684916496277 / Valid loss: 6.8495058332170755

Epoch: 37
Training loss: 0.42353859543800354 / Valid loss: 6.873068968454997
Training loss: 0.3280651867389679 / Valid loss: 6.7587628818693615
Training loss: 0.4312743842601776 / Valid loss: 6.870133681524368
Training loss: 0.7213282585144043 / Valid loss: 6.8087525526682535
Training loss: 0.48998257517814636 / Valid loss: 6.8538070451645625

Epoch: 38
Training loss: 0.3917848765850067 / Valid loss: 6.765246945335751
Training loss: 0.357817679643631 / Valid loss: 6.76726332846142
Training loss: 0.41595977544784546 / Valid loss: 6.762485940115792
Training loss: 0.3384285271167755 / Valid loss: 6.8026490415845595
Training loss: 0.32240891456604004 / Valid loss: 6.9024126370747885

Epoch: 39
Training loss: 0.5113993287086487 / Valid loss: 6.83139526049296
Training loss: 0.3025866746902466 / Valid loss: 6.79451983315604
Training loss: 0.3561829924583435 / Valid loss: 6.755090431939988
Training loss: 0.45146700739860535 / Valid loss: 6.957076218014672

Epoch: 40
Training loss: 0.36804628372192383 / Valid loss: 6.874037115914481
Training loss: 0.38985177874565125 / Valid loss: 6.743153272356306
Training loss: 0.26375043392181396 / Valid loss: 6.882572759900774
Training loss: 0.31998610496520996 / Valid loss: 6.782429118383498
Training loss: 0.3033495545387268 / Valid loss: 6.772621184303647

Epoch: 41
Training loss: 0.2733774185180664 / Valid loss: 6.866429837544759
Training loss: 0.25624948740005493 / Valid loss: 6.7811607837677
Training loss: 0.2962963581085205 / Valid loss: 6.848663548060826
Training loss: 1.2327595949172974 / Valid loss: 6.76028528213501
Training loss: 0.4783816933631897 / Valid loss: 6.814179098038446

Epoch: 42
Training loss: 0.39118099212646484 / Valid loss: 6.759415899004255
Training loss: 0.3444925844669342 / Valid loss: 6.80299927847726
Training loss: 0.3744836449623108 / Valid loss: 6.756099392118908
Training loss: 0.32826337218284607 / Valid loss: 6.747801853361584
Training loss: 0.37627869844436646 / Valid loss: 6.76649592263358

Epoch: 43
Training loss: 0.3811500370502472 / Valid loss: 6.794272372836159
Training loss: 0.32206419110298157 / Valid loss: 6.8181998207455585
Training loss: 0.2588002681732178 / Valid loss: 6.7701265902746295
Training loss: 0.5416268110275269 / Valid loss: 6.752507813771566
Training loss: 0.29299452900886536 / Valid loss: 6.75570479120527

Epoch: 44
Training loss: 0.23972025513648987 / Valid loss: 6.765600036439442
Training loss: 0.3293181359767914 / Valid loss: 6.805912362961542
Training loss: 0.3223400413990021 / Valid loss: 6.786621938432966
Training loss: 0.3097377419471741 / Valid loss: 6.794602176121303
Training loss: 0.43824344873428345 / Valid loss: 6.807029651460193

Epoch: 45
Training loss: 0.2801916003227234 / Valid loss: 6.743092972891671
Training loss: 0.4561864137649536 / Valid loss: 6.796846952892485
Training loss: 0.522200882434845 / Valid loss: 6.771978337424142
Training loss: 0.3911826014518738 / Valid loss: 6.707783487864903
Training loss: 0.38306760787963867 / Valid loss: 6.825072869800386

Epoch: 46
Training loss: 0.30472809076309204 / Valid loss: 6.757801737104144
Training loss: 0.32752498984336853 / Valid loss: 6.805779695510864
Training loss: 0.4997628927230835 / Valid loss: 6.775128409976051
Training loss: 0.3409487307071686 / Valid loss: 6.736865511394682
Training loss: 0.6677675247192383 / Valid loss: 6.694364036832537

Epoch: 47
Training loss: 0.19558504223823547 / Valid loss: 6.702166289374942
Training loss: 0.3180335760116577 / Valid loss: 6.729248537336077
Training loss: 0.48342302441596985 / Valid loss: 6.785060024261474
Training loss: 0.3862532377243042 / Valid loss: 6.806991767883301
Training loss: 0.2732636332511902 / Valid loss: 6.76103454771496

Epoch: 48
Training loss: 0.24597975611686707 / Valid loss: 6.728623362949916
Training loss: 0.4236012101173401 / Valid loss: 6.843870162963867
Training loss: 0.37488868832588196 / Valid loss: 6.704499503544398
Training loss: 0.2817808985710144 / Valid loss: 6.730080336616153
Training loss: 0.44567030668258667 / Valid loss: 6.786397407168434

Epoch: 49
Training loss: 0.4393573999404907 / Valid loss: 6.731303719111851
Training loss: 0.25647956132888794 / Valid loss: 6.797152814410982
Training loss: 0.20093435049057007 / Valid loss: 6.73744230497451
Training loss: 0.2900494635105133 / Valid loss: 6.7258472170148575

Epoch: 50
Training loss: 0.46962541341781616 / Valid loss: 6.699434284936814
Training loss: 0.3220330476760864 / Valid loss: 6.674120330810547
Training loss: 0.29386743903160095 / Valid loss: 6.711546103159587
Training loss: 0.29852235317230225 / Valid loss: 6.708015450977144
Training loss: 0.5148974657058716 / Valid loss: 6.816843416577294

Epoch: 51
Training loss: 0.23548740148544312 / Valid loss: 6.736036421003796
Training loss: 0.4178692698478699 / Valid loss: 6.767370832534064
Training loss: 0.28324565291404724 / Valid loss: 6.730896827152797
Training loss: 0.25252464413642883 / Valid loss: 6.707066404251825
Training loss: 0.35402363538742065 / Valid loss: 6.711458492279053

Epoch: 52
Training loss: 0.25875067710876465 / Valid loss: 6.724168273380824
Training loss: 0.2623487710952759 / Valid loss: 6.756116574151175
Training loss: 0.2128627449274063 / Valid loss: 6.717803519112723
Training loss: 0.1926773190498352 / Valid loss: 6.704392324175154
Training loss: 0.36973828077316284 / Valid loss: 6.713899035680861

Epoch: 53
Training loss: 0.24603763222694397 / Valid loss: 6.763773277827672
Training loss: 0.4876273274421692 / Valid loss: 6.707433228265671
Training loss: 0.2852274179458618 / Valid loss: 6.747799033210391
Training loss: 0.6705142855644226 / Valid loss: 6.7391944567362465
Training loss: 0.4465499520301819 / Valid loss: 6.76401474362328

Epoch: 54
Training loss: 0.5303177833557129 / Valid loss: 6.687629495348249
Training loss: 0.3286805748939514 / Valid loss: 6.794836509795416
Training loss: 0.3106897175312042 / Valid loss: 6.75164741334461
Training loss: 0.2318662703037262 / Valid loss: 6.7098256270090735
Training loss: 0.3557831346988678 / Valid loss: 6.758794512067523

Epoch: 55
Training loss: 0.5168576240539551 / Valid loss: 6.705766225996472
Training loss: 0.4875376224517822 / Valid loss: 6.759159515017555
Training loss: 0.5993860960006714 / Valid loss: 6.781763281141009
Training loss: 0.27772843837738037 / Valid loss: 6.810182339804513
Training loss: 0.3392465114593506 / Valid loss: 6.739926226933798

Epoch: 56
Training loss: 0.6266573071479797 / Valid loss: 6.710631915501186
Training loss: 0.1744554191827774 / Valid loss: 6.765591453370594
Training loss: 0.3134199380874634 / Valid loss: 6.69522210984003
Training loss: 0.31472039222717285 / Valid loss: 6.670461872645787
Training loss: 0.5152013897895813 / Valid loss: 6.690105004537673

Epoch: 57
Training loss: 0.7524474859237671 / Valid loss: 6.766784797395979
Training loss: 0.3706541359424591 / Valid loss: 6.710080396561396
Training loss: 0.35724395513534546 / Valid loss: 6.749285752432687
Training loss: 0.6074902415275574 / Valid loss: 6.689384891873314
Training loss: 0.44736865162849426 / Valid loss: 6.72520558493478

Epoch: 58
Training loss: 0.3819587826728821 / Valid loss: 6.689128784906297
Training loss: 0.4916685223579407 / Valid loss: 6.735916532788958
Training loss: 0.3611557185649872 / Valid loss: 6.711106672741118
Training loss: 0.534123420715332 / Valid loss: 6.67636771656218
Training loss: 0.22396443784236908 / Valid loss: 6.724246406555176

Epoch: 59
Training loss: 0.2273387461900711 / Valid loss: 6.680069655463809
Training loss: 0.42568856477737427 / Valid loss: 6.669782334282285
Training loss: 0.3666840195655823 / Valid loss: 6.721442597252982
Training loss: 0.4067525565624237 / Valid loss: 6.671936652773902

Epoch: 60
Training loss: 0.34881871938705444 / Valid loss: 6.710927477337065
Training loss: 0.6154779195785522 / Valid loss: 6.726460093543643
Training loss: 0.23126104474067688 / Valid loss: 6.669532276335216
Training loss: 0.3360059857368469 / Valid loss: 6.721975612640381
Training loss: 0.39391452074050903 / Valid loss: 6.6899144717625205

Epoch: 61
Training loss: 0.44558197259902954 / Valid loss: 6.7684749989282516
Training loss: 0.20760712027549744 / Valid loss: 6.768717820303781
Training loss: 0.550693929195404 / Valid loss: 6.6894766716730025
Training loss: 0.41413360834121704 / Valid loss: 6.741762708482288
Training loss: 0.3054914176464081 / Valid loss: 6.715096432822091

Epoch: 62
Training loss: 0.15472835302352905 / Valid loss: 6.666775571732294
Training loss: 0.27374690771102905 / Valid loss: 6.676443340664818
Training loss: 0.1859350949525833 / Valid loss: 6.6890191532316665
Training loss: 0.36541715264320374 / Valid loss: 6.7309935161045615
Training loss: 0.4565538763999939 / Valid loss: 6.716866742996943

Epoch: 63
Training loss: 0.3241569399833679 / Valid loss: 6.617739561625889
Training loss: 0.2074940949678421 / Valid loss: 6.634667342049735
Training loss: 0.2874569594860077 / Valid loss: 6.625707553681873
Training loss: 0.4158697724342346 / Valid loss: 6.709875547318232
Training loss: 0.2488686442375183 / Valid loss: 6.7120491209484285

Epoch: 64
Training loss: 0.20841088891029358 / Valid loss: 6.670325551714216
Training loss: 0.17572864890098572 / Valid loss: 6.69535178002857
Training loss: 0.5030312538146973 / Valid loss: 6.683894500278291
Training loss: 0.2802730202674866 / Valid loss: 6.668777388618106
Training loss: 0.29570499062538147 / Valid loss: 6.65590085756211

Epoch: 65
Training loss: 0.21561826765537262 / Valid loss: 6.657608425049554
Training loss: 0.4194198250770569 / Valid loss: 6.685143191473824
Training loss: 0.19582059979438782 / Valid loss: 6.695774296351842
Training loss: 0.26846858859062195 / Valid loss: 6.663293915703183
Training loss: 1.200953483581543 / Valid loss: 6.636239810216995

Epoch: 66
Training loss: 0.20386239886283875 / Valid loss: 6.635817682175409
Training loss: 0.2242625504732132 / Valid loss: 6.6748885200137185
Training loss: 0.6934322118759155 / Valid loss: 6.612638042086647
Training loss: 0.2251012921333313 / Valid loss: 6.662507272901989
Training loss: 0.48607316613197327 / Valid loss: 6.653824186325073

Epoch: 67
Training loss: 0.28059470653533936 / Valid loss: 6.696042982737223
Training loss: 0.1762937605381012 / Valid loss: 6.748717868895758
Training loss: 0.23315705358982086 / Valid loss: 6.679172847384498
Training loss: 0.9508079886436462 / Valid loss: 6.686927713666644
Training loss: 0.2847082018852234 / Valid loss: 6.6408798830849785

Epoch: 68
Training loss: 0.2023329734802246 / Valid loss: 6.638641952332996
Training loss: 0.32127800583839417 / Valid loss: 6.684152616773333
Training loss: 0.22909942269325256 / Valid loss: 6.686097304026286
Training loss: 0.25614750385284424 / Valid loss: 6.698540562675113
Training loss: 0.2671150267124176 / Valid loss: 6.679765060969761

Epoch: 69
Training loss: 0.41309303045272827 / Valid loss: 6.706050623030889
Training loss: 0.2782416343688965 / Valid loss: 6.664101877666655
Training loss: 0.3861317038536072 / Valid loss: 6.688184724535261
Training loss: 0.31186074018478394 / Valid loss: 6.675781572432745

Epoch: 70
Training loss: 0.2071351408958435 / Valid loss: 6.688096720831735
Training loss: 0.5145910978317261 / Valid loss: 6.686439979644049
Training loss: 0.35519811511039734 / Valid loss: 6.624032397497268
Training loss: 0.3331165909767151 / Valid loss: 6.6752090204329715
Training loss: 0.4087202548980713 / Valid loss: 6.661813890366327

Epoch: 71
Training loss: 0.3515656292438507 / Valid loss: 6.696151760646275
Training loss: 0.5540785789489746 / Valid loss: 6.669122055598668
Training loss: 0.2891699969768524 / Valid loss: 6.689207860401699
Training loss: 0.23078607022762299 / Valid loss: 6.65930077234904
Training loss: 0.2650478482246399 / Valid loss: 6.637486718949818

Epoch: 72
Training loss: 0.22568164765834808 / Valid loss: 6.663834410622006
Training loss: 0.5306540727615356 / Valid loss: 6.627140587852114
Training loss: 0.2193690687417984 / Valid loss: 6.640688666843233
Training loss: 0.34747231006622314 / Valid loss: 6.651382991245815
Training loss: 0.5703251361846924 / Valid loss: 6.67217553229559

Epoch: 73
Training loss: 0.44613921642303467 / Valid loss: 6.643679614294143
Training loss: 0.42118895053863525 / Valid loss: 6.644640227726527
Training loss: 0.2059280276298523 / Valid loss: 6.678271491186959
Training loss: 0.2047095000743866 / Valid loss: 6.634040482838949
Training loss: 0.26169097423553467 / Valid loss: 6.679233503341675

Epoch: 74
Training loss: 0.21150918304920197 / Valid loss: 6.645373253595261
Training loss: 0.2562561631202698 / Valid loss: 6.603808500653222
Training loss: 0.4858044683933258 / Valid loss: 6.637781792595273
Training loss: 0.43979257345199585 / Valid loss: 6.656432694480532
Training loss: 0.2181214690208435 / Valid loss: 6.632199003582909

Epoch: 75
Training loss: 0.27400001883506775 / Valid loss: 6.663445495423817
Training loss: 0.3727610111236572 / Valid loss: 6.663954471406482
Training loss: 0.5653051733970642 / Valid loss: 6.638429282960438
Training loss: 0.20947265625 / Valid loss: 6.633637993676322
Training loss: 0.40302059054374695 / Valid loss: 6.609807087126232

Epoch: 76
Training loss: 0.319435179233551 / Valid loss: 6.619022117342268
Training loss: 0.468672513961792 / Valid loss: 6.669357222602481
Training loss: 0.17770695686340332 / Valid loss: 6.673665396372477
Training loss: 0.33879488706588745 / Valid loss: 6.613214574541364
Training loss: 0.27779558300971985 / Valid loss: 6.71521782875061

Epoch: 77
Training loss: 0.2996503710746765 / Valid loss: 6.683221017746698
Training loss: 0.35909560322761536 / Valid loss: 6.705450166974749
Training loss: 0.3195039629936218 / Valid loss: 6.642677543276832
Training loss: 0.41818860173225403 / Valid loss: 6.663578564780099
Training loss: 0.44790446758270264 / Valid loss: 6.616634650457473

Epoch: 78
Training loss: 0.36747539043426514 / Valid loss: 6.666807201930455
Training loss: 0.18169009685516357 / Valid loss: 6.6603870119367325
Training loss: 0.15213361382484436 / Valid loss: 6.6445341973077685
Training loss: 0.3188200891017914 / Valid loss: 6.673798642839704
Training loss: 0.49758583307266235 / Valid loss: 6.664251005081903

Epoch: 79
Training loss: 0.2632230520248413 / Valid loss: 6.633800034295945
Training loss: 0.34705644845962524 / Valid loss: 6.6278168405805316
Training loss: 0.1976604163646698 / Valid loss: 6.662951056162516
Training loss: 0.19321796298027039 / Valid loss: 6.697877266293481
ModuleList(
  (0): Linear(in_features=5376, out_features=2000, bias=True)
  (1): Dropout(p=0.0, inplace=False)
  (2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=2000, out_features=100, bias=True)
  (5): Dropout(p=0.0, inplace=False)
  (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=100, out_features=16, bias=True)
  (9): Dropout(p=0, inplace=False)
  (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (11): LeakyReLU(negative_slope=0.02)
  (12): Linear(in_features=16, out_features=1, bias=True)
)
Loss on test set of optimal model: 5.645615534555344
