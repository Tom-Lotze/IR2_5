********************************************************************************************
** WARNING: 'The 'pre2019' module environment is deprecated. Please consider switching
             to the '2019' or '2020' module environment. You can read more about our
             software policy on this page:
             https://userinfo.surfsara.nl/documentation/software-policy-lisacartesius

             If you have any question, please contact us via http://servicedesk.surfsara.nl.'
********************************************************************************************
Using device: cuda

GeForce GTX 1080 Ti
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.05
learning_rate : 0.001
nr_epochs : 100
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : True
weightdecay : 0.0001
momentum : 0.9
embedder : TFIDF
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=31191, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.05, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
neural net:
 [tensor([[ 1.4603e-03, -1.4531e-03,  2.1772e-03,  ...,  6.4021e-04,
         -2.6256e-03, -4.8083e-03],
        [ 4.7762e-03,  2.5457e-03,  2.0257e-03,  ..., -2.7362e-03,
         -1.1119e-03, -6.6421e-04],
        [-2.8967e-03, -4.1528e-03, -3.7788e-03,  ..., -4.1366e-03,
          9.5038e-05,  3.3919e-03],
        ...,
        [-2.3854e-03, -2.9863e-03,  2.0743e-03,  ..., -4.6862e-03,
          3.0439e-03,  3.5491e-03],
        [-3.2048e-03, -9.5860e-04,  1.3232e-04,  ..., -5.3426e-03,
         -9.2631e-05,  8.6153e-05],
        [-5.0829e-03,  4.0579e-04, -1.1557e-04,  ..., -4.7570e-03,
          2.0814e-03, -7.3431e-04]], device='cuda:0'), tensor([-4.2872e-03, -2.8137e-04, -4.2487e-03,  3.0075e-03,  3.8793e-03,
         2.1033e-03,  1.8394e-04, -5.0561e-03, -4.6186e-03, -2.0387e-03,
         4.8876e-03, -4.8692e-03, -3.6438e-03,  2.8436e-04, -2.8957e-03,
        -3.3653e-03, -3.2414e-03, -4.7570e-03, -1.4138e-03, -4.2962e-04,
        -1.5121e-03, -4.2443e-03,  3.7558e-03, -8.3244e-04,  2.8223e-03,
        -4.6388e-03, -3.8677e-03,  3.8929e-03,  1.5835e-03,  1.8667e-03,
         2.5174e-03, -2.1806e-03,  2.4941e-03, -3.0671e-03,  4.3574e-03,
         4.5217e-03,  6.7695e-04, -3.6519e-03,  3.7522e-03,  2.1329e-04,
        -4.4436e-03, -4.8648e-04,  4.7425e-03, -2.1365e-03, -5.3389e-03,
        -2.5912e-03,  3.3590e-03,  2.8456e-03, -1.1641e-03,  5.6059e-03,
         5.0873e-03,  1.4636e-03,  5.0301e-03, -4.6538e-03,  3.1408e-03,
         2.0524e-03, -6.6424e-04,  3.7065e-03, -5.2581e-04, -3.1312e-03,
         5.6386e-03,  2.6796e-03,  3.3508e-03,  1.3535e-03,  5.1095e-03,
        -4.2748e-03, -1.3317e-03, -5.1039e-03, -5.5110e-03,  4.0019e-03,
        -4.0129e-03, -5.6020e-03,  5.5239e-03, -2.4303e-03, -2.4823e-03,
        -4.4928e-03, -2.5715e-03,  4.9895e-03,  4.5194e-03,  4.4513e-03,
         3.2325e-03, -3.9907e-03,  1.6797e-03, -1.4459e-03, -1.3264e-04,
        -1.1353e-04, -1.3141e-03, -1.0645e-03,  1.9162e-04, -4.4247e-03,
         3.8981e-03,  4.4404e-03, -5.2092e-04,  4.6693e-03, -5.4588e-03,
         4.8128e-03, -2.6335e-03,  6.5991e-04,  2.0050e-03,  3.5056e-03,
        -5.5735e-03,  3.1568e-03,  2.8387e-04,  4.4501e-03, -2.9663e-03,
        -2.6960e-03,  2.6986e-03,  2.1970e-03, -3.1094e-03, -6.3469e-04,
         5.3475e-03,  3.9474e-03, -5.1097e-03,  5.1000e-03,  3.9965e-03,
        -1.5317e-03, -2.0813e-03,  2.6524e-03,  3.0959e-03,  4.3093e-03,
         1.0240e-03,  1.6123e-03, -1.0612e-03, -3.7665e-03,  3.4602e-03,
        -5.3727e-03,  1.8319e-03,  2.3601e-03, -2.8822e-03, -1.5455e-03,
        -2.1343e-03, -4.5842e-03,  1.4805e-03,  5.2766e-03, -5.3803e-03,
        -4.1220e-03, -7.0321e-04,  4.6787e-03, -3.5915e-03, -9.1436e-05,
         3.2326e-03, -2.7002e-04,  1.0660e-03, -5.4622e-03, -5.5190e-03,
        -3.9786e-03, -5.5439e-03, -5.2763e-03,  4.4491e-03,  1.1320e-04,
        -3.2675e-03,  5.5357e-03, -2.2430e-03,  5.0122e-03,  6.2611e-04,
         4.7414e-03,  5.0886e-03,  1.6699e-03,  2.3793e-03, -5.4903e-03,
         4.5676e-03,  5.3202e-03, -7.2416e-04, -1.7050e-03,  7.7869e-04,
        -2.5147e-03, -1.0416e-03, -1.9439e-03, -5.4500e-03,  4.4211e-03,
         2.8368e-03,  1.0308e-03, -3.0901e-03, -4.1916e-03, -3.4752e-03,
        -5.2389e-03,  2.5899e-05,  3.4031e-03,  3.3484e-04,  5.1864e-03,
         2.4938e-03,  3.4427e-03,  4.7742e-03, -4.7518e-03, -3.6168e-03,
         5.3644e-03,  7.9528e-04, -8.2803e-04,  5.4880e-04,  4.0646e-03,
        -5.4084e-03,  5.5048e-04, -5.3544e-03,  2.4839e-03, -3.7883e-03,
         5.3698e-03,  2.6407e-03, -4.1027e-04,  3.6338e-03,  4.6782e-03,
        -1.8032e-03,  1.4101e-03,  5.0021e-03, -4.7550e-03, -1.1075e-03,
        -3.3914e-04, -1.9021e-03,  3.1109e-03,  1.5869e-03,  4.1054e-03,
        -5.5542e-03,  1.3445e-03, -1.9080e-04,  1.6222e-03,  2.6826e-04,
        -1.0208e-03,  4.4000e-03, -5.5002e-03,  3.3561e-03, -3.0085e-03,
         4.5116e-03, -4.7058e-03, -5.2870e-03,  3.8890e-03,  5.2510e-03,
        -5.5240e-03, -2.0113e-04,  4.2917e-04, -3.6603e-03, -2.8450e-03,
         2.2309e-03, -2.5128e-03,  5.1836e-03,  3.1270e-03, -3.4699e-03,
         2.7425e-03, -4.8151e-03, -3.4544e-03, -4.5859e-03, -3.8935e-03,
        -2.5022e-03, -5.1751e-03, -1.3348e-03, -8.6909e-04,  4.7190e-04,
         1.0742e-03,  4.5017e-03,  2.8497e-03,  1.7222e-03,  3.4807e-03,
        -9.4850e-04,  2.5664e-03,  3.1178e-03,  7.8251e-04,  3.0384e-03,
         4.7425e-03, -2.2457e-03, -4.5466e-03, -3.4947e-03,  2.3713e-03,
        -1.1498e-03, -7.0857e-04,  5.0506e-03,  3.6106e-03, -7.6323e-04,
         3.3671e-03,  3.0555e-03, -4.3535e-03,  1.1326e-03, -2.5999e-03,
         3.2402e-03,  1.9335e-04, -7.7986e-04,  3.9947e-03,  4.2541e-03,
         5.4228e-03, -4.9988e-03, -1.7690e-03, -4.1806e-03,  2.6264e-03,
        -3.0971e-03, -2.4463e-03,  4.4550e-03, -2.5655e-03,  2.4357e-03,
        -4.3450e-03, -4.2508e-03,  4.9932e-03, -4.7407e-03, -5.1958e-03,
        -5.1343e-03, -2.5251e-03,  3.5387e-03, -5.3392e-03, -5.0818e-03,
        -3.5292e-03, -1.8343e-03,  2.8704e-03, -7.9873e-04, -4.8465e-03],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), tensor([[ 0.0297, -0.0364, -0.0548,  ...,  0.0235, -0.0288,  0.0187],
        [ 0.0350, -0.0002, -0.0224,  ...,  0.0047,  0.0151,  0.0315],
        [-0.0100, -0.0457,  0.0471,  ...,  0.0423,  0.0520, -0.0191],
        ...,
        [ 0.0206, -0.0263, -0.0121,  ...,  0.0101,  0.0072,  0.0018],
        [-0.0452,  0.0335,  0.0477,  ..., -0.0206, -0.0181,  0.0116],
        [ 0.0414,  0.0409, -0.0383,  ..., -0.0111, -0.0377,  0.0287]],
       device='cuda:0'), tensor([-0.0141, -0.0165, -0.0269, -0.0492, -0.0059, -0.0315, -0.0040, -0.0547,
         0.0228, -0.0319,  0.0218, -0.0356, -0.0496,  0.0501, -0.0114, -0.0289,
        -0.0311,  0.0162, -0.0490,  0.0342,  0.0049,  0.0413, -0.0363,  0.0307,
         0.0461,  0.0230,  0.0222, -0.0027, -0.0041, -0.0047,  0.0353, -0.0317],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), tensor([[-0.0668, -0.0564,  0.1272, -0.0653,  0.0422, -0.0853, -0.1272, -0.1251,
          0.1655,  0.0482,  0.0848,  0.1497, -0.1656, -0.0278, -0.1610,  0.0195,
         -0.0068, -0.0712, -0.0536,  0.0346,  0.1110,  0.0094,  0.0820, -0.1289,
          0.1166,  0.0536, -0.1625,  0.0917, -0.0575, -0.0731,  0.1686, -0.0517]],
       device='cuda:0'), tensor([0.1101], device='cuda:0')]

Epoch: 0
Training loss: 14.43508529663086 / Valid loss: 15.70700608208066
Training loss: 5.806911945343018 / Valid loss: 6.039643687293643
Training loss: 4.984628677368164 / Valid loss: 5.707058107285272
Training loss: 4.1456074714660645 / Valid loss: 5.7050646214258105
Training loss: 4.240859031677246 / Valid loss: 5.79888429187593

Epoch: 1
Training loss: 6.23227071762085 / Valid loss: 5.603230442319598
Training loss: 4.890556335449219 / Valid loss: 5.763649933678764
Training loss: 4.864645004272461 / Valid loss: 5.849528199150448
Training loss: 6.274462699890137 / Valid loss: 5.538008194877988
Training loss: 6.921233177185059 / Valid loss: 5.714519396282378

Epoch: 2
Training loss: 4.323671340942383 / Valid loss: 5.6688309578668505
Training loss: 4.790783882141113 / Valid loss: 6.142070284343901
Training loss: 4.67188835144043 / Valid loss: 6.2631026540483745
Training loss: 5.122450828552246 / Valid loss: 6.954346000580561
Training loss: 4.310911655426025 / Valid loss: 6.454893094017392

Epoch: 3
Training loss: 3.958191394805908 / Valid loss: 5.940739558991932
Training loss: 4.648426532745361 / Valid loss: 5.981432871591477
Training loss: 3.5918796062469482 / Valid loss: 5.942988595508393
Training loss: 3.9828577041625977 / Valid loss: 5.848102807998657
Training loss: 4.403785705566406 / Valid loss: 6.408702902566819

Epoch: 4
Training loss: 2.8678388595581055 / Valid loss: 5.869504608426776
Training loss: 3.3809187412261963 / Valid loss: 6.070954967680431
Training loss: 4.432804107666016 / Valid loss: 6.183844112214588
Training loss: 3.507838249206543 / Valid loss: 6.03640482993353
Training loss: 4.544912338256836 / Valid loss: 6.181197466169085

Epoch: 5
Training loss: 2.974426507949829 / Valid loss: 6.108203851609003
Training loss: 3.3637824058532715 / Valid loss: 6.8492277871994744
Training loss: 3.486255168914795 / Valid loss: 6.678690324510847
Training loss: 3.006758689880371 / Valid loss: 6.208817388897851
Training loss: 3.255690097808838 / Valid loss: 6.5800786926632835

Epoch: 6
Training loss: 2.749117136001587 / Valid loss: 6.177335509799776
Training loss: 3.530362367630005 / Valid loss: 6.404930430366879
Training loss: 3.305222988128662 / Valid loss: 6.54715530531747
Training loss: 2.6659040451049805 / Valid loss: 7.3125747862316315
Training loss: 3.033618211746216 / Valid loss: 6.754833003452846

Epoch: 7
Training loss: 1.837477684020996 / Valid loss: 6.366491531190418
Training loss: 3.1778297424316406 / Valid loss: 6.516170765104748
Training loss: 2.498274803161621 / Valid loss: 6.583873038064866
Training loss: 2.8637847900390625 / Valid loss: 6.3327438922155475
Training loss: 2.8923559188842773 / Valid loss: 6.301718591508411

Epoch: 8
Training loss: 2.617459774017334 / Valid loss: 6.3233329227992465
Training loss: 1.7933489084243774 / Valid loss: 6.344354084559849
Training loss: 2.2205705642700195 / Valid loss: 6.544272254762196
Training loss: 2.029939889907837 / Valid loss: 6.70663929893857
Training loss: 1.7568018436431885 / Valid loss: 6.545516061782837

Epoch: 9
Training loss: 1.8082631826400757 / Valid loss: 6.61060391834804
Training loss: 1.0904574394226074 / Valid loss: 6.691169934045701
Training loss: 1.9652760028839111 / Valid loss: 6.430512437366304
Training loss: 2.038214921951294 / Valid loss: 6.618647430056617
Training loss: 2.9309089183807373 / Valid loss: 6.6302271070934475

Epoch: 10
Training loss: 1.5624595880508423 / Valid loss: 6.362307062603178
Training loss: 1.7078862190246582 / Valid loss: 6.848728661310105
Training loss: 1.5236930847167969 / Valid loss: 6.764141137259347
Training loss: 1.8336997032165527 / Valid loss: 6.836758318401518
Training loss: 1.6639389991760254 / Valid loss: 6.914831411270868

Epoch: 11
Training loss: 1.1489155292510986 / Valid loss: 6.8736739953358965
Training loss: 2.4694502353668213 / Valid loss: 6.816507107870919
Training loss: 1.3698331117630005 / Valid loss: 6.564934467134021
Training loss: 1.7200263738632202 / Valid loss: 6.6861953417460125
Training loss: 1.9693816900253296 / Valid loss: 6.967013858613514

Epoch: 12
Training loss: 1.8533744812011719 / Valid loss: 6.666253593989781
Training loss: 1.5561846494674683 / Valid loss: 6.931498150598435
Training loss: 1.2875230312347412 / Valid loss: 6.606097748166039
Training loss: 2.0047478675842285 / Valid loss: 6.868746062687465
Training loss: 1.8059061765670776 / Valid loss: 6.516999017624628

Epoch: 13
Training loss: 2.385989189147949 / Valid loss: 6.680470627830142
Training loss: 1.7457984685897827 / Valid loss: 6.580384990147182
Training loss: 1.66440749168396 / Valid loss: 6.802100020363217
Training loss: 1.259628176689148 / Valid loss: 6.955012462252662
Training loss: 1.6810646057128906 / Valid loss: 6.6562538760049

Epoch: 14
Training loss: 1.1401861906051636 / Valid loss: 7.223111055010841
Training loss: 1.8446342945098877 / Valid loss: 6.823634769803002
Training loss: 1.3077315092086792 / Valid loss: 6.972215116591681
Training loss: 1.4173754453659058 / Valid loss: 6.567668378920782
Training loss: 1.4572291374206543 / Valid loss: 6.701854276657104

Epoch: 15
Training loss: 1.0117640495300293 / Valid loss: 6.656002121879941
Training loss: 1.506253719329834 / Valid loss: 6.617264743078323
Training loss: 0.6808078289031982 / Valid loss: 6.904321665990921
Training loss: 1.0979596376419067 / Valid loss: 6.711217757633754
Training loss: 1.3785204887390137 / Valid loss: 6.727352151416597

Epoch: 16
Training loss: 0.8978800177574158 / Valid loss: 6.711272171565464
Training loss: 1.0393953323364258 / Valid loss: 6.707830769675119
Training loss: 1.5142625570297241 / Valid loss: 7.055029364994594
Training loss: 1.349472999572754 / Valid loss: 6.728325907389323
Training loss: 1.4896153211593628 / Valid loss: 6.630430269241333

Epoch: 17
Training loss: 1.3129103183746338 / Valid loss: 6.921214071909587
Training loss: 1.5391227006912231 / Valid loss: 7.000782943907238
Training loss: 1.194145679473877 / Valid loss: 6.886785643441336
Training loss: 1.6119108200073242 / Valid loss: 6.503174618312291
Training loss: 0.5685034394264221 / Valid loss: 6.943393534705752

Epoch: 18
Training loss: 1.0744073390960693 / Valid loss: 6.864866025107247
Training loss: 1.2957450151443481 / Valid loss: 6.886594736008417
Training loss: 1.0948817729949951 / Valid loss: 6.9729691460019065
Training loss: 1.062009334564209 / Valid loss: 6.770037419455392
Training loss: 1.2929012775421143 / Valid loss: 6.767581095014299

Epoch: 19
Training loss: 0.964250922203064 / Valid loss: 6.880395212627593
Training loss: 0.7504986524581909 / Valid loss: 6.787756029764811
Training loss: 1.271336317062378 / Valid loss: 6.759092751003447
Training loss: 1.0166444778442383 / Valid loss: 6.867658955710275
Training loss: 0.9823315143585205 / Valid loss: 6.929263033185687

Epoch: 20
Training loss: 0.8529847860336304 / Valid loss: 6.895277563730875
Training loss: 1.02128267288208 / Valid loss: 6.654011290413993
Training loss: 1.2666579484939575 / Valid loss: 6.676991682960874
Training loss: 1.1584852933883667 / Valid loss: 6.890815934680757
Training loss: 1.9108984470367432 / Valid loss: 6.813810271308536

Epoch: 21
Training loss: 0.6549380421638489 / Valid loss: 6.701768902369908
Training loss: 1.2348124980926514 / Valid loss: 6.731303187779018
Training loss: 1.3423840999603271 / Valid loss: 6.783470621563139
Training loss: 1.0403733253479004 / Valid loss: 7.009909852345785
Training loss: 0.8187627792358398 / Valid loss: 6.722082955496652

Epoch: 22
Training loss: 0.7971802949905396 / Valid loss: 6.774026484716506
Training loss: 0.579062819480896 / Valid loss: 6.876524857112339
Training loss: 1.7729334831237793 / Valid loss: 6.797220445814586
Training loss: 1.0328435897827148 / Valid loss: 6.650057147798084
Training loss: 0.8587691783905029 / Valid loss: 6.932350858052572

Epoch: 23
Training loss: 0.9896079897880554 / Valid loss: 6.625829272043138
Training loss: 0.860556423664093 / Valid loss: 6.6483321916489375
Training loss: 0.5784403085708618 / Valid loss: 6.838297421591623
Training loss: 1.1863245964050293 / Valid loss: 6.737479232606434
Training loss: 1.4567608833312988 / Valid loss: 6.862365927015032

Epoch: 24
Training loss: 0.723570704460144 / Valid loss: 6.867839395432245
Training loss: 0.9448079466819763 / Valid loss: 6.742198764710199
Training loss: 1.2139077186584473 / Valid loss: 6.804561240332467
Training loss: 0.8143652677536011 / Valid loss: 6.849922091620309
Training loss: 0.9512346386909485 / Valid loss: 6.861515422094436

Epoch: 25
Training loss: 1.670403003692627 / Valid loss: 6.647811828340803
Training loss: 1.2795581817626953 / Valid loss: 6.69390968368167
Training loss: 0.9436565041542053 / Valid loss: 6.803208105904716
Training loss: 0.7704916596412659 / Valid loss: 6.707381225767589
Training loss: 1.0831050872802734 / Valid loss: 6.868210588182722

Epoch: 26
Training loss: 0.4946228861808777 / Valid loss: 6.831779116675968
Training loss: 0.9512187838554382 / Valid loss: 6.6638947895595
Training loss: 1.3868534564971924 / Valid loss: 6.750208940960112
Training loss: 1.2137254476547241 / Valid loss: 6.780607268923805
Training loss: 0.9315159320831299 / Valid loss: 6.711378964923677

Epoch: 27
Training loss: 0.5734151601791382 / Valid loss: 6.567165065947033
Training loss: 1.1397590637207031 / Valid loss: 6.728134759267172
Training loss: 1.3040151596069336 / Valid loss: 6.877537618364607
Training loss: 0.6874693036079407 / Valid loss: 6.979436211358934
Training loss: 0.9514347314834595 / Valid loss: 7.0067513511294415

Epoch: 28
Training loss: 0.37154000997543335 / Valid loss: 6.811716179620652
Training loss: 0.6182929873466492 / Valid loss: 6.705300462813605
Training loss: 0.5758489370346069 / Valid loss: 6.833121177128383
Training loss: 1.100707769393921 / Valid loss: 6.847500494548252
Training loss: 0.7749161720275879 / Valid loss: 6.756613690512521

Epoch: 29
Training loss: 0.5696996450424194 / Valid loss: 6.735395965122041
Training loss: 1.459810495376587 / Valid loss: 6.891812506176176
Training loss: 0.5179651975631714 / Valid loss: 6.889649214063372
Training loss: 1.3662595748901367 / Valid loss: 6.6508291062854585
Training loss: 1.272326111793518 / Valid loss: 6.618229014532907

Epoch: 30
Training loss: 1.0659964084625244 / Valid loss: 6.699658457438151
Training loss: 0.42629268765449524 / Valid loss: 6.719439733596075
Training loss: 0.5407994985580444 / Valid loss: 6.859870070502872
Training loss: 0.6403616070747375 / Valid loss: 6.973296247209821
Training loss: 0.912874698638916 / Valid loss: 6.853565765562512

Epoch: 31
Training loss: 0.7526288032531738 / Valid loss: 6.843948545910063
Training loss: 0.8177490830421448 / Valid loss: 6.816964278902327
Training loss: 0.5504616498947144 / Valid loss: 6.650111057644799
Training loss: 0.48164111375808716 / Valid loss: 6.763202721732004
Training loss: 0.8625043630599976 / Valid loss: 6.900519502730597

Epoch: 32
Training loss: 1.785025954246521 / Valid loss: 6.835835797446115
Training loss: 0.5799094438552856 / Valid loss: 6.899351533253988
Training loss: 0.6386349201202393 / Valid loss: 6.701279067993164
Training loss: 0.9411942362785339 / Valid loss: 6.626535710834322
Training loss: 1.034902811050415 / Valid loss: 6.710639058975946

Epoch: 33
Training loss: 0.43139249086380005 / Valid loss: 6.736893885476249
Training loss: 0.8437517881393433 / Valid loss: 6.870060972940355
Training loss: 0.6771745681762695 / Valid loss: 6.650549146107265
Training loss: 0.6462082862854004 / Valid loss: 6.609754312606085
Training loss: 0.5594222545623779 / Valid loss: 6.862153144109817

Epoch: 34
Training loss: 0.800857424736023 / Valid loss: 6.789800126211984
Training loss: 0.9795212745666504 / Valid loss: 6.781261789231073
Training loss: 0.7125125527381897 / Valid loss: 6.7560112771533785
Training loss: 0.6869948506355286 / Valid loss: 6.788377707345145
Training loss: 1.165325403213501 / Valid loss: 6.774565392448789

Epoch: 35
Training loss: 0.7988170385360718 / Valid loss: 6.872665041968936
Training loss: 0.4931814670562744 / Valid loss: 6.579067109879993
Training loss: 0.36421719193458557 / Valid loss: 6.800093791598365
Training loss: 0.8726500868797302 / Valid loss: 6.691855221702939
Training loss: 0.5309679508209229 / Valid loss: 6.904933470771426

Epoch: 36
Training loss: 0.5023792386054993 / Valid loss: 6.907184341975621
Training loss: 0.547666609287262 / Valid loss: 6.612311942236764
Training loss: 0.549466609954834 / Valid loss: 6.6904490879603795
Training loss: 0.4406430125236511 / Valid loss: 6.857480103628976
Training loss: 0.7058720588684082 / Valid loss: 6.827255848475865

Epoch: 37
Training loss: 0.548691987991333 / Valid loss: 6.800240289597284
Training loss: 0.7679964900016785 / Valid loss: 6.7960723786127
Training loss: 0.8943206667900085 / Valid loss: 6.834389759245373
Training loss: 0.4491361975669861 / Valid loss: 6.8290522666204545
Training loss: 0.7137494683265686 / Valid loss: 6.7002501396905805

Epoch: 38
Training loss: 0.6306920051574707 / Valid loss: 6.916773689360846
Training loss: 0.9951966404914856 / Valid loss: 6.673320609047299
Training loss: 0.4434407353401184 / Valid loss: 6.946395553861346
Training loss: 0.43914055824279785 / Valid loss: 6.778911849430629
Training loss: 0.7572067975997925 / Valid loss: 6.770140502566383

Epoch: 39
Training loss: 0.7824007272720337 / Valid loss: 6.71154534476144
Training loss: 0.777877688407898 / Valid loss: 6.573737203507196
Training loss: 0.7280241847038269 / Valid loss: 6.759626088823591
Training loss: 0.8642131090164185 / Valid loss: 6.713016859690348
Training loss: 0.758922815322876 / Valid loss: 6.663397557394845

Epoch: 40
Training loss: 0.6290934681892395 / Valid loss: 6.826661010015578
Training loss: 0.743506669998169 / Valid loss: 6.698313358851841
Training loss: 0.7507015466690063 / Valid loss: 6.646430133637928
Training loss: 0.7798920273780823 / Valid loss: 6.7833158311389745
Training loss: 0.33230867981910706 / Valid loss: 6.755115990411667

Epoch: 41
Training loss: 0.8875288367271423 / Valid loss: 6.74621855872018
Training loss: 0.622032880783081 / Valid loss: 6.649997588566372
Training loss: 1.2998669147491455 / Valid loss: 6.671483434949603
Training loss: 1.0486925840377808 / Valid loss: 6.619525182814825
Training loss: 0.4404774010181427 / Valid loss: 6.744084151585897

Epoch: 42
Training loss: 0.45793047547340393 / Valid loss: 6.645376296270461
Training loss: 0.5274839997291565 / Valid loss: 6.758569145202637
Training loss: 0.9727970361709595 / Valid loss: 6.587997039159139
Training loss: 0.5780758261680603 / Valid loss: 6.776440911065965
Training loss: 0.6159061789512634 / Valid loss: 6.816785646620251

Epoch: 43
Training loss: 0.5109121203422546 / Valid loss: 6.798910274959746
Training loss: 0.6098017692565918 / Valid loss: 6.751562331971668
Training loss: 1.0545520782470703 / Valid loss: 6.755255964824132
Training loss: 0.6616754531860352 / Valid loss: 6.633392799468267
Training loss: 0.4284782409667969 / Valid loss: 6.841932476134527

Epoch: 44
Training loss: 0.5669259428977966 / Valid loss: 6.78246035802932
Training loss: 0.718971848487854 / Valid loss: 6.782122330438523
Training loss: 0.5402103662490845 / Valid loss: 6.837366730826242
Training loss: 0.3015294671058655 / Valid loss: 6.68420268921625
Training loss: 0.4662964940071106 / Valid loss: 6.7551656745729

Epoch: 45
Training loss: 1.0607757568359375 / Valid loss: 6.693910339900426
Training loss: 1.0612114667892456 / Valid loss: 6.845968786875407
Training loss: 0.4555680453777313 / Valid loss: 6.778778616587321
Training loss: 0.6371443867683411 / Valid loss: 6.800350275493804
Training loss: 0.3952941298484802 / Valid loss: 6.838850993201846

Epoch: 46
Training loss: 0.45157769322395325 / Valid loss: 6.761674270175752
Training loss: 0.45437318086624146 / Valid loss: 6.61201304481143
Training loss: 0.5445019006729126 / Valid loss: 6.756317066010975
Training loss: 0.8602228760719299 / Valid loss: 6.68449551264445
Training loss: 0.798506498336792 / Valid loss: 6.663473783220564

Epoch: 47
Training loss: 0.3348437547683716 / Valid loss: 6.531182361784436
Training loss: 0.6669518947601318 / Valid loss: 6.757806948253086
Training loss: 0.6650765538215637 / Valid loss: 6.659172775631859
Training loss: 0.41985124349594116 / Valid loss: 6.720545119330996
Training loss: 0.33098548650741577 / Valid loss: 6.66086676461356

Epoch: 48
Training loss: 0.430527001619339 / Valid loss: 6.625119699750628
Training loss: 0.5794103145599365 / Valid loss: 6.559008975256057
Training loss: 0.5385177135467529 / Valid loss: 6.730686033339728
Training loss: 0.5853655934333801 / Valid loss: 6.717574682689849
Training loss: 0.6266592741012573 / Valid loss: 6.592255231312343

Epoch: 49
Training loss: 0.4317992627620697 / Valid loss: 6.761347593579973
Training loss: 0.35583728551864624 / Valid loss: 6.772642453511556
Training loss: 0.5154427886009216 / Valid loss: 6.662661000660488
Training loss: 0.5778061151504517 / Valid loss: 6.580588788077945
Training loss: 0.6744961738586426 / Valid loss: 6.515518492744082

Epoch: 50
Training loss: 0.7296337485313416 / Valid loss: 6.57850904010591
Training loss: 0.6536073684692383 / Valid loss: 6.8097014904022215
Training loss: 0.39819246530532837 / Valid loss: 6.7532709666660855
Training loss: 0.6055879592895508 / Valid loss: 6.724706865492321
Training loss: 0.5882235169410706 / Valid loss: 6.689198800495693

Epoch: 51
Training loss: 0.8099554777145386 / Valid loss: 6.608710902077811
Training loss: 0.4761649966239929 / Valid loss: 6.721000603267124
Training loss: 0.41082262992858887 / Valid loss: 6.592151884805588
Training loss: 0.44695085287094116 / Valid loss: 6.576157928648449
Training loss: 0.462213397026062 / Valid loss: 6.813709259033203

Epoch: 52
Training loss: 0.9153731465339661 / Valid loss: 6.762073991412208
Training loss: 1.2566163539886475 / Valid loss: 6.575543594360352
Training loss: 0.4556870460510254 / Valid loss: 6.646929527464367
Training loss: 0.3879019618034363 / Valid loss: 6.536136504581997
Training loss: 0.7337977886199951 / Valid loss: 6.683533872876849

Epoch: 53
Training loss: 0.5171126127243042 / Valid loss: 6.779945146469843
Training loss: 0.2871229350566864 / Valid loss: 6.641764840625581
Training loss: 0.4324432909488678 / Valid loss: 6.779971263522193
Training loss: 0.4715580642223358 / Valid loss: 6.844323074249994
Training loss: 0.6013810634613037 / Valid loss: 6.629532963888986

Epoch: 54
Training loss: 0.7380712628364563 / Valid loss: 6.777848706926618
Training loss: 0.48603102564811707 / Valid loss: 6.8526777835119335
Training loss: 0.42923998832702637 / Valid loss: 6.628343927292597
Training loss: 0.9167628288269043 / Valid loss: 6.75283427692595
Training loss: 0.6005866527557373 / Valid loss: 6.679623295011974

Epoch: 55
Training loss: 0.4162328243255615 / Valid loss: 6.608815717697143
Training loss: 0.6182903051376343 / Valid loss: 6.763143750599452
Training loss: 0.32568269968032837 / Valid loss: 6.694594855535598
Training loss: 0.4630584120750427 / Valid loss: 6.7386764208475745
Training loss: 0.2813263237476349 / Valid loss: 6.748376133328392

Epoch: 56
Training loss: 0.5165181159973145 / Valid loss: 6.672450208663941
Training loss: 0.6525261998176575 / Valid loss: 6.544176292419434
Training loss: 0.5696641206741333 / Valid loss: 6.760293143136161
Training loss: 0.638335108757019 / Valid loss: 6.628197156815302
Training loss: 0.3663899898529053 / Valid loss: 6.685630687077841

Epoch: 57
Training loss: 0.4255680441856384 / Valid loss: 6.742789386567615
Training loss: 0.6599527597427368 / Valid loss: 6.711997029894874
Training loss: 0.5540198087692261 / Valid loss: 6.700900563739595
Training loss: 0.5195808410644531 / Valid loss: 6.76486105691819
Training loss: 0.40096694231033325 / Valid loss: 6.479988888331822

Epoch: 58
Training loss: 0.42670202255249023 / Valid loss: 6.59949441183181
Training loss: 0.5470710396766663 / Valid loss: 6.923846085866292
Training loss: 0.5943847894668579 / Valid loss: 6.697584274836949
Training loss: 0.29198020696640015 / Valid loss: 6.776324190412249
Training loss: 0.6397751569747925 / Valid loss: 6.642466735839844

Epoch: 59
Training loss: 0.3833427131175995 / Valid loss: 6.760351510274978
Training loss: 0.3219681978225708 / Valid loss: 6.6330857072557725
Training loss: 0.44699251651763916 / Valid loss: 6.761754006431216
Training loss: 0.5087723731994629 / Valid loss: 6.66353566305978
Training loss: 0.7874751091003418 / Valid loss: 6.646181678771972

Epoch: 60
Training loss: 0.6378201842308044 / Valid loss: 6.734005414871943
Training loss: 0.48924264311790466 / Valid loss: 6.739629581996373
Training loss: 0.46358004212379456 / Valid loss: 6.816499691917783
Training loss: 0.2515760064125061 / Valid loss: 6.82413372085208
Training loss: 1.0761346817016602 / Valid loss: 6.7100111030396965

Epoch: 61
Training loss: 0.3667822480201721 / Valid loss: 6.579935001191639
Training loss: 0.4097406566143036 / Valid loss: 6.693127950032552
Training loss: 0.457866370677948 / Valid loss: 6.658247684297107
Training loss: 0.5337554812431335 / Valid loss: 6.767969494774228
Training loss: 0.8442045450210571 / Valid loss: 6.709710852305094

Epoch: 62
Training loss: 1.2201144695281982 / Valid loss: 6.823530185790289
Training loss: 0.48616814613342285 / Valid loss: 6.68135834194365
Training loss: 0.41575098037719727 / Valid loss: 6.698352745601109
Training loss: 1.038379192352295 / Valid loss: 6.676197978428432
Training loss: 0.5227100849151611 / Valid loss: 6.900255455289568

Epoch: 63
Training loss: 0.4170846939086914 / Valid loss: 6.85176214490618
Training loss: 0.8303258419036865 / Valid loss: 6.672611935933431
Training loss: 0.36357954144477844 / Valid loss: 6.717600400107248
Training loss: 0.5806604623794556 / Valid loss: 6.743711419332595
Training loss: 0.40274345874786377 / Valid loss: 6.649262995946975

Epoch: 64
Training loss: 0.3940083980560303 / Valid loss: 6.570825227101644
Training loss: 0.580193281173706 / Valid loss: 6.638476371765137
Training loss: 0.36709192395210266 / Valid loss: 6.756815124693371
Training loss: 0.41622161865234375 / Valid loss: 6.687672692253476
Training loss: 0.27800965309143066 / Valid loss: 6.685191669918242

Epoch: 65
Training loss: 0.35171133279800415 / Valid loss: 6.655673685527983
Training loss: 0.495930016040802 / Valid loss: 6.674061870574951
Training loss: 0.3678845167160034 / Valid loss: 6.743474288213821
Training loss: 0.2816295027732849 / Valid loss: 6.610894493829637
Training loss: 0.34460514783859253 / Valid loss: 6.716750156311762

Epoch: 66
Training loss: 0.35587361454963684 / Valid loss: 6.527751806804112
Training loss: 0.5625596046447754 / Valid loss: 6.643417553674607
Training loss: 0.4520612359046936 / Valid loss: 6.756428800310407
Training loss: 0.5134771466255188 / Valid loss: 6.735524039041429
Training loss: 0.5200262665748596 / Valid loss: 6.708139097122919

Epoch: 67
Training loss: 0.2513011693954468 / Valid loss: 6.617162386576335
Training loss: 0.35968753695487976 / Valid loss: 6.64072079431443
Training loss: 0.80804443359375 / Valid loss: 6.796839423406691
Training loss: 0.2644164562225342 / Valid loss: 6.585559134256272
Training loss: 0.5022660493850708 / Valid loss: 6.631233351571219

Epoch: 68
Training loss: 0.3874362111091614 / Valid loss: 6.6090742860521585
Training loss: 0.3911052346229553 / Valid loss: 6.809581064042591
Training loss: 0.3793623149394989 / Valid loss: 6.690119897751581
Training loss: 0.2939475178718567 / Valid loss: 6.654219466163998
Training loss: 0.4318283200263977 / Valid loss: 6.735419645763579

Epoch: 69
Training loss: 0.3240770995616913 / Valid loss: 6.7214768409729
Training loss: 0.3924916088581085 / Valid loss: 6.487458149592082
Training loss: 0.6286175847053528 / Valid loss: 6.7054387410481775
Training loss: 0.7217611074447632 / Valid loss: 6.6483073279971165
Training loss: 0.6212203502655029 / Valid loss: 6.70787779490153

Epoch: 70
Training loss: 0.3784211575984955 / Valid loss: 6.699981080918085
Training loss: 0.3135164678096771 / Valid loss: 6.780148730959211
Training loss: 0.8880934119224548 / Valid loss: 6.7381793340047205
Training loss: 0.3651953935623169 / Valid loss: 6.702301175253732
Training loss: 0.394470751285553 / Valid loss: 6.549035344805036

Epoch: 71
Training loss: 0.4693302810192108 / Valid loss: 6.582528559366862
Training loss: 0.3120913505554199 / Valid loss: 6.6912448065621515
Training loss: 0.3997476100921631 / Valid loss: 6.464511242366973
Training loss: 0.5591268539428711 / Valid loss: 6.700605083647228
Training loss: 0.4515921473503113 / Valid loss: 6.691401470275152

Epoch: 72
Training loss: 0.5952092409133911 / Valid loss: 6.612079293387277
Training loss: 0.4618127942085266 / Valid loss: 6.853043910435268
Training loss: 0.30834537744522095 / Valid loss: 6.653526451474145
Training loss: 0.5471241474151611 / Valid loss: 6.569553032375517
Training loss: 0.32901012897491455 / Valid loss: 6.817990371159145

Epoch: 73
Training loss: 0.43391403555870056 / Valid loss: 6.780657050723121
Training loss: 0.5821295380592346 / Valid loss: 6.636933776310512
Training loss: 0.42057862877845764 / Valid loss: 6.911446873346964
Training loss: 0.23337236046791077 / Valid loss: 6.710179630915324
Training loss: 0.4067104756832123 / Valid loss: 6.636515326727004

Epoch: 74
Training loss: 0.44496434926986694 / Valid loss: 6.717452405747913
Training loss: 0.5017589330673218 / Valid loss: 6.788089570545015
Training loss: 0.9968310594558716 / Valid loss: 6.719906275612967
Training loss: 0.5026737451553345 / Valid loss: 6.817033454350063
Training loss: 0.3100823163986206 / Valid loss: 6.659475044977097

Epoch: 75
Training loss: 0.5519582629203796 / Valid loss: 6.699331483386811
Training loss: 0.3611642122268677 / Valid loss: 6.648033466793242
Training loss: 0.4851278066635132 / Valid loss: 6.652384483246577
Training loss: 0.3668864965438843 / Valid loss: 6.596852493286133
Training loss: 0.7179313898086548 / Valid loss: 6.705369758605957

Epoch: 76
Training loss: 0.4254438877105713 / Valid loss: 6.797626633871169
Training loss: 0.370421826839447 / Valid loss: 6.669719920839582
Training loss: 0.5580277442932129 / Valid loss: 6.73507418405442
Training loss: 0.4136580228805542 / Valid loss: 6.614050188518706
Training loss: 0.7285890579223633 / Valid loss: 6.704083737872896

Epoch: 77
Training loss: 0.7076737284660339 / Valid loss: 6.56528156598409
Training loss: 0.44199228286743164 / Valid loss: 6.751853050504412
Training loss: 0.45421987771987915 / Valid loss: 6.540453038896833
Training loss: 0.6731060743331909 / Valid loss: 6.564408370426723
Training loss: 0.43935438990592957 / Valid loss: 6.717085988180978

Epoch: 78
Training loss: 0.6753875017166138 / Valid loss: 6.5928116707574755
Training loss: 0.33771181106567383 / Valid loss: 6.687130769093831
Training loss: 0.3599472641944885 / Valid loss: 6.6629590352376304
Training loss: 0.8119645714759827 / Valid loss: 6.525113943644932
Training loss: 0.29098471999168396 / Valid loss: 6.593925149100167

Epoch: 79
Training loss: 0.6039958596229553 / Valid loss: 6.634159560430618
Training loss: 0.35401222109794617 / Valid loss: 6.56099883261181
Training loss: 0.3187083899974823 / Valid loss: 6.544349316188267
Training loss: 0.2787838578224182 / Valid loss: 6.565017409551711
Training loss: 0.29164472222328186 / Valid loss: 6.685382111867269

Epoch: 80
Training loss: 0.4367273151874542 / Valid loss: 6.5823886099315825
Training loss: 0.645135760307312 / Valid loss: 6.540732047671363
Training loss: 0.5293889045715332 / Valid loss: 6.825033324105399
Training loss: 0.3596353530883789 / Valid loss: 6.655407483237131
Training loss: 0.2988887429237366 / Valid loss: 6.610848499479748

Epoch: 81
Training loss: 0.5454487204551697 / Valid loss: 6.671510989325387
Training loss: 0.3336160182952881 / Valid loss: 6.687984682264782
Training loss: 0.39364224672317505 / Valid loss: 6.470678038824172
Training loss: 0.3482149839401245 / Valid loss: 6.676362369174049
Training loss: 0.29537874460220337 / Valid loss: 6.594658965156192

Epoch: 82
Training loss: 0.5226759910583496 / Valid loss: 6.581914529346284
Training loss: 0.4312063455581665 / Valid loss: 6.6297895567757745
Training loss: 0.3170022964477539 / Valid loss: 6.759487399600801
Training loss: 0.3671112060546875 / Valid loss: 6.5726848965599425
Training loss: 0.338772714138031 / Valid loss: 6.611494450342088

Epoch: 83
Training loss: 0.3797651529312134 / Valid loss: 6.528963429587228
Training loss: 0.8073760271072388 / Valid loss: 6.518167134693691
Training loss: 0.3773375153541565 / Valid loss: 6.628552752449399
Training loss: 0.21970593929290771 / Valid loss: 6.630058715457008
Training loss: 0.3267703652381897 / Valid loss: 6.664384821483067

Epoch: 84
Training loss: 0.4690066874027252 / Valid loss: 6.501053626196725
Training loss: 0.43403682112693787 / Valid loss: 6.738237222035726
Training loss: 0.26436933875083923 / Valid loss: 6.624668053218296
Training loss: 0.5086009502410889 / Valid loss: 6.643488286790394
Training loss: 0.411965012550354 / Valid loss: 6.563261772337414

Epoch: 85
Training loss: 0.3338455557823181 / Valid loss: 6.618447657993862
Training loss: 0.3191409111022949 / Valid loss: 6.656881164369129
Training loss: 0.42375126481056213 / Valid loss: 6.7605475879850845
Training loss: 0.4395422339439392 / Valid loss: 6.767650465738206
Training loss: 0.36963263154029846 / Valid loss: 6.866137699853806

Epoch: 86
Training loss: 0.27626723051071167 / Valid loss: 6.729549326215472
Training loss: 1.0537183284759521 / Valid loss: 6.681450457800002
Training loss: 0.25807416439056396 / Valid loss: 6.638763507207234
Training loss: 0.2781254053115845 / Valid loss: 6.5708229564485094
Training loss: 0.49000176787376404 / Valid loss: 6.663998621986026

Epoch: 87
Training loss: 0.2948750853538513 / Valid loss: 6.693745799291701
Training loss: 0.6280204057693481 / Valid loss: 6.679762308938162
Training loss: 0.45167338848114014 / Valid loss: 6.618900271824428
Training loss: 0.2616122364997864 / Valid loss: 6.59958766301473
Training loss: 0.3876555562019348 / Valid loss: 6.744061585835048

Epoch: 88
Training loss: 0.2031019777059555 / Valid loss: 6.682578091394333
Training loss: 0.3915556073188782 / Valid loss: 6.854692036764963
Training loss: 0.3968835473060608 / Valid loss: 6.775367155529204
Training loss: 0.6857262849807739 / Valid loss: 6.607603826976958
Training loss: 0.3051246404647827 / Valid loss: 6.713304024650937

Epoch: 89
Training loss: 1.2062568664550781 / Valid loss: 6.646757507324219
Training loss: 0.3832709491252899 / Valid loss: 6.675157319931757
Training loss: 0.33603692054748535 / Valid loss: 6.631699784596761
Training loss: 0.48701000213623047 / Valid loss: 6.663333039056687
Training loss: 0.4076909124851227 / Valid loss: 6.659985356103807

Epoch: 90
Training loss: 0.22625181078910828 / Valid loss: 6.641382258278983
Training loss: 0.6994098424911499 / Valid loss: 6.707309679757981
Training loss: 0.40600648522377014 / Valid loss: 6.706034088134766
Training loss: 0.32004091143608093 / Valid loss: 6.636679563068208
Training loss: 0.2877073287963867 / Valid loss: 6.716918668292817

Epoch: 91
Training loss: 0.4974299967288971 / Valid loss: 6.611720112391881
Training loss: 0.389018177986145 / Valid loss: 6.623916905266898
Training loss: 0.4637583792209625 / Valid loss: 6.597112051645914
Training loss: 0.20130287110805511 / Valid loss: 6.60061286517552
Training loss: 0.43709731101989746 / Valid loss: 6.602854158764794

Epoch: 92
Training loss: 0.6425575017929077 / Valid loss: 6.585700318926857
Training loss: 0.28972846269607544 / Valid loss: 6.6719458988734655
Training loss: 0.49488556385040283 / Valid loss: 6.492717148008801
Training loss: 0.47835972905158997 / Valid loss: 6.552994108200073
Training loss: 0.39752912521362305 / Valid loss: 6.602073110852923

Epoch: 93
Training loss: 0.3307216763496399 / Valid loss: 6.654115349905831
Training loss: 0.3840268850326538 / Valid loss: 6.765066937037877
Training loss: 0.5188523530960083 / Valid loss: 6.639917473565965
Training loss: 0.31102028489112854 / Valid loss: 6.876524895713443
Training loss: 0.26113075017929077 / Valid loss: 6.720470026561192

Epoch: 94
Training loss: 0.4031141996383667 / Valid loss: 6.684895626703898
Training loss: 0.34131407737731934 / Valid loss: 6.554145077296666
Training loss: 0.3507377505302429 / Valid loss: 6.712366045088995
Training loss: 0.35676729679107666 / Valid loss: 6.6841720149630595
Training loss: 0.281162291765213 / Valid loss: 6.620018068949381

Epoch: 95
Training loss: 0.23109300434589386 / Valid loss: 6.666217263539632
Training loss: 0.5548861026763916 / Valid loss: 6.830344100225539
Training loss: 0.4490646421909332 / Valid loss: 6.609132117316836
Training loss: 0.22954852879047394 / Valid loss: 6.523069354466029
Training loss: 0.25496721267700195 / Valid loss: 6.594842411222912

Epoch: 96
Training loss: 0.40620899200439453 / Valid loss: 6.767880700883412
Training loss: 1.0827548503875732 / Valid loss: 6.654582168942406
Training loss: 0.948384165763855 / Valid loss: 6.593726185389928
Training loss: 0.3312312960624695 / Valid loss: 6.7468036197480705
Training loss: 0.20968398451805115 / Valid loss: 6.614517761412121

Epoch: 97
Training loss: 0.2534278929233551 / Valid loss: 6.508655979519799
Training loss: 0.32312750816345215 / Valid loss: 6.724347809382848
Training loss: 0.5395901203155518 / Valid loss: 6.6933811573755175
Training loss: 0.33868032693862915 / Valid loss: 6.769193939935594
Training loss: 0.464852899312973 / Valid loss: 6.581854652223133

Epoch: 98
Training loss: 0.7010256052017212 / Valid loss: 6.693232245672316
Training loss: 0.6410868167877197 / Valid loss: 6.698711140950521
Training loss: 0.347697913646698 / Valid loss: 6.887342462085543
Training loss: 0.3421071469783783 / Valid loss: 6.50819959640503
Training loss: 0.3477572798728943 / Valid loss: 6.748034554436093

Epoch: 99
Training loss: 0.39474406838417053 / Valid loss: 6.604744525182815
Training loss: 0.257387638092041 / Valid loss: 6.782752473013741
Training loss: 0.3350118398666382 / Valid loss: 6.627549870808919
Training loss: 0.2540608048439026 / Valid loss: 6.881113928840274
Training loss: 0.5817409753799438 / Valid loss: 6.589274953660511
neural net:
 [tensor([[ 1.3915e-03, -1.3847e-03,  3.2112e-03,  ...,  5.2682e-03,
         -3.6732e-03, -5.5532e-03],
        [ 4.5481e-03,  2.4224e-03,  2.3290e-03,  ..., -1.4332e-03,
         -4.2462e-03,  6.3199e-05],
        [-2.7599e-03, -3.9486e-03, -5.8505e-03,  ...,  3.4376e-03,
         -7.7966e-03,  1.5038e-02],
        ...,
        [-2.2713e-03, -2.8436e-03,  6.9665e-03,  ..., -2.0557e-02,
          2.7284e-03, -2.6775e-05],
        [-3.0521e-03, -9.1297e-04, -2.6122e-04,  ...,  6.8584e-03,
         -2.5074e-03, -8.5093e-03],
        [-4.8371e-03,  3.8633e-04, -1.5287e-03,  ..., -5.1475e-03,
          6.9833e-05, -9.3774e-04]], device='cuda:0'), tensor([ 1.3370e-02,  1.5941e-02,  7.7337e-03, -4.5309e-03,  8.4150e-04,
        -9.3254e-03,  1.1841e-02,  7.3909e-03, -2.7191e-02,  2.6811e-02,
        -1.9300e-02,  1.1167e-02,  1.8039e-03,  3.1247e-03,  3.0296e-03,
        -1.7668e-02, -7.6461e-02,  1.1979e-02, -1.2766e-02, -1.1202e-03,
        -9.1138e-03, -1.9219e-02, -2.9702e-02, -8.0371e-03,  2.1247e-02,
        -4.6579e-02,  2.7651e-03, -3.8363e-03, -1.3323e-01,  1.7101e-03,
        -1.0577e-02, -3.8632e-03,  2.1654e-03, -1.2506e-03,  8.9621e-03,
         2.1848e-03, -1.5417e-02, -2.8762e-02,  2.5134e-02,  7.0382e-03,
         1.2075e-02,  1.9449e-02, -1.7913e-02, -6.8289e-02,  1.2419e-03,
         2.6581e-03, -1.8682e-02,  1.9838e-02, -2.7297e-02,  8.6612e-03,
         4.1515e-03,  9.1249e-03,  3.4333e-02, -3.0093e-02, -1.7752e-02,
        -8.7427e-02,  4.3007e-03,  7.1642e-03, -3.6134e-03,  1.3643e-02,
        -2.8530e-02, -1.6588e-02, -7.7431e-03,  3.1082e-02,  1.9599e-02,
         7.6022e-03, -3.9010e-02,  1.0875e-02, -1.0058e-01,  1.4039e-03,
         4.6835e-03,  1.3270e-02, -7.7889e-03,  3.3912e-03,  1.5237e-02,
         6.4797e-04, -5.1796e-03,  8.6320e-03, -2.1037e-01,  6.6204e-03,
        -1.8751e-02, -7.5787e-02,  6.6931e-03, -1.2093e-02, -6.2785e-03,
        -1.0905e-02,  1.2553e-02, -2.3143e-03,  5.2746e-03,  6.0514e-03,
        -9.4062e-02,  8.0482e-03, -3.5252e-02, -8.1569e-03, -4.9043e-03,
        -4.5399e-03, -1.2174e-03, -1.0943e-02,  7.6261e-03,  2.4852e-02,
         4.7280e-03,  6.6957e-03, -1.9254e-02,  3.4037e-03,  1.4252e-02,
         6.2050e-02, -4.8122e-03, -1.4592e-02, -3.8415e-03, -1.2388e-01,
         1.4697e-02, -4.8572e-03,  9.2269e-03,  1.3664e-02, -1.4953e-02,
         1.2107e-02, -5.0127e-02,  1.3765e-02,  1.1074e-02, -4.7484e-03,
         2.9371e-03,  5.0555e-03,  1.0244e-02,  1.1104e-02, -1.7555e-02,
         2.4784e-03, -1.0133e-01, -7.4399e-04, -1.6768e-02,  4.5138e-03,
        -6.0141e-02, -7.2105e-03,  2.6340e-02, -6.6329e-02,  3.8758e-03,
         8.4344e-03,  1.5739e-02,  2.0456e-02,  2.6440e-03, -1.2573e-02,
        -1.6181e-02,  1.3068e-03,  5.4702e-02,  1.4731e-03, -8.4454e-03,
         1.4108e-02,  3.6778e-02, -2.5218e-02, -1.6780e-02,  2.8548e-03,
         6.0383e-03,  3.6603e-03,  6.1109e-03, -1.0876e-02,  5.5283e-03,
         1.8222e-02, -8.9051e-02,  4.6355e-03,  1.1194e-02,  1.6022e-02,
        -8.2437e-03,  1.0196e-02,  1.3714e-02, -9.9724e-02,  1.1701e-02,
         1.1199e-02,  9.9636e-03,  1.0151e-02, -6.7187e-03, -3.1619e-02,
        -5.7865e-02,  8.3623e-03, -2.7942e-03,  9.8180e-03,  9.2211e-03,
        -2.4726e-02, -7.0651e-02,  3.2490e-03,  4.5355e-03,  1.5530e-02,
        -1.2675e-01,  7.5525e-03,  5.0307e-03, -5.2212e-02,  2.2637e-02,
         3.5304e-03,  2.8894e-03, -5.1866e-02,  1.3048e-02,  1.9142e-02,
         6.5595e-04, -1.1609e-01,  2.4429e-02, -1.2239e-01,  5.0066e-03,
         4.4558e-03, -1.8565e-02, -1.3018e-01, -1.6146e-02, -1.1838e-02,
         1.4580e-02,  5.2748e-03,  4.8011e-03, -5.8702e-03,  4.5845e-03,
        -1.3852e-02,  1.7042e-02, -1.1877e-02, -2.0666e-02, -1.5278e-02,
         1.4288e-02, -1.8372e-02,  7.7207e-03,  2.9885e-03, -5.8118e-03,
        -1.2855e-01, -2.7304e-03,  9.2283e-03,  3.2798e-03, -9.5046e-02,
        -1.0048e-02, -4.4886e-03,  9.7477e-03,  8.8340e-03,  1.1234e-02,
         2.1363e-02,  4.8240e-03,  3.3488e-02,  1.6361e-02,  6.7854e-03,
         1.1621e-02,  1.2673e-02,  1.9048e-02,  1.8093e-02,  7.7028e-03,
         1.6719e-02, -1.4232e-02, -6.3957e-03, -6.0827e-03, -2.7426e-02,
         1.1102e-02,  1.1396e-03, -9.6873e-02,  8.6129e-03, -1.0299e-02,
        -1.0148e-03, -2.9312e-03,  9.5631e-04, -9.9238e-03, -1.5829e-02,
        -1.9268e-02,  1.0120e-02,  9.9042e-03, -1.8257e-02, -1.6243e-02,
        -2.8995e-02, -5.5605e-02,  1.6306e-03,  1.2313e-02,  1.2348e-02,
         4.7949e-03,  5.6149e-03, -2.2668e-02, -1.3996e-02, -2.0162e-02,
         1.5803e-02, -1.1267e-02, -2.7802e-03,  5.7778e-03, -9.5116e-02,
         5.5913e-03,  1.1330e-02,  1.1012e-02,  1.9536e-03, -6.5734e-02,
         1.1905e-02,  1.2840e-02,  8.9000e-03, -7.6864e-02,  1.4117e-02,
         1.4456e-02, -1.8495e-02,  1.9874e-04, -1.4344e-01,  1.0414e-02,
        -1.6419e-02, -1.4019e-03,  5.1319e-03,  5.1849e-03,  8.7379e-03,
         6.2718e-03, -4.3537e-03, -1.6148e-02,  1.2082e-02, -7.8418e-02,
         9.2438e-03,  2.0845e-02,  3.0806e-03, -1.1962e-02,  1.2087e-02],
       device='cuda:0'), tensor([0.9457, 0.8939, 0.9394, 0.8976, 0.8890, 0.9413, 0.9027, 0.8849, 0.8924,
        1.0163, 0.8655, 0.9399, 0.9904, 0.9992, 0.9949, 0.8699, 0.8605, 1.0105,
        0.8801, 0.9529, 0.9068, 0.8999, 0.9064, 0.9209, 0.9929, 0.8783, 0.9107,
        0.8905, 0.8609, 0.9910, 1.0534, 0.9243, 1.0614, 1.0893, 1.0053, 0.9717,
        0.9247, 0.8642, 0.9858, 1.0658, 0.9708, 0.8714, 0.9215, 0.8720, 0.9735,
        0.9711, 0.8673, 0.9749, 0.9080, 0.9560, 0.9854, 0.9336, 0.8729, 0.8655,
        0.9211, 0.8595, 0.9198, 0.9845, 0.9043, 0.9042, 0.9786, 0.8791, 1.0145,
        0.8856, 1.0049, 0.9561, 0.8681, 0.9615, 0.8701, 0.9681, 1.0164, 0.9245,
        0.9342, 0.9443, 1.0088, 0.9251, 0.8869, 0.9580, 0.8584, 0.9307, 0.9315,
        0.8733, 0.8809, 0.9290, 0.8890, 0.9288, 0.9716, 0.8948, 0.8422, 0.9286,
        0.8636, 0.9730, 0.8596, 0.9287, 0.9678, 1.0757, 0.9259, 0.9438, 0.8843,
        0.9382, 0.9174, 1.0071, 0.9045, 0.9695, 0.9312, 0.8791, 0.8676, 0.8693,
        1.0772, 0.8582, 0.8906, 0.8863, 0.9759, 0.9614, 0.8816, 0.8994, 0.8648,
        1.0295, 0.9183, 0.8974, 0.9705, 1.0033, 0.8876, 0.9396, 0.8941, 0.9856,
        0.8662, 0.9532, 0.8975, 0.9893, 0.8643, 0.9567, 0.8774, 0.8707, 0.9291,
        0.8842, 0.9355, 0.9384, 0.8977, 0.9785, 0.9579, 0.8605, 0.8825, 0.9487,
        0.9189, 1.0132, 0.9687, 0.8617, 0.9302, 0.8870, 0.9381, 0.8829, 0.9346,
        0.9460, 0.9286, 0.9995, 0.8693, 0.8933, 0.8595, 0.9615, 0.9527, 0.9820,
        0.8902, 0.8662, 0.8841, 0.8947, 0.9563, 0.9503, 0.9718, 0.8681, 0.8588,
        0.9208, 0.9533, 0.9894, 0.9776, 0.8542, 0.8597, 0.9085, 0.9012, 0.9835,
        0.8721, 0.9873, 0.9133, 0.8615, 0.8757, 0.9120, 0.9824, 0.8597, 1.0525,
        0.8752, 0.9404, 0.8702, 0.9272, 0.8590, 0.9323, 0.9268, 0.8927, 0.8742,
        0.9481, 0.8829, 0.8892, 0.9525, 0.9977, 0.9133, 0.9151, 0.8788, 0.9752,
        0.8777, 0.9228, 0.8678, 0.9593, 0.8975, 0.9500, 1.0279, 0.8999, 0.8611,
        0.9056, 0.9099, 0.8995, 0.8604, 0.8888, 0.9296, 0.9062, 0.8920, 0.9404,
        0.9655, 0.9680, 0.8702, 0.9109, 0.9121, 0.8857, 0.9700, 1.0166, 0.9875,
        0.9040, 0.8921, 0.9360, 0.9317, 0.9141, 1.0801, 0.9581, 0.9198, 0.8671,
        0.9345, 0.9068, 0.9814, 0.9449, 0.9221, 0.8722, 0.9161, 0.9176, 0.8915,
        0.9450, 1.0112, 0.9801, 0.8641, 0.8772, 0.9458, 1.0139, 0.9883, 0.9117,
        0.9787, 0.8717, 1.0665, 0.9433, 0.8915, 0.8611, 0.9181, 0.9526, 0.8631,
        0.9664, 0.9244, 0.9299, 0.9619, 0.8605, 0.9451, 0.9612, 0.9979, 0.8780,
        0.9819, 0.9345, 0.9180, 0.9532, 0.8641, 0.9614, 0.9739, 0.8627, 0.9541,
        1.0162, 0.9494, 0.9445, 0.9669, 0.9070, 0.9542, 0.8684, 0.9724, 0.9731,
        1.0589, 0.9244, 0.9208], device='cuda:0'), tensor([-0.0296, -0.0557, -0.0887, -0.2273, -0.3694, -0.1423, -0.0380, -0.0447,
        -0.2392, -0.4951, -0.4013, -0.0417, -0.1973, -0.0961, -0.4728, -0.3452,
        -0.0595, -0.1079, -0.3473, -0.1137, -0.3507, -0.2035, -0.2276, -0.0301,
        -0.0943, -0.0539, -0.0362, -0.3541, -0.0578, -0.3480, -0.2736, -0.2377,
        -0.1310, -0.3741, -0.3171, -0.3368, -0.2953, -0.0499, -0.5270, -0.2699,
        -0.0414, -0.0524, -0.1315, -0.0522, -0.0283, -0.0671, -0.3688, -0.4931,
        -0.1380, -0.0272, -0.0399, -0.0329, -0.0542, -0.0558, -0.1237, -0.0585,
        -0.0302, -0.0582, -0.2739, -0.0331, -0.1695, -0.3605, -0.2401, -0.0445,
        -0.5632, -0.0431, -0.0518, -0.0632, -0.0515, -0.2935, -0.5612, -0.0272,
        -0.3101, -0.0513, -0.0058, -0.0318, -0.3448, -0.0442, -0.0590, -0.0526,
        -0.1947, -0.0476, -0.0486, -0.0232, -0.3235, -0.1940, -0.4575, -0.3411,
        -0.0804, -0.0314, -0.0561, -0.0564, -0.0615, -0.2562, -0.1082, -0.1405,
        -0.2344, -0.0299, -0.2628, -0.0397, -0.0321, -0.4921, -0.0572, -0.0524,
        -0.0306, -0.0493, -0.3454, -0.4103, -0.1599, -0.0594, -0.0464, -0.3716,
        -0.0359, -0.4558, -0.3693, -0.0357, -0.0547, -0.6028, -0.0254, -0.2589,
        -0.1605, -0.5553, -0.0427, -0.0307, -0.3295, -0.5176, -0.0533, -0.0210,
        -0.3513, -0.2206, -0.0564, -0.0462, -0.0508, -0.0513, -0.1167, -0.1644,
        -0.0351, -0.0176, -0.0435, -0.0500, -0.2936, -0.0584, -0.0544, -0.0663,
        -0.0614, -0.3783, -0.0299, -0.0583, -0.1622, -0.0487, -0.0387, -0.0991,
        -0.0424, -0.2359, -0.1946, -0.5885, -0.0511, -0.0464, -0.4300, -0.2301,
        -0.0431, -0.0120, -0.1996, -0.0546, -0.0584, -0.0420, -0.0246, -0.0247,
        -0.0511, -0.0518, -0.0589, -0.0362, -0.0322, -0.2117, -0.0620, -0.4624,
        -0.0580, -0.1259, -0.0344, -0.5148, -0.0577, -0.0623, -0.0432, -0.0572,
        -0.0560, -0.0411, -0.2305, -0.0596, -0.0520, -0.0510, -0.0882, -0.0532,
        -0.0308, -0.0609, -0.0400, -0.0372, -0.2189, -0.0547, -0.0546, -0.0682,
        -0.0451, -0.0506, -0.1174, -0.3444, -0.0405, -0.2625, -0.4618, -0.3666,
        -0.1567, -0.3871, -0.0303, -0.1882, -0.0470, -0.2584, -0.3351, -0.0549,
        -0.0375, -0.1767, -0.0376, -0.0568, -0.2173, -0.1143, -0.0324, -0.0449,
        -0.0218, -0.0408, -0.0487, -0.0527, -0.0319, -0.0403, -0.0443, -0.1367,
        -0.5893, -0.0690, -0.0399, -0.0432, -0.0717, -0.1712, -0.0360, -0.3064,
        -0.0297, -0.0402, -0.0524, -0.0686, -0.2163, -0.0456, -0.0322, -0.0505,
        -0.3712, -0.0739, -0.1370, -0.0430, -0.0572, -0.1111, -0.0134, -0.0577,
        -0.0493, -0.0289, -0.1035, -0.6482, -0.3051, -0.0133, -0.3387, -0.3068,
        -0.1357, -0.0477, -0.4362, -0.1347, -0.0395, -0.0582, -0.1728, -0.0356,
        -0.0380, -0.1718, -0.0599, -0.0552, -0.0309, -0.1843, -0.0557, -0.0558,
        -0.0263, -0.0354, -0.0270, -0.0571, -0.1416, -0.1159, -0.3897, -0.0196,
        -0.1342, -0.0470, -0.0308, -0.0494, -0.2506, -0.0256, -0.0530, -0.1396,
        -0.3907, -0.2648, -0.3325, -0.0284], device='cuda:0'), tensor([[ 0.0377, -0.0280, -0.0283,  ..., -0.0133, -0.0072,  0.0007],
        [ 0.0484,  0.0217,  0.0178,  ..., -0.0669,  0.0267,  0.0590],
        [-0.0125, -0.0096,  0.0667,  ...,  0.0543,  0.1566, -0.0427],
        ...,
        [ 0.0205, -0.0537, -0.0276,  ...,  0.0233,  0.1649, -0.0107],
        [-0.1516,  0.0609,  0.0495,  ...,  0.0027, -0.0736,  0.0839],
        [ 0.0788,  0.0357, -0.0484,  ..., -0.1994, -0.0456,  0.0401]],
       device='cuda:0'), tensor([ 0.0121,  0.0716, -0.0241, -0.0522,  0.0138, -0.0523, -0.0240, -0.0467,
         0.0758, -0.0088,  0.0484,  0.0005, -0.0805,  0.0741, -0.0194,  0.0112,
         0.0074,  0.0435, -0.0198,  0.0398,  0.0271,  0.1201, -0.0258,  0.1039,
         0.0904,  0.0806,  0.0483,  0.0520,  0.0197, -0.0216,  0.0586,  0.0386],
       device='cuda:0'), tensor([0.7564, 0.7739, 0.6964, 0.7206, 0.7060, 0.8304, 0.7302, 0.7332, 0.7092,
        0.7478, 0.7552, 0.6187, 0.7832, 0.7115, 0.7976, 0.7166, 0.6986, 0.7540,
        0.7359, 0.7662, 0.6375, 0.7362, 0.7323, 0.7847, 0.6917, 0.7435, 0.7317,
        0.7097, 0.7589, 0.7642, 0.6693, 0.7229], device='cuda:0'), tensor([-0.0503, -0.0253,  0.0688,  0.1118,  0.1513,  0.0945,  0.0224,  0.1107,
         0.1691, -0.0301,  0.1646,  0.2515, -0.0390,  0.1646,  0.1736,  0.0647,
         0.2757, -0.0127,  0.0360, -0.0715,  0.2061,  0.0653,  0.0266, -0.1289,
         0.1620,  0.1812,  0.1924,  0.1052,  0.0310,  0.1120,  0.2090,  0.1070],
       device='cuda:0'), tensor([[0.3199, 0.3709, 0.3244, 0.3161, 0.3289, 0.2895, 0.2652, 0.2451, 0.3566,
         0.3316, 0.3073, 0.3506, 0.2204, 0.3482, 0.2920, 0.3075, 0.3400, 0.3076,
         0.2924, 0.3292, 0.3479, 0.3962, 0.2901, 0.3736, 0.3616, 0.3516, 0.2977,
         0.3309, 0.2801, 0.2821, 0.3400, 0.3454]], device='cuda:0'), tensor([-0.0801], device='cuda:0')]
tensor([[ 0.7067],
        [ 0.5478],
        [ 1.6610],
        [ 4.8506],
        [ 1.9070],
        [-0.0248],
        [ 2.3587],
        [ 0.8002],
        [-0.0898],
        [ 3.9426],
        [ 2.4986],
        [ 4.8014],
        [ 2.7219],
        [ 1.0713],
        [ 0.5967],
        [ 4.8488],
        [ 2.8262],
        [ 1.9201],
        [ 2.0391],
        [ 1.7250],
        [ 0.0388],
        [ 6.0917],
        [ 2.6154],
        [ 1.8213],
        [ 0.8835],
        [ 1.6827],
        [ 0.6426],
        [ 1.5205],
        [ 1.5864],
        [ 2.7363],
        [ 7.3538],
        [ 9.1213],
        [ 2.1854],
        [ 1.9854],
        [ 3.9382],
        [ 4.5963],
        [ 0.2143],
        [-0.0496],
        [ 2.7041],
        [ 0.7237],
        [ 4.1090],
        [ 2.4124],
        [ 3.7886],
        [ 0.9236],
        [ 2.8602],
        [ 1.9784],
        [ 4.5373],
        [ 1.8102],
        [ 4.8318],
        [ 3.7414],
        [ 5.2830],
        [ 1.0165],
        [ 2.7452],
        [ 2.9796],
        [ 2.5698],
        [ 2.7678],
        [ 0.7321],
        [ 1.6445],
        [ 0.9692],
        [ 7.6948],
        [ 7.6971],
        [ 1.7630],
        [ 4.7611],
        [ 4.9724]], device='cuda:0')
neural net:
 [tensor([[ 1.3915e-03, -1.3847e-03,  3.2112e-03,  ...,  5.2682e-03,
         -3.6732e-03, -5.5532e-03],
        [ 4.5481e-03,  2.4224e-03,  2.3290e-03,  ..., -1.4332e-03,
         -4.2462e-03,  6.3199e-05],
        [-2.7599e-03, -3.9486e-03, -5.8505e-03,  ...,  3.4376e-03,
         -7.7966e-03,  1.5038e-02],
        ...,
        [-2.2713e-03, -2.8436e-03,  6.9665e-03,  ..., -2.0557e-02,
          2.7284e-03, -2.6775e-05],
        [-3.0521e-03, -9.1297e-04, -2.6122e-04,  ...,  6.8584e-03,
         -2.5074e-03, -8.5093e-03],
        [-4.8371e-03,  3.8633e-04, -1.5287e-03,  ..., -5.1475e-03,
          6.9833e-05, -9.3774e-04]], device='cuda:0'), tensor([ 1.3370e-02,  1.5941e-02,  7.7337e-03, -4.5309e-03,  8.4150e-04,
        -9.3254e-03,  1.1841e-02,  7.3909e-03, -2.7191e-02,  2.6811e-02,
        -1.9300e-02,  1.1167e-02,  1.8039e-03,  3.1247e-03,  3.0296e-03,
        -1.7668e-02, -7.6461e-02,  1.1979e-02, -1.2766e-02, -1.1202e-03,
        -9.1138e-03, -1.9219e-02, -2.9702e-02, -8.0371e-03,  2.1247e-02,
        -4.6579e-02,  2.7651e-03, -3.8363e-03, -1.3323e-01,  1.7101e-03,
        -1.0577e-02, -3.8632e-03,  2.1654e-03, -1.2506e-03,  8.9621e-03,
         2.1848e-03, -1.5417e-02, -2.8762e-02,  2.5134e-02,  7.0382e-03,
         1.2075e-02,  1.9449e-02, -1.7913e-02, -6.8289e-02,  1.2419e-03,
         2.6581e-03, -1.8682e-02,  1.9838e-02, -2.7297e-02,  8.6612e-03,
         4.1515e-03,  9.1249e-03,  3.4333e-02, -3.0093e-02, -1.7752e-02,
        -8.7427e-02,  4.3007e-03,  7.1642e-03, -3.6134e-03,  1.3643e-02,
        -2.8530e-02, -1.6588e-02, -7.7431e-03,  3.1082e-02,  1.9599e-02,
         7.6022e-03, -3.9010e-02,  1.0875e-02, -1.0058e-01,  1.4039e-03,
         4.6835e-03,  1.3270e-02, -7.7889e-03,  3.3912e-03,  1.5237e-02,
         6.4797e-04, -5.1796e-03,  8.6320e-03, -2.1037e-01,  6.6204e-03,
        -1.8751e-02, -7.5787e-02,  6.6931e-03, -1.2093e-02, -6.2785e-03,
        -1.0905e-02,  1.2553e-02, -2.3143e-03,  5.2746e-03,  6.0514e-03,
        -9.4062e-02,  8.0482e-03, -3.5252e-02, -8.1569e-03, -4.9043e-03,
        -4.5399e-03, -1.2174e-03, -1.0943e-02,  7.6261e-03,  2.4852e-02,
         4.7280e-03,  6.6957e-03, -1.9254e-02,  3.4037e-03,  1.4252e-02,
         6.2050e-02, -4.8122e-03, -1.4592e-02, -3.8415e-03, -1.2388e-01,
         1.4697e-02, -4.8572e-03,  9.2269e-03,  1.3664e-02, -1.4953e-02,
         1.2107e-02, -5.0127e-02,  1.3765e-02,  1.1074e-02, -4.7484e-03,
         2.9371e-03,  5.0555e-03,  1.0244e-02,  1.1104e-02, -1.7555e-02,
         2.4784e-03, -1.0133e-01, -7.4399e-04, -1.6768e-02,  4.5138e-03,
        -6.0141e-02, -7.2105e-03,  2.6340e-02, -6.6329e-02,  3.8758e-03,
         8.4344e-03,  1.5739e-02,  2.0456e-02,  2.6440e-03, -1.2573e-02,
        -1.6181e-02,  1.3068e-03,  5.4702e-02,  1.4731e-03, -8.4454e-03,
         1.4108e-02,  3.6778e-02, -2.5218e-02, -1.6780e-02,  2.8548e-03,
         6.0383e-03,  3.6603e-03,  6.1109e-03, -1.0876e-02,  5.5283e-03,
         1.8222e-02, -8.9051e-02,  4.6355e-03,  1.1194e-02,  1.6022e-02,
        -8.2437e-03,  1.0196e-02,  1.3714e-02, -9.9724e-02,  1.1701e-02,
         1.1199e-02,  9.9636e-03,  1.0151e-02, -6.7187e-03, -3.1619e-02,
        -5.7865e-02,  8.3623e-03, -2.7942e-03,  9.8180e-03,  9.2211e-03,
        -2.4726e-02, -7.0651e-02,  3.2490e-03,  4.5355e-03,  1.5530e-02,
        -1.2675e-01,  7.5525e-03,  5.0307e-03, -5.2212e-02,  2.2637e-02,
         3.5304e-03,  2.8894e-03, -5.1866e-02,  1.3048e-02,  1.9142e-02,
         6.5595e-04, -1.1609e-01,  2.4429e-02, -1.2239e-01,  5.0066e-03,
         4.4558e-03, -1.8565e-02, -1.3018e-01, -1.6146e-02, -1.1838e-02,
         1.4580e-02,  5.2748e-03,  4.8011e-03, -5.8702e-03,  4.5845e-03,
        -1.3852e-02,  1.7042e-02, -1.1877e-02, -2.0666e-02, -1.5278e-02,
         1.4288e-02, -1.8372e-02,  7.7207e-03,  2.9885e-03, -5.8118e-03,
        -1.2855e-01, -2.7304e-03,  9.2283e-03,  3.2798e-03, -9.5046e-02,
        -1.0048e-02, -4.4886e-03,  9.7477e-03,  8.8340e-03,  1.1234e-02,
         2.1363e-02,  4.8240e-03,  3.3488e-02,  1.6361e-02,  6.7854e-03,
         1.1621e-02,  1.2673e-02,  1.9048e-02,  1.8093e-02,  7.7028e-03,
         1.6719e-02, -1.4232e-02, -6.3957e-03, -6.0827e-03, -2.7426e-02,
         1.1102e-02,  1.1396e-03, -9.6873e-02,  8.6129e-03, -1.0299e-02,
        -1.0148e-03, -2.9312e-03,  9.5631e-04, -9.9238e-03, -1.5829e-02,
        -1.9268e-02,  1.0120e-02,  9.9042e-03, -1.8257e-02, -1.6243e-02,
        -2.8995e-02, -5.5605e-02,  1.6306e-03,  1.2313e-02,  1.2348e-02,
         4.7949e-03,  5.6149e-03, -2.2668e-02, -1.3996e-02, -2.0162e-02,
         1.5803e-02, -1.1267e-02, -2.7802e-03,  5.7778e-03, -9.5116e-02,
         5.5913e-03,  1.1330e-02,  1.1012e-02,  1.9536e-03, -6.5734e-02,
         1.1905e-02,  1.2840e-02,  8.9000e-03, -7.6864e-02,  1.4117e-02,
         1.4456e-02, -1.8495e-02,  1.9874e-04, -1.4344e-01,  1.0414e-02,
        -1.6419e-02, -1.4019e-03,  5.1319e-03,  5.1849e-03,  8.7379e-03,
         6.2718e-03, -4.3537e-03, -1.6148e-02,  1.2082e-02, -7.8418e-02,
         9.2438e-03,  2.0845e-02,  3.0806e-03, -1.1962e-02,  1.2087e-02],
       device='cuda:0'), tensor([0.9457, 0.8939, 0.9394, 0.8976, 0.8890, 0.9413, 0.9027, 0.8849, 0.8924,
        1.0163, 0.8655, 0.9399, 0.9904, 0.9992, 0.9949, 0.8699, 0.8605, 1.0105,
        0.8801, 0.9529, 0.9068, 0.8999, 0.9064, 0.9209, 0.9929, 0.8783, 0.9107,
        0.8905, 0.8609, 0.9910, 1.0534, 0.9243, 1.0614, 1.0893, 1.0053, 0.9717,
        0.9247, 0.8642, 0.9858, 1.0658, 0.9708, 0.8714, 0.9215, 0.8720, 0.9735,
        0.9711, 0.8673, 0.9749, 0.9080, 0.9560, 0.9854, 0.9336, 0.8729, 0.8655,
        0.9211, 0.8595, 0.9198, 0.9845, 0.9043, 0.9042, 0.9786, 0.8791, 1.0145,
        0.8856, 1.0049, 0.9561, 0.8681, 0.9615, 0.8701, 0.9681, 1.0164, 0.9245,
        0.9342, 0.9443, 1.0088, 0.9251, 0.8869, 0.9580, 0.8584, 0.9307, 0.9315,
        0.8733, 0.8809, 0.9290, 0.8890, 0.9288, 0.9716, 0.8948, 0.8422, 0.9286,
        0.8636, 0.9730, 0.8596, 0.9287, 0.9678, 1.0757, 0.9259, 0.9438, 0.8843,
        0.9382, 0.9174, 1.0071, 0.9045, 0.9695, 0.9312, 0.8791, 0.8676, 0.8693,
        1.0772, 0.8582, 0.8906, 0.8863, 0.9759, 0.9614, 0.8816, 0.8994, 0.8648,
        1.0295, 0.9183, 0.8974, 0.9705, 1.0033, 0.8876, 0.9396, 0.8941, 0.9856,
        0.8662, 0.9532, 0.8975, 0.9893, 0.8643, 0.9567, 0.8774, 0.8707, 0.9291,
        0.8842, 0.9355, 0.9384, 0.8977, 0.9785, 0.9579, 0.8605, 0.8825, 0.9487,
        0.9189, 1.0132, 0.9687, 0.8617, 0.9302, 0.8870, 0.9381, 0.8829, 0.9346,
        0.9460, 0.9286, 0.9995, 0.8693, 0.8933, 0.8595, 0.9615, 0.9527, 0.9820,
        0.8902, 0.8662, 0.8841, 0.8947, 0.9563, 0.9503, 0.9718, 0.8681, 0.8588,
        0.9208, 0.9533, 0.9894, 0.9776, 0.8542, 0.8597, 0.9085, 0.9012, 0.9835,
        0.8721, 0.9873, 0.9133, 0.8615, 0.8757, 0.9120, 0.9824, 0.8597, 1.0525,
        0.8752, 0.9404, 0.8702, 0.9272, 0.8590, 0.9323, 0.9268, 0.8927, 0.8742,
        0.9481, 0.8829, 0.8892, 0.9525, 0.9977, 0.9133, 0.9151, 0.8788, 0.9752,
        0.8777, 0.9228, 0.8678, 0.9593, 0.8975, 0.9500, 1.0279, 0.8999, 0.8611,
        0.9056, 0.9099, 0.8995, 0.8604, 0.8888, 0.9296, 0.9062, 0.8920, 0.9404,
        0.9655, 0.9680, 0.8702, 0.9109, 0.9121, 0.8857, 0.9700, 1.0166, 0.9875,
        0.9040, 0.8921, 0.9360, 0.9317, 0.9141, 1.0801, 0.9581, 0.9198, 0.8671,
        0.9345, 0.9068, 0.9814, 0.9449, 0.9221, 0.8722, 0.9161, 0.9176, 0.8915,
        0.9450, 1.0112, 0.9801, 0.8641, 0.8772, 0.9458, 1.0139, 0.9883, 0.9117,
        0.9787, 0.8717, 1.0665, 0.9433, 0.8915, 0.8611, 0.9181, 0.9526, 0.8631,
        0.9664, 0.9244, 0.9299, 0.9619, 0.8605, 0.9451, 0.9612, 0.9979, 0.8780,
        0.9819, 0.9345, 0.9180, 0.9532, 0.8641, 0.9614, 0.9739, 0.8627, 0.9541,
        1.0162, 0.9494, 0.9445, 0.9669, 0.9070, 0.9542, 0.8684, 0.9724, 0.9731,
        1.0589, 0.9244, 0.9208], device='cuda:0'), tensor([-0.0296, -0.0557, -0.0887, -0.2273, -0.3694, -0.1423, -0.0380, -0.0447,
        -0.2392, -0.4951, -0.4013, -0.0417, -0.1973, -0.0961, -0.4728, -0.3452,
        -0.0595, -0.1079, -0.3473, -0.1137, -0.3507, -0.2035, -0.2276, -0.0301,
        -0.0943, -0.0539, -0.0362, -0.3541, -0.0578, -0.3480, -0.2736, -0.2377,
        -0.1310, -0.3741, -0.3171, -0.3368, -0.2953, -0.0499, -0.5270, -0.2699,
        -0.0414, -0.0524, -0.1315, -0.0522, -0.0283, -0.0671, -0.3688, -0.4931,
        -0.1380, -0.0272, -0.0399, -0.0329, -0.0542, -0.0558, -0.1237, -0.0585,
        -0.0302, -0.0582, -0.2739, -0.0331, -0.1695, -0.3605, -0.2401, -0.0445,
        -0.5632, -0.0431, -0.0518, -0.0632, -0.0515, -0.2935, -0.5612, -0.0272,
        -0.3101, -0.0513, -0.0058, -0.0318, -0.3448, -0.0442, -0.0590, -0.0526,
        -0.1947, -0.0476, -0.0486, -0.0232, -0.3235, -0.1940, -0.4575, -0.3411,
        -0.0804, -0.0314, -0.0561, -0.0564, -0.0615, -0.2562, -0.1082, -0.1405,
        -0.2344, -0.0299, -0.2628, -0.0397, -0.0321, -0.4921, -0.0572, -0.0524,
        -0.0306, -0.0493, -0.3454, -0.4103, -0.1599, -0.0594, -0.0464, -0.3716,
        -0.0359, -0.4558, -0.3693, -0.0357, -0.0547, -0.6028, -0.0254, -0.2589,
        -0.1605, -0.5553, -0.0427, -0.0307, -0.3295, -0.5176, -0.0533, -0.0210,
        -0.3513, -0.2206, -0.0564, -0.0462, -0.0508, -0.0513, -0.1167, -0.1644,
        -0.0351, -0.0176, -0.0435, -0.0500, -0.2936, -0.0584, -0.0544, -0.0663,
        -0.0614, -0.3783, -0.0299, -0.0583, -0.1622, -0.0487, -0.0387, -0.0991,
        -0.0424, -0.2359, -0.1946, -0.5885, -0.0511, -0.0464, -0.4300, -0.2301,
        -0.0431, -0.0120, -0.1996, -0.0546, -0.0584, -0.0420, -0.0246, -0.0247,
        -0.0511, -0.0518, -0.0589, -0.0362, -0.0322, -0.2117, -0.0620, -0.4624,
        -0.0580, -0.1259, -0.0344, -0.5148, -0.0577, -0.0623, -0.0432, -0.0572,
        -0.0560, -0.0411, -0.2305, -0.0596, -0.0520, -0.0510, -0.0882, -0.0532,
        -0.0308, -0.0609, -0.0400, -0.0372, -0.2189, -0.0547, -0.0546, -0.0682,
        -0.0451, -0.0506, -0.1174, -0.3444, -0.0405, -0.2625, -0.4618, -0.3666,
        -0.1567, -0.3871, -0.0303, -0.1882, -0.0470, -0.2584, -0.3351, -0.0549,
        -0.0375, -0.1767, -0.0376, -0.0568, -0.2173, -0.1143, -0.0324, -0.0449,
        -0.0218, -0.0408, -0.0487, -0.0527, -0.0319, -0.0403, -0.0443, -0.1367,
        -0.5893, -0.0690, -0.0399, -0.0432, -0.0717, -0.1712, -0.0360, -0.3064,
        -0.0297, -0.0402, -0.0524, -0.0686, -0.2163, -0.0456, -0.0322, -0.0505,
        -0.3712, -0.0739, -0.1370, -0.0430, -0.0572, -0.1111, -0.0134, -0.0577,
        -0.0493, -0.0289, -0.1035, -0.6482, -0.3051, -0.0133, -0.3387, -0.3068,
        -0.1357, -0.0477, -0.4362, -0.1347, -0.0395, -0.0582, -0.1728, -0.0356,
        -0.0380, -0.1718, -0.0599, -0.0552, -0.0309, -0.1843, -0.0557, -0.0558,
        -0.0263, -0.0354, -0.0270, -0.0571, -0.1416, -0.1159, -0.3897, -0.0196,
        -0.1342, -0.0470, -0.0308, -0.0494, -0.2506, -0.0256, -0.0530, -0.1396,
        -0.3907, -0.2648, -0.3325, -0.0284], device='cuda:0'), tensor([[ 0.0377, -0.0280, -0.0283,  ..., -0.0133, -0.0072,  0.0007],
        [ 0.0484,  0.0217,  0.0178,  ..., -0.0669,  0.0267,  0.0590],
        [-0.0125, -0.0096,  0.0667,  ...,  0.0543,  0.1566, -0.0427],
        ...,
        [ 0.0205, -0.0537, -0.0276,  ...,  0.0233,  0.1649, -0.0107],
        [-0.1516,  0.0609,  0.0495,  ...,  0.0027, -0.0736,  0.0839],
        [ 0.0788,  0.0357, -0.0484,  ..., -0.1994, -0.0456,  0.0401]],
       device='cuda:0'), tensor([ 0.0121,  0.0716, -0.0241, -0.0522,  0.0138, -0.0523, -0.0240, -0.0467,
         0.0758, -0.0088,  0.0484,  0.0005, -0.0805,  0.0741, -0.0194,  0.0112,
         0.0074,  0.0435, -0.0198,  0.0398,  0.0271,  0.1201, -0.0258,  0.1039,
         0.0904,  0.0806,  0.0483,  0.0520,  0.0197, -0.0216,  0.0586,  0.0386],
       device='cuda:0'), tensor([0.7564, 0.7739, 0.6964, 0.7206, 0.7060, 0.8304, 0.7302, 0.7332, 0.7092,
        0.7478, 0.7552, 0.6187, 0.7832, 0.7115, 0.7976, 0.7166, 0.6986, 0.7540,
        0.7359, 0.7662, 0.6375, 0.7362, 0.7323, 0.7847, 0.6917, 0.7435, 0.7317,
        0.7097, 0.7589, 0.7642, 0.6693, 0.7229], device='cuda:0'), tensor([-0.0503, -0.0253,  0.0688,  0.1118,  0.1513,  0.0945,  0.0224,  0.1107,
         0.1691, -0.0301,  0.1646,  0.2515, -0.0390,  0.1646,  0.1736,  0.0647,
         0.2757, -0.0127,  0.0360, -0.0715,  0.2061,  0.0653,  0.0266, -0.1289,
         0.1620,  0.1812,  0.1924,  0.1052,  0.0310,  0.1120,  0.2090,  0.1070],
       device='cuda:0'), tensor([[0.3199, 0.3709, 0.3244, 0.3161, 0.3289, 0.2895, 0.2652, 0.2451, 0.3566,
         0.3316, 0.3073, 0.3506, 0.2204, 0.3482, 0.2920, 0.3075, 0.3400, 0.3076,
         0.2924, 0.3292, 0.3479, 0.3962, 0.2901, 0.3736, 0.3616, 0.3516, 0.2977,
         0.3309, 0.2801, 0.2821, 0.3400, 0.3454]], device='cuda:0'), tensor([-0.0801], device='cuda:0')]
tensor([[6.0821],
        [1.9457],
        [4.3453],
        [1.7252],
        [4.5883],
        [2.0033],
        [2.7265],
        [3.7111],
        [3.2169],
        [3.5098],
        [1.7310],
        [1.1880],
        [2.0570],
        [3.6725],
        [8.1242],
        [2.9140],
        [3.7902],
        [3.8302],
        [2.3278],
        [3.2522],
        [1.8350],
        [2.2794],
        [3.1506],
        [4.5204],
        [2.3889],
        [2.4404],
        [1.5856],
        [2.8651],
        [2.3539],
        [2.5252],
        [1.4274],
        [3.6528],
        [2.9657],
        [1.8891],
        [3.5415],
        [1.5905],
        [4.4492],
        [2.3059],
        [3.4742],
        [2.9590],
        [3.4666],
        [2.2493],
        [1.6455],
        [2.3645],
        [1.0934],
        [2.6696],
        [2.5444],
        [2.3004],
        [2.5012],
        [5.0869],
        [3.7393],
        [1.9333],
        [4.2865],
        [3.2804],
        [2.7354],
        [2.6485],
        [1.5449],
        [4.0578],
        [1.8919],
        [1.4989],
        [3.2113],
        [2.5604],
        [1.6869],
        [0.6916]], device='cuda:0')
Loss on train set via eval test: 0.1245206147356301
Loss on test set: 6.807690561385382
