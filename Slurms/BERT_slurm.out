********************************************************************************************
** WARNING: 'The 'pre2019' module environment is deprecated. Please consider switching
             to the '2019' or '2020' module environment. You can read more about our
             software policy on this page:
             https://userinfo.surfsara.nl/documentation/software-policy-lisacartesius

             If you have any question, please contact us via http://servicedesk.surfsara.nl.'
********************************************************************************************
Using device: cuda

GeForce GTX 1080 Ti
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
Training regression with following parameters:
dnn_hidden_units : 300, 32
dropout_probs : 0.3, 0.05
learning_rate : 0.001
nr_epochs : 100
batch_size : 64
eval_freq : 100
data_dir : Data/
neg_slope : 0.02
optimizer : SGD
amsgrad : True
batchnorm : True
weightdecay : 0.0001
momentum : 0.9
embedder : Bert
Training started
Device : cuda
ModuleList(
  (0): Linear(in_features=5376, out_features=300, bias=True)
  (1): Dropout(p=0.3, inplace=False)
  (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): LeakyReLU(negative_slope=0.02)
  (4): Linear(in_features=300, out_features=32, bias=True)
  (5): Dropout(p=0.05, inplace=False)
  (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): LeakyReLU(negative_slope=0.02)
  (8): Linear(in_features=32, out_features=1, bias=True)
)
neural net:
 [tensor([[ 0.0035, -0.0035,  0.0052,  ..., -0.0130,  0.0056, -0.0088],
        [ 0.0045, -0.0089, -0.0107,  ...,  0.0052,  0.0048, -0.0076],
        [ 0.0014, -0.0062,  0.0084,  ..., -0.0093,  0.0089,  0.0006],
        ...,
        [-0.0058,  0.0055,  0.0019,  ...,  0.0132,  0.0079,  0.0033],
        [-0.0119,  0.0065, -0.0104,  ..., -0.0040,  0.0074, -0.0118],
        [-0.0084, -0.0022, -0.0104,  ...,  0.0046, -0.0053,  0.0026]],
       device='cuda:0'), tensor([ 6.1908e-03,  9.4730e-03,  4.0844e-03,  7.2053e-03, -7.5650e-03,
        -7.8832e-03, -7.8220e-03,  1.4596e-03,  1.1989e-02, -7.6588e-03,
         1.2660e-02,  7.7451e-03, -1.6862e-03, -5.7928e-03, -5.9773e-03,
        -1.2504e-02,  1.2614e-02, -1.6331e-03, -1.3230e-02,  6.1551e-03,
        -3.5333e-03, -1.3258e-03,  6.8282e-03,  4.9591e-03,  1.1026e-02,
         6.1257e-03,  6.7206e-03,  4.7627e-03,  1.0540e-02,  3.7899e-03,
        -9.5683e-03,  8.3697e-03,  3.5639e-03, -7.2611e-03, -8.4501e-03,
        -7.3323e-03,  1.0752e-02,  1.6913e-03, -5.4175e-03, -1.0077e-02,
        -8.5742e-03, -1.3630e-02, -8.0114e-04, -7.1776e-03,  7.3167e-03,
        -1.0892e-02,  3.3710e-03,  1.1669e-02,  3.1884e-03,  6.6005e-03,
         7.4364e-03,  7.6632e-03,  1.2209e-02,  1.2305e-02, -9.6422e-03,
         9.5229e-04,  8.5223e-04, -6.6562e-03,  1.0333e-02,  3.8328e-03,
        -6.5618e-03, -7.2025e-03,  1.3016e-02,  9.5085e-03,  1.5434e-03,
         1.2693e-02, -1.3313e-02, -7.8086e-03, -1.0331e-02,  8.2957e-03,
        -6.1494e-03, -5.6646e-05,  7.7152e-03, -1.1186e-02, -8.7701e-03,
         4.5456e-03, -5.8034e-03, -1.2297e-02, -5.7203e-03, -1.2337e-02,
         1.1607e-02, -4.3958e-03,  1.2340e-02, -6.7009e-03, -1.0314e-02,
        -3.6307e-03,  3.9819e-03, -1.2677e-05, -2.8444e-03,  7.5662e-03,
         1.2744e-02, -1.2558e-02, -7.3229e-03,  9.1467e-03, -5.1329e-03,
         6.8300e-03,  4.7728e-03, -6.7125e-03, -8.7461e-04,  8.9765e-03,
         1.1259e-02,  1.6875e-03, -9.4385e-03, -6.8914e-03,  1.0094e-02,
        -1.3397e-02,  1.2217e-02,  7.1941e-03,  6.0293e-04, -9.2256e-03,
         8.0634e-03, -1.2569e-02,  5.7503e-03,  8.4083e-03,  6.3846e-03,
         6.8085e-03,  1.0586e-02,  1.1642e-02, -8.5935e-03, -2.6303e-03,
        -3.0700e-03, -9.7075e-03, -9.1235e-03, -1.3539e-02,  9.5483e-03,
         1.5743e-03, -6.7769e-03, -4.9824e-03,  1.3496e-02,  9.1415e-03,
         1.0382e-02,  1.0293e-02,  7.6196e-03, -8.5856e-03, -1.7091e-03,
        -6.6779e-03, -7.6665e-03,  9.9550e-04, -2.8481e-04, -6.7325e-03,
        -7.1988e-03,  3.0063e-03, -6.8882e-03, -2.6528e-03,  2.2587e-03,
        -4.9416e-03, -4.8614e-03, -1.2035e-02,  1.1177e-02, -7.9536e-03,
         6.1995e-03,  5.0235e-03, -7.3600e-03, -1.9053e-03, -1.0862e-02,
        -8.2104e-03,  4.5341e-03, -1.1624e-03, -1.3237e-02,  8.7562e-03,
         1.5708e-04,  5.8453e-03, -4.5865e-03,  6.3260e-03, -7.5384e-03,
         1.0783e-02,  6.8170e-03,  4.4673e-03,  1.0586e-02,  6.0070e-03,
        -3.1499e-03,  1.2032e-03, -2.3416e-03, -1.2767e-02, -1.0061e-02,
         2.8987e-03,  1.3508e-02, -2.5000e-03,  1.2125e-02, -6.1701e-04,
         1.3316e-02, -9.0667e-03, -1.1850e-02,  1.2170e-02,  1.0577e-02,
         1.0409e-02,  4.1206e-03, -3.9714e-03, -7.7134e-03, -8.9969e-03,
         4.4566e-03,  2.4342e-03,  1.6708e-03, -4.9487e-03,  1.3483e-02,
         4.5443e-03, -8.6954e-03, -2.7178e-03,  9.3358e-03, -7.6390e-03,
         1.0803e-02, -3.4699e-03, -1.2594e-02,  2.0801e-03,  5.7544e-03,
         4.5966e-03,  1.1982e-02, -1.3398e-02,  1.1950e-02, -5.1176e-03,
        -7.4220e-03,  6.2815e-03,  2.1421e-03, -3.4979e-03, -8.1106e-03,
         2.3853e-03,  1.1085e-02, -8.5641e-03,  1.1652e-02,  1.2154e-02,
         3.3821e-03,  5.4173e-03,  3.6133e-03,  1.1921e-02, -1.2202e-04,
         1.3291e-03,  7.9977e-03, -9.1828e-03,  1.2680e-02,  9.6885e-03,
        -9.4037e-03,  7.0275e-03,  1.1817e-02,  6.3789e-03, -3.3498e-03,
         1.3211e-02,  4.4777e-03,  1.3477e-02, -5.0517e-03,  1.3482e-02,
         6.8885e-03,  4.5033e-03,  6.4059e-03, -7.5613e-04,  3.7614e-03,
        -5.6941e-03,  4.3891e-03, -3.4414e-03, -1.2314e-02, -3.0129e-03,
        -2.1002e-03,  9.8752e-03,  1.2174e-02, -1.7293e-03, -4.4426e-03,
        -2.8396e-03,  6.4866e-03, -8.7991e-03, -3.5962e-03, -3.6791e-03,
         5.7245e-05,  5.0450e-03, -7.6964e-03, -3.9388e-04, -1.2701e-02,
        -1.1177e-03, -7.2794e-03,  5.5947e-03,  1.3496e-02, -9.7928e-03,
         6.8743e-03,  5.6490e-04,  6.4588e-03, -2.4593e-03,  4.0530e-03,
        -3.0683e-03,  2.1734e-03, -1.2843e-02,  1.0774e-02, -2.5377e-03,
        -1.0668e-02,  1.3098e-02,  8.8518e-04, -6.1425e-03, -1.3382e-02,
        -1.2049e-02,  7.2464e-03, -4.1364e-03, -4.7744e-03,  1.3551e-02,
         3.9614e-03,  3.7871e-03,  1.2677e-02, -3.5220e-03,  6.4114e-03,
         1.0300e-02,  8.0989e-04,  7.3246e-03,  3.3239e-03,  8.7276e-03],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), tensor([[ 5.2854e-02,  5.4947e-02, -1.9604e-02,  ..., -2.1188e-02,
         -4.7861e-05, -4.8300e-02],
        [ 3.3754e-02, -2.2243e-02, -1.4574e-02,  ..., -3.5948e-02,
          1.8371e-02, -4.2570e-02],
        [-1.6167e-02,  2.9327e-02, -1.7276e-02,  ..., -3.5623e-03,
          4.9161e-02,  2.9653e-02],
        ...,
        [ 1.2322e-02, -9.9669e-03,  4.9586e-02,  ..., -3.3535e-02,
         -1.8332e-03, -2.4213e-02],
        [ 1.0364e-02,  5.4379e-02,  3.8070e-02,  ..., -1.1329e-02,
          8.8845e-03, -1.5984e-02],
        [ 5.0398e-02, -5.2963e-03,  2.1117e-02,  ...,  5.6946e-02,
         -4.2045e-02,  2.9271e-02]], device='cuda:0'), tensor([-0.0175, -0.0209, -0.0067, -0.0028, -0.0570, -0.0423,  0.0394, -0.0517,
         0.0144, -0.0506,  0.0564, -0.0331,  0.0373,  0.0541, -0.0478,  0.0267,
         0.0228,  0.0303,  0.0351, -0.0326, -0.0167,  0.0306,  0.0571, -0.0312,
         0.0333,  0.0066, -0.0441, -0.0218, -0.0514,  0.0541,  0.0277,  0.0457],
       device='cuda:0'), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0'), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), tensor([[ 0.0520,  0.0254,  0.0417,  0.1638, -0.1763, -0.1683,  0.0689,  0.1291,
          0.1346,  0.1424,  0.1130, -0.1295, -0.1496, -0.0811,  0.0259,  0.1741,
         -0.1267, -0.1199,  0.1764,  0.1093, -0.0709,  0.0586,  0.0379, -0.0369,
         -0.0302,  0.0330, -0.0031, -0.0057, -0.1549,  0.0699,  0.0153,  0.0740]],
       device='cuda:0'), tensor([-0.0790], device='cuda:0')]

Epoch: 0
Training loss: 13.930601119995117 / Valid loss: 16.556846727643695
Training loss: 3.531541585922241 / Valid loss: 5.670454004832677
Training loss: 4.804642677307129 / Valid loss: 5.642841788700649
Training loss: 5.218390464782715 / Valid loss: 5.675773127873739
Training loss: 4.277702808380127 / Valid loss: 5.581893323716663

Epoch: 1
Training loss: 5.169252395629883 / Valid loss: 5.514728916259039
Training loss: 5.035224914550781 / Valid loss: 5.445218717484247
Training loss: 4.275619983673096 / Valid loss: 5.54001916930789
Training loss: 4.130034446716309 / Valid loss: 5.626433928807576
Training loss: 4.9304399490356445 / Valid loss: 5.528393277667818

Epoch: 2
Training loss: 4.311848163604736 / Valid loss: 5.489704988116309
Training loss: 4.171500205993652 / Valid loss: 5.528871306918917
Training loss: 4.581844329833984 / Valid loss: 5.519865229016259
Training loss: 5.250172138214111 / Valid loss: 5.538011042277018
Training loss: 4.768006801605225 / Valid loss: 5.48284268833342

Epoch: 3
Training loss: 3.258040428161621 / Valid loss: 5.480120656603859
Training loss: 6.456144332885742 / Valid loss: 5.503665631158011
Training loss: 6.227110862731934 / Valid loss: 5.49132494472322
Training loss: 4.628060340881348 / Valid loss: 5.504633599235898
Training loss: 5.837442398071289 / Valid loss: 5.468642614001319

Epoch: 4
Training loss: 4.1218390464782715 / Valid loss: 5.5150923819769
Training loss: 3.988144636154175 / Valid loss: 5.586585787364415
Training loss: 5.536558628082275 / Valid loss: 5.564151997793289
Training loss: 3.5385050773620605 / Valid loss: 5.564602522622971
Training loss: 6.0717620849609375 / Valid loss: 5.573944809323265

Epoch: 5
Training loss: 4.224312782287598 / Valid loss: 5.534787069048201
Training loss: 4.923149585723877 / Valid loss: 5.583995026633853
Training loss: 3.1473147869110107 / Valid loss: 5.637420554388137
Training loss: 5.238935470581055 / Valid loss: 5.653192858468919
Training loss: 5.889276504516602 / Valid loss: 5.578010325204758

Epoch: 6
Training loss: 4.963746070861816 / Valid loss: 5.530899906158448
Training loss: 6.308085918426514 / Valid loss: 5.730139852705456
Training loss: 5.790842056274414 / Valid loss: 5.704905530384608
Training loss: 5.413238525390625 / Valid loss: 5.658217818396432
Training loss: 5.588495254516602 / Valid loss: 5.699164894648961

Epoch: 7
Training loss: 4.701962471008301 / Valid loss: 5.65589709736052
Training loss: 5.319433212280273 / Valid loss: 5.754926107043311
Training loss: 5.663423538208008 / Valid loss: 5.696326410202753
Training loss: 4.0507378578186035 / Valid loss: 5.770732198442731
Training loss: 4.891251564025879 / Valid loss: 5.784535875774565

Epoch: 8
Training loss: 5.013442039489746 / Valid loss: 5.673888183775402
Training loss: 2.5040132999420166 / Valid loss: 5.813339308329991
Training loss: 4.160885334014893 / Valid loss: 5.829583924157279
Training loss: 4.836391925811768 / Valid loss: 5.792780460630144
Training loss: 4.9122419357299805 / Valid loss: 5.76085562251863

Epoch: 9
Training loss: 4.639671325683594 / Valid loss: 5.737889285314651
Training loss: 3.326178550720215 / Valid loss: 5.8238409837087
Training loss: 5.799674987792969 / Valid loss: 5.953509303501674
Training loss: 5.496761322021484 / Valid loss: 5.854629789079938
Training loss: 3.942525863647461 / Valid loss: 5.782990051451184

Epoch: 10
Training loss: 3.599902629852295 / Valid loss: 5.882679080963134
Training loss: 4.338655948638916 / Valid loss: 5.882766333080474
Training loss: 4.971693992614746 / Valid loss: 5.953856020882016
Training loss: 2.708071231842041 / Valid loss: 5.901724676858811
Training loss: 4.950852394104004 / Valid loss: 5.9846849214463

Epoch: 11
Training loss: 3.866006374359131 / Valid loss: 5.8829429217747276
Training loss: 2.6201162338256836 / Valid loss: 5.911446414675031
Training loss: 3.222883462905884 / Valid loss: 5.922115739186605
Training loss: 3.550039291381836 / Valid loss: 5.9814375968206495
Training loss: 6.736564636230469 / Valid loss: 5.947413162958054

Epoch: 12
Training loss: 2.8023793697357178 / Valid loss: 5.906735016050793
Training loss: 4.5782999992370605 / Valid loss: 6.033841239838373
Training loss: 3.9680726528167725 / Valid loss: 5.989717983064198
Training loss: 3.4206647872924805 / Valid loss: 5.977214656557355
Training loss: 3.8720316886901855 / Valid loss: 6.036121815726871

Epoch: 13
Training loss: 2.5205025672912598 / Valid loss: 5.982526749656314
Training loss: 4.245814323425293 / Valid loss: 6.049062613078526
Training loss: 3.6999895572662354 / Valid loss: 6.146177764165969
Training loss: 4.4755096435546875 / Valid loss: 6.085800354821341
Training loss: 4.016820430755615 / Valid loss: 6.1872988791692825

Epoch: 14
Training loss: 2.884705066680908 / Valid loss: 6.063606809434437
Training loss: 2.739684581756592 / Valid loss: 6.159433208193097
Training loss: 4.547239303588867 / Valid loss: 6.135288304374331
Training loss: 2.4620609283447266 / Valid loss: 6.169400049391247
Training loss: 5.413201332092285 / Valid loss: 6.210640664327713

Epoch: 15
Training loss: 4.61216926574707 / Valid loss: 6.1016394183749245
Training loss: 3.2590749263763428 / Valid loss: 6.213655033565703
Training loss: 3.2639923095703125 / Valid loss: 6.106159637087868
Training loss: 2.9742610454559326 / Valid loss: 6.152637116114298
Training loss: 3.8122408390045166 / Valid loss: 6.434505426316035

Epoch: 16
Training loss: 3.693918466567993 / Valid loss: 6.287404382796515
Training loss: 3.7933735847473145 / Valid loss: 6.256161678405035
Training loss: 3.1319966316223145 / Valid loss: 6.318925217219761
Training loss: 3.5736684799194336 / Valid loss: 6.400255007970901
Training loss: 3.9738197326660156 / Valid loss: 6.189396038509551

Epoch: 17
Training loss: 3.053746223449707 / Valid loss: 6.3151915005275185
Training loss: 3.6155757904052734 / Valid loss: 6.237120346795945
Training loss: 2.762166738510132 / Valid loss: 6.18828832081386
Training loss: 3.2854671478271484 / Valid loss: 6.1936770484561015
Training loss: 3.782097578048706 / Valid loss: 6.189570068177723

Epoch: 18
Training loss: 3.3415942192077637 / Valid loss: 6.194442674091884
Training loss: 2.9189655780792236 / Valid loss: 6.347582762581961
Training loss: 2.9870316982269287 / Valid loss: 6.432843290056501
Training loss: 2.5684146881103516 / Valid loss: 6.27772068977356
Training loss: 4.566215515136719 / Valid loss: 6.283059678758893

Epoch: 19
Training loss: 2.8220973014831543 / Valid loss: 6.219338271731422
Training loss: 2.4467718601226807 / Valid loss: 6.334908242452713
Training loss: 3.160829782485962 / Valid loss: 6.296823058809553
Training loss: 3.9243383407592773 / Valid loss: 6.339158666701544
Training loss: 3.0777511596679688 / Valid loss: 6.305897939772833

Epoch: 20
Training loss: 3.3148250579833984 / Valid loss: 6.285206613086519
Training loss: 3.1047136783599854 / Valid loss: 6.4280450253259565
Training loss: 3.7249178886413574 / Valid loss: 6.318487526121594
Training loss: 2.353877544403076 / Valid loss: 6.268608967463176
Training loss: 4.326116561889648 / Valid loss: 6.3385789144606814

Epoch: 21
Training loss: 2.324143409729004 / Valid loss: 6.342696074077061
Training loss: 3.678887128829956 / Valid loss: 6.418085427511306
Training loss: 2.9428458213806152 / Valid loss: 6.4392268839336575
Training loss: 2.6394882202148438 / Valid loss: 6.313000978742327
Training loss: 2.352994441986084 / Valid loss: 6.38570644287836

Epoch: 22
Training loss: 1.9101958274841309 / Valid loss: 6.381631056467692
Training loss: 2.321016788482666 / Valid loss: 6.424162178947812
Training loss: 2.9354753494262695 / Valid loss: 6.490477736790975
Training loss: 3.865241765975952 / Valid loss: 6.55445298013233
Training loss: 2.53408145904541 / Valid loss: 6.5477844329107375

Epoch: 23
Training loss: 2.649138927459717 / Valid loss: 6.499771676744733
Training loss: 1.9638911485671997 / Valid loss: 6.541747293018159
Training loss: 2.821789026260376 / Valid loss: 6.517119636989775
Training loss: 2.899805784225464 / Valid loss: 6.480046631041027
Training loss: 2.939570426940918 / Valid loss: 6.3849413894471665

Epoch: 24
Training loss: 2.7104997634887695 / Valid loss: 6.412493885131109
Training loss: 2.568939685821533 / Valid loss: 6.958593886239188
Training loss: 2.4790122509002686 / Valid loss: 6.543829804375058
Training loss: 3.402552366256714 / Valid loss: 6.597983178638277
Training loss: 2.9698381423950195 / Valid loss: 6.570664682842437

Epoch: 25
Training loss: 3.2485365867614746 / Valid loss: 6.487678972880046
Training loss: 2.6523900032043457 / Valid loss: 6.581855696723575
Training loss: 2.918275833129883 / Valid loss: 6.574765250796363
Training loss: 2.288717746734619 / Valid loss: 6.576678739275251
Training loss: 4.231385231018066 / Valid loss: 6.508523019154866

Epoch: 26
Training loss: 2.488403797149658 / Valid loss: 6.464650424321492
Training loss: 2.503349781036377 / Valid loss: 6.4724921726045155
Training loss: 2.7348520755767822 / Valid loss: 6.482537696475075
Training loss: 2.887481451034546 / Valid loss: 6.530499158586775
Training loss: 1.7458873987197876 / Valid loss: 6.523930358886719

Epoch: 27
Training loss: 2.307431697845459 / Valid loss: 6.580012607574463
Training loss: 1.7509386539459229 / Valid loss: 6.517878875278291
Training loss: 3.1056642532348633 / Valid loss: 6.510267680031912
Training loss: 2.325291633605957 / Valid loss: 6.665057790847052
Training loss: 2.5081210136413574 / Valid loss: 6.6831371784210205

Epoch: 28
Training loss: 1.530068278312683 / Valid loss: 6.526379907698859
Training loss: 1.8600151538848877 / Valid loss: 6.526224526904878
Training loss: 3.1027121543884277 / Valid loss: 6.522832084837414
Training loss: 1.500227689743042 / Valid loss: 6.597483548663911
Training loss: 2.2360572814941406 / Valid loss: 6.58938630876087

Epoch: 29
Training loss: 2.0216894149780273 / Valid loss: 6.59528074037461
Training loss: 2.527930498123169 / Valid loss: 6.489182960419428
Training loss: 2.641831159591675 / Valid loss: 6.696459225245884
Training loss: 2.668839931488037 / Valid loss: 6.579600211552211
Training loss: 1.67850923538208 / Valid loss: 6.590031242370605

Epoch: 30
Training loss: 1.8792706727981567 / Valid loss: 6.679408700125558
Training loss: 2.147426128387451 / Valid loss: 6.697324775514149
Training loss: 2.342949867248535 / Valid loss: 6.556637119111561
Training loss: 2.094614028930664 / Valid loss: 6.6304075786045615
Training loss: 2.190310478210449 / Valid loss: 6.7184756574176605

Epoch: 31
Training loss: 2.66898775100708 / Valid loss: 6.714188430422828
Training loss: 1.3395885229110718 / Valid loss: 6.586913058871315
Training loss: 1.7800624370574951 / Valid loss: 6.5694290479024255
Training loss: 3.2988741397857666 / Valid loss: 6.64605177470616
Training loss: 2.3805744647979736 / Valid loss: 6.6322385061354865

Epoch: 32
Training loss: 1.8827377557754517 / Valid loss: 6.5492203916822165
Training loss: 2.0882482528686523 / Valid loss: 6.623232074010939
Training loss: 2.0675578117370605 / Valid loss: 6.612973440261114
Training loss: 2.1023359298706055 / Valid loss: 6.532480669021607
Training loss: 1.895084261894226 / Valid loss: 6.6583106994628904

Epoch: 33
Training loss: 1.528965950012207 / Valid loss: 6.76826368967692
Training loss: 2.294678211212158 / Valid loss: 6.620521690731957
Training loss: 2.477874755859375 / Valid loss: 6.688595567430768
Training loss: 1.7876899242401123 / Valid loss: 6.68285942304702
Training loss: 2.1501684188842773 / Valid loss: 6.639665263039725

Epoch: 34
Training loss: 1.8466556072235107 / Valid loss: 6.634074767430623
Training loss: 1.976917028427124 / Valid loss: 6.671724003837222
Training loss: 1.869726538658142 / Valid loss: 6.62430477142334
Training loss: 2.0900206565856934 / Valid loss: 6.773813674563454
Training loss: 2.243086338043213 / Valid loss: 6.756731294450306

Epoch: 35
Training loss: 2.3041787147521973 / Valid loss: 6.659629149664016
Training loss: 1.8840886354446411 / Valid loss: 6.628761523110526
Training loss: 3.0045371055603027 / Valid loss: 6.617353148687453
Training loss: 2.0190038681030273 / Valid loss: 6.60260104678926
Training loss: 1.9440205097198486 / Valid loss: 6.672582197189331

Epoch: 36
Training loss: 1.843945026397705 / Valid loss: 6.709347148168654
Training loss: 2.7640137672424316 / Valid loss: 6.724860802150908
Training loss: 2.216557502746582 / Valid loss: 6.6697835422697525
Training loss: 2.348792791366577 / Valid loss: 6.620582317170642
Training loss: 2.249934673309326 / Valid loss: 6.590014130728585

Epoch: 37
Training loss: 1.8087096214294434 / Valid loss: 6.6108936446053645
Training loss: 2.7243354320526123 / Valid loss: 6.601340616317023
Training loss: 2.613199234008789 / Valid loss: 6.627313039416358
Training loss: 1.8859210014343262 / Valid loss: 6.701983286085583
Training loss: 1.935924768447876 / Valid loss: 6.78309121131897

Epoch: 38
Training loss: 2.420578718185425 / Valid loss: 6.608095598220825
Training loss: 2.138028383255005 / Valid loss: 6.723089100065685
Training loss: 1.8798669576644897 / Valid loss: 6.699796118055072
Training loss: 1.6043370962142944 / Valid loss: 6.673064200083415
Training loss: 1.7310028076171875 / Valid loss: 6.829348268963042

Epoch: 39
Training loss: 1.571817398071289 / Valid loss: 6.665902056012835
Training loss: 1.869643211364746 / Valid loss: 6.748458882740565
Training loss: 2.0382025241851807 / Valid loss: 6.76907103402274
Training loss: 2.096113681793213 / Valid loss: 6.660524109431676
Training loss: 1.5757439136505127 / Valid loss: 6.696244741621472

Epoch: 40
Training loss: 2.15858793258667 / Valid loss: 6.654723233268374
Training loss: 1.4127506017684937 / Valid loss: 6.650458814984276
Training loss: 1.309342622756958 / Valid loss: 6.70603225798834
Training loss: 2.594672918319702 / Valid loss: 6.664328840800694
Training loss: 2.0069141387939453 / Valid loss: 6.652937557583764

Epoch: 41
Training loss: 1.126526951789856 / Valid loss: 6.62013027100336
Training loss: 1.1614052057266235 / Valid loss: 6.646353465034848
Training loss: 1.8734493255615234 / Valid loss: 6.684980269840786
Training loss: 2.4359912872314453 / Valid loss: 6.758556359154838
Training loss: 2.436507225036621 / Valid loss: 6.6367995262146

Epoch: 42
Training loss: 1.9361984729766846 / Valid loss: 6.654213887169248
Training loss: 1.3503586053848267 / Valid loss: 6.773292205447242
Training loss: 1.4940786361694336 / Valid loss: 6.621325974237351
Training loss: 1.7554309368133545 / Valid loss: 6.680746126174927
Training loss: 1.928849458694458 / Valid loss: 6.640324270157587

Epoch: 43
Training loss: 1.4718443155288696 / Valid loss: 6.727827499026344
Training loss: 2.7302825450897217 / Valid loss: 6.764437107812791
Training loss: 1.342885971069336 / Valid loss: 6.672410242898124
Training loss: 1.62461519241333 / Valid loss: 6.691459108534313
Training loss: 2.246650218963623 / Valid loss: 6.711026169004894

Epoch: 44
Training loss: 1.163482427597046 / Valid loss: 6.700686881655739
Training loss: 1.130936622619629 / Valid loss: 6.621792395909627
Training loss: 1.5483523607254028 / Valid loss: 6.626452041807629
Training loss: 1.764488697052002 / Valid loss: 6.8396369548071
Training loss: 1.2923691272735596 / Valid loss: 6.750418613070534

Epoch: 45
Training loss: 1.942577838897705 / Valid loss: 6.711346165339152
Training loss: 1.2838406562805176 / Valid loss: 6.721186606089274
Training loss: 1.4675824642181396 / Valid loss: 6.701606007984706
Training loss: 1.7800811529159546 / Valid loss: 6.724439714068458
Training loss: 2.6791954040527344 / Valid loss: 6.824350729442778

Epoch: 46
Training loss: 1.8622164726257324 / Valid loss: 6.779054276148478
Training loss: 1.6058921813964844 / Valid loss: 6.72167800721668
Training loss: 1.7782070636749268 / Valid loss: 6.697429652441116
Training loss: 2.1771397590637207 / Valid loss: 6.684687010447184
Training loss: 1.4328253269195557 / Valid loss: 6.6912867546081545

Epoch: 47
Training loss: 1.2903940677642822 / Valid loss: 6.683285599663144
Training loss: 1.8411240577697754 / Valid loss: 6.7142445178258985
Training loss: 1.395061731338501 / Valid loss: 6.711065964471726
Training loss: 1.5405018329620361 / Valid loss: 6.7023765609377906
Training loss: 2.118659734725952 / Valid loss: 6.657102734701974

Epoch: 48
Training loss: 1.4892078638076782 / Valid loss: 6.682340726398286
Training loss: 1.8403255939483643 / Valid loss: 6.778252919514974
Training loss: 1.7121894359588623 / Valid loss: 6.708922881171817
Training loss: 1.324607253074646 / Valid loss: 6.73833053452628
Training loss: 1.6718863248825073 / Valid loss: 6.747479148138137

Epoch: 49
Training loss: 1.269693374633789 / Valid loss: 6.677031834920247
Training loss: 1.986769437789917 / Valid loss: 6.687090801057361
Training loss: 1.0910248756408691 / Valid loss: 6.775187787555513
Training loss: 1.3553640842437744 / Valid loss: 6.835079529171899
Training loss: 1.5544769763946533 / Valid loss: 6.880240626562209

Epoch: 50
Training loss: 1.939821720123291 / Valid loss: 6.802974215007963
Training loss: 1.2692489624023438 / Valid loss: 6.798801054273333
Training loss: 1.3130412101745605 / Valid loss: 6.712849900836036
Training loss: 2.056403636932373 / Valid loss: 6.684672296614874
Training loss: 1.300306797027588 / Valid loss: 6.723520528702509

Epoch: 51
Training loss: 1.7814586162567139 / Valid loss: 6.724106218701317
Training loss: 1.9752757549285889 / Valid loss: 6.775675680523827
Training loss: 1.8562430143356323 / Valid loss: 6.761546541395641
Training loss: 1.1512153148651123 / Valid loss: 6.766123544602167
Training loss: 1.5202962160110474 / Valid loss: 6.767053431556338

Epoch: 52
Training loss: 1.7648506164550781 / Valid loss: 6.71528218133109
Training loss: 1.8604989051818848 / Valid loss: 6.750388145446777
Training loss: 1.337533712387085 / Valid loss: 6.710472574688139
Training loss: 1.503401279449463 / Valid loss: 6.739590942291986
Training loss: 1.4278877973556519 / Valid loss: 6.72378252801441

Epoch: 53
Training loss: 1.6467421054840088 / Valid loss: 6.716267267862956
Training loss: 1.4867463111877441 / Valid loss: 6.656991331917899
Training loss: 2.0886459350585938 / Valid loss: 6.701277278718495
Training loss: 2.0240020751953125 / Valid loss: 6.818992605663481
Training loss: 1.5783576965332031 / Valid loss: 6.740770153772264

Epoch: 54
Training loss: 1.3632259368896484 / Valid loss: 6.769328798566546
Training loss: 2.004835844039917 / Valid loss: 6.732434022994268
Training loss: 1.8060415983200073 / Valid loss: 6.626293404897054
Training loss: 1.981173038482666 / Valid loss: 6.8068963868277415
Training loss: 1.4649466276168823 / Valid loss: 6.773235757010323

Epoch: 55
Training loss: 1.230387568473816 / Valid loss: 6.756114537375314
Training loss: 2.5248732566833496 / Valid loss: 6.759454225358509
Training loss: 1.4907517433166504 / Valid loss: 6.716279722395397
Training loss: 1.3549834489822388 / Valid loss: 6.789696130298433
Training loss: 1.7136391401290894 / Valid loss: 6.689329015640985

Epoch: 56
Training loss: 1.4402474164962769 / Valid loss: 6.690253530229841
Training loss: 1.4681285619735718 / Valid loss: 6.639913059416271
Training loss: 1.1178923845291138 / Valid loss: 6.772415442693801
Training loss: 1.2043969631195068 / Valid loss: 6.8597396419161845
Training loss: 1.3043286800384521 / Valid loss: 6.652490304765247

Epoch: 57
Training loss: 1.3487383127212524 / Valid loss: 6.724936439877465
Training loss: 1.4232940673828125 / Valid loss: 6.832337938036237
Training loss: 1.3303214311599731 / Valid loss: 6.764053381057012
Training loss: 1.9861310720443726 / Valid loss: 6.774621631985619
Training loss: 1.22324800491333 / Valid loss: 6.766134965987432

Epoch: 58
Training loss: 1.0078595876693726 / Valid loss: 6.734367602212089
Training loss: 1.2707428932189941 / Valid loss: 6.736689301899501
Training loss: 1.3117401599884033 / Valid loss: 6.806420675913492
Training loss: 1.5879638195037842 / Valid loss: 6.68796048391433
Training loss: 1.8174042701721191 / Valid loss: 6.759024956112817

Epoch: 59
Training loss: 2.038550853729248 / Valid loss: 6.628471326828003
Training loss: 1.662083625793457 / Valid loss: 6.696656583604359
Training loss: 2.2491250038146973 / Valid loss: 6.716803552990868
Training loss: 1.8317930698394775 / Valid loss: 6.79605195635841
Training loss: 1.8049137592315674 / Valid loss: 6.8154974392482215

Epoch: 60
Training loss: 1.5404958724975586 / Valid loss: 6.717917083558582
Training loss: 1.2736847400665283 / Valid loss: 6.666271341414679
Training loss: 1.1720085144042969 / Valid loss: 6.730328391847157
Training loss: 1.7192103862762451 / Valid loss: 6.6923734483264745
Training loss: 1.4637761116027832 / Valid loss: 6.798699683234805

Epoch: 61
Training loss: 1.313367486000061 / Valid loss: 6.770954259236654
Training loss: 1.6314276456832886 / Valid loss: 6.761543773469471
Training loss: 1.4705934524536133 / Valid loss: 6.745005380539667
Training loss: 1.0058889389038086 / Valid loss: 6.617492689405169
Training loss: 1.6896660327911377 / Valid loss: 6.741579042162214

Epoch: 62
Training loss: 0.9731235504150391 / Valid loss: 6.750537336440313
Training loss: 1.4199285507202148 / Valid loss: 6.707157500584921
Training loss: 1.4014003276824951 / Valid loss: 6.689111988885062
Training loss: 1.418652892112732 / Valid loss: 6.686278654280163
Training loss: 1.3526341915130615 / Valid loss: 6.789522382191249

Epoch: 63
Training loss: 1.4454281330108643 / Valid loss: 6.670786912100656
Training loss: 1.7979230880737305 / Valid loss: 6.687475660869054
Training loss: 1.5461628437042236 / Valid loss: 6.732686056409563
Training loss: 1.1285951137542725 / Valid loss: 6.712600367409842
Training loss: 1.6272358894348145 / Valid loss: 6.823097301664807

Epoch: 64
Training loss: 1.052520990371704 / Valid loss: 6.70562973022461
Training loss: 1.5194543600082397 / Valid loss: 6.743686921255929
Training loss: 1.3062247037887573 / Valid loss: 6.672434974852062
Training loss: 1.5380147695541382 / Valid loss: 6.711818913051061
Training loss: 0.9398899674415588 / Valid loss: 6.701340084984189

Epoch: 65
Training loss: 1.4954184293746948 / Valid loss: 6.7051605746859595
Training loss: 1.167712926864624 / Valid loss: 6.608431071326846
Training loss: 0.9234734773635864 / Valid loss: 6.725110885075161
Training loss: 1.0613821744918823 / Valid loss: 6.824208729607719
Training loss: 1.2489598989486694 / Valid loss: 6.687274814787365

Epoch: 66
Training loss: 1.4281642436981201 / Valid loss: 6.715060429345994
Training loss: 2.3581714630126953 / Valid loss: 6.829178980418614
Training loss: 1.870497703552246 / Valid loss: 6.762143625531878
Training loss: 2.394848585128784 / Valid loss: 6.7019656431107295
Training loss: 1.4451360702514648 / Valid loss: 6.725055733181182

Epoch: 67
Training loss: 1.7307369709014893 / Valid loss: 6.755952826000395
Training loss: 0.793510913848877 / Valid loss: 6.678494446618217
Training loss: 1.2430630922317505 / Valid loss: 6.757479595002674
Training loss: 1.1139190196990967 / Valid loss: 6.670404929206485
Training loss: 0.9830577373504639 / Valid loss: 6.774774335679554

Epoch: 68
Training loss: 1.2914283275604248 / Valid loss: 6.752375561850411
Training loss: 1.339430570602417 / Valid loss: 6.659132137752715
Training loss: 1.463036298751831 / Valid loss: 6.651210280827113
Training loss: 1.3161917924880981 / Valid loss: 6.737080760229201
Training loss: 1.2982749938964844 / Valid loss: 6.721725591023763

Epoch: 69
Training loss: 1.1947122812271118 / Valid loss: 6.6636029243469235
Training loss: 0.8675093650817871 / Valid loss: 6.697130834488641
Training loss: 1.1121623516082764 / Valid loss: 6.702011830466134
Training loss: 1.378380537033081 / Valid loss: 6.683154424031575
Training loss: 0.7530165910720825 / Valid loss: 6.749200882230486

Epoch: 70
Training loss: 1.3115423917770386 / Valid loss: 6.751449294317336
Training loss: 1.196580171585083 / Valid loss: 6.7194196928115115
Training loss: 0.8281173706054688 / Valid loss: 6.702612020855859
Training loss: 1.2601438760757446 / Valid loss: 6.702893706730434
Training loss: 1.315327525138855 / Valid loss: 6.757553706850325

Epoch: 71
Training loss: 1.1568377017974854 / Valid loss: 6.798297723134358
Training loss: 1.1448463201522827 / Valid loss: 6.678515743073963
Training loss: 0.8601843118667603 / Valid loss: 6.683359856832595
Training loss: 1.5396122932434082 / Valid loss: 6.6380603381565635
Training loss: 1.1979222297668457 / Valid loss: 6.6949006353105815

Epoch: 72
Training loss: 1.7615245580673218 / Valid loss: 6.962992055075509
Training loss: 1.4716684818267822 / Valid loss: 6.693999744596935
Training loss: 1.3061461448669434 / Valid loss: 6.646029758453369
Training loss: 1.0992941856384277 / Valid loss: 6.60345169930231
Training loss: 1.0274426937103271 / Valid loss: 6.659593237014044

Epoch: 73
Training loss: 1.0071167945861816 / Valid loss: 6.7277948311397004
Training loss: 1.533583164215088 / Valid loss: 6.664386719749087
Training loss: 1.9097424745559692 / Valid loss: 6.7117089180719285
Training loss: 0.9470469951629639 / Valid loss: 6.717108617510115
Training loss: 1.388614296913147 / Valid loss: 6.66989319438026

Epoch: 74
Training loss: 1.1890087127685547 / Valid loss: 6.675595051901681
Training loss: 1.1287999153137207 / Valid loss: 6.7089948608761745
Training loss: 1.4269663095474243 / Valid loss: 6.76509428024292
Training loss: 1.2015185356140137 / Valid loss: 6.650439673378354
Training loss: 1.2041833400726318 / Valid loss: 6.718629078637986

Epoch: 75
Training loss: 1.071929931640625 / Valid loss: 6.681779797871908
Training loss: 1.2479419708251953 / Valid loss: 6.682120768229167
Training loss: 1.5273804664611816 / Valid loss: 6.683052328654698
Training loss: 1.4349110126495361 / Valid loss: 6.75940545627049
Training loss: 1.377680778503418 / Valid loss: 6.700925708952404

Epoch: 76
Training loss: 1.463853120803833 / Valid loss: 6.6935562338147845
Training loss: 1.1056511402130127 / Valid loss: 6.722923719315302
Training loss: 0.9633333683013916 / Valid loss: 6.740454555693127
Training loss: 1.6688225269317627 / Valid loss: 6.727343899863107
Training loss: 1.2066733837127686 / Valid loss: 6.8732163974217

Epoch: 77
Training loss: 0.994330644607544 / Valid loss: 6.709336859839303
Training loss: 1.2165143489837646 / Valid loss: 6.704484717051188
Training loss: 1.2644848823547363 / Valid loss: 6.748776320048741
Training loss: 1.1192864179611206 / Valid loss: 6.792215610685803
Training loss: 1.3232356309890747 / Valid loss: 6.735466096514747

Epoch: 78
Training loss: 0.954835057258606 / Valid loss: 6.7416508084251765
Training loss: 0.9319121241569519 / Valid loss: 6.684671129499163
Training loss: 1.024776816368103 / Valid loss: 6.743982848666963
Training loss: 1.543152928352356 / Valid loss: 6.75698834827968
Training loss: 1.1956697702407837 / Valid loss: 6.692509832836333

Epoch: 79
Training loss: 0.9713560342788696 / Valid loss: 6.729613008953276
Training loss: 1.5063438415527344 / Valid loss: 6.712537225087484
Training loss: 1.0461796522140503 / Valid loss: 6.667363298506964
Training loss: 1.260087490081787 / Valid loss: 6.71606992994036
Training loss: 1.0139316320419312 / Valid loss: 6.791647486459642

Epoch: 80
Training loss: 1.0510852336883545 / Valid loss: 6.678684666043236
Training loss: 1.4049409627914429 / Valid loss: 6.657055337088448
Training loss: 1.5686798095703125 / Valid loss: 6.674669763020106
Training loss: 1.3794746398925781 / Valid loss: 6.704526814960298
Training loss: 1.0085210800170898 / Valid loss: 6.78897723924546

Epoch: 81
Training loss: 1.1566412448883057 / Valid loss: 6.776919819059826
Training loss: 1.4534237384796143 / Valid loss: 6.762672733125233
Training loss: 1.1408491134643555 / Valid loss: 6.754528527032761
Training loss: 1.5586124658584595 / Valid loss: 6.755475493839809
Training loss: 1.135288953781128 / Valid loss: 6.803004728044782

Epoch: 82
Training loss: 1.8841092586517334 / Valid loss: 6.786001552854265
Training loss: 1.0823969841003418 / Valid loss: 6.722558945701236
Training loss: 1.2746496200561523 / Valid loss: 6.741951492854527
Training loss: 1.1269878149032593 / Valid loss: 6.75969641095116
Training loss: 1.0838514566421509 / Valid loss: 6.683160627455939

Epoch: 83
Training loss: 0.8964053392410278 / Valid loss: 6.758379904429118
Training loss: 1.0774898529052734 / Valid loss: 6.69592417762393
Training loss: 1.3510247468948364 / Valid loss: 6.7257432301839195
Training loss: 0.9596583843231201 / Valid loss: 6.74037663596017
Training loss: 1.206413984298706 / Valid loss: 6.679377210707892

Epoch: 84
Training loss: 0.6385658979415894 / Valid loss: 6.681344867887951
Training loss: 1.2222694158554077 / Valid loss: 6.747637825920468
Training loss: 1.7067756652832031 / Valid loss: 6.6629538831256685
Training loss: 1.4986131191253662 / Valid loss: 6.7106902849106564
Training loss: 1.2076690196990967 / Valid loss: 6.770153690519787

Epoch: 85
Training loss: 1.5011754035949707 / Valid loss: 6.6959691410972955
Training loss: 0.8535914421081543 / Valid loss: 6.675704111371721
Training loss: 1.5390899181365967 / Valid loss: 6.7298990249633786
Training loss: 1.215035319328308 / Valid loss: 6.742816645758492
Training loss: 1.1675541400909424 / Valid loss: 6.708455685206822

Epoch: 86
Training loss: 1.0136626958847046 / Valid loss: 6.740301565896897
Training loss: 1.1531682014465332 / Valid loss: 6.782101299649193
Training loss: 0.625296950340271 / Valid loss: 6.6922206197466165
Training loss: 1.1193339824676514 / Valid loss: 6.763225941430955
Training loss: 1.0549523830413818 / Valid loss: 6.756693576631092

Epoch: 87
Training loss: 1.4008736610412598 / Valid loss: 6.657325331370036
Training loss: 1.5762139558792114 / Valid loss: 6.686747482844761
Training loss: 1.0173795223236084 / Valid loss: 6.695484656379336
Training loss: 2.0310750007629395 / Valid loss: 6.697906049092611
Training loss: 0.8911489248275757 / Valid loss: 6.7024311973935085

Epoch: 88
Training loss: 0.7706273198127747 / Valid loss: 6.709204144704909
Training loss: 1.421036720275879 / Valid loss: 6.7210066068740115
Training loss: 1.347719669342041 / Valid loss: 6.707858889443534
Training loss: 1.3582321405410767 / Valid loss: 6.669243921552385
Training loss: 0.9809350967407227 / Valid loss: 6.703244266055879

Epoch: 89
Training loss: 1.1781964302062988 / Valid loss: 6.747471645900181
Training loss: 0.9840997457504272 / Valid loss: 6.696579644793556
Training loss: 1.01051664352417 / Valid loss: 6.682814804712931
Training loss: 1.1528265476226807 / Valid loss: 6.73842712583996
Training loss: 0.9269729256629944 / Valid loss: 6.6677481605893085

Epoch: 90
Training loss: 1.0136170387268066 / Valid loss: 6.752661405290876
Training loss: 0.7901768088340759 / Valid loss: 6.684377266111828
Training loss: 1.2611258029937744 / Valid loss: 6.701669438680013
Training loss: 1.4122101068496704 / Valid loss: 6.675627563113258
Training loss: 1.040107250213623 / Valid loss: 6.7017089389619375

Epoch: 91
Training loss: 1.1872107982635498 / Valid loss: 6.750608916509719
Training loss: 1.4415576457977295 / Valid loss: 6.746756676265171
Training loss: 1.3528163433074951 / Valid loss: 6.758981477646601
Training loss: 1.1128134727478027 / Valid loss: 6.688108017331078
Training loss: 1.128652811050415 / Valid loss: 6.71679995400565

Epoch: 92
Training loss: 0.6303426623344421 / Valid loss: 6.705101117633638
Training loss: 1.0113083124160767 / Valid loss: 6.668464760553269
Training loss: 1.0663878917694092 / Valid loss: 6.662240748178391
Training loss: 1.4116218090057373 / Valid loss: 6.687368797120594
Training loss: 1.0381832122802734 / Valid loss: 6.659184010823568

Epoch: 93
Training loss: 0.875133216381073 / Valid loss: 6.706664316994804
Training loss: 1.5037264823913574 / Valid loss: 6.650368567875454
Training loss: 1.1948189735412598 / Valid loss: 6.66454496383667
Training loss: 0.8263563513755798 / Valid loss: 6.654223355792817
Training loss: 0.6493769884109497 / Valid loss: 6.672081384204683

Epoch: 94
Training loss: 0.9765952229499817 / Valid loss: 6.703967689332508
Training loss: 0.8066350221633911 / Valid loss: 6.651108364831834
Training loss: 1.24006986618042 / Valid loss: 6.715958593005226
Training loss: 1.4916622638702393 / Valid loss: 6.683341105779012
Training loss: 0.7712010145187378 / Valid loss: 6.618206292106992

Epoch: 95
Training loss: 0.7878865003585815 / Valid loss: 6.739622188749768
Training loss: 1.1025890111923218 / Valid loss: 6.669905539921352
Training loss: 1.216185212135315 / Valid loss: 6.730372008823213
Training loss: 1.0557138919830322 / Valid loss: 6.685115073976062
Training loss: 1.0631195306777954 / Valid loss: 6.736676479521252

Epoch: 96
Training loss: 0.48580530285835266 / Valid loss: 6.659851478395008
Training loss: 0.7372503280639648 / Valid loss: 6.653440262022473
Training loss: 1.1226913928985596 / Valid loss: 6.742840346835909
Training loss: 1.1214884519577026 / Valid loss: 6.674363422393799
Training loss: 1.3117941617965698 / Valid loss: 6.725112726574852

Epoch: 97
Training loss: 0.9400950074195862 / Valid loss: 6.746326237633115
Training loss: 0.9404789805412292 / Valid loss: 6.738839117685954
Training loss: 1.229048252105713 / Valid loss: 6.709299291883196
Training loss: 2.0289087295532227 / Valid loss: 6.730719961438861
Training loss: 1.2870018482208252 / Valid loss: 6.684153025490897

Epoch: 98
Training loss: 0.9713738560676575 / Valid loss: 6.707232488904681
Training loss: 0.9635635614395142 / Valid loss: 6.674637576511928
Training loss: 1.1155481338500977 / Valid loss: 6.683363691965739
Training loss: 1.321288824081421 / Valid loss: 6.712747278667631
Training loss: 0.7726095914840698 / Valid loss: 6.702564868472871

Epoch: 99
Training loss: 0.9563413262367249 / Valid loss: 6.68213229633513
Training loss: 0.8489862680435181 / Valid loss: 6.680335667019799
Training loss: 0.8987122178077698 / Valid loss: 6.676394271850586
Training loss: 0.7011114358901978 / Valid loss: 6.723664229256766
Training loss: 0.9732711315155029 / Valid loss: 6.700943320138114
neural net:
 [tensor([[ 0.0163,  0.0188,  0.0086,  ..., -0.0222,  0.0101,  0.0086],
        [-0.0448, -0.0216, -0.0461,  ...,  0.0112, -0.0011, -0.0133],
        [-0.0607, -0.0262, -0.0074,  ..., -0.0204,  0.0017,  0.0148],
        ...,
        [ 0.0094,  0.0154, -0.0289,  ...,  0.0068,  0.0058,  0.0041],
        [-0.0160,  0.0210, -0.0083,  ..., -0.0031,  0.0162, -0.0116],
        [-0.0162,  0.0263, -0.0105,  ..., -0.0116,  0.0108,  0.0146]],
       device='cuda:0'), tensor([-1.0714e-02, -2.1523e-03, -4.2817e-03, -1.0067e-02, -1.4667e-02,
        -2.2468e-02, -1.5533e-02, -9.8564e-03, -1.9927e-03, -2.7076e-02,
        -4.4101e-05, -5.2547e-03, -5.7580e-03, -5.5729e-03, -2.2397e-02,
        -2.3076e-02,  4.7217e-03, -9.8505e-03, -1.2838e-02, -1.0090e-02,
        -1.5151e-02, -3.5023e-03, -1.2938e-02, -2.2220e-03, -9.1797e-04,
        -6.7367e-03,  1.7211e-03, -1.5087e-02,  2.0487e-03, -1.0375e-02,
        -2.7936e-02,  1.4162e-03, -5.8346e-03, -1.8913e-02, -9.7003e-03,
        -1.9519e-02, -1.6364e-03, -3.9415e-03, -1.8130e-02, -1.6992e-02,
        -2.1070e-02, -1.6110e-02, -6.2410e-03, -1.9688e-02,  3.4666e-03,
        -1.1834e-02, -5.1916e-03,  6.8607e-05, -1.0146e-02,  2.4895e-03,
         1.5054e-02, -3.2582e-04,  5.6768e-03,  4.9145e-03, -9.2746e-03,
         2.7392e-03,  4.9242e-03, -1.0040e-03,  2.8618e-03, -7.0158e-04,
        -2.1202e-02, -1.0164e-02, -2.2835e-03, -1.0469e-02, -8.7926e-03,
        -2.3792e-04, -2.0656e-02, -1.9229e-02, -1.8959e-02,  2.3980e-02,
        -2.0654e-02, -1.6105e-02, -9.6964e-03, -3.0371e-02, -1.7455e-02,
        -8.2818e-03, -6.8829e-03, -1.2721e-02, -1.5253e-02, -2.7319e-02,
         2.2477e-04, -6.0484e-03,  1.7027e-02, -1.8265e-02, -1.2134e-02,
        -1.8541e-02, -8.0732e-03, -7.3433e-03, -3.0062e-03,  7.9572e-03,
        -6.8942e-03, -1.7758e-02, -1.0218e-02, -5.7125e-04, -2.5453e-02,
         8.6391e-03, -1.6898e-02, -8.9368e-03, -6.4519e-04, -5.7022e-03,
         2.7114e-03, -1.6990e-03, -1.3350e-02, -1.2141e-02,  8.3932e-03,
        -2.4920e-02,  1.7258e-03, -2.7004e-03,  1.2287e-03, -1.2366e-02,
         3.3823e-03, -2.4729e-02,  5.8147e-04, -3.9817e-04, -1.1930e-02,
        -7.5801e-03,  3.6636e-03,  1.0072e-02, -1.7630e-02, -2.5704e-03,
        -9.6351e-03, -1.5216e-02, -2.6092e-02, -2.1292e-02, -1.3517e-02,
        -1.1949e-02,  1.0703e-03, -2.6654e-02, -9.6804e-04,  6.2375e-03,
         1.0529e-04,  3.7213e-04, -5.2462e-03, -1.7907e-02, -2.1613e-02,
        -8.6236e-03, -1.0906e-02, -5.5852e-03, -9.7459e-03, -1.8539e-02,
        -1.3253e-02, -9.6908e-04, -6.4190e-03, -5.0116e-03, -1.8221e-02,
        -1.4018e-02, -2.5336e-02, -2.5467e-02,  2.0137e-02, -1.9436e-02,
         1.5421e-03, -1.1058e-02, -2.3136e-03, -1.1611e-02, -1.3822e-02,
        -1.4694e-02, -7.4848e-03, -1.2733e-02, -2.5761e-02, -3.5967e-03,
        -3.9924e-04, -9.2781e-03, -1.6095e-02,  9.7115e-03, -1.2093e-02,
         6.3330e-03, -7.1059e-03,  7.0722e-03, -4.2886e-03,  3.1534e-03,
         4.7584e-04,  1.4076e-04, -1.4384e-02, -2.5056e-02, -2.4029e-02,
        -1.0766e-04, -1.6074e-03, -1.3929e-02,  1.2358e-02, -1.5058e-02,
         1.1677e-02, -8.2807e-03, -2.6107e-02, -9.4084e-04,  3.1200e-03,
        -5.6817e-03, -4.4648e-04, -6.0193e-03, -6.5709e-03, -1.5902e-02,
        -7.7144e-03, -5.8661e-03, -2.4668e-04, -2.0905e-02,  3.1694e-04,
        -6.8735e-03, -2.3474e-02, -1.7096e-02, -2.2540e-03, -1.1014e-02,
         5.4741e-04, -1.0888e-02, -2.4429e-02, -8.6382e-03,  3.7578e-03,
         1.9215e-03, -2.4433e-03, -3.3969e-02, -4.2952e-03, -1.0306e-02,
        -1.1465e-02, -1.2948e-02, -1.7829e-03, -8.5838e-03, -1.7853e-02,
        -1.3881e-02, -3.1850e-03, -1.0585e-02,  7.8547e-03,  7.2728e-03,
         5.1033e-03,  2.5680e-03, -5.9980e-03, -3.6935e-03,  2.4040e-04,
        -8.2440e-03, -1.5194e-03, -1.9757e-02,  4.2626e-04, -2.7204e-03,
        -9.6636e-03, -7.8971e-03,  1.5399e-02,  4.4841e-03, -1.4285e-02,
         1.1407e-02, -1.8802e-04, -1.8065e-03, -2.0896e-02,  4.9279e-03,
         2.2361e-04, -3.5620e-03, -1.5285e-03, -1.7015e-02, -5.3660e-03,
        -2.1452e-02, -9.5441e-03, -9.4070e-03, -3.8489e-03, -1.4591e-02,
        -2.3760e-03, -5.0427e-03,  5.8253e-03, -2.9230e-02, -7.6341e-03,
        -9.5615e-03,  9.7380e-03, -1.5575e-02, -2.1437e-02, -1.7516e-02,
         3.1055e-03, -3.5561e-03, -1.4059e-02,  2.7918e-03, -1.8384e-02,
         3.8792e-03, -1.0112e-02, -1.1170e-02,  4.5408e-03, -1.0721e-02,
         2.3291e-03, -8.3716e-03, -2.3833e-03, -1.2281e-02, -9.6942e-03,
        -1.3988e-02,  3.2435e-03, -2.4637e-02,  8.5472e-03, -1.5577e-03,
        -2.2306e-02,  1.8236e-02,  1.7613e-03, -8.8949e-03, -2.2337e-02,
        -1.8910e-02,  7.9921e-04, -4.4800e-03, -1.3653e-02,  6.7509e-03,
        -7.9701e-03, -1.0149e-02, -1.2478e-03, -2.1092e-02,  7.6679e-03,
         2.1742e-03,  1.8738e-03,  1.1468e-03, -7.9041e-03,  2.4149e-03],
       device='cuda:0'), tensor([0.8523, 0.8830, 0.8941, 0.7722, 0.9041, 0.8330, 0.8283, 0.8790, 0.8630,
        0.8061, 0.9054, 0.8135, 0.8813, 0.8310, 0.8615, 0.9008, 0.8534, 0.8923,
        0.8545, 0.9642, 0.8372, 0.8647, 0.9331, 0.8526, 0.7706, 0.8782, 0.8558,
        0.7606, 0.8040, 0.7878, 0.7698, 0.8750, 0.8961, 0.8251, 0.8962, 0.8728,
        0.9116, 0.8875, 0.8650, 0.8686, 0.8436, 0.9042, 0.8955, 0.8156, 0.9159,
        0.8464, 0.8804, 0.9083, 0.8788, 0.8963, 0.8715, 0.9184, 0.9057, 0.8536,
        0.8856, 0.8503, 0.8759, 0.8629, 0.8546, 0.9018, 0.9045, 0.8978, 0.8486,
        0.7847, 0.8331, 0.8060, 0.8091, 0.9081, 0.8935, 0.8873, 0.9026, 0.8188,
        0.8891, 0.8537, 0.9088, 0.8842, 0.8128, 0.8817, 0.8352, 0.8844, 0.8018,
        0.8864, 0.9115, 0.7731, 0.9336, 0.8591, 0.8122, 0.8697, 0.8375, 0.8607,
        0.7837, 0.8505, 0.8975, 0.8943, 0.8078, 0.8447, 0.8682, 0.9233, 0.8768,
        0.8204, 0.8599, 0.9104, 0.8711, 0.8912, 0.9004, 0.8820, 0.8895, 0.7844,
        0.8536, 0.8999, 0.8938, 0.8417, 0.8806, 0.8392, 0.9092, 0.9423, 0.8402,
        0.9040, 0.8416, 0.8757, 0.8691, 0.8446, 0.8217, 0.8957, 0.8722, 0.7390,
        0.8535, 0.7917, 0.8265, 0.9280, 0.9317, 0.8228, 0.9035, 0.8279, 0.7866,
        0.8714, 0.8836, 0.9138, 0.8674, 0.8346, 0.8326, 0.8481, 0.8897, 0.8849,
        0.7644, 0.8757, 0.7687, 0.9096, 0.8386, 0.8691, 0.8684, 0.8017, 0.8852,
        0.8395, 0.8908, 0.9211, 0.7778, 0.9015, 0.8621, 0.8429, 0.8902, 0.8049,
        0.8572, 0.8777, 0.8429, 0.9001, 0.8895, 0.8604, 0.8767, 0.8683, 0.8762,
        0.8733, 0.9391, 0.8549, 0.8794, 0.9251, 0.9059, 0.8775, 0.8776, 0.8023,
        0.8664, 0.8823, 0.8833, 0.9105, 0.8661, 0.8992, 0.8602, 0.9021, 0.8819,
        0.8804, 0.9255, 0.8930, 0.8526, 0.8788, 0.8470, 0.8070, 0.8359, 0.8034,
        0.9283, 0.8545, 0.8877, 0.8124, 0.8706, 0.8632, 0.8941, 0.8991, 0.9456,
        0.7835, 0.9134, 0.8776, 0.8439, 0.8527, 0.8930, 0.9242, 0.8869, 0.9142,
        0.8889, 0.8846, 0.8858, 0.9342, 0.8782, 0.8943, 0.8352, 0.8244, 0.9021,
        0.8501, 0.8709, 0.7883, 0.7695, 0.8978, 0.8605, 0.9094, 0.8968, 0.9257,
        0.8590, 0.8458, 0.8489, 0.8407, 0.8438, 0.8592, 0.8013, 0.8407, 0.8222,
        0.7871, 0.8881, 0.8226, 0.8566, 0.8458, 0.7979, 0.8382, 0.9075, 0.8359,
        0.8105, 0.7781, 0.8599, 0.8319, 0.8599, 0.8538, 0.7847, 0.7663, 0.8424,
        0.9210, 0.8969, 0.9425, 0.8826, 0.8532, 0.8962, 0.8745, 0.8461, 0.9148,
        0.8395, 0.8019, 0.8914, 0.7956, 0.7965, 0.8537, 0.8457, 0.9426, 0.8872,
        0.8969, 0.7707, 0.8391, 0.8610, 0.9082, 0.8572, 0.8713, 0.9147, 0.8014,
        0.8307, 0.9126, 0.8292, 0.8401, 0.8279, 0.7891, 0.9032, 0.8839, 0.8697,
        0.8449, 0.8551, 0.8560], device='cuda:0'), tensor([-0.3656, -0.4230, -0.4634, -0.5483, -0.3954, -0.3818, -0.3931, -0.3669,
        -0.4536, -0.3202, -0.5698, -0.5453, -0.2304, -0.1134, -0.3456, -0.3520,
        -0.4299, -0.4035, -0.2220, -0.4353, -0.3746, -0.2871, -0.4621, -0.4054,
        -0.5656, -0.3666, -0.3732, -0.5829, -0.5232, -0.4919, -0.5634, -0.2568,
        -0.4092, -0.4487, -0.3585, -0.4656, -0.4693, -0.3834, -0.5186, -0.4107,
        -0.4208, -0.5181, -0.3717, -0.4262, -0.4602, -0.1849, -0.3517, -0.6032,
        -0.2144, -0.4335, -0.3046, -0.3896, -0.5302, -0.4076, -0.4693, -0.4259,
        -0.4086, -0.2898, -0.3918, -0.5300, -0.4004, -0.5166, -0.4491, -0.5014,
        -0.4661, -0.2175, -0.3372, -0.3462, -0.3712, -0.3711, -0.3861, -0.3821,
        -0.3132, -0.3098, -0.3250, -0.3551, -0.3581, -0.2458, -0.3524, -0.4094,
        -0.5149, -0.3365, -0.3563, -0.6157, -0.4045, -0.3484, -0.3467, -0.4834,
        -0.2641, -0.1934, -0.4647, -0.3206, -0.3879, -0.4741, -0.5853, -0.1901,
        -0.6201, -0.3543, -0.2647, -0.4430, -0.4315, -0.5355, -0.2097, -0.3495,
        -0.5189, -0.4135, -0.4281, -0.5173, -0.3512, -0.5111, -0.3315, -0.4344,
        -0.3110, -0.3111, -0.5224, -0.5043, -0.3266, -0.3926, -0.3615, -0.3109,
        -0.2604, -0.4000, -0.4548, -0.5236, -0.3952, -0.5692, -0.3437, -0.5390,
        -0.4643, -0.4303, -0.3216, -0.3723, -0.5542, -0.4380, -0.5632, -0.2522,
        -0.2403, -0.4244, -0.3602, -0.4461, -0.4101, -0.1648, -0.4152, -0.3936,
        -0.6406, -0.4442, -0.5554, -0.5776, -0.4807, -0.3354, -0.4014, -0.4637,
        -0.4772, -0.4192, -0.4211, -0.3943, -0.5205, -0.4408, -0.4085, -0.4500,
        -0.5384, -0.4430, -0.3937, -0.2590, -0.1848, -0.5013, -0.4201, -0.2545,
        -0.4110, -0.2361, -0.5196, -0.2716, -0.5158, -0.3460, -0.2926, -0.3262,
        -0.4478, -0.2760, -0.3569, -0.4249, -0.2638, -0.3247, -0.3062, -0.4481,
        -0.3705, -0.4189, -0.2825, -0.4118, -0.3292, -0.4962, -0.5155, -0.5177,
        -0.3134, -0.4869, -0.5237, -0.4537, -0.4482, -0.5559, -0.3706, -0.4777,
        -0.3488, -0.5219, -0.4828, -0.3770, -0.4629, -0.2408, -0.4582, -0.5247,
        -0.4797, -0.2574, -0.1581, -0.3808, -0.2775, -0.3611, -0.1954, -0.5303,
        -0.4451, -0.2093, -0.3483, -0.3747, -0.1862, -0.3211, -0.3642, -0.3014,
        -0.4335, -0.4043, -0.3403, -0.5115, -0.5198, -0.4303, -0.3105, -0.4686,
        -0.4814, -0.3703, -0.3844, -0.2864, -0.2452, -0.6321, -0.4294, -0.2501,
        -0.5562, -0.2732, -0.3267, -0.6720, -0.4550, -0.5296, -0.4640, -0.3653,
        -0.2498, -0.4138, -0.3693, -0.4067, -0.2574, -0.5618, -0.2191, -0.5117,
        -0.3131, -0.3685, -0.6259, -0.6117, -0.2253, -0.3354, -0.4689, -0.4379,
        -0.2918, -0.1665, -0.3490, -0.4099, -0.5437, -0.2865, -0.2594, -0.4464,
        -0.2507, -0.4987, -0.4867, -0.5167, -0.2513, -0.4292, -0.4996, -0.4217,
        -0.5353, -0.2277, -0.2650, -0.4804, -0.2855, -0.3777, -0.5274, -0.1725,
        -0.2043, -0.2786, -0.4401, -0.2348, -0.4035, -0.3997, -0.4302, -0.3219,
        -0.4593, -0.3302, -0.2944, -0.2873], device='cuda:0'), tensor([[ 3.4109e-02,  9.1734e-02, -4.1183e-02,  ..., -6.4396e-02,
         -9.4019e-02, -9.8472e-02],
        [ 2.1526e-02, -1.2445e-01,  2.3803e-02,  ...,  1.6595e-01,
         -1.2128e-02, -1.5193e-01],
        [ 4.8804e-02,  4.4249e-05, -8.6528e-02,  ..., -8.1237e-02,
          1.5921e-02,  2.1255e-02],
        ...,
        [ 4.9683e-02, -2.5813e-01,  4.9488e-02,  ...,  6.7236e-02,
         -1.8030e-01, -8.5665e-02],
        [-4.2704e-02, -3.9639e-02,  8.6321e-02,  ...,  2.4168e-02,
         -5.3294e-02, -4.9079e-02],
        [ 1.6582e-01, -5.1246e-02, -2.0097e-02,  ...,  7.8505e-02,
         -2.5024e-01,  1.4379e-01]], device='cuda:0'), tensor([ 0.1401,  0.1908,  0.1241,  0.1817,  0.0056,  0.0458,  0.2215,  0.0930,
         0.1934,  0.0400,  0.1077,  0.0534,  0.1514,  0.0921, -0.0074,  0.0921,
        -0.0210,  0.1997,  0.2174,  0.1319,  0.2371,  0.1694,  0.0540,  0.0178,
         0.0295,  0.0853,  0.0120,  0.1975,  0.1266,  0.1222,  0.0742,  0.1518],
       device='cuda:0'), tensor([0.9832, 0.9785, 0.9443, 0.9287, 0.8924, 0.8804, 0.9764, 0.9171, 0.9588,
        0.9254, 0.9866, 0.9379, 0.8942, 0.8389, 0.9005, 0.9171, 0.9356, 0.9129,
        0.9485, 0.9530, 0.9171, 0.9364, 0.9357, 0.8731, 0.9240, 0.9633, 0.9646,
        0.9746, 0.9262, 0.9736, 0.9527, 0.9814], device='cuda:0'), tensor([-0.4048, -0.3639, -0.3941, -0.3777, -0.2320, -0.1727, -0.3571, -0.4234,
        -0.6599, -0.5220, -0.3510, -0.4048, -0.4063, -0.3493, -0.3811, -0.3414,
        -0.3450, -0.4216, -0.5051, -0.3156, -0.3855, -0.3735, -0.3344, -0.2370,
        -0.3286, -0.3714, -0.2579, -0.4455, -0.3973, -0.4183, -0.4055, -0.4456],
       device='cuda:0'), tensor([[0.5273, 0.4908, 0.5062, 0.5001, 0.4003, 0.3778, 0.4984, 0.5295, 0.6596,
         0.5458, 0.5148, 0.4874, 0.4351, 0.3998, 0.4742, 0.5048, 0.4977, 0.4835,
         0.5446, 0.5130, 0.4641, 0.5212, 0.4431, 0.3680, 0.4358, 0.4753, 0.4844,
         0.5302, 0.4765, 0.5204, 0.5108, 0.5361]], device='cuda:0'), tensor([0.1499], device='cuda:0')]
tensor([[1.0900],
        [3.4753],
        [3.8524],
        [2.4042],
        [2.1835],
        [9.5365],
        [6.6645],
        [2.3372],
        [2.1254],
        [0.3591],
        [2.6623],
        [1.3008],
        [1.9131],
        [4.0727],
        [2.5164],
        [2.4670],
        [2.3714],
        [2.4921],
        [1.4779],
        [4.1019],
        [4.5593],
        [2.4331],
        [2.1100],
        [5.0153],
        [4.2517],
        [1.2420],
        [5.3469],
        [1.7326],
        [7.3102],
        [3.2155],
        [3.4769],
        [0.9849],
        [2.4532],
        [0.0862],
        [0.5977],
        [3.0451],
        [1.4131],
        [2.6642],
        [0.1254],
        [0.5076],
        [7.1512],
        [2.1700],
        [0.8293],
        [1.8751],
        [3.4743],
        [2.2231],
        [0.2776],
        [2.6471],
        [1.2578],
        [2.0070],
        [3.2098],
        [2.4207],
        [4.6986],
        [2.1492],
        [0.5809],
        [1.1524],
        [1.0906],
        [3.9307],
        [1.8091],
        [4.1052],
        [5.6422],
        [3.4607],
        [1.0503],
        [2.9353]], device='cuda:0')
neural net:
 [tensor([[ 0.0163,  0.0188,  0.0086,  ..., -0.0222,  0.0101,  0.0086],
        [-0.0448, -0.0216, -0.0461,  ...,  0.0112, -0.0011, -0.0133],
        [-0.0607, -0.0262, -0.0074,  ..., -0.0204,  0.0017,  0.0148],
        ...,
        [ 0.0094,  0.0154, -0.0289,  ...,  0.0068,  0.0058,  0.0041],
        [-0.0160,  0.0210, -0.0083,  ..., -0.0031,  0.0162, -0.0116],
        [-0.0162,  0.0263, -0.0105,  ..., -0.0116,  0.0108,  0.0146]],
       device='cuda:0'), tensor([-1.0714e-02, -2.1523e-03, -4.2817e-03, -1.0067e-02, -1.4667e-02,
        -2.2468e-02, -1.5533e-02, -9.8564e-03, -1.9927e-03, -2.7076e-02,
        -4.4101e-05, -5.2547e-03, -5.7580e-03, -5.5729e-03, -2.2397e-02,
        -2.3076e-02,  4.7217e-03, -9.8505e-03, -1.2838e-02, -1.0090e-02,
        -1.5151e-02, -3.5023e-03, -1.2938e-02, -2.2220e-03, -9.1797e-04,
        -6.7367e-03,  1.7211e-03, -1.5087e-02,  2.0487e-03, -1.0375e-02,
        -2.7936e-02,  1.4162e-03, -5.8346e-03, -1.8913e-02, -9.7003e-03,
        -1.9519e-02, -1.6364e-03, -3.9415e-03, -1.8130e-02, -1.6992e-02,
        -2.1070e-02, -1.6110e-02, -6.2410e-03, -1.9688e-02,  3.4666e-03,
        -1.1834e-02, -5.1916e-03,  6.8607e-05, -1.0146e-02,  2.4895e-03,
         1.5054e-02, -3.2582e-04,  5.6768e-03,  4.9145e-03, -9.2746e-03,
         2.7392e-03,  4.9242e-03, -1.0040e-03,  2.8618e-03, -7.0158e-04,
        -2.1202e-02, -1.0164e-02, -2.2835e-03, -1.0469e-02, -8.7926e-03,
        -2.3792e-04, -2.0656e-02, -1.9229e-02, -1.8959e-02,  2.3980e-02,
        -2.0654e-02, -1.6105e-02, -9.6964e-03, -3.0371e-02, -1.7455e-02,
        -8.2818e-03, -6.8829e-03, -1.2721e-02, -1.5253e-02, -2.7319e-02,
         2.2477e-04, -6.0484e-03,  1.7027e-02, -1.8265e-02, -1.2134e-02,
        -1.8541e-02, -8.0732e-03, -7.3433e-03, -3.0062e-03,  7.9572e-03,
        -6.8942e-03, -1.7758e-02, -1.0218e-02, -5.7125e-04, -2.5453e-02,
         8.6391e-03, -1.6898e-02, -8.9368e-03, -6.4519e-04, -5.7022e-03,
         2.7114e-03, -1.6990e-03, -1.3350e-02, -1.2141e-02,  8.3932e-03,
        -2.4920e-02,  1.7258e-03, -2.7004e-03,  1.2287e-03, -1.2366e-02,
         3.3823e-03, -2.4729e-02,  5.8147e-04, -3.9817e-04, -1.1930e-02,
        -7.5801e-03,  3.6636e-03,  1.0072e-02, -1.7630e-02, -2.5704e-03,
        -9.6351e-03, -1.5216e-02, -2.6092e-02, -2.1292e-02, -1.3517e-02,
        -1.1949e-02,  1.0703e-03, -2.6654e-02, -9.6804e-04,  6.2375e-03,
         1.0529e-04,  3.7213e-04, -5.2462e-03, -1.7907e-02, -2.1613e-02,
        -8.6236e-03, -1.0906e-02, -5.5852e-03, -9.7459e-03, -1.8539e-02,
        -1.3253e-02, -9.6908e-04, -6.4190e-03, -5.0116e-03, -1.8221e-02,
        -1.4018e-02, -2.5336e-02, -2.5467e-02,  2.0137e-02, -1.9436e-02,
         1.5421e-03, -1.1058e-02, -2.3136e-03, -1.1611e-02, -1.3822e-02,
        -1.4694e-02, -7.4848e-03, -1.2733e-02, -2.5761e-02, -3.5967e-03,
        -3.9924e-04, -9.2781e-03, -1.6095e-02,  9.7115e-03, -1.2093e-02,
         6.3330e-03, -7.1059e-03,  7.0722e-03, -4.2886e-03,  3.1534e-03,
         4.7584e-04,  1.4076e-04, -1.4384e-02, -2.5056e-02, -2.4029e-02,
        -1.0766e-04, -1.6074e-03, -1.3929e-02,  1.2358e-02, -1.5058e-02,
         1.1677e-02, -8.2807e-03, -2.6107e-02, -9.4084e-04,  3.1200e-03,
        -5.6817e-03, -4.4648e-04, -6.0193e-03, -6.5709e-03, -1.5902e-02,
        -7.7144e-03, -5.8661e-03, -2.4668e-04, -2.0905e-02,  3.1694e-04,
        -6.8735e-03, -2.3474e-02, -1.7096e-02, -2.2540e-03, -1.1014e-02,
         5.4741e-04, -1.0888e-02, -2.4429e-02, -8.6382e-03,  3.7578e-03,
         1.9215e-03, -2.4433e-03, -3.3969e-02, -4.2952e-03, -1.0306e-02,
        -1.1465e-02, -1.2948e-02, -1.7829e-03, -8.5838e-03, -1.7853e-02,
        -1.3881e-02, -3.1850e-03, -1.0585e-02,  7.8547e-03,  7.2728e-03,
         5.1033e-03,  2.5680e-03, -5.9980e-03, -3.6935e-03,  2.4040e-04,
        -8.2440e-03, -1.5194e-03, -1.9757e-02,  4.2626e-04, -2.7204e-03,
        -9.6636e-03, -7.8971e-03,  1.5399e-02,  4.4841e-03, -1.4285e-02,
         1.1407e-02, -1.8802e-04, -1.8065e-03, -2.0896e-02,  4.9279e-03,
         2.2361e-04, -3.5620e-03, -1.5285e-03, -1.7015e-02, -5.3660e-03,
        -2.1452e-02, -9.5441e-03, -9.4070e-03, -3.8489e-03, -1.4591e-02,
        -2.3760e-03, -5.0427e-03,  5.8253e-03, -2.9230e-02, -7.6341e-03,
        -9.5615e-03,  9.7380e-03, -1.5575e-02, -2.1437e-02, -1.7516e-02,
         3.1055e-03, -3.5561e-03, -1.4059e-02,  2.7918e-03, -1.8384e-02,
         3.8792e-03, -1.0112e-02, -1.1170e-02,  4.5408e-03, -1.0721e-02,
         2.3291e-03, -8.3716e-03, -2.3833e-03, -1.2281e-02, -9.6942e-03,
        -1.3988e-02,  3.2435e-03, -2.4637e-02,  8.5472e-03, -1.5577e-03,
        -2.2306e-02,  1.8236e-02,  1.7613e-03, -8.8949e-03, -2.2337e-02,
        -1.8910e-02,  7.9921e-04, -4.4800e-03, -1.3653e-02,  6.7509e-03,
        -7.9701e-03, -1.0149e-02, -1.2478e-03, -2.1092e-02,  7.6679e-03,
         2.1742e-03,  1.8738e-03,  1.1468e-03, -7.9041e-03,  2.4149e-03],
       device='cuda:0'), tensor([0.8523, 0.8830, 0.8941, 0.7722, 0.9041, 0.8330, 0.8283, 0.8790, 0.8630,
        0.8061, 0.9054, 0.8135, 0.8813, 0.8310, 0.8615, 0.9008, 0.8534, 0.8923,
        0.8545, 0.9642, 0.8372, 0.8647, 0.9331, 0.8526, 0.7706, 0.8782, 0.8558,
        0.7606, 0.8040, 0.7878, 0.7698, 0.8750, 0.8961, 0.8251, 0.8962, 0.8728,
        0.9116, 0.8875, 0.8650, 0.8686, 0.8436, 0.9042, 0.8955, 0.8156, 0.9159,
        0.8464, 0.8804, 0.9083, 0.8788, 0.8963, 0.8715, 0.9184, 0.9057, 0.8536,
        0.8856, 0.8503, 0.8759, 0.8629, 0.8546, 0.9018, 0.9045, 0.8978, 0.8486,
        0.7847, 0.8331, 0.8060, 0.8091, 0.9081, 0.8935, 0.8873, 0.9026, 0.8188,
        0.8891, 0.8537, 0.9088, 0.8842, 0.8128, 0.8817, 0.8352, 0.8844, 0.8018,
        0.8864, 0.9115, 0.7731, 0.9336, 0.8591, 0.8122, 0.8697, 0.8375, 0.8607,
        0.7837, 0.8505, 0.8975, 0.8943, 0.8078, 0.8447, 0.8682, 0.9233, 0.8768,
        0.8204, 0.8599, 0.9104, 0.8711, 0.8912, 0.9004, 0.8820, 0.8895, 0.7844,
        0.8536, 0.8999, 0.8938, 0.8417, 0.8806, 0.8392, 0.9092, 0.9423, 0.8402,
        0.9040, 0.8416, 0.8757, 0.8691, 0.8446, 0.8217, 0.8957, 0.8722, 0.7390,
        0.8535, 0.7917, 0.8265, 0.9280, 0.9317, 0.8228, 0.9035, 0.8279, 0.7866,
        0.8714, 0.8836, 0.9138, 0.8674, 0.8346, 0.8326, 0.8481, 0.8897, 0.8849,
        0.7644, 0.8757, 0.7687, 0.9096, 0.8386, 0.8691, 0.8684, 0.8017, 0.8852,
        0.8395, 0.8908, 0.9211, 0.7778, 0.9015, 0.8621, 0.8429, 0.8902, 0.8049,
        0.8572, 0.8777, 0.8429, 0.9001, 0.8895, 0.8604, 0.8767, 0.8683, 0.8762,
        0.8733, 0.9391, 0.8549, 0.8794, 0.9251, 0.9059, 0.8775, 0.8776, 0.8023,
        0.8664, 0.8823, 0.8833, 0.9105, 0.8661, 0.8992, 0.8602, 0.9021, 0.8819,
        0.8804, 0.9255, 0.8930, 0.8526, 0.8788, 0.8470, 0.8070, 0.8359, 0.8034,
        0.9283, 0.8545, 0.8877, 0.8124, 0.8706, 0.8632, 0.8941, 0.8991, 0.9456,
        0.7835, 0.9134, 0.8776, 0.8439, 0.8527, 0.8930, 0.9242, 0.8869, 0.9142,
        0.8889, 0.8846, 0.8858, 0.9342, 0.8782, 0.8943, 0.8352, 0.8244, 0.9021,
        0.8501, 0.8709, 0.7883, 0.7695, 0.8978, 0.8605, 0.9094, 0.8968, 0.9257,
        0.8590, 0.8458, 0.8489, 0.8407, 0.8438, 0.8592, 0.8013, 0.8407, 0.8222,
        0.7871, 0.8881, 0.8226, 0.8566, 0.8458, 0.7979, 0.8382, 0.9075, 0.8359,
        0.8105, 0.7781, 0.8599, 0.8319, 0.8599, 0.8538, 0.7847, 0.7663, 0.8424,
        0.9210, 0.8969, 0.9425, 0.8826, 0.8532, 0.8962, 0.8745, 0.8461, 0.9148,
        0.8395, 0.8019, 0.8914, 0.7956, 0.7965, 0.8537, 0.8457, 0.9426, 0.8872,
        0.8969, 0.7707, 0.8391, 0.8610, 0.9082, 0.8572, 0.8713, 0.9147, 0.8014,
        0.8307, 0.9126, 0.8292, 0.8401, 0.8279, 0.7891, 0.9032, 0.8839, 0.8697,
        0.8449, 0.8551, 0.8560], device='cuda:0'), tensor([-0.3656, -0.4230, -0.4634, -0.5483, -0.3954, -0.3818, -0.3931, -0.3669,
        -0.4536, -0.3202, -0.5698, -0.5453, -0.2304, -0.1134, -0.3456, -0.3520,
        -0.4299, -0.4035, -0.2220, -0.4353, -0.3746, -0.2871, -0.4621, -0.4054,
        -0.5656, -0.3666, -0.3732, -0.5829, -0.5232, -0.4919, -0.5634, -0.2568,
        -0.4092, -0.4487, -0.3585, -0.4656, -0.4693, -0.3834, -0.5186, -0.4107,
        -0.4208, -0.5181, -0.3717, -0.4262, -0.4602, -0.1849, -0.3517, -0.6032,
        -0.2144, -0.4335, -0.3046, -0.3896, -0.5302, -0.4076, -0.4693, -0.4259,
        -0.4086, -0.2898, -0.3918, -0.5300, -0.4004, -0.5166, -0.4491, -0.5014,
        -0.4661, -0.2175, -0.3372, -0.3462, -0.3712, -0.3711, -0.3861, -0.3821,
        -0.3132, -0.3098, -0.3250, -0.3551, -0.3581, -0.2458, -0.3524, -0.4094,
        -0.5149, -0.3365, -0.3563, -0.6157, -0.4045, -0.3484, -0.3467, -0.4834,
        -0.2641, -0.1934, -0.4647, -0.3206, -0.3879, -0.4741, -0.5853, -0.1901,
        -0.6201, -0.3543, -0.2647, -0.4430, -0.4315, -0.5355, -0.2097, -0.3495,
        -0.5189, -0.4135, -0.4281, -0.5173, -0.3512, -0.5111, -0.3315, -0.4344,
        -0.3110, -0.3111, -0.5224, -0.5043, -0.3266, -0.3926, -0.3615, -0.3109,
        -0.2604, -0.4000, -0.4548, -0.5236, -0.3952, -0.5692, -0.3437, -0.5390,
        -0.4643, -0.4303, -0.3216, -0.3723, -0.5542, -0.4380, -0.5632, -0.2522,
        -0.2403, -0.4244, -0.3602, -0.4461, -0.4101, -0.1648, -0.4152, -0.3936,
        -0.6406, -0.4442, -0.5554, -0.5776, -0.4807, -0.3354, -0.4014, -0.4637,
        -0.4772, -0.4192, -0.4211, -0.3943, -0.5205, -0.4408, -0.4085, -0.4500,
        -0.5384, -0.4430, -0.3937, -0.2590, -0.1848, -0.5013, -0.4201, -0.2545,
        -0.4110, -0.2361, -0.5196, -0.2716, -0.5158, -0.3460, -0.2926, -0.3262,
        -0.4478, -0.2760, -0.3569, -0.4249, -0.2638, -0.3247, -0.3062, -0.4481,
        -0.3705, -0.4189, -0.2825, -0.4118, -0.3292, -0.4962, -0.5155, -0.5177,
        -0.3134, -0.4869, -0.5237, -0.4537, -0.4482, -0.5559, -0.3706, -0.4777,
        -0.3488, -0.5219, -0.4828, -0.3770, -0.4629, -0.2408, -0.4582, -0.5247,
        -0.4797, -0.2574, -0.1581, -0.3808, -0.2775, -0.3611, -0.1954, -0.5303,
        -0.4451, -0.2093, -0.3483, -0.3747, -0.1862, -0.3211, -0.3642, -0.3014,
        -0.4335, -0.4043, -0.3403, -0.5115, -0.5198, -0.4303, -0.3105, -0.4686,
        -0.4814, -0.3703, -0.3844, -0.2864, -0.2452, -0.6321, -0.4294, -0.2501,
        -0.5562, -0.2732, -0.3267, -0.6720, -0.4550, -0.5296, -0.4640, -0.3653,
        -0.2498, -0.4138, -0.3693, -0.4067, -0.2574, -0.5618, -0.2191, -0.5117,
        -0.3131, -0.3685, -0.6259, -0.6117, -0.2253, -0.3354, -0.4689, -0.4379,
        -0.2918, -0.1665, -0.3490, -0.4099, -0.5437, -0.2865, -0.2594, -0.4464,
        -0.2507, -0.4987, -0.4867, -0.5167, -0.2513, -0.4292, -0.4996, -0.4217,
        -0.5353, -0.2277, -0.2650, -0.4804, -0.2855, -0.3777, -0.5274, -0.1725,
        -0.2043, -0.2786, -0.4401, -0.2348, -0.4035, -0.3997, -0.4302, -0.3219,
        -0.4593, -0.3302, -0.2944, -0.2873], device='cuda:0'), tensor([[ 3.4109e-02,  9.1734e-02, -4.1183e-02,  ..., -6.4396e-02,
         -9.4019e-02, -9.8472e-02],
        [ 2.1526e-02, -1.2445e-01,  2.3803e-02,  ...,  1.6595e-01,
         -1.2128e-02, -1.5193e-01],
        [ 4.8804e-02,  4.4249e-05, -8.6528e-02,  ..., -8.1237e-02,
          1.5921e-02,  2.1255e-02],
        ...,
        [ 4.9683e-02, -2.5813e-01,  4.9488e-02,  ...,  6.7236e-02,
         -1.8030e-01, -8.5665e-02],
        [-4.2704e-02, -3.9639e-02,  8.6321e-02,  ...,  2.4168e-02,
         -5.3294e-02, -4.9079e-02],
        [ 1.6582e-01, -5.1246e-02, -2.0097e-02,  ...,  7.8505e-02,
         -2.5024e-01,  1.4379e-01]], device='cuda:0'), tensor([ 0.1401,  0.1908,  0.1241,  0.1817,  0.0056,  0.0458,  0.2215,  0.0930,
         0.1934,  0.0400,  0.1077,  0.0534,  0.1514,  0.0921, -0.0074,  0.0921,
        -0.0210,  0.1997,  0.2174,  0.1319,  0.2371,  0.1694,  0.0540,  0.0178,
         0.0295,  0.0853,  0.0120,  0.1975,  0.1266,  0.1222,  0.0742,  0.1518],
       device='cuda:0'), tensor([0.9832, 0.9785, 0.9443, 0.9287, 0.8924, 0.8804, 0.9764, 0.9171, 0.9588,
        0.9254, 0.9866, 0.9379, 0.8942, 0.8389, 0.9005, 0.9171, 0.9356, 0.9129,
        0.9485, 0.9530, 0.9171, 0.9364, 0.9357, 0.8731, 0.9240, 0.9633, 0.9646,
        0.9746, 0.9262, 0.9736, 0.9527, 0.9814], device='cuda:0'), tensor([-0.4048, -0.3639, -0.3941, -0.3777, -0.2320, -0.1727, -0.3571, -0.4234,
        -0.6599, -0.5220, -0.3510, -0.4048, -0.4063, -0.3493, -0.3811, -0.3414,
        -0.3450, -0.4216, -0.5051, -0.3156, -0.3855, -0.3735, -0.3344, -0.2370,
        -0.3286, -0.3714, -0.2579, -0.4455, -0.3973, -0.4183, -0.4055, -0.4456],
       device='cuda:0'), tensor([[0.5273, 0.4908, 0.5062, 0.5001, 0.4003, 0.3778, 0.4984, 0.5295, 0.6596,
         0.5458, 0.5148, 0.4874, 0.4351, 0.3998, 0.4742, 0.5048, 0.4977, 0.4835,
         0.5446, 0.5130, 0.4641, 0.5212, 0.4431, 0.3680, 0.4358, 0.4753, 0.4844,
         0.5302, 0.4765, 0.5204, 0.5108, 0.5361]], device='cuda:0'), tensor([0.1499], device='cuda:0')]
tensor([[4.0384],
        [1.8814],
        [3.4624],
        [1.2667],
        [1.9897],
        [0.7601],
        [1.7099],
        [4.0905],
        [1.9526],
        [4.2121],
        [3.3976],
        [3.0743],
        [2.9729],
        [3.2987],
        [3.1549],
        [3.0596],
        [3.6458],
        [6.3038],
        [3.3365],
        [1.3305],
        [4.3858],
        [1.6718],
        [2.8273],
        [2.2348],
        [1.6893],
        [1.3097],
        [1.7033],
        [2.0105],
        [3.8789],
        [2.7431],
        [1.7764],
        [2.1557],
        [5.1886],
        [1.3459],
        [0.8889],
        [3.2780],
        [1.6888],
        [3.5853],
        [3.8648],
        [1.1041],
        [1.3578],
        [1.0364],
        [1.9571],
        [2.4588],
        [2.3186],
        [2.6176],
        [2.8176],
        [1.4591],
        [2.2817],
        [6.1901],
        [1.1853],
        [1.3631],
        [2.6064],
        [3.9968],
        [0.5606],
        [4.3496],
        [3.0029],
        [3.0590],
        [2.5925],
        [3.0043],
        [3.6953],
        [2.1938],
        [0.8809],
        [3.0036]], device='cuda:0')
Loss on train set via eval test: 0.32653663413865225
Loss on test set: 6.438380323137555
